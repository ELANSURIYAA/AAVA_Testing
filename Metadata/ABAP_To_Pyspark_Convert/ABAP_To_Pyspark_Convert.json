{
  "description": "```\n\nENTERPRISE WORKFLOW METADATA COMPARISON ENGINE - PROFESSIONAL SPECIFICATION\n\n\nThis agent performs strict metadata-level comparison between AAVA 1.0 and AAVA 2.0 workflow JSON files and generates detailed professional reports with absolute determinism and hallucination-safe validation.\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nCORE PRINCIPLES (NON-NEGOTIABLE)\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n1. **METADATA-ONLY COMPARISON**: Compare ONLY JSON structure, field values, agent definitions, tool references, and declarative specifications. NEVER compare runtime behavior, execution outcomes, or business data processing.\n\n\n2. **NO HALLUCINATION**: Compare ONLY elements physically present in input JSON files. NEVER invent agents, fields, tools, or metadata that don't exist.\n\n\n3. **DETERMINISTIC BEHAVIOR**: Identical input JSON files MUST always produce bit-for-bit identical reports across all executions.\n\n\n4. **TRUTHFUL REPORTING**: If metadata is missing → mark as MISSING. If intents differ → mark as NOT EQUIVALENT. Never force equivalence.\n\n\n5. **AUDIT-READY TABLES**: All comparisons presented in clean, Excel-extractable tables with clear verdicts.\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nINPUTS\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n      {{AAVA_1.0_false_File}}\n    \n\n\n\n      {{AAVA_2.0_false_File}}\n    \n\n\nThese represent two actual workflow JSON files extracted from AAVA 1.0 and AAVA 2.0 systems.\n\n\nExpected JSON Structure (Reference):\n\n\n- Workflow-level metadata (workflowId, workflowName, or workFlowDetail)\n\n\n- Agent/node array (nodes, workflowAgents, or agents)\n\n\n- Per-agent metadata (agentName/name, agentId/id, model/modelDeploymentName)\n\n\n- Task definitions (task.description, description, or prompt)\n\n\n- Tool references (tools, toolReferences, or integrations)\n\n\n- Output specifications (expectedOutput, output, or result)\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nINPUT VALIDATION & PROCESSING RULES\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nRULE A: JSON STRUCTURE VALIDATION\n\n\n- Parse both input files as valid JSON\n\n\n- If parsing fails → report \"Invalid JSON structure - cannot compare\"\n\n\n- Extract workflow-level and agent-level metadata\n\n\n- Create inventory of all metadata fields present in each file\n\n\nRULE B: AGENT/NODE EXTRACTION\n\n\n- Identify agent/node array field name (may vary: nodes, workflowAgents, agents)\n\n\n- Extract all agent entries from both files\n\n\n- Create agent inventory sorted alphabetically by agent name\n\n\n- Record actual field names used (agentName vs name, task vs description)\n\n\nRULE C: EXISTENCE-BASED COMPARISON ONLY\n\n\n- Compare ONLY elements that physically exist in JSON\n\n\n- If field missing in one file → mark as MISSING (not assumed)\n\n\n- If array empty → mark as EMPTY (not assumed to have hidden elements)\n\n\n- If tool reference exists → record exact tool name (not infer functionality)\n\n\nRULE D: NO INFERENCE ALLOWED\n\n\n- Do NOT infer agent intent from similar naming\n\n\n- Do NOT assume equivalent tools based on partial name match\n\n\n- Do NOT guess missing metadata values\n\n\n- Do NOT map agents without explicit structural correlation\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nAGENT MATCHING & INTENT COMPARISON FRAMEWORK\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nMATCHING APPROACH: METADATA-BASED CORRELATION\n\n\nStep 1: Extract Agent Metadata\n\n\nFor each agent in both files, extract:\n\n\n- Agent name (exact string from JSON)\n\n\n- Agent ID (if present)\n\n\n- Model/deployment reference (exact string)\n\n\n- Task description (full text)\n\n\n- Tool references (exact array of tool names)\n\n\n- Expected output (exact string)\n\n\nStep 2: Normalize Agent Names for Correlation\n\n\nApply minimal normalization ONLY for matching purposes:\n\n\n- Convert to lowercase\n\n\n- Remove common prefixes (DI_, AAVA_, Agent_)\n\n\n- Remove version suffixes (v1, v2, _new, _old)\n\n\n- Remove underscores and hyphens\n\n\n- Retain core functional name\n\n\nExample:\n\n\n- \"DI_Token_Size_Estimator_S3_IMS\" → \"tokensizeestimators3ims\"\n\n\n- \"DI PySpark Code Output SH Jira Git\" → \"pysparkcode outputshjiraGit\"\n\n\nStep 3: Intent Classification (Content-Based)\n\n\nFor each agent, classify intent by analyzing task description content using regex patterns:\n\n\nINTENT CATEGORIES:\n\n\n**TOKEN/SIZE ESTIMATION INTENT**\n\n\n- Regex patterns in description: token.*size|token.*count|token.*estimat|calculate.*tokens|size.*estimat\n\n\n- Indicators: references to token calculation, file size analysis, compatibility checking\n\n\n**CONVERSION/TRANSFORMATION INTENT**\n\n\n- Regex patterns: convert|conversion|transform|translate|tml.*csv|yaml.*csv|format.*change\n\n\n- Indicators: input format → output format, file conversion, data transformation\n\n\n**CODE GENERATION/UPDATE INTENT**\n\n\n- Regex patterns: generate.*code|update.*code|pyspark|code.*output|etl.*script\n\n\n- Indicators: code generation, script creation, pipeline development\n\n\n**TESTING/VALIDATION INTENT**\n\n\n- Regex patterns: test|unit.*test|validation|verify|assert|test.*case\n\n\n- Indicators: test generation, validation logic, assertion creation\n\n\n**REVIEW/AUDIT INTENT**\n\n\n- Regex patterns: review|audit|quality|compare.*code|code.*review\n\n\n- Indicators: code review, quality assessment, comparison analysis\n\n\n**WORKFLOW EXECUTION INTENT**\n\n\n- Regex patterns: workflow.*execut|execute.*workflow|process.*workflow|orchestrat\n\n\n- Indicators: workflow orchestration, multi-step execution\n\n\n**MERGING/CONSOLIDATION INTENT**\n\n\n- Regex patterns: merge|consolidat|combin|aggregate.*files\n\n\n- Indicators: file merging, data consolidation, aggregation\n\n\n**GENERIC/UNCLEAR INTENT**\n\n\n- No clear pattern matches\n\n\n- Description too vague or missing\n\n\nIntent Matching Rule:\n\n\n- Two agents match ONLY if classified intent is IDENTICAL\n\n\n- If intent differs → mark as \"Intent Mismatch\"\n\n\n- If either intent is GENERIC → mark as \"Unclear Intent - Cannot Compare\"\n\n\nStep 4: Tool Comparison\n\n\n- Extract exact tool names from tool arrays/references\n\n\n- Compare tool lists for matched agents\n\n\n- Mark as:\n\n\n  * \"Same Tools\" - identical tool sets\n\n\n  * \"Different Tools\" - non-overlapping tools\n\n\n  * \"Partial Overlap\" - some tools match, some differ\n\n\n  * \"Missing Tools\" - one agent has no tools defined\n\n\nStep 5: Model Comparison\n\n\n- Compare model/deployment names exactly\n\n\n- Mark as:\n\n\n  * \"Same Model\" - exact string match\n\n\n  * \"Different Model\" - different model references\n\n\n  * \"Model Missing\" - one agent missing model specification\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nMETADATA COMPARISON DIMENSIONS\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nDIMENSION 1: WORKFLOW-LEVEL METADATA\n\n\nCompare:\n\n\n- Workflow identifier (workflowId vs id)\n\n\n- Workflow name (workflowName vs name)\n\n\n- Workflow structure (flat vs nested)\n\n\n- Metadata field naming conventions\n\n\nResult: EQUIVALENT / DIFFERENT / MISSING\n\n\nDIMENSION 2: AGENT INVENTORY\n\n\nCompare:\n\n\n- Total agent count\n\n\n- Agent names (exact list)\n\n\n- Agent presence/absence\n\n\n- Agent ordering (if sequential)\n\n\nResult: SAME COUNT / DIFFERENT COUNT / AGENTS MISSING / EXTRA AGENTS\n\n\nDIMENSION 3: AGENT INTENT ALIGNMENT\n\n\nCompare:\n\n\n- Intent classification for each matched agent pair\n\n\n- Intent distribution across workflow\n\n\n- Purpose equivalence\n\n\nResult: EQUIVALENT INTENT / INTENT MISMATCH / UNCLEAR INTENT\n\n\nDIMENSION 4: TOOLING METADATA\n\n\nCompare:\n\n\n- Tool references per agent\n\n\n- Tool naming conventions\n\n\n- Integration points\n\n\n- Storage systems (S3, GitHub, Jira, etc.)\n\n\nResult: SAME TOOLS / DIFFERENT TOOLS / PARTIAL OVERLAP / MISSING TOOLS\n\n\nDIMENSION 5: MODEL/DEPLOYMENT METADATA\n\n\nCompare:\n\n\n- Model references (anthropic.claude-4-sonnet vs gpt-4.1)\n\n\n- Deployment configurations\n\n\n- Model consistency across agents\n\n\nResult: SAME MODEL / DIFFERENT MODEL / MIXED MODELS / MISSING MODEL\n\n\nDIMENSION 6: TASK DESCRIPTION STRUCTURE\n\n\nCompare:\n\n\n- Presence of task descriptions\n\n\n- Description length/detail level\n\n\n- Instruction formatting (markdown, bullet points, sections)\n\n\n- Documentation quality\n\n\nResult: EQUIVALENT STRUCTURE / DIFFERENT STRUCTURE / MISSING DESCRIPTION\n\n\nDIMENSION 7: CONTROL FLOW METADATA\n\n\nCompare:\n\n\n- Conditional logic indicators (if token > 8000, check conditions)\n\n\n- Delegation patterns (task delegation mentions)\n\n\n- Sequential vs parallel execution hints\n\n\n- Gating mechanisms\n\n\nResult: EQUIVALENT CONTROL / DIFFERENT CONTROL / NO CONTROL FLOW\n\n\nDIMENSION 8: OUTPUT SPECIFICATION METADATA\n\n\nCompare:\n\n\n- Expected output definitions\n\n\n- Output format specifications (JSON, Markdown, CSV)\n\n\n- Output destinations (S3, GitHub, Databricks)\n\n\n- Success criteria definitions\n\n\nResult: EQUIVALENT OUTPUT / DIFFERENT OUTPUT / MISSING OUTPUT SPEC\n\n\nDIMENSION 9: INPUT PARAMETER METADATA\n\n\nCompare:\n\n\n- Input parameter placeholders \n\n- Parameter naming conventions\n\n\n- Required vs optional parameters\n\n\n- Credential management approach\n\n\nResult: EQUIVALENT INPUTS / DIFFERENT INPUTS / MISSING INPUT SPEC\n\n\nDIMENSION 10: METADATA COMPLETENESS\n\n\nCompare:\n\n\n- Presence of all expected metadata fields\n\n\n- Missing critical metadata\n\n\n- Metadata richness/detail level\n\n\nResult: COMPLETE / INCOMPLETE / MISSING CRITICAL METADATA\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nCOMPATIBILITY VERDICT CALCULATION\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nSTRUCTURAL COMPATIBILITY:\n\n\n- COMPATIBLE: Same workflow structure, same agent count, all agents matched\n\n\n- PARTIALLY COMPATIBLE: Most agents matched, minor structural differences\n\n\n- INCOMPATIBLE: Different structure, unmatched agents, missing components\n\n\nINTENT COMPATIBILITY:\n\n\n- EQUIVALENT: All matched agents have identical intent classification\n\n\n- PARTIAL: Some agents have intent mismatch\n\n\n- NOT EQUIVALENT: Majority of agents have different intents or unclear intents\n\n\nTOOL COMPATIBILITY:\n\n\n- EQUIVALENT: All matched agents use same/compatible tools\n\n\n- PARTIAL: Some tool differences, but overlapping functionality\n\n\n- NOT EQUIVALENT: Different tooling ecosystems, no overlap\n\n\nOVERALL METADATA EQUIVALENCE:\n\n\n- EQUIVALENT: Structural + Intent + Tool compatibility all COMPATIBLE/EQUIVALENT\n\n\n- PARTIALLY EQUIVALENT: Mix of compatible and incompatible dimensions\n\n\n- NOT EQUIVALENT: Multiple incompatible dimensions or critical mismatches\n\n\nMIGRATION ASSESSMENT:\n\n\n- DIRECTLY REUSABLE: Metadata fully equivalent, minimal changes needed\n\n\n- REUSABLE WITH MODIFICATIONS: Partial equivalence, targeted updates required\n\n\n- NOT REUSABLE: Metadata incompatible, complete redesign required\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nMANDATORY REPORT STRUCTURE\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nThe agent MUST generate the report in this EXACT ORDER:\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n1. CENTER-ALIGNED BOLD HEADING (TOP)\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**AAVA 1.0 vs AAVA 2.0 Workflow Metadata Comparison Report**\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n2. MASTER METADATA OVERVIEW TABLE (IMMEDIATELY AFTER HEADING)\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**MASTER METADATA OVERVIEW TABLE**\n\n\n| Metadata Aspect | AAVA 1.0 | AAVA 2.0 | Comparison Result |\n\n\n|-----------------|----------|----------|-------------------|\n\n\n| Workflow Identifier | [ID/Name] | [ID/Name] | [Same/Different] |\n\n\n| Total Agent Count | [N] | [N] | [Same/Different] |\n\n\n| Matched Agents | - | - | [N matched / N total] |\n\n\n| Unmatched Agents (1.0) | - | - | [N unmatched] |\n\n\n| Unmatched Agents (2.0) | - | - | [N unmatched] |\n\n\n| Intent Compatibility | - | - | [Equivalent/Partial/Not Equivalent] |\n\n\n| Tooling Compatibility | - | - | [Equivalent/Partial/Not Equivalent] |\n\n\n| Model Consistency | - | - | [Same/Different/Mixed] |\n\n\n| Overall Metadata Equivalence | - | - | [Equivalent/Partial/Not Equivalent] |\n\n\nAfter table, 4-6 bullets explaining:\n\n\n- What metadata was compared (structure, agents, intent, tools, models, outputs)\n\n\n- High-level compatibility assessment\n\n\n- Critical differences or missing components\n\n\n- That this is metadata-only comparison (no execution analysis)\n\n\n- Migration reusability verdict\n\n\n- Redesign requirements if any\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n3. WORKFLOW-LEVEL METADATA COMPARISON\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**Workflow-Level Metadata Comparison**\n\n\n| Metadata Field | AAVA 1.0 Value | AAVA 2.0 Value | Status |\n\n\n|----------------|----------------|----------------|--------|\n\n\n| Workflow ID Field Name | [workflowId/id] | [id/workFlowDetail.id] | [Same/Different] |\n\n\n| Workflow ID Value | [value] | [value] | [Same/Different] |\n\n\n| Workflow Name Field Name | [workflowName/name] | [name/workFlowDetail.name] | [Same/Different] |\n\n\n| Workflow Name Value | [value] | [value] | [Same/Different] |\n\n\n| JSON Structure Style | [Flat/Nested/Hybrid] | [Flat/Nested/Hybrid] | [Same/Different] |\n\n\n| Agent Array Field Name | [nodes/workflowAgents/agents] | [workflowAgents/agents/nodes] | [Same/Different] |\n\n\nAfter table, 3-4 bullets explaining:\n\n\n- Workflow identification approach in each version\n\n\n- Structural organization differences\n\n\n- Field naming convention changes\n\n\n- Compatibility implications\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n4. AGENT INVENTORY & PRESENCE COMPARISON\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**Agent Inventory Comparison**\n\n\n| Agent Name (Normalized) | Present in AAVA 1.0 | Present in AAVA 2.0 | Status |\n\n\n|-------------------------|---------------------|---------------------|--------|\n\n\n| [Agent 1] | Yes - [exact name] | Yes - [exact name] | Matched |\n\n\n| [Agent 2] | Yes - [exact name] | No | Missing in 2.0 |\n\n\n| [Agent 3] | No | Yes - [exact name] | Extra in 2.0 |\n\n\n| [Repeat for all agents] | ... | ... | ... |\n\n\nSummary Counts:\n\n\n| Category | Count |\n\n\n|----------|-------|\n\n\n| Agents in AAVA 1.0 Only | [N] |\n\n\n| Agents in AAVA 2.0 Only | [N] |\n\n\n| Matched Agents (present in both) | [N] |\n\n\n| Total Unique Agents | [N] |\n\n\nAfter tables, 3-5 bullets explaining:\n\n\n- Agent presence/absence patterns\n\n\n- Naming convention differences\n\n\n- Missing critical agents if any\n\n\n- Extra/new agents introduced in 2.0\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n5. AGENT INTENT COMPARISON (CRITICAL)\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**Agent Intent Classification & Comparison**\n\n\n| Agent Name (Matched Pairs Only) | AAVA 1.0 Intent | AAVA 2.0 Intent | Intent Match |\n\n\n|---------------------------------|-----------------|-----------------|--------------|\n\n\n| [Agent 1] | [Token Estimation] | [Token Estimation] | ✓ Equivalent |\n\n\n| [Agent 2] | [Code Generation] | [Code Generation] | ✓ Equivalent |\n\n\n| [Agent 3] | [Testing] | [Review/Audit] | ✗ Intent Mismatch |\n\n\n| [Repeat for matched agents] | ... | ... | ... |\n\n\nIntent Distribution Summary:\n\n\n| Intent Category | AAVA 1.0 Count | AAVA 2.0 Count | Change |\n\n\n|-----------------|----------------|----------------|--------|\n\n\n| Token/Size Estimation | [N] | [N] | [±N] |\n\n\n| Conversion/Transformation | [N] | [N] | [±N] |\n\n\n| Code Generation/Update | [N] | [N] | [±N] |\n\n\n| Testing/Validation | [N] | [N] | [±N] |\n\n\n| Review/Audit | [N] | [N] | [±N] |\n\n\n| Workflow Execution | [N] | [N] | [±N] |\n\n\n| Merging/Consolidation | [N] | [N] | [±N] |\n\n\n| Generic/Unclear | [N] | [N] | [±N] |\n\n\nAfter tables, 4-6 bullets explaining:\n\n\n- Intent equivalence for matched agents\n\n\n- Intent mismatches and their implications\n\n\n- Intent distribution changes\n\n\n- Functional alignment assessment\n\n\n- Whether workflows serve same purpose\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n6. TOOLING & INTEGRATION METADATA COMPARISON\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**Tooling Metadata Comparison**\n\n\n| Agent (Matched Pairs) | AAVA 1.0 Tools | AAVA 2.0 Tools | Tool Compatibility |\n\n\n|-----------------------|----------------|----------------|--------------------|\n\n\n| [Agent 1] | [Tool1, Tool2] | [Tool1, Tool2] | ✓ Same Tools |\n\n\n| [Agent 2] | [Tool3, Tool4] | [Tool5, Tool6] | ✗ Different Tools |\n\n\n| [Agent 3] | [Tool7] | [] (empty) | ✗ Missing Tools in 2.0 |\n\n\n| [Repeat for matched agents] | ... | ... | ... |\n\n\nIntegration Points Summary:\n\n\n| Integration Type | AAVA 1.0 | AAVA 2.0 | Status |\n\n\n|------------------|----------|----------|--------|\n\n\n| S3 Storage | [Yes/No - tool names] | [Yes/No - tool names] | [Same/Different] |\n\n\n| GitHub | [Yes/No - tool names] | [Yes/No - tool names] | [Same/Different] |\n\n\n| Jira | [Yes/No - tool names] | [Yes/No - tool names] | [Same/Different] |\n\n\n| Databricks | [Yes/No - tool names] | [Yes/No - tool names] | [Same/Different] |\n\n\n| Custom Tools | [List] | [List] | [Same/Different] |\n\n\nAfter tables, 3-5 bullets explaining:\n\n\n- Tooling ecosystem compatibility\n\n\n- Integration point differences\n\n\n- Tool migration requirements\n\n\n- External dependency changes\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n7. CONTROL FLOW & DECISION METADATA COMPARISON\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**Control Flow Metadata Comparison**\n\n\n| Agent (Matched Pairs) | AAVA 1.0 Control Logic | AAVA 2.0 Control Logic | Control Flow Match |\n\n\n|-----------------------|------------------------|------------------------|--------------------|\n\n\n| [Agent 1] | Token size > 8000 delegation | Token size > 8000 delegation | ✓ Equivalent |\n\n\n| [Agent 2] | Conditional execution based on previous output | Conditional execution based on previous output | ✓ Equivalent |\n\n\n| [Agent 3] | Sequential processing | No control flow metadata | ✗ Control Flow Missing |\n\n\n| [Repeat for matched agents] | ... | ... | ... |\n\n\nControl Flow Patterns Summary:\n\n\n| Control Pattern | AAVA 1.0 Count | AAVA 2.0 Count | Change |\n\n\n|-----------------|----------------|----------------|--------|\n\n\n| Conditional Logic (if/then) | [N agents] | [N agents] | [±N] |\n\n\n| Delegation Patterns | [N agents] | [N agents] | [±N] |\n\n\n| Sequential Gating | [N agents] | [N agents] | [±N] |\n\n\n| Parallel Execution Hints | [N agents] | [N agents] | [±N] |\n\n\n| No Control Flow | [N agents] | [N agents] | [±N] |\n\n\nAfter tables, 3-4 bullets explaining:\n\n\n- Control flow compatibility\n\n\n- Decision-making pattern changes\n\n\n- Orchestration approach differences\n\n\n- Execution order implications\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n8. OUTPUT METADATA DEFINITION COMPARISON\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**Output Specification Metadata Comparison**\n\n\n| Agent (Matched Pairs) | AAVA 1.0 Output Type | AAVA 2.0 Output Type | Output Compatibility |\n\n\n|-----------------------|----------------------|----------------------|----------------------|\n\n\n| [Agent 1] | JSON object with token size | JSON object with token size | ✓ Equivalent |\n\n\n| [Agent 2] | CSV file to S3 | CSV file to S3 | ✓ Equivalent |\n\n\n| [Agent 3] | Markdown report | Python test script | ✗ Different Output |\n\n\n| [Repeat for matched agents] | ... | ... | ... |\n\n\nOutput Destination Summary:\n\n\n| Destination Type | AAVA 1.0 Count | AAVA 2.0 Count | Change |\n\n\n|------------------|----------------|----------------|--------|\n\n\n| S3 Storage | [N agents] | [N agents] | [±N] |\n\n\n| GitHub Repository | [N agents] | [N agents] | [±N] |\n\n\n| Databricks | [N agents] | [N agents] | [±N] |\n\n\n| In-Memory/Return | [N agents] | [N agents] | [±N] |\n\n\n| Not Specified | [N agents] | [N agents] | [±N] |\n\n\nAfter tables, 3-4 bullets explaining:\n\n\n- Output format compatibility\n\n\n- Destination alignment\n\n\n- Data contract changes\n\n\n- Integration impact\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n9. MISSING & EXTRA METADATA (EXPLICIT SECTION)\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**Metadata Present in AAVA 1.0 but Missing in AAVA 2.0:**\n\n\n- [List each agent/component/field that exists in 1.0 but not in 2.0]\n\n\n- [Example: \"DI_Token_Size_Estimator_S3_IMS agent - no equivalent in AAVA 2.0\"]\n\n\n- [Example: \"IMS_S3TokenCounterTool tool reference - not used in AAVA 2.0\"]\n\n\n- [If none: \"All AAVA 1.0 metadata elements have equivalents in AAVA 2.0\"]\n\n\n**Metadata Present in AAVA 2.0 but Missing in AAVA 1.0:**\n\n\n- [List each agent/component/field that exists in 2.0 but not in 1.0]\n\n\n- [Example: \"DI_JiraStoryReaderTool tool reference - not present in AAVA 1.0\"]\n\n\n- [Example: \"Test scenario documentation in task descriptions - new in AAVA 2.0\"]\n\n\n- [If none: \"All AAVA 2.0 metadata elements have equivalents in AAVA 1.0\"]\n\n\n**Critical Missing Metadata:**\n\n\n- [List any critical metadata fields that are missing in either version]\n\n\n- [Example: \"AAVA 2.0 agents missing expected output specifications\"]\n\n\n- [If none: \"No critical metadata missing in either version\"]\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n10. OVERALL METADATA COMPATIBILITY VERDICT\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**Metadata Compatibility Assessment**\n\n\n| Compatibility Dimension | Verdict | Justification |\n\n\n|-------------------------|---------|---------------|\n\n\n| Structural Compatibility | [Compatible/Partially Compatible/Incompatible] | [Explanation based on workflow structure, agent count, matching success] |\n\n\n| Intent Compatibility | [Equivalent/Partial/Not Equivalent] | [Explanation based on intent classification matching] |\n\n\n| Tool Compatibility | [Equivalent/Partial/Not Equivalent] | [Explanation based on tool alignment] |\n\n\n| Model Compatibility | [Same/Different/Mixed] | [Explanation based on model references] |\n\n\n| Control Flow Compatibility | [Equivalent/Different] | [Explanation based on control logic patterns] |\n\n\n| Output Specification Compatibility | [Equivalent/Different] | [Explanation based on output definitions] |\n\n\n| **Overall Metadata Equivalence** | **[Equivalent/Partially Equivalent/Not Equivalent]** | **[Summary assessment considering all dimensions]** |\n\n\n**Migration Reusability Assessment:**\n\n\n| Assessment Aspect | Status |\n\n\n|-------------------|--------|\n\n\n| Can AAVA 2.0 Directly Replace AAVA 1.0? | [Yes/No/Partially] |\n\n\n| Metadata Redesign Required? | [No/Minimal/Moderate/Complete] |\n\n\n| Agent Mapping Possible? | [Yes/Partially/No] |\n\n\n| Tool Migration Feasible? | [Yes/Partially/No] |\n\n\n| **Overall Reusability Verdict** | **[Directly Reusable/Reusable with Modifications/Not Reusable]** |\n\n\nAfter tables, 4-6 bullets explaining:\n\n\n- Overall metadata equivalence justification\n\n\n- Reusability constraints\n\n\n- Migration effort estimation\n\n\n- Redesign requirements\n\n\n- Compatibility risks\n\n\n- Recommendation for migration approach\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n11. FINAL SUMMARY ( SHORT & CLEAR)\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n**Final Summary:**\n\n\n[2-3 sentences providing clear verdict on metadata compatibility, reusability status, and migration feasibility. Example:]\n\n\n\"AAVA 1.0 and AAVA 2.0 workflows exhibit [equivalent/partial/incompatible] metadata structures with [N] matched agents out of [total] components. [Intent/Tooling/Control Flow] compatibility is [status], making the workflows [directly reusable/reusable with modifications/not reusable] for migration purposes. [Specific redesign requirements or migration risks if applicable].\"\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nFORMATTING & QUALITY STANDARDS\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n1. Heading: CENTER-ALIGNED and BOLD\n\n\n2. Tables: Clean, Excel-extractable, properly aligned\n\n\n3. Every table MUST be followed by 3-6 explanatory bullets\n\n\n4. Language: Professional, audit-ready, consulting-grade\n\n\n5. NO dates anywhere in report\n\n\n6. NO AI self-reference or self-praise\n\n\n7. NO placeholders - use actual extracted metadata\n\n\n8. NO assumptions - only compare what exists in JSON\n\n\n9. Verdicts: Use exact terms defined in configuration\n\n\n10. Deterministic: Same inputs = identical outputs always\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nABSOLUTE PROHIBITIONS\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nThe agent MUST NEVER:\n\n\n- Compare runtime behavior or execution outcomes\n\n\n- Infer functionality not explicitly stated in metadata\n\n\n- Assume agent equivalence from name similarity alone\n\n\n- Invent metadata fields or values\n\n\n- Force equivalence when metadata differs\n\n\n- Skip unmatched agents in reporting\n\n\n- Use probabilistic or adaptive reasoning\n\n\n- Produce different outputs for same inputs\n\n\n- Include dates in the report\n\n\n- Use template/placeholder text\n\n\n- Make execution or business logic assumptions\n\n\nThe agent MUST ALWAYS:\n\n\n- Compare only JSON metadata as physically present\n\n\n- Mark missing metadata explicitly\n\n\n- Report intent mismatches truthfully\n\n\n- Include all agents (matched and unmatched)\n\n\n- Follow exact report structure defined\n\n\n- Produce deterministic outputs\n\n\n- Use professional audit-ready language\n\n\n- Generate Excel-compatible tables\n\n\n- Provide clear verdicts based on metadata analysis\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\nDETERMINISM & REPRODUCIBILITY GUARANTEE\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n- Identical JSON inputs → Identical reports (bit-for-bit)\n\n\n- Alphabetical sorting for consistency\n\n\n- Fixed classification rules (no variance)\n\n\n- Exact verdict terms (no synonyms)\n\n\n- No timestamps or run-specific data\n\n\n- No randomness in matching or classification\n\n\n- Intent classification purely regex-based\n\n\n- Verdicts mechanically derived from metadata\n\n\n- Same metadata values → Same verdicts always\n\n\nThis configuration ensures metadata-only comparison, hallucination safety, truthful reporting, audit-ready tables, and complete determinism for professional workflow migration assessments.\n\n\n```\n\n\n---MANDATORY TOOL USAGE:\nYou MUST call the DirectoryRead and FileReadTool with the user's question\nDO NOT attempt to answer without calling the tool\nDO NOT generate synthetic or assumed information\nTool calling is REQUIRED - no exceptions./n  - workflow_1402.json\n  - workflow_4100_filtered.json",
  "expected_output": "```\n\nREPORT: Professional metadata comparison with strict structure\n\nSTRUCTURE:\n\n- Bold center title\n\n- Master metadata overview table (immediately after heading)\n\n- 11 sections in exact order as specified\n\n- 2-3 sentence final summary\n\nCONTENT:\n\n- Workflow-level metadata comparison\n\n- Agent inventory and presence analysis\n\n- Intent classification and compatibility\n\n- Tooling and integration metadata comparison\n\n- Control flow and decision metadata comparison\n\n- Output specification comparison\n\n- Explicit missing/extra metadata lists\n\n- Overall compatibility verdict with reusability assessment\n\n- Professional closing summary\n\nVALIDATION:\n\n- Metadata-only comparison (no execution assumptions)\n\n- Real JSON elements only (no hallucination)\n\n- Exact agent names and tool references from inputs\n\n- Intent classification based on task description content\n\n- Truthful reporting of missing/unmatched components\n\nQUALITY:\n\n- Excel-extractable tables\n\n- Clear verdicts using exact terminology\n\n- Audit-ready professional language\n\n- Deterministic outputs\n\n- No dates, no placeholders, no AI self-reference\n\nFORMAT:\n\n- All tables followed by explanatory bullets\n\n- Consistent verdict terminology\n\n- Clean structured presentation\n\n- Migration reusability assessment included\n\n```\n\n---",
  "summary": "```\n\nENTERPRISE WORKFLOW METADATA COMPARISON ENGINE - PROFESSIONAL SPECIFICATION\n\n\nThis agent performs...",
  "raw": "**AAVA 1.0 vs AAVA 2.0 Workflow Metadata Comparison Report**\n---\n\n**MASTER METADATA OVERVIEW TABLE**\n\n| Metadata Aspect           | AAVA 1.0                                 | AAVA 2.0                                 | Comparison Result           |\n|--------------------------|-------------------------------------------|------------------------------------------|----------------------------|\n| Workflow Identifier      | workflowId: 1402                         | workFlowDetail.id: 4100                  | Different                   |\n| Total Agent Count        | 5                                        | 5                                        | Same                       |\n| Matched Agents           | -                                        | -                                        | 5 matched / 5 total         |\n| Unmatched Agents (1.0)   | -                                        | -                                        | 0 unmatched                 |\n| Unmatched Agents (2.0)   | -                                        | -                                        | 0 unmatched                 |\n| Intent Compatibility     | -                                        | -                                        | Equivalent                  |\n| Tooling Compatibility    | -                                        | -                                        | Equivalent                  |\n| Model Consistency        | -                                        | -                                        | Different                   |\n| Overall Metadata Equivalence | -                                    | -                                        | Partially Equivalent        |\n\n- Compared workflow structure, agent definitions, intent classification, tool references, model deployments, control flow, output specifications, and field naming conventions.\n- Both workflows have identical agent sets and intent distribution; however, workflow identifiers and model references differ.\n- No agents or tools are missing or extra; naming conventions and metadata field names vary between versions.\n- This is a strict metadata-only comparison—no execution or runtime behavior was analyzed.\n- Migration is feasible with minor model and identifier adjustments; direct reusability is possible with targeted modifications.\n\n---\n\n**Workflow-Level Metadata Comparison**\n\n| Metadata Field           | AAVA 1.0 Value           | AAVA 2.0 Value                 | Status         |\n|-------------------------|--------------------------|---------------------------------|----------------|\n| Workflow ID Field Name  | workflowId               | workFlowDetail.id               | Different      |\n| Workflow ID Value       | 1402                     | 4100                            | Different      |\n| Workflow Name Field Name| workflowName             | workFlowDetail.name             | Different      |\n| Workflow Name Value     | ABAP_To_Pyspark_Convert  | ABAP To Pyspark Convert         | Different      |\n| JSON Structure Style    | Flat                     | Nested (workFlowDetail + agents)| Different      |\n| Agent Array Field Name  | nodes                    | workflowAgents                  | Different      |\n\n- AAVA 1.0 uses flat structure with 'workflowId' and 'workflowName'; AAVA 2.0 uses nested 'workFlowDetail' and 'workflowAgents'.\n- Field naming conventions differ: underscores in 1.0, camel case/nested in 2.0.\n- Workflow identifiers and names are not the same; migration may require mapping.\n- Agent array field names are distinct, requiring structural adaptation for migration.\n\n---\n\n**Agent Inventory Comparison**\n\n| Agent Name (Normalized)         | Present in AAVA 1.0 (Exact)        | Present in AAVA 2.0 (Exact)        | Status           |\n|---------------------------------|------------------------------------|------------------------------------|------------------|\n| abaptopysparkconverter          | Yes - ABAP_to_PySpark_Converter    | Yes - ABAP to PySpark Converter    | Matched          |\n| abaptopysparkunittesting        | Yes - ABAP_to_PySpark_Unit_Testing | Yes - ABAP to PySpark Unit Testing | Matched          |\n| abaptopysparkconversiontester   | Yes - ABAP_to_PySpark_Conversion_Tester | Yes - ABAP to PySpark Conversion Tester | Matched    |\n| abaptopysparkrecontest          | Yes - ABAP_To_PySpark_Recon_Tester | Yes - ABAP To PySpark Recon Tester | Matched          |\n| abaptopysparkreviewer           | Yes - ABAP_To_PySpark_Reviewer     | Yes - ABAP To PySpark Reviewer     | Matched          |\n\nSummary Counts:\n\n| Category                | Count |\n|-------------------------|-------|\n| Agents in AAVA 1.0 Only | 0     |\n| Agents in AAVA 2.0 Only | 0     |\n| Matched Agents          | 5     |\n| Total Unique Agents     | 5     |\n\n- All agents are present in both versions; no agents missing or extra.\n- Naming conventions differ slightly (underscores in 1.0, spaces/camel case in 2.0).\n- No critical agents missing; agent set is stable across migration.\n\n---\n\n**Agent Intent Classification & Comparison**\n\n| Agent Name (Matched Pairs)      | AAVA 1.0 Intent           | AAVA 2.0 Intent           | Intent Match    |\n|---------------------------------|---------------------------|---------------------------|-----------------|\n| abaptopysparkconverter          | Conversion/Transformation | Conversion/Transformation | ✓ Equivalent    |\n| abaptopysparkunittesting        | Testing/Validation        | Testing/Validation        | ✓ Equivalent    |\n| abaptopysparkconversiontester   | Review/Audit              | Review/Audit              | ✓ Equivalent    |\n| abaptopysparkrecontest          | Testing/Validation        | Testing/Validation        | ✓ Equivalent    |\n| abaptopysparkreviewer           | Review/Audit              | Review/Audit              | ✓ Equivalent    |\n\nIntent Distribution Summary:\n\n| Intent Category           | AAVA 1.0 Count | AAVA 2.0 Count | Change |\n|--------------------------|----------------|----------------|--------|\n| Conversion/Transformation| 1              | 1              | 0      |\n| Testing/Validation       | 2              | 2              | 0      |\n| Review/Audit             | 2              | 2              | 0      |\n| Token/Size Estimation    | 0              | 0              | 0      |\n| Code Generation/Update   | 0              | 0              | 0      |\n| Workflow Execution       | 0              | 0              | 0      |\n| Merging/Consolidation    | 0              | 0              | 0      |\n| Generic/Unclear          | 0              | 0              | 0      |\n\n- Intent alignment is perfect; all matched agents retain their original purpose.\n- No intent mismatches or unclear intents; functional equivalence is maintained.\n- Intent distribution unchanged; workflows serve the same business purpose.\n- No risk of intent drift during migration.\n\n---\n\n**Tooling Metadata Comparison**\n\n| Agent (Matched Pairs)            | AAVA 1.0 Tools | AAVA 2.0 Tools | Tool Compatibility    |\n|----------------------------------|----------------|----------------|----------------------|\n| abaptopysparkconverter           | []             | []             | ✓ Same Tools         |\n| abaptopysparkunittesting         | []             | []             | ✓ Same Tools         |\n| abaptopysparkconversiontester    | []             | []             | ✓ Same Tools         |\n| abaptopysparkrecontest           | []             | []             | ✓ Same Tools         |\n| abaptopysparkreviewer            | []             | []             | ✓ Same Tools         |\n\nIntegration Points Summary:\n\n| Integration Type | AAVA 1.0 | AAVA 2.0 | Status   |\n|------------------|----------|----------|----------|\n| S3 Storage       | No       | No       | Same     |\n| GitHub           | No       | No       | Same     |\n| Jira             | No       | No       | Same     |\n| Databricks       | No       | No       | Same     |\n| Custom Tools     | None     | None     | Same     |\n\n- No tools or integrations are defined in either version.\n- Tooling ecosystem is identical; no migration of external dependencies required.\n- No tool migration requirements or compatibility risks.\n- Workflows are self-contained with respect to tooling.\n\n---\n\n**Control Flow Metadata Comparison**\n\n| Agent (Matched Pairs)            | AAVA 1.0 Control Logic | AAVA 2.0 Control Logic | Control Flow Match |\n|----------------------------------|------------------------|------------------------|--------------------|\n| abaptopysparkconverter           | Sequential conversion steps | Sequential conversion steps | ✓ Equivalent    |\n| abaptopysparkunittesting         | Sequential test generation  | Sequential test generation  | ✓ Equivalent    |\n| abaptopysparkconversiontester    | Sequential comparison/review| Sequential comparison/review| ✓ Equivalent    |\n| abaptopysparkrecontest           | Sequential validation      | Sequential validation      | ✓ Equivalent    |\n| abaptopysparkreviewer            | Sequential review          | Sequential review          | ✓ Equivalent    |\n\nControl Flow Patterns Summary:\n\n| Control Pattern           | AAVA 1.0 Count | AAVA 2.0 Count | Change |\n|--------------------------|----------------|----------------|--------|\n| Conditional Logic        | 0              | 0              | 0      |\n| Delegation Patterns      | 0              | 0              | 0      |\n| Sequential Gating        | 5              | 5              | 0      |\n| Parallel Execution Hints | 0              | 0              | 0      |\n| No Control Flow          | 0              | 0              | 0      |\n\n- Control flow is sequential across all agents; no conditional or parallel logic.\n- No differences in orchestration or gating mechanisms.\n- Execution order and decision logic are structurally identical.\n- No migration risk for control flow patterns.\n\n---\n\n**Output Specification Metadata Comparison**\n\n| Agent (Matched Pairs)            | AAVA 1.0 Output Type          | AAVA 2.0 Output Type          | Output Compatibility |\n|----------------------------------|-------------------------------|-------------------------------|---------------------|\n| abaptopysparkconverter           | Converted PySpark Code + API Cost | Converted PySpark Code + API Cost | ✓ Equivalent    |\n| abaptopysparkunittesting         | Test Case List + Pytest Script + API Cost | Test Case List + Pytest Script + API Cost | ✓ Equivalent    |\n| abaptopysparkconversiontester    | Test Case List + Pytest Script + API Cost | Test Case List + Pytest Script + API Cost | ✓ Equivalent    |\n| abaptopysparkrecontest           | Python Validation Script + API Cost | Python Validation Script + API Cost | ✓ Equivalent    |\n| abaptopysparkreviewer            | Code Review Report + API Cost | Code Review Report + API Cost | ✓ Equivalent    |\n\nOutput Destination Summary:\n\n| Destination Type     | AAVA 1.0 Count | AAVA 2.0 Count | Change |\n|----------------------|----------------|----------------|--------|\n| S3 Storage           | 0              | 0              | 0      |\n| GitHub Repository    | 0              | 0              | 0      |\n| Databricks           | 0              | 0              | 0      |\n| In-Memory/Return     | 5              | 5              | 0      |\n| Not Specified        | 0              | 0              | 0      |\n\n- Output specifications are identical for all agents.\n- Output format and destination are aligned; no contract changes.\n- No integration impact or data migration risks.\n- Outputs are consistent and migration-ready.\n\n---\n\n**Metadata Present in AAVA 1.0 but Missing in AAVA 2.0:**\n- Field names: 'workflowId', 'workflowName', and 'nodes' are not present in AAVA 2.0 (replaced by nested structure).\n- Model field 'model' used in 1.0, replaced by 'modelDeploymentName' in 2.0.\n\n**Metadata Present in AAVA 2.0 but Missing in AAVA 1.0:**\n- Field names: 'workFlowDetail' (with nested 'id' and 'name'), 'workflowAgents', 'agentId', and 'modelDeploymentName'.\n- No new agents or tool references introduced.\n\n**Critical Missing Metadata:**\n- No critical metadata missing in either version; all required agent/task/output fields are present.\n\n---\n\n**Metadata Compatibility Assessment**\n\n| Compatibility Dimension        | Verdict            | Justification                                               |\n|-------------------------------|--------------------|-------------------------------------------------------------|\n| Structural Compatibility      | Partially Compatible| Workflow structure and field names differ; agent set matches |\n| Intent Compatibility          | Equivalent         | All agents have identical intent classification              |\n| Tool Compatibility            | Equivalent         | No tools defined in either version                           |\n| Model Compatibility           | Different          | Model references differ: 'o3-mini'/'gpt-4o' vs 'gpt-4.1'     |\n| Control Flow Compatibility    | Equivalent         | Sequential logic in both versions                            |\n| Output Specification Compatibility | Equivalent    | Output types and formats are identical                       |\n| **Overall Metadata Equivalence** | **Partially Equivalent** | **Structural and model differences require minor mapping**   |\n\n**Migration Reusability Assessment:**\n\n| Assessment Aspect                  | Status         |\n|------------------------------------|---------------|\n| Can AAVA 2.0 Directly Replace AAVA 1.0? | Partially    |\n| Metadata Redesign Required?        | Minimal       |\n| Agent Mapping Possible?            | Yes           |\n| Tool Migration Feasible?           | Yes           |\n| **Overall Reusability Verdict**    | **Reusable with Modifications** |\n\n- Metadata equivalence is high, with only field name and model reference differences.\n- Agent mapping is direct; no missing or extra agents.\n- Migration effort is minimal, requiring updates to workflow identifiers and model deployment references.\n- No redesign needed for intent, tool, control flow, or output specifications.\n- Compatibility risks are limited to structural adaptation and model mapping.\n\n---\n\n**Final Summary:**\n\nAAVA 1.0 and AAVA 2.0 workflows exhibit partially equivalent metadata structures with all 5 agents matched and identical intent, tool, control flow, and output specifications. Structural and model reference differences require minimal mapping, making the workflows reusable with targeted modifications for migration purposes. No critical metadata is missing, and direct agent mapping is feasible."
}