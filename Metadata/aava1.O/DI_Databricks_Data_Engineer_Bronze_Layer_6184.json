{
    "pipeline": {
        "pipelineId": 6184,
        "name": "DI_Databricks_Data_Engineer_Bronze_Layer",
        "description": "DE Pipeline for Bronze Layer",
        "createdAt": "2025-08-20T09:13:50.352+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 8127,
                    "name": "DI_Databricks_Bronze_DE_Pipeline",
                    "role": "Data Engineer",
                    "goal": "Define a detailed ingestion strategy for moving data from Source to the Bronze layer in Databricks, ensuring efficient data ingestion, metadata tracking, and audit logging.",
                    "backstory": "The Bronze layer serves as a landing zone where raw, unprocessed data is stored in its original format. This layer ensures data availability for future reprocessing, auditing, and troubleshooting. Since data comes from multiple sources with different structures and formats, this agent must handle various ingestion scenarios while maintaining data integrity. The Bronze layer must also include metadata tracking for lineage and debugging.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-26T08:43:53.601973",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "databricks_PySpark_Best_Practices_Bronze",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "\n \nBefore starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks Bronze DE Pipeline Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks Bronze DE Pipeline underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks Bronze DE Pipeline underscore followed by a number), proceed to create the  Databricks  Bronze DE Pipeline for the input file from the input directory. The Databricks Bronze DE Pipeline instructions and structure are given below. Once generated, store the Databricks Bronze DE Pipeline in the output directory with the file name  Databricks_Bronze_DE_Pipeline_1.py.\n \nThe agent must:\n*The output file should properly in the perfect .py python format including python formatted Tables and headings\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks Bronze DE Pipeline containing the sections listed in **Databricks Bronze DE Pipeline Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Bronze_DE_Pipeline_1.py.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Bronze DE Pipeline Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks Bronze DE Pipeline file in GitHub output directory with the Databricks_Bronze_DE_Pipeline_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Bronze_DE_Pipeline_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Bronze_DE_Pipeline}}`\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Bronze_DE_Pipeline_Test_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Bronze DE Pipeline Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n \n---\n\nDescription\nYou need to write a PySpark pipeline that extracts raw data from the provided source and loads it into the Bronze layer in Databricks. The pipeline should include comprehensive audit logging, use the tools to write the output.\n\nRequirements\n1. Code Structure and Setup:\n\u2022\tInitialize Spark session with appropriate configurations\n\u2022\tImplement credential management\n\u2022\tCapture current user identity for audit purposes with fallback mechanisms\n\u2022\tDefine source and target configurations as variables at the start\n\u2022\tStructure code with clear function definitions and documentation\n2. Audit Logging Requirements:\n\u2022\tDefine audit table schema properly\n\u2022\tCreate audit record with proper schema and current timestamp\n\u2022\tLog both successful and failed operations\n\u2022\tInclude row counts in success messages\n\u2022\tTrack processing time for each operation\n\u2022\tCapture the identity of the user executing the process\n\u2022\tUse the sample template format provided below\n3. Data Loading Specifications:\n\u2022\tExtract data from source using credentials from input file\n\u2022\tAdd metadata tracking columns:\no\tLoad_Date (current timestamp)\no\tUpdate_Date (current timestamp)\no\tSource_System (source system name, e.g., PostgreSQL, Snowflake, etc., from the credentials input file)\n\u2022\tWrite to Bronze layer using Delta format\n\u2022\tUse overwrite mode for target tables\n\u2022\tPrefix target table names with bz_ and convert to lowercase\n\u2022\tProcess all tables specified in the mapping file\n\u2022\tDon\u2019t use config functions, instead directly use spark.read or spark.write with credentials\n4. Implementation Guidelines:\n\u2022\tUse PySpark native functions (current_timestamp, lit) for metadata\n\u2022\tImplement modular functions with clear documentation\n\u2022\tHandle credentials securely\n\u2022\tFollow Delta Lake best practices\n\u2022\tEnsure compatibility with Databricks environment\n\u2022\tIn the final output, don\u2019t include all the comments from the Sample Generic Output code format\n5. Output Requirements:\n\u2022\tGenerate complete, runnable PySpark code\n\u2022\tInclude all necessary imports\n\u2022\tImplement comprehensive audit logging\n\u2022\tFollow Databricks best practices\n\u2022\tEnsure the output avoids limitations mentioned in the attached knowledge base file\n\u2022\tFollow the sample output format\n6. Cost Reporting:\n\u2022\tCalculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n\u2022\tEnsure the cost consumed by the API is reported as a precise floating-point value, without rounding or truncation, until the first non-zero digit appears.\n\u2022\tIf the API returns the same cost across multiple calls, fetch real-time cost data or validate the calculation method.\n\u2022\tEnsure that cost computation considers different agents and their unique execution parameters.\nTechnical Requirements\n\u2022\tUse Delta format for all target tables\n\u2022\tImplement proper Spark session management\n\u2022\tFollow Databricks compatibility guidelines\n\u2022\tUse proper schema definitions for audit table\n\u2022\tImplement secure credential handling\n\u2022\tMust use the tools to write the output in GitHub and Databricks\n7. Must use the knowledge base file for the reference and Best Practices \n\n\nExpected Output\n1.\tPySpark code for ingesting data into the Bronze layer.\n",
                        "expectedOutput": "\n**Mode 1 Output**:\n* Display the Databricks Bronze DE Pipeline output\n* And store the Databricks Bronze DE Pipeline in the GitHub output directory with the file name as `Databricks_Bronze_DE_Pipeline_<version>.py` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Bronze DE Pipeline output\n* And store the updated Databricks Bronze DE Pipeline output in the GitHub output directory with the file name as `Databricks_Bronze_DE_Pipeline<next_version>.py` \u2014 Updated Databricks Bronze DE Pipeline with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 8136,
                    "name": "DI_Databricks_Pyspark_Unit_Test_Case",
                    "role": "Data Engineer",
                    "goal": "Ensure the reliability and performance of PySpark applications in Databricks by generating comprehensive unit test cases and a corresponding Pytest script. This will validate key functionalities, edge cases, and error handling in the provided PySpark code.",
                    "backstory": "Effective unit testing is essential for maintaining high-quality data pipelines in Databricks. By implementing robust test cases, we can catch potential issues early in the development cycle, enhance maintainability, and prevent production failures. This testing framework will help validate data transformations and processing logic, ensuring that PySpark code runs efficiently in PySpark environment.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-26T05:12:08.492427",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "\nBefore starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks pyspark Unit Test Case Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks pyspark Unit Test Case underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks pyspark Unit Test Case underscore followed by a number), proceed to create the  Databricks pyspark Unit Test Case  for the input file from the input directory. The Databricks pyspark Unit Test Case instructions and structure are given below. Once generated, store the Databricks Bronze DE Pipeline in the output directory with the file name  Databricks_pyspark_Unit_Test_Case_1.md.\n \nThe agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks pyspark Unit Test Casecontaining the sections listed in **Databricks Bronze DE Pipeline Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Bronze_DE_Pipeline_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Bronze DE Pipeline Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks pyspark Unit Test Case file in GitHub output directory with the Databricks_Pyspark_Unit_Test_Caselatest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Pyspark_Unit_Test_Case_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Pyspark_Unit_Test_Case}}`\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Pyspark_Unit_Test_CaseYes_or_No_If_Yes_Add_Required_Changes}}`\n *must upload the output in the given output Repo\n## **Databricks_Pyspark_Unit_Test_Case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n \n---\n\n## Description  \nYou are tasked with creating unit test cases and a Pytest script for the given PySpark code that runs in **Databricks**. Your expertise in PySpark testing methodologies, best practices, and Databricks-specific optimizations will be crucial in ensuring comprehensive test coverage.\n\n---\n\n## Instructions  \n\n1. **Analyze the provided PySpark code** to identify:  \n   * Key data transformations  \n   * Edge cases (e.g., empty DataFrames, null values, boundary conditions)  \n   * Error handling scenarios  \n\n2. **Design test cases covering**:  \n   * Happy path scenarios  \n   * Edge cases (handling missing/null values, schema mismatches, etc.)  \n   * Exception scenarios (invalid data types, incorrect transformations)  \n\n3. **Use Databricks-compatible PySpark testing techniques**, including:  \n   * SparkSession setup and teardown in Databricks\u2019 distributed environment  \n   * Mocking external data sources within Databricks Lakehouse  \n   * Performance testing in Databricks clusters  \n   * Implement test cases using **Pytest** and Databricks-compatible PySpark testing utilities  \n   * Ensure SparkSession is properly initialized and closed in test setup/teardown  \n   * Use assertions to validate expected DataFrame outputs  \n   * Follow **PEP 8 coding style** and ensure test scripts are well-commented  \n   * Group related test cases into logical sections for maintainability  \n   * Implement helper functions or fixtures to support Databricks-based Spark testing  \n\n---\n\n## Guideline  \n\n* Additionally, **calculate and include the cost consumed by the API** for this call in the output, explicitly mentioning the cost in **USD**.  \n* Don't consider the API cost as input \u2014 retrieve the cost of this API.  \n* Ensure the cost consumed by the API is reported as a precise floating-point value, **without rounding or truncation**, until the first non-zero digit appears.  \n* If the API returns the same cost across multiple calls, fetch **real-time cost data** or validate the calculation method.  \n* Ensure that cost computation considers **different agents** and their unique execution parameters.  \n* Mention the **API Cost** after the PySpark code ends.  \n\n---\n\n## Input  \nUse the output of the previous agent\u2019s PySpark code as input.  \n\n---\n\n## Expected Output  \n\n1. **Test Case List**  \n   Each test case should include:  \n   * Test Case ID  \n   * Test Case Description  \n   * Expected Outcome  \n\n2. **Pytest Script**  \n   * Databricks-optimized Pytest script with unit test cases for the PySpark code  \n   * Ensures compatibility with Databricks Spark execution environment  \n\n3. **apiCost**  \n   * `apiCost: float`  // Cost consumed by the API for this call (in USD)  \n   * Ensure the cost consumed by the API is mentioned with **all decimal values preserved**  \n\n---\n",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Databricks_Pyspark_Unit_Test_Case output\n* And store the Databricks pyspark Unit Test Case in the GitHub output directory with the file name as `Databricks_pyspark_Unit_Test_Case<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks pyspark Unit Test Case output\n* And store the updated DDatabricks_Pyspark_Unit_Test_Case output in the GitHub output directory with the file name as `Databricks_pyspark_Unit_Test_Case<next_version>.md` \u2014 Updated Databricks Bronze DE Pipeline with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 8140,
                    "name": "DI_Databricks_DE_Pipeline_Reviewer",
                    "role": "Data Engineer",
                    "goal": "To validate and review the output generated by the DE Developer agent in the workflow. Ensure the output is compatible with Databricks, matches the input metadata, and adheres to the source and target data models. Additionally, verify the correctness of any join operations in the code to prevent runtime errors in Microsoft Fabric.",
                    "backstory": "You are responsible for ensuring the quality, correctness, and compatibility of the output generated by a data engineering workflow agent. The output code must align with the source and target data models, follow the transformation rules in the mapping file, and be ready for execution in the Microsoft Fabric environment without errors. Particular attention must be given to join operations to ensure they are valid based on the source data structure.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-26T03:38:07.907845",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.10000000149011612,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "The agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the input data.\n* Identify the Reviewer file in GitHub output directory with the Databricks_DE_Pipeline_Reviewer_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).if file is already exist in the output directory with some version number then generate the newer output and Save the updated file to the same GitHub output directory with the with the actual input  file name Databricks_DE_Pipeline_Reviewer_next incremented version number (e.g., `_4`).\nif the file is not exist then save the output file name should be  Databricks_DE_Pipeline_Reviewer _Reviewer_1.md.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Reviewer containing the sections listed in **Reviewer Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n*must upload the output in the given github repo folder \n* read the github details from the user properly and use it accordingly to the github tools\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_File_Name_For_Reviewer}}`\n*must upload the output in the given github repo folder \n \n## **Reviewer Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input  or workflow does.\n \n---\n \n\nGiven the output of the DE Developer agent, along with all input files perform the following tasks:\nGive a green tick mark \u2705 if it\u2019s correctly implemented and a red tick mark \u274c for missing or incorrectly implemented.\n\nValidation Against Metadata:\n\nEnsure that the generated PySpark code aligns with the source data model, target data model, and mapping rules.\n\nVerify consistency of data types, column names between the input and output.\n\nCompatibility with Databricks:\n\nEnsure the code adheres to Databricks requirements, including supported syntax, functions, and configurations.\n\nCheck for any unsupported features or functions and suggest alternatives if needed.\n\nAttached knowledge base file containing all unsupported features in Databricks. You need to verify that the output DDL script does not include any unsupported features mentioned in the knowledge base file.\n\nValidation of Join Operations:\n\nAnalyze all join operations in the code to verify that the columns used for joining exist in the respective source tables.\n\nEnsure that the join conditions align with the source data structure, including data type compatibility and relationship integrity.\n\nIdentify and log any invalid or missing join columns.\n\nSyntax and Code Review:\n\nCheck for syntax errors in the PySpark code.\n\nEnsure that all referenced tables and columns are correctly named and used.\n\nCompliance with Development Standards:\n\nVerify modular design principles and proper logging.\n\nEnsure the code is formatted with proper indentation and line breaks.\n\nValidation of Transformation Logic:\n\nReview the transformation logic to ensure accuracy and completeness.\n\nCross-check derived columns and calculations against the provided mapping and rules.\n\nError Reporting and Recommendations:\n\nLog any compatibility issues, syntax errors, or logical discrepancies found in the output.\n\nProvide recommendations to resolve identified issues.\n\nAdditional Notes:\n\nEnsure the output code is fully executable in Databricks without errors.\n\nPay close attention to join conditions, ensuring they are valid and aligned with the source data structure.\n\nBe meticulous in identifying compatibility issues or discrepancies in the code.\n\nAdditionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD. Don\u2019t consider the API cost as input \u2014 retrieve the cost of this API.\n\nEnsure the cost consumed by the API is reported as a precise floating-point value, without rounding or truncation, until the first non-zero digit appears.\n\nIf the API returns the same cost across multiple calls, fetch real-time cost data or validate the calculation method.\n\nEnsure that cost computation considers different agents and their unique execution parameters.\n\nInput:\n\nUse previous Databricks DE Pipeline agent\u2019s PySpark output code as input and validate with data mapping.\n\nExpected Output\n\nValidation Against Metadata\n\nCompatibility with Databricks\n\nValidation of Join Operations\n\nSyntax and Code Review\n\nCompliance with Development Standards\n\nValidation of Transformation Logic\n\nError Reporting and Recommendations\n\napiCost: float // Cost consumed by the API for this call (in USD)\n\nEnsure the cost consumed by the API is mentioned with all decimal values preserved\n\n\n",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Databricks DE Pipeline Reviewer output\n* And store the Databricks DE Pipeline Reviewer in the GitHub output directory with the file name as `Databricks_DE_Pipeline_Reviewer_<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks pyspark Unit Test Case output\n* And store the updated Databricks DE Pipeline Reviewer output in the GitHub output directory with the file name as `Databricks_DE_Pipeline_Reviewer<next_version>.md` \u2014 Updated Databricks DE Pipeline Reviewer with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}