{
    "pipeline": {
        "pipelineId": 8190,
        "name": "DI_Azure_Synapse_To_PySpark_Doc&Analyze",
        "description": "Synapse to pyspark Documentation, Analysis and plan workflow",
        "createdAt": "2025-11-10T06:23:50.122+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9537,
                    "name": "DI_Synapse_Documentation",
                    "role": "Data Engineer",
                    "goal": "This document outlines the purpose and functionality of Azure Synapse components such as pipelines, dataflows, stored procedures, SQL scripts, and parameter files. It serves as a guide to understanding how these Synapse components facilitate data ingestion, transformation, processing, and orchestration across the data pipeline.",
                    "backstory": "Azure Synapse has become a core cloud-based data integration and analytics platform for modern enterprises, enabling both batch and near real-time data pipelines for data warehousing, regulatory reporting, and advanced analytics. Its components (pipelines, dataflows, SQL scripts, JSON configurations, etc.) support scalable processing, metadata-driven transformations, and seamless orchestration across cloud and hybrid environments. Organizations rely on it to ensure data quality, enrichment, and automated pipeline execution.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-12T07:11:54.797178",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "====================================================\nAuthor:        Ascendion AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n====================================================\n1. Overview of Pipeline/Component\nDescribe the purpose of the Azure Synapse pipeline, dataflow, SQL script, or linked service.\n\nExplain the business logic or requirement the pipeline or component addresses.\n\n2. Component Structure and Design\nDescribe the layout and logical grouping of activities within the pipeline or dataflow.\n\nHighlight key components such as:\n\nSource (Azure Blob, Data Lake, SQL DB, etc.)\n\nCopy Activity\n\nDataflow Transformation (Join, Aggregate, Derived Column, Filter, Conditional Split, Lookup)\n\nStored Procedure Activity\n\nForEach, Wait, Web, or Custom Activities\n\nSink (Azure SQL, Synapse Table, Parquet, CSV, etc.)\n\nExplain the connection flow between activities and the use of parameters, variables, and triggers.\n\n3. Data Flow and Processing Logic\nList the key data sources, staging/intermediate datasets, and final outputs.\n\nFor each logical step:\n\nDescribe what it does (e.g., filtering, joining, aggregating, mapping).\n\nMention any SQL scripts, stored procedures, or notebooks used.\n\nInclude any business rules or transformations applied.\n\n4. Data Mapping (Lineage)\nMap fields from source datasets to target datasets in the following format:\ngive the below details as a marked down table\n\nTarget Table Name : <actual target table/view>\nTarget Column Name : <actual column>\nSource Table Name : <actual source table/view>\nSource Column Name : <actual column>\nRemarks : <1:1 Mapping | Transformation | Validation - include logic description>\n5. Transformation Logic\nDocument each derived column expression, computed column, or SQL transformation used in the pipeline or dataflow.\n\nExplain what each transformation does and which fields are involved.\n\nNote any user-defined functions (UDFs) or reusable templates applied.\n\n6. Complexity Analysis\ngive the below details as a marked down table\nProvide a high-level complexity summary:\n\nNumber of Pipeline Activities: <integer>\n\nNumber of Dataflow Transformations: <integer>\n\nSQL Scripts or Stored Procedures Used: <count>\n\nJoins Used: <list of types or None>\n\nLookup Tables or Reference Data: <count or 'None'>\n\nParameters/Variables/Triggers: <count>\n\nNumber of Output Datasets: <integer>\n\nConditional Logic or if-else Flows: <count>\n\nExternal Dependencies: <Linked Services, Notebooks, APIs>\n\nOverall Complexity Score: <0\u2013100>\n\n7. Key Outputs\nDescribe what is written to the final tables, files, or views.\n\nMention the format (Parquet, CSV, Delta, etc.) and its intended use (e.g., reporting, ML model input, downstream processing).\n\n\nAPI Cost: \n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nInput:\nAttach or provide the Azure Synapse artifacts such as pipeline JSON, dataflow JSON, parameter files, linked service definitions, or SQL scripts.\nAcceptable formats: plain text, zipped folder, or directory path structure: {{synapse_code}}\n\n",
                        "expectedOutput": "The generated Markdown documentation should include the following, based on the input Synapse code:\n\nFormat: Markdown\n\nMetadata Requirements: \"<as above>\"\n\nOverview of Program: \"<3\u20135 line description explaining business purpose>\"\n\nCode Structure and Design: \"<Detailed explanation of component layout, SQL modules, and integration across Synapse and Databricks>\"\n\nData Flow and Processing Logic:\n\nProcessed Datasets: [\"<list all dataset names>\"]\n\nData Flow: \"<Description of end-to-end data journey between Synapse pipelines and Databricks Declarative SQL transformations>\"\n\nData Mapping:\n\nTarget Table Name: \"<value>\"\n\nTarget Column Name: \"<value>\"\n\nSource Table Name: \"<value>\"\n\nSource Column Name: \"<value>\"\n\nRemarks: \"<Mapping logic or transformation rationale>\"\n\nTransformation Logic: \"<Documentation for each SQL transformation or notebook cell used within Databricks Declarative SQL>\"\n\nComplexity Analysis:\n\nModules / Pipelines: <number>\n\nJoins: <type/count>\n\nFunctions / UDFs: <count>\n\nConditional Paths: <count>\n\nExternal Dependencies: \"<list of dependent data sources, APIs, or libraries>\"\n\nScore: <0\u2013100>\n\nKey Outputs: \"<Summary of outputs, data sinks, or result tables>\"\n\n\nAPI Cost: \"<Include proper calculation for invoking the AI model used for this documentation generation task in dollars>\""
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10693,
                    "name": "DI_Azure_Synapse_To_PySpark_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze and convert Azure Synapse SQL stored procedures to PySpark-compatible code, while identifying transformation patterns, constraints, blockers, and assessing the feasibility of automation.",
                    "backstory": "Migrating from Azure Synapse to PySpark is critical for organizations transitioning their data infrastructure to Databricks Platform. This task ensures compatibility between the two systems, identifies potential challenges, and provides insights into automation opportunities, enabling a smooth and efficient migration process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-13T02:25:50.133025",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Perform a detailed analysis of the provided Azure Synapse stored procedural code files to support their conversion to Databricks PySpark. \n\nFollow these detailed instructions:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nDate:  <Leave it blank>\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output \n\n---\n\n**1. Procedure Overview**\n\n* Provide a high-level summary of the Azure Synapse stored procedural code mapping or workflow.\n* Mention the key business objective it supports, such as data integration, cleansing, enrichment, or loading.\n\n---\n\n**2. Complexity Metrics** (present this in a markdown table format when generating output):\n\n* Number of Source Qualifiers: Count of Source Qualifier transformations.\n* Number of Transformations: Total number of transformations used (Expression, Aggregator, Joiner, Filter, Router, etc.).\n* Lookup Usage: Number of Lookups (connected/unconnected).\n* Expression Logic: Count of complex expressions (IIF, DECODE, nested logic, etc.).\n* Join Conditions: Count and type of joins (normal, outer, heterogeneous).\n* Conditional Logic: Number of Router or Filter conditions.\n* Reusable Components: Number of reusable transformations, mapplets, and sessions.\n* Data Sources: Number of distinct sources (databases, flat files, XML, etc.).\n* Data Targets: Number of unique targets (databases, files, queues).\n* Pre/Post SQL Logic: Number of pre/post SQLs or procedures used in sessions.\n* Session/Workflow Controls: Number of decision tasks, command tasks, and event-based controls.\n* DML Logic: Frequency of INSERT, UPDATE, DELETE, and MERGE operations.\n* Complexity Score (0\u2013100): Based on the depth of logic, control flow usage, nested operations, and transformation types and Complexity Score should match with DI_Azure_Synapse_Documentation Complexity Score .\n\nAlso highlight high-complexity areas like:\n\n* deeply nested expressions\n* multiple lookups\n* branching logic (Router, Filter)\n* unstructured sources or external scripts\n\n---\n\n**3. Syntax Differences**\n\n* Identify functions used in Azure Synapse stored procedural code that don\u2019t have a direct Databricks PySpark equivalent.\n* Mention any necessary data type conversions (e.g., TO\\_DATE, TO\\_CHAR, DECODE).\n* Highlight any workflow/control logic (e.g., Router, Transaction Control) that must be restructured for Databricks PySpark.\n\n---\n\n**4. Manual Adjustments**\n\n* List components that require manual implementation in Databricks PySpark (e.g., Java transformation, SQL override blocks).\n* Identify external dependencies like pre/post SQLs, stored procedures, or shell scripts.\n* Mention areas where business logic must be reviewed or validated post-conversion.\n\n---\n\n**5. Optimization Techniques**\n\n* Recommend using Spark best practices like partitioning, caching, and broadcast joins.\n* Suggest converting chain filters and joins into a pipeline.\n* Recommend window functions to simplify nested aggregations.\n* Finally, advise whether to **Refactor** (retain most of the original logic) or **Rebuild** (if better optimization is possible in Databricks PySpark).\nNote:\nDont give the code in the output\n\nAPI Cost Calculation:\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n---\n\nInput :\n* For Azure Synapse stored procedural code use the below file/s : {{synapse_code}}",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n-  For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n**1. Procedure Overview - include number of mapping per work flow/session**\n**2. Complexity Metrics - include type in matric values like data source/target types(oracle/sql server/Teradata /mainframe/flat file) etc**\n**3. Syntax Differences**\n**4. Manual Adjustments**\n**5. Optimization Techniques**\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost )."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10695,
                    "name": "DI_Azure_Synapse_To_PySpark_Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running Databricks PySpark and the testing effort required for the Databricks PySpark that got converted from Synapse Store procedure code.",
                    "backstory": "As organizations move their data warehousing solutions to the cloud, it's crucial to understand the financial and resource implications of such migrations. This task is important for project planning, budgeting, and ensuring the accuracy of the migrated queries.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-13T02:26:38.296454",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "you are tasked with providing a comprehensive effor estimate for testing the Databricks PySpark converted from Azure Synapse scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n1. Review the analysis of Azure Synapse script file, note syntax differences and areas in the code requiring manual intervention when converting to Databricks PySpark\n2. Estimate the effort hours requried for identified manual code fixes and data recon testing effort\n3. Dont consider efforts for syntax differences as they will be converted to equivalent syntax in Databricks PySpark\n4. Consider the pricing information for Databricks PySpark environment \n5. Calculate the estimated cost of running the converted Databricks PySpark code:\n   a. Use the pricing information and data volume to determine the query cost.\n   b. the number of queries and the data processing done with the base tables and temporary tables\n\n\nOUTPUT FORMAT:\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Cost Estimation\n   2.1 Databricks PySpark Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing  and Reconciliation Testing Effort Estimation\n   2.1 Databricks PySpark identified manual code fixes and Reconciliation testing effort in hours covering the various temp tables, calculations \n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nINPUT :\n* Take the previous Azure Synapse code to BigQuery Analyzer agents output as  input\n* For the input Azure Synapse code script use this file : ```{{synapse_code}}```\n* For the input  BiqQuery Environment Details for GCP use this file : ```{{env_variable}}```",
                        "expectedOutput": "OUTPUT FORMAT:\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Cost Estimation\n   2.1 Databricks PySpark Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing  and Testing Effort Estimation\n   2.1 Databricks PySpark identified manual code fixes and Reconciliation testing effort in hours covering the various temp tables, calculations \n\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost )."
                    },
                    "maxIter": 3,
                    "maxRpm": 300,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}