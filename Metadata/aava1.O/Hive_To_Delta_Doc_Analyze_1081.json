{
    "pipeline": {
        "pipelineId": 1081,
        "name": "Hive_To_Delta_Doc_&_Analyze",
        "description": "Hive_To_Delta_Doc_&_Analyze",
        "createdAt": "2025-03-14T09:38:27.469+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1284,
                    "name": "Hive to Delta Documentation",
                    "role": "Data Engineer",
                    "goal": "Analyze and document a Hive script to create a comprehensive guide for business and technical teams, explaining existing business rules and facilitating future modifications.",
                    "backstory": "Clear documentation of Hive scripts is crucial for maintaining and evolving complex data systems. By creating a comprehensive guide, we ensure that both business and technical teams can understand the current rules and make informed decisions about future changes, reducing errors and improving efficiency.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-11T17:28:46.283721",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Please create a detailed documentation for the given Hive code. The documentation must follow a well-organized structure, thoroughly explaining the purpose, logic, and technical aspects of the code.\n\n1. Script Overview\nPurpose: Provide a high-level explanation of the SQL script's purpose, objectives, and the business problem it addresses.\nData Sources: Outline the key tables, views, and macros that are present.\nKPI Definitions: Clearly define key metrics (e.g., revenue, churn rate) and their calculation logic.\n\n2. Query Breakdown\nBreak down each query, subquery, or section of the script step-by-step.\nExplain the logic implemented in a way that can be understood by a Business Analyst, including joins, filters, aggregations, or conditions used.\nHighlight syntax differences between Hive and Delta where applicable.\n\n3. Data Mapping Details:\n*Provide data mapping details, including transformations applied to the data in the below format:  \n*Destination Table | Destination Column | Source Table | Source Column | Mapping\n* Mapping column will have the details whether its 1 to  1 mapping or the transformation rule or the validation rule  \n\n4. Complexity Metrics\nNumber of Lines: Count of lines in the script.\nTables Used: Number of tables referenced in the script.\nJoins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\nTemporary Tables: Number of Common Table Expressions (CTEs) or derived tables used.\nAggregate Functions: Number of aggregate functions (e.g., COUNT, SUM, AVG, OLAP functions).\nDML Statements: Count of DML statements by type (SELECT, INSERT, UPDATE, DELETE, MERGE).\nConditional Logic: Number of conditional statements (CASE, IF, ELSE, etc.).\n\n5. Key Outputs\nExplain the outputs generated by the SQL script, such as result sets, updated tables, or reports.\nMention the final tables or views where data is stored.\n\n6. Error Handling and Optimization\nDocument any error-handling mechanisms, such as conditional logic or exception handling used in the script.\nInclude details of query optimizations, such as partitioning, statistics collection, or execution plan improvements.\nHighlight any performance improvements made when migrating from Hive to Delta.\n\n7. Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD\n \n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n*Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n \n\nInput :\n* For Hive code use the below file :\n```%1$s``` ",
                        "expectedOutput": "1. Script Overview:\nProvide a high-level explanation of the script's purpose, objectives, and the business problem it addresses.\nData Sources: Outline the key tables, views, or macros present in the script.\nKPI Definitions: Clearly define key metrics (e.g., revenue, churn rate) and their calculation logic.\n\n2. Query Breakdown:\nBreak down each query, subquery, or section of the script step by step.\nExplain the logic implemented in a way that can be understood by a Business Analyst, including joins, filters, aggregations, or conditions used.\n\n3. Data Mapping Details:\n*Provide data mapping details, including transformations applied to the data in the below format:  \n*Destination Table | Destination Column | Source Table | Source Column | Mapping\n* Mapping column will have the details whether its 1 to  1 mapping or the transformation rule or the validation rule  \n\n4. Complexity Metrics:\nNumber of Lines: Count of lines in the script.\nTables Used: Number of tables referenced in the script.\nJoins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\nTemporary Tables: Number of Common Table Expressions (CTEs) or derived tables used.\nAggregate Functions: Number of aggregate functions (e.g., COUNT, SUM, AVG, OLAP functions).\nDML Statements: Count of DML statements by type (SELECT, INSERT, UPDATE, DELETE, MERGE).\nConditional Logic: Number of conditional statements (CASE, IF, ELSE, etc.).\n\n5. Key Outputs:\nExplain the outputs generated by the script, such as result sets, updated tables, or reports.\nMention the final tables or views where data is stored.\n\n6. Error Handling and Optimization:\nDocument any error-handling mechanisms, such as conditional logic or exception handling used in the script.\nInclude details of query optimizations, such as partitioning, statistics collection, or execution plan improvements.\nHighlight any performance improvements made when migrating from Hive to Delta.\n\n7. API Cost Calculation:\napiCost: float // Cost consumed by the API for this call (in USD).\nEnsure the cost consumed by the API is reported with full decimal precision."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1274,
                    "name": "Hive to Delta Analyser",
                    "role": "Data Engineer",
                    "goal": "Analyze each provided Hive file to extract detailed metrics, identify potential conversion challenges, and recommend solutions for a smooth transition to Delta. Generate a separate output session for each input file.",
                    "backstory": "The scripts are written for a Hive environment and require detailed analysis to assess their structure, complexity, and compatibility with Delta. This analysis will help identify areas requiring manual intervention, optimization opportunities, and potential challenges during the conversion process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-11T10:42:42.536396",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Parse and analyze each Hive file independently to generate a comprehensive report. Ensure that if multiple files given as input then do analysis for each file is presented as a distinct session. Each session must include:\n\n1. Script Overview:\n* Provide a high-level description of the script\u2019s purpose and primary business objectives.\n\n2. Complexity Metrics\n\nNumber of Lines: Count the total number of lines in the Hive script.\nTables Used: Count the number of tables referenced in the script.\nJoins: Identify and count the number of joins used, categorizing them by type (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\nTemporary Tables: Count the number of Common Table Expressions (CTEs), subqueries, and temporary tables created using WITH or INSERT OVERWRITE LOCAL DIRECTORY.\nAggregate Functions: Identify and count the number of aggregate functions such as SUM(), COUNT(), AVG(), MAX(), MIN(), and OLAP functions (RANK(), DENSE_RANK(), NTILE()).\nDML Statements: Count the number of DML statements by type (SELECT, INSERT, UPDATE, DELETE, MERGE, LOAD, EXPORT).\nConditional Logic: Count the occurrences of conditional statements like CASE, IF, WHEN, and COALESCE.\n\n3. Syntax Differences\n\nIdentify Hive-specific syntax differences that require modifications when converting to Delta.\nCommon differences include:\nPartitioning Syntax (DISTRIBUTE BY in Hive vs. PARTITION BY in Delta).\nWindow Functions (Delta uses OVER(PARTITION BY ORDER BY), while Hive might use DISTRIBUTE BY and SORT BY).\nLateral Views & Explode (Delta has UNNEST() instead of LATERAL VIEW EXPLODE()).\nData Types (e.g., STRING in Hive vs. STRING in Delta, BIGINT vs. INT64).\nDate Functions (e.g., from_unixtime() in Hive vs. TIMESTAMP_SECONDS() in Delta).\n\n4. Manual Adjustments\n\nProvide recommendations for converting Hive to Delta, including:\nFunction replacements (e.g., UNIX_TIMESTAMP() \u2192 TIMESTAMP_SECONDS(), REGEXP_EXTRACT() \u2192 SAFE.REGEXP_EXTRACT()).\nSyntax Adjustments:\nINSERT OVERWRITE to Delta\u2019s MERGE.\nMAP data types converted to ARRAYS or STRUCTS.\nWorkarounds for unsupported features, such as:\nLATERAL VIEW EXPLODE() \u2192 Use UNNEST().\nDISTRIBUTE BY \u2192 Use PARTITION BY.\n\n5. Conversion Complexity\n\nCompute a complexity score (0\u2013100) based on:\nNumber of syntax differences.\nAmount of manual adjustments required.\nComplexity of query logic (e.g., CTEs, nested subqueries, advanced window functions).\nHighlight high-complexity areas such as:\nLateral Views and Explode functions.\nRecursive CTEs.\nPartitioned table queries.\nWindow functions.\n\n6. Optimization Techniques\n\nSuggest optimization strategies for Delta:\nPartitioning & Clustering: Identify opportunities to use Delta\u2019s native partitioning and clustering to improve performance.\nQuery Restructuring: Recommend changes in join order, materialized views, or denormalization for cost efficiency.\nStorage Format Adjustments: If Hive stores data in ORC/Parquet, recommend best storage practices in Delta.\nRefactor vs. Rebuild Decision:\nRefactor: Minimal changes to work efficiently in Delta.\nRebuild: Major restructuring for improved performance.\nProvide a reason for choosing Refactor or Rebuild.\n\n7. API Cost Estimation\n\nCalculate and report the cost consumed by the API for this analysis.\nEnsure:\nThe cost is reported as a floating-point value.\nThe cost is explicitly mentioned in USD (e.g., apiCost: 0.1234 USD).\nInclude all decimal values to maintain accuracy.\nInput :\n* For Hive code use the below file :\n```%1$s```",
                        "expectedOutput": "1.Script Overview :\nProvide a high-level description of the Hive script\u2019s purpose and primary business objectives.\n\n2.Complexity Metrics :\nNumber of Lines: Count of lines in the Hive script.\nTables Used: Number of tables referenced in the Hive script.\nJoins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\nTemporary Tables: Number of Common Table Expressions (CTEs), subqueries, and temporary tables (e.g., WITH clauses, INSERT OVERWRITE LOCAL DIRECTORY).\nAggregate Functions: Number of aggregate functions such as SUM(), COUNT(), AVG(), MAX(), MIN(), and OLAP functions (RANK(), DENSE_RANK(), NTILE()).\nDML Statements: Number of DML statements by type (SELECT, INSERT, UPDATE, DELETE, MERGE, LOAD, EXPORT).\nConditional Logic: Number of conditional statements like CASE, IF, WHEN, and COALESCE.\n\n3. Syntax Differences:\nIdentify the number of syntax differences between the Hive code and the expected Delta equivalent.\nCommon differences include:\nPartitioning Syntax (DISTRIBUTE BY in Hive vs. PARTITION BY in Delta).\nWindow Functions (OVER(PARTITION BY ORDER BY) differences).\nLateral Views & Explode (LATERAL VIEW EXPLODE() in Hive vs. UNNEST() in Delta).\nData Types (STRING vs. STRING, BIGINT vs. INT64).\nDate Functions (from_unixtime() vs. TIMESTAMP_SECONDS()).\n\n4. Manual Adjustments\n\nRecommend specific manual adjustments for functions and clauses incompatible with Delta, including:\nFunction replacements (e.g., UNIX_TIMESTAMP() \u2192 TIMESTAMP_SECONDS(), REGEXP_EXTRACT() \u2192 SAFE.REGEXP_EXTRACT()).\nSyntax adjustments for features like date and window functions.\nWorkarounds for unsupported features (e.g., LATERAL VIEW EXPLODE() replaced with UNNEST(), DISTRIBUTE BY converted to PARTITION BY).\n\n5.Conversion Complexity\nCalculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.\nHighlight high-complexity areas such as:\nLateral Views and Explode functions.\nRecursive CTEs.\nPartitioned table queries.\nWindow functions.\n\n6.Optimization Techniques\nSuggest optimization strategies for Delta:\nPartitioning & Clustering: Identify opportunities to use Delta\u2019s native partitioning and clustering.\nQuery Restructuring: Recommend changes in join order, materialized views, or denormalization for cost efficiency.\nStorage Format Adjustments: If Hive stores data in ORC/Parquet, suggest best storage practices in Delta.\nRefactor vs. Rebuild Decision:\nRefactor: Minimal changes to work efficiently in Delta.\nRebuild: Major restructuring for improved performance.\nProvide a reason for choosing Refactor or Rebuild.\n\n7. API Cost Estimation\napiCost: float // Cost consumed by the API for this call (in USD).\nEnsure the cost consumed by the API is reported as a floating-point value with all decimal places included."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1411,
                    "name": "Hive to Delta Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running Delta code and the testing effort required for the Delta code that got converted from Hive scripts.",
                    "backstory": "As organizations move their data warehousing solutions to the cloud, it's crucial to understand the financial and resource implications of such migrations. This task is important for project planning, budgeting, and ensuring the accuracy of the migrated queries.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T08:34:34.719449",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with providing a comprehensive effort estimate for testing the Delta converted from Hive scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n1. Review the analysis of Hive script file, noting syntax differences when converting to Delta and areas requiring manual intervention.\n2. Consider the pricing information for Azure Databricks Delta environment \n4. Calculate the estimated cost of running the converted Delta code:\n   a. Use the pricing information and data volume to determine the code cost.\n   b. the number of code and the data processing done with the base tables and temporary tables\n5. Estimate the code fixing and data recon testing effort required:\n\nINPUT :\n* Take the previous Hive to Delta Analyser agent output as  input\n* For the input Hive script use this file : ```%1$s```\n* For the input  Delta Environment Details for Azure Databricks  use this file : ```%2$s```",
                        "expectedOutput": "1. Cost Estimation\n   2.1 Delta Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing  and Testing Effort Estimation\n   2.1 Delta code manual code fixes and unit testing effort covering the various temp tables, calculations in hours\n   2.2 Output validation effort comparing the output from Hive script and Delta script in hours\n   2.3 Total Estimated Effort in Hours\n         - provide the reason for the total effort hours and how it was arrived\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost )."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}