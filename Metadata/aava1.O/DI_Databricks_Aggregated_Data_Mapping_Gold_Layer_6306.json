{
    "pipeline": {
        "pipelineId": 6306,
        "name": "DI_Databricks_Aggregated_Data_Mapping_Gold_Layer",
        "description": "This workflow is for recommending and creating the gold Fact data mapping",
        "createdAt": "2025-08-25T06:36:23.955+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 8175,
                    "name": "DI_Databricks_Gold_Aggregated_Transformation_Recommender",
                    "role": "Data modeler",
                    "goal": "Analyze the Model Conceptual, Data Constraints, Silver Layer Physical DDL script, Gold Layer Physical DDL script, and Sample Data to generate comprehensive transformation rules specifically for Aggregated Tables in the Gold layer.",
                    "backstory": "Aggregated tables are essential for optimizing query performance and enabling efficient analytical reporting by precomputing summarized data. By automatically generating transformation rules for Aggregated Tables, we ensure that aggregation logic aligns with business needs, maintains data accuracy, and supports efficient analytics.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-21T06:59:20.975421",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks Gold Aggregated Transformation Recommender Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks Gold Aggregated Transformation Recommender underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks Gold Aggregated Transformation Recommender followed by a number , proceed to create the  Databricks Gold Aggregated Transformation Recommender for the input file from the input directory. The Databricks Gold Aggregated Transformation Recommender instructions and structure are given below. Once generated, store the Databricks Gold Aggregated Transformation Recommender in the output directory with the file name  Databricks_Gold_Aggregated_Transformation_Recommender_1.md.\n \nThe agent must:\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks Databricks Gold Aggregated Transformation Recommender containing the sections listed in **Databricks Gold Aggregated Transformation Recommender  Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Gold_Aggregated_Transformation_Recommender_1.md.\n*The output file should properly in the md format including md formatted Tables and headings\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Gold Aggregated Transformation Recommender Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks Gold Aggregated Transformation Recommender file in GitHub output directory with the Databricks_Gold_Aggregated_Transformation_Recommender_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Gold_Aggregated_Transformation_Recommender_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Gold_Aggregated_Transformation_Recommender}}\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Gold_Aggregated_Transformation_Recommender_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Gold Aggregated Transformation RecommenderStructure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\nYou will read the Model Conceptual, Data Constraints, Silver Layer Physical DDL script, Gold Layer Physical DDL script, and Sample Data and generate transformation rules only for Aggregated Tables.\n\nINSTRUCTIONS:\n\n1. Parse the Silver Layer DDL script to extract only Aggregated Tables and their column definitions.\n2. Analyze the Model Conceptual and Data Constraints file to identify aggregation logic, business rules, and required summary metrics.\n3. Inspect Sample Data to detect patterns, outliers, and inconsistencies in aggregations.\n4. Generate transformation rules for Aggregated Tables, including:\n* Aggregation Methods: Define SUM, COUNT, AVERAGE, MAX, MIN, MEDIAN, DISTINCT COUNT, etc.\n* Grouping Logic: Specify how data should be grouped (e.g., by date, category, region, customer segment).\n* Window Functions: Implement calculations that require row-based aggregation (e.g., rolling averages, cumulative sums).\n* Granularity Checks: Ensure aggregated data maintains consistency and aligns with reporting requirements.\n* Data Normalization & Formatting: Ensure consistent formats (e.g., decimal precision, rounding, date bucketization).\n5. Provide SQL transformations for each rule, ensuring alignment with the Silver Layer schema.\n6. Ensure traceability of transformations by linking each rule back to its source from the Model Conceptual, Data Constraints, Silver Layer schema to Gold layer.\n\n1. Transformation Rules for Aggregated Tables:\n* [Rule Name]: [Description]\n    - Rationale: [Explanation]\n    - SQL Example: [Sample SQL transformation]\n2. Ensure API cost consumption is included in the output, explicitly reporting the cost as a floating-point value in USD (e.g., apiCost: actual cost).",
                        "expectedOutput": "\n**Mode 1 Output**:\n* Display the Databricks Gold Aggregated Transformation Recommender output\n* And store the Databricks_Gold_Aggregated_Transformation_Recommender in the GitHub output directory with the file name as `Databricks_Gold_Aggregated_Transformation_Recommender_<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Gold Aggregated Transformation Recommender output\n* And store the updated Databricks Gold Aggregated Transformation Recommender output in the GitHub output directory with the file name as `Databricks_Gold_Aggregated_Transformation_Recommender_next_version>.md` \u2014 Updated Databricks Databricks Gold Aggregated Transformation Recommender with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 8176,
                    "name": "DI_Databricks_Gold_Aggregated_Transformation_Data_Mapping",
                    "role": "Data modeler",
                    "goal": "Create a comprehensive data mapping for Aggregated Tables in the Gold Layer, incorporating necessary aggregation logic, validation rules, and cleansing mechanisms.",
                    "backstory": "This task ensures that Aggregated Tables in the Gold Layer are correctly structured, maintain accurate summary metrics, and comply with business reporting needs. A well-defined aggregation model enhances query performance, reduces data redundancy, and ensures consistency in analytical applications.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-21T07:15:27.409609",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "datamappingoutputformat",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks Gold Aggregated Transformation Data Mapping Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks Gold Aggregated Transformation Data Mapping underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks Gold Aggregated Transformation Data Mapping followed by a number , proceed to create the  Databricks Gold Aggregated Transformation Data Mapping for the input file from the input directory. The Databricks Gold Aggregated Transformation Data Mappinginstructions and structure are given below. Once generated, store the Databricks Gold Aggregated Transformation Data Mapping in the output directory with the file name  Databricks_Gold_Aggregated_Transformation_Data_Mapping_1.md.\n \nThe agent must:\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks Gold Aggregated Transformation Data Mapping containing the sections listed in **Databricks Gold Aggregated Transformation Data Mapping  Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Gold_Aggregated_Transformation_Data_Mapping_1.md.\n*The output file should properly in the md format including md formatted Tables and headings\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Gold Aggregated Transformation Data Mapping Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks Gold Aggregated Transformation Data Mapping file in GitHub output directory with the Databricks_Gold_Aggregated_Transformation_Data_Mapping_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Gold_Aggregated_Transformation_Data_Mapping_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Gold_Aggregated_Transformation_Data_Mapping}}\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Gold_Aggregated_Transformation_Data_Mapping_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Gold Aggregated Transformation Data Mapping Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n \nYou are tasked with creating a detailed data mapping specifically for Aggregated Tables in the Gold Layer. This mapping will incorporate necessary aggregation rules, validations, and cleansing mechanisms at the metric level.\nYour work will be based on the Silver and Gold Layer Physical Model provided and previous Databricks Gold Aggregated Transformation Recommender Agents recommendations.\n\nINSTRUCTIONS:\n\n1. Review the provided Silver and Gold Layer Physical Model DDL script.\n2. Create a detailed data mapping for Aggregated Tables from the Silver to Gold Layer, ensuring:\n* Aggregation Methods (e.g., SUM, COUNT, AVERAGE, DISTINCT COUNT).\n* Grouping Logic (e.g., aggregating by time, region, category).\n* Validation Rules for ensuring consistency (e.g., preventing duplicate aggregation, handling NULL values).\n* Cleansing Logic (e.g., rounding errors, enforcing decimal precision, outlier removal).\n3. Ensure all transformations and rules are compatible with PySpark and Databricks.\n4. Include explanations for complex transformations and business rules.\n\nInputs:\n* Also take input from previous Databricks Gold Aggregated Transformation Recommender Agent\u2019s output recommendations as input.\n\nExpected Output\n1. Overview: Summary of the data mapping approach and key considerations.\n2. Data Mapping for Aggregated Tables:\nThe mapping output should be in tabular format with the following fields for each Aggregated Table and its columns:\n* Target Layer: Gold\n* Target Table: Proper table name as per the Gold Layer DDL script\n* Target Field: Proper field name as per the Gold Layer DDL script\n* Source Layer: Silver\n* Source Table: Proper table name as per the Silver Layer DDL script\n* Source Field: Proper field name as per the Silver Layer DDL script\n* Aggregation Rule: Required aggregation logic (e.g., SUM, AVERAGE, COUNT, DISTINCT COUNT).\n* Validation Rule: Required validation rules from the Data Constraints file.\n* Transformation Rule: Required transformation rules from the previous Fabric Gold Aggregated Transformation Recommender Agents output recommendations (e.g., data normalization, time bucketization, metric roll-ups).\n3. Ensure API cost consumption is included in the output, explicitly reporting the cost as a floating-point value in USD (e.g., apiCost: actual cost).",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Databricks Gold Aggregated Transformation Data Mapping output\n* And store the Databricks Gold Aggregated Transformation Data Mapping in the GitHub output directory with the file name as `Databricks_Gold_Aggregated_Transformation_Data_Mapping_<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Gold Aggregated Transformation Data Mapping output\n* And store the updated Databricks Gold Aggregated Transformation Data Mapping output in the GitHub output directory with the file name as `Databricks_Gold_Aggregated_Transformation_Data_Mapping_next_version>.md` \u2014 Updated Databricks Gold Aggregated Transformation Data Mapping with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 8125,
                    "name": "DI_Databricks_Gold_Model_Reviewer",
                    "role": "Data modeler",
                    "goal": "This agent is to evaluate and validate the physical data model for alignment with reporting requirement and compatibility with Databricks and PySpark.\n",
                    "backstory": "Evaluate and validate the physical data model for alignment with reporting requirements , source data structure, and compatibility with Databricks and PySpark.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-25T04:54:56.12927",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "The agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the input data.\n* Identify the Reviewer file in GitHub output directory with the actual input file name Databricks_Gold_Model_Reviewer_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).if file is already exist in the output directory with some version number then generate the newer output and Save the updated file to the same GitHub output directory with the with the actual input file name Databricks_Gold_Model_Reviewer_next incremented version number (e.g., `_4`).\nif the file is not exist then save the output file name should be the actual input file name, followed by _Reviewer_1.md.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Reviewer containing the sections listed in **Reviewer Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and File present in the github input directory: `{{GitHub_Details_For_Databricks_Gold_Model_Reviewer}}`\n \n## **Reviewer Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n \n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the Input or workflow does.\n \n---\n\nYou are tasked with thoroughly evaluating the physical data model and associated DDL scripts. Give a green tick mark \u2705 if it\u2019s correctly implemented and a red tick mark \u274c for missing or incorrectly implemented. Your evaluation should cover multiple aspects to ensure the model's quality, completeness, and compatibility.\n\nINSTRUCTIONS:\n1.\tReview the provided physical data model and DDL scripts.\n2.\tCompare the model against the reporting requirements or model conceptual:\na. Identify all required data elements.\nb. Confirm all required tables and columns from the Gold Layer are present and correctly structured.\nc. Check for appropriate data types and sizes.\nd. Verify correct categorization of Facts, Dimensions, and Code tables, ensuring accurate data modeling.\n3.\tAnalyze the model's alignment with the source data structure:\na. Ensure all source data elements are accounted for.\nb. Verify that data transformations are correctly represented.\nc. Validate all aggregations, calculations, and business rules for accuracy and PySpark compatibility.\n4.\tAssess the model for adherence to best practices:\na. Check for proper normalization.\nb. Evaluate indexing strategies.\nc. Review naming conventions and consistency.\nd. Confirm inclusion of load_date, update_date, source_system columns and verify audit/error tables for tracking data issues.\n5.\tIdentify any missing requirements or inconsistencies in the model.\n6.\tEvaluate the DDL scripts for compatibility with Microsoft Fabric and Spark:\na. Verify syntax compatibility.\nb. Check for any unsupported data types or features.\n7.\tDocument any deviations from best practices or potential optimizations.\n8.\tProvide recommendations for addressing identified issues or improvements.\n9.\tAttached knowledge base file containing all the unsupported features in Microsoft Fabric. You need to verify that the output DDL script does not include any unsupported features mentioned in the knowledge base file.\n\nOUTPUT FORMAT:\nProvide a comprehensive evaluation report in the following structure:\n1.\tAlignment with Conceptual Data Model\n1.1 \u2705 Green Tick: Covered Requirements\n1.2 \u274c Red Tick: Missing Requirements\n2.\tSource Data Structure Compatibility\n2.1 \u2705 Green Tick: Aligned Elements\n2.2 \u274c Red Tick: Misaligned or Missing Elements\n3.\tBest Practices Assessment\n3.1 \u2705 Green Tick: Adherence to Best Practices\n3.2 \u274c Red Tick: Deviations from Best Practices\n4.\tDDL Script Compatibility\n4.1 Microsoft Fabric Compatibility\n4.2 Spark Compatibility\n4.3 Used any unsupported features in Microsoft Fabric\n5.\tIdentified Issues and Recommendations\n6.\tapiCost: float // Cost consumed by the API for this call (in USD)\nExpected Output:\nProvide a comprehensive evaluation report in the following structure:\n1.\tAlignment with Conceptual Data Model\n1.1 \u2705 Green Tick: Covered Requirements\n1.2 \u274c Red Tick: Missing Requirements\n2.\tSource Data Structure Compatibility\n2.1 \u2705 Green Tick: Aligned Elements\n2.2 \u274c Red Tick: Misaligned or Missing Elements\n3.\tBest Practices Assessment\n3.1 \u2705 Green Tick: Adherence to Best Practices\n3.2 \u274c Red Tick: Deviations from Best Practices\n4.\tDDL Script Compatibility\n4.1 Microsoft Fabric Compatibility\n4.2 Spark Compatibility\n4.3 Used any unsupported features in Microsoft Fabric\n5.\tIdentified Issues and Recommendations\n6.\tapiCost: float // Cost consumed by the API for this call (in USD)\n\n\n",
                        "expectedOutput": "\n**Mode 1 Output**:\n\n* Display the Databricks_Gold_Model_Reviewer output\n\n* And store the Databricks_Gold_Model_Reviewer output in the GitHub output directory with the file name as `Databricks_Gold_Model_Reviewer<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n\n* Display the updated Databricks_Gold_Model_Reviewer output\n\n* And store the updated Reviewer output in the GitHub output directory with the file name as Databricks_Gold_Model_Reviewer_<next_version>.md` \u2014 Updated Reviewer with requested changes applied, preserving structure and formatting.\n\n\n"
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}