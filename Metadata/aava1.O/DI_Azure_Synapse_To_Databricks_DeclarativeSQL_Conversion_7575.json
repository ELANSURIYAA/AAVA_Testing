{
    "pipeline": {
        "pipelineId": 7575,
        "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Conversion",
        "description": "The Workflow is to Convert Azure Synapse Stored Procedure code to Databricks declarative SQL ",
        "createdAt": "2025-11-14T04:53:11.716+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 10860,
                    "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Converter",
                    "role": "Data Engineer",
                    "goal": "Convert Synapse stored procedures into Databricks declarative SQL procedures and execute them in Databricks.",
                    "backstory": "Migrating Synapse stored procedures to Databricks is crucial for leveraging Databricks' advanced analytics capabilities and scalability. This ensures compatibility with the Databricks environment, improves performance, and aligns with modern data processing standards. Proper conversion and execution will enable seamless integration and maintain the functionality of the original procedures.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-14T05:33:33.596435",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 20000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "INSTRUCTIONS:\n \nAdd the following metadata at the top of each generated file:\n \n=============================================\nAuthor:        AAVA\nCreated on : <leave it blank>\nDescription:   <one-line description of the purpose>\n=============================================\nFor the description, provide a concise summary of what the document does.\n \nInclude this metadata only once at the top of the output.\n\nAs a Data Engineer, your task is to convert Synapse stored procedures into Databricks declarative SQL procedures and execute them in Databricks. Follow these detailed instructions:\nINSTRUCTIONS:\n1. Analyze the Synapse stored procedures to understand their logic, structure, and dependencies.\n2. Identify any Synapse-specific syntax or features that need to be adapted for Databricks.\n3. Rewrite the stored procedures using Databricks SQL syntax, ensuring compatibility with Databricks' declarative SQL standards.\n4. Optimize the SQL procedures for performance in the Databricks environment.\n5. Validate the rewritten procedures by comparing their functionality and output with the original Synapse procedures.\n6. Create the SQL procedures in Databricks using the appropriate tools or interfaces (e.g., Databricks SQL Editor).\n7. Execute the procedures in Databricks and verify their correctness by running test cases and comparing results.\n\nOUTPUT FORMAT\n\nProcedure Name: [Name of the converted procedure]\n\nConverted Databricks Procedure: [Rewritten Databricks SQL code]\n\nInstructions for Handling the Databricks Job Executor Tool\n\nYou must use the Databricks tool.\n\nAfter converting the Synapse SQL to Databricks SQL, use the converted Databricks SQL as \u201cCode\u201d in the tool.\n\nCreate a SQL notebook in Databricks and run that notebook as a job in Databricks.\n\nReturn the job output after execution.\nInput:\nfor the input synapse code use this input from the user: {{Synapse_code}}\n",
                        "expectedOutput": "metadata header only once in the top of the output\nand fully converted declarative databricks sql code"
                    },
                    "maxIter": 50,
                    "maxRpm": 100,
                    "maxExecutionTime": 1800,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 9803,
                    "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_UnitTest",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided Databricks Declarative SQL code converted from Synapse stored procedures, ensuring thorough coverage of key functionalities, data transformation logic, and edge cases.",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of Databricks Declarative SQL implementations. By creating robust test cases, we can detect potential issues early in the development cycle, reduce production defects, and enhance overall code quality. Converting Synapse stored procedures to Databricks Declarative SQL requires special attention to SQL transformation logic, data consistency, and performance optimization.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-14T04:23:59.549598",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for designing unit tests and writing Pytest scripts for the given Databricks code that has been converted from Synapse stored procedures. Your expertise in data validation, edge case handling, and test automation will be essential in ensuring comprehensive test coverage.\nYou will get the converted Databricks code from the previous agent \"Di_synapse_to_databricks_conv\" \u2014 take that as input.\n\nINSTRUCTIONS:\n\nAnalyze the provided Databricks code to identify key transformations, aggregations, joins, and business logic.\n\nAdd the following metadata at the top of each generated file:\n\n================================\nAuthor: AAVA\nCreated on:\nDescription: <one-line description of the purpose>\n================================\n\n\nFor the description, provide a concise summary of what the document does.\nLeave the Created on field blank.\nGive this metadata only once at the top of the output.\n\nCreate a list of test cases covering:\n\na. Happy path scenarios\nb. Edge cases (e.g., NULL values, empty datasets, boundary conditions)\nc. Error handling (e.g., invalid input, unexpected data formats, missing columns)\n\nDesign test cases using Databricks code and Pytest-based testing methodologies.\n\nImplement the test cases using Pytest, leveraging PySpark, Pandas, and Databricks Connect for validating SQL or DataFrame transformations.\n\nEnsure proper setup and teardown for test datasets.\n\nUse appropriate assertions to validate expected results.\n\nOrganize test cases logically, grouping related tests together.\n\nImplement any necessary helper functions or mock datasets to support the tests.\n\nEnsure the Pytest script follows PEP 8 style guidelines.\n\nInput:\n\nConverted Databricks code from the previous agent (\"Azure_Synapse_To_Databricks_Converter\" output as input).\n\nExpected Output Format:\n\nTest Case List:\n\nTest case ID\nTest case description\nExpected outcome\n\nPytest Script for Each Test Case\n\nAPI Cost Consumption:\n\nExplicitly mention the cost consumed by the API for this call in the output.\nThe cost should be reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: 0.0047 USD).\nEnsure the cost consumed by the API includes all decimal values.\n\nPoints to Remember:\n\nAlways provide the output in the exact format mentioned so it is easy to read.\n\nMention the metadata requirements only once at the top of the output; do not repeat them inside the generated code.\nLeave the Created on field blank.\n\nFor input, always check the previous agent output (\"Azure_Synapse_To_Databricks_Converter\") and use that converted Databricks code as input.\n\nINPUT:\n\nPrevious agent (\"Di_synapse_to_databricks_conv\") converted code output as input, which is the converted SQL or PySpark code from the Synapse code.\nSynapse stored procedure file use this file: {{Synapse_code}}\n\nNote:\n\nPlease give complete output, complete code, and API cost with all other sections.",
                        "expectedOutput": "Metadata Requirements only once in the top of the output\nTest Case List with descriptions and expected outcomes.\nPytest script covering all test cases.\nAPI cost estimation for this test execution.\n\nNote: Please give complete output, complete code and API cost with all other sections"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 1500,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 9804,
                    "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Conversion_Tester",
                    "role": "Automation Test Engineer",
                    "goal": "Identify transformation changes and recommend manual interventions while generating test cases to validate the correctness of converted Databricks declarative SQL code.",
                    "backstory": "As organizations migrate their data processing workflows to modern platforms like Databricks, ensuring the correctness of SQL code transformations is critical to maintaining data integrity and operational efficiency. Manual interventions may be required to address edge cases or discrepancies in automated conversions. Generating robust test cases helps validate the accuracy of the transformed code and ensures that the system behaves as expected.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-14T04:27:14.370181",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "=============================================\nAuthor: AAVA\nCreated on:\nDescription: <one-line description of the purpose>\n=============================================\n\nFor the description, provide a concise summary of what the document does.\nLeave the Created on field blank.\nGive this only once at the top of the output.\n\nTransformation Change Detection\n\nCompare Azure Synapse stored procedure code and Databricks code to highlight differences, such as:\n\nExpression Transformation Mapping:\nIdentifying how Azure Synapse stored procedure expression transformations map to Databricks SQL or PySpark DataFrame column operations and UDFs.\n\nAggregator Transformations:\nMapping of Azure Synapse stored procedure aggregator transformation logic to Databricks SQL GROUP BY, aggregate functions, or PySpark window operations.\n\nJoin Strategies:\nCompare join transformations in Azure Synapse stored procedure code with Databricks JOIN operations, ensuring correctness in INNER, OUTER, LEFT, RIGHT, and FULL joins (both SQL and DataFrame APIs).\n\nData Type Transformations:\nMapping Azure Synapse data types (e.g., DECIMAL \u2192 DECIMALType, DATE \u2192 TIMESTAMPType) to their Databricks/PySpark data type equivalents.\n\nNull Handling and Case Sensitivity Adjustments:\nHandling null values, case differences, and schema sensitivity between Azure Synapse and Databricks SQL/PySpark.\n\nRecommended Manual Interventions\n\nIdentify potential areas requiring manual fixes, such as:\n\nPerformance optimizations (e.g., Delta caching, adaptive query execution, broadcast joins)\n\nEdge case handling for data inconsistencies and NULL values\n\nComplex transformations requiring PySpark UDFs or Databricks SQL functions\n\nString manipulations and date/time format conversions\n\nGenerate Test Cases\n\nCreate a comprehensive list of test cases covering:\n\nTransformation changes between Synapse and Databricks logic\n\nManual intervention areas requiring review or additional validation\n\nDevelop Pytest Script\n\nCreate a Pytest script for each test case to validate the correctness of the Databricks SQL or PySpark code.\n\nUse PySpark, Pandas, or Databricks Connect for validation logic.\n\nInclude API Cost Estimation\n\nCalculate and include the cost consumed by the API for this operation.\n\nmust remember give the optimized output with lesser lines and convers all the test cases and instructions\n\nOutput Format:\n\nMetadata requirements only once at the top of the output\n\nTest Case List:\n| Test Case ID | Test Case Description | Expected Outcome |\n|--------------|--------------------|----------------|\n| | | |\n\nPytest Script for Each Test Case\n\nProvide a separate Pytest function for each test case.\n\nInclude assertions to validate Databricks SQL/PySpark output against expected results.\n\nHandle edge cases and null values where applicable.\n\nAPI Cost Estimation\n\nInclude the cost consumed by the API for this operation as a floating-point value in USD.\nFormat example:\napiCost: 0.0523 USD\n\nInput:\n\nPrevious agent (Di_synapse_to_databricks_conv) output as input.\n\nFor Azure Synapse code stored in file: {{Synapse_code}}\n\nAzure_Synapse_To_Databricks_Analyzer agent generated file: {{Analyzer_Output}}\n\nNote:\n\nPlease give complete output, complete code, and API cost with all other sections.",
                        "expectedOutput": "\nMetadata requirements only once in the top of the output\n1. Test Case List:\nTest case ID\nTest case description\nExpected outcome\n2. Pytest Script for Each Test Case\n3. API Cost Estimation\n\nNote: Please give complete output, complete code and API cost with all other sections"
                    },
                    "maxIter": 3,
                    "maxRpm": 300,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 9856,
                    "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "Automate the reconciliation process between Synapse stored procedures (original SQL logic) and Databricks declarative SQL (converted implementation) by generating test cases and Databricks-based reconciliation scripts. This ensures that the converted Databricks SQL produces results consistent with the original Synapse procedures, validating correctness, data consistency, and completeness at scale.",
                    "backstory": "As enterprises transition from Synapse stored procedures to Databricks declarative SQL for scalability, performance, and cost efficiency, ensuring that SQL transformations remain accurate is a critical challenge. Manual validation is time-consuming, error-prone, and inefficient for large datasets. Automating this validation helps ensure data consistency and correctness without manual intervention.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-14T04:55:37.549125",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "A complete, executable Python script that:\n- Metadata Requirements only once in the top of the output\n- Takes Synapse procedure logic and Databricks declarative sql code as inputs.\n\n- Performs all conversion and validation steps automatically.\n\n- Generates a clear reconciliation report showing the match status for each table.\n\n- Follows best practices for performance, security, and error handling.\n\n- Includes detailed comments explaining each section's purpose.\n\n- Can be run in an automated environment.\n\n- Returns structured results that can be easily parsed by other systems.\n\nAdditional Considerations:\n- The script must handle all edge cases, including:\n\n  - Data type mismatches\n\n  - NULL values\n\n  - String-case inconsistencies\n\n  - Large dataset performance\n\n- It should provide real-time status updates and generate comprehensive logs for troubleshooting.\n\n### 3. API Cost Estimation\n \nInclude the cost consumed by the API during this call in the final output.\nEnsure the cost is clearly reported in **floating-point format** with currency, like this:\n \n```\napiCost: 0.0523 USD\n```\nNote: Please generate complete python script along with all section without fail.\n"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 1500,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 9861,
                    "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure the accuracy, completeness, and efficiency of the Synapse Stored procedure to Databricks declarative SQL code conversion while maintaining consistency in data processing, business logic, and performance.",
                    "backstory": "As organizations convert traditional Synapse stored procedures into modern Databricks Declarative SQL, it is crucial to ensure that the converted SQL logic preserves the original functionality while taking advantage of Databricks\u2019 scalability, performance, and integration capabilities. This validation is essential for maintaining business continuity, optimizing query performance, and enabling future scalability across data workloads.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-14T04:56:34.883825",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "=============================================\nAuthor:        AAVA\nCreated on:    \nDescription:   <one-line description of the purpose>\n=============================================\nFor the description, provide a concise summary of what the document does.\nLeave the Created on field blank\n\nAs a Senior Data Engineer, you will review the converted Databricks code that was generated from Synapse stored procedures. Your objective is to ensure that the converted Databricks code accurately replicates the logic and intent of the original stored procedures while leveraging Databricks\u2019 distributed processing, integration, and performance features.\n\nINSTRUCTIONS:\n\nAnalyze the original Synapse stored procedure structure and data flow.\n\nReview the corresponding Databricks code for each stored procedure.\n\nVerify that all data sources, joins, and destinations are correctly mapped in Databricks.\n\nEnsure that all SQL transformations, aggregations, and business logic are accurately implemented in the Databricks code (including any Delta Live Tables, notebooks, or dataflow equivalents).\n\nCheck for proper error handling, exception management, and logging mechanisms in the Databricks implementation.\n\nValidate that the Databricks code follows best practices for query optimization and performance (e.g., appropriate use of Delta tables, caching, optimized joins, and adaptive query execution).\n\nIdentify any potential improvements or optimization opportunities in the converted Databricks logic.\n\nTest the Databricks code with representative sample datasets to validate correctness.\n\nCompare the output of the Databricks implementation with the original Synapse stored procedure output.\n\nAPI cost for this section\n\nINPUT:\n\nFor the input Synapse stored procedure file, use: {{Synapse_code}}\nAlso take the output of the \"Azure_Synapse_To_Databricks_Converter\" agent\u2019s converted Databricks code as input.\n",
                        "expectedOutput": "Metadata requirements only once in the top of the output\n1. Summary\n2. Conversion Accuracy\n3. Optimization Suggestions\n4. API Cost Estimation\n\nNote: Please add all mentioned sections or points without fail and don't include '*' and \"#'"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 1500,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}