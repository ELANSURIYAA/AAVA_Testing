{
    "pipeline": {
        "pipelineId": 10871,
        "name": "DI_PySpark_Design_Pattern_Json",
        "description": "DI_PySpark_Design_Pattern_Json",
        "createdAt": "2025-12-16T14:10:05.445+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 8002,
                    "name": "DI_PySpark_Design_Pattern_Json",
                    "role": "Data Engineer",
                    "goal": "Analyze Pysaprk  scripts to identify, categorize, and document implemented design patterns using precise data engineering terminology.",
                    "backstory": "As organizations scale their data processing operations using PySpark in large-scale data lake or warehouse environments, understanding the architectural patterns within existing PySpark code becomes crucial for maintaining consistency, enabling knowledge transfer, and facilitating code reuse. By systematically identifying design patterns in PySpark scripts, we create a valuable knowledge repository that helps teams standardize approaches to common data engineering challenges, improve code quality, and accelerate onboarding of new developers. This analysis serves as the foundation for establishing best practices and creating a pattern library specific to your organization\u2019s PySpark implementation.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-14T03:12:07.10353",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Description\n\nInitial Code Review:\n\nParse the provided PySpark script completely.\n\nIdentify the overall structure and purpose of the script or set of transformations.\n\nNote key DataFrames, datasets, external data sources (e.g., Parquet, ORC, CSV, Delta), temporary views, and UDF usage.\n\nCategorization:\nGroup identified patterns into logical categories:\n\nPattern Consideration: Code\n\nSource File Read: SF\n\nSource Table / View: ST\n\nSource External Data / API: SA\n\nTransformation Arithmetic Calculations: TAC\n\nTransformation Statistical Calculations: TSC\n\nTransformation External Code / UDF: TX\n\nTransformation Aggregate Functions (SUM, COUNT, etc.): TAG\n\nJoin with Another Dataset: JL\n\nTemporary Views / Cached Tables: TEM\n\nData Quality and Rejection Logic: DQ\n\nWrite Operations - UPDATE: DU\n\nWrite Operations - INSERT / Save: DI\n\nWrite Operations - DELETE / Filter Out: DD\n\nWrite Operations - MERGE / Upsert: DM\n\nUnion / Union All: DUN\n\nAppend Mode Writes: DA\n\nSorting / Ordering: DS\n\nTarget File Output (e.g., write to file): TF\n\nTarget Table Output: TT\n\nTarget External Table / API Output: TA\n\nOthers not in above category: OO\n\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}\n\nSpecial Rules:\n\nDo not include full paths like /mnt/data/processed/sales_data.parquet, instead return only sales_data.parquet (or table name without schema).\n\nMaintain the original PySpark script filename as the JSON key.\n\nEach referenced dataset/module should be a string inside the list.\n\nEnsure values are deduplicated.\n\nOutput must be in proper JSON format, starting with { and ending with }, with no extra text above or below.\n\nProgram file names must end with .py or .ipynb.\n\nOutput must be perfectly valid JSON so it can be saved directly to a .json file.",
                        "expectedOutput": "{\n  \"<filename_1>.py\": {\n    \"design_pattern\": [\"<CODE_1>\", \"<CODE_2>\", \"...\", \"OO\"],\n    \"oo_reason\": \"<Brief reason why OO was added>\"\n  },\n  \"<filename_2>.ipynb\": {\n    \"design_pattern\": [\"<CODE_1>\", \"<CODE_2>\", \"...\", \"OO\"],\n    \"oo_reason\": \"<Brief reason why OO was added>\"\n  }\n}\n"
                    },
                    "maxIter": 20,
                    "maxRpm": 20,
                    "maxExecutionTime": 180,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 1736,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "AAVA Domain",
        "domainId": 697,
        "projectId": 1735,
        "project": "AAVA Learning",
        "teamId": 1736,
        "team": "Data Crew",
        "callbacks": []
    }
}