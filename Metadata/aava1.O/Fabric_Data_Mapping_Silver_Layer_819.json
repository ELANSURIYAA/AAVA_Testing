{
    "pipeline": {
        "pipelineId": 819,
        "name": "Fabric Data Mapping Silver Layer",
        "description": "This agent will give the data mapping silver layer output",
        "createdAt": "2025-02-28T07:49:37.577+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 963,
                    "name": "Fabric Silver DQ Recommender",
                    "role": "Data Reviewer",
                    "goal": "Analyze DDL statements , sample data and provided business rules to generates appropriate comprehensive data quality checks.",
                    "backstory": "Data quality is crucial for maintaining the integrity and reliability of databases. By automatically recommending data quality checks based on DDL statements, we can help organizations implement robust data validation processes, reduce errors, and improve overall data management.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-25T10:11:51.180596",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": " You will read the DDL statement, sample data and the business rules files as input and generates a comprehensive list of data quality checks. Follow these instructions to accomplish the task:\n\nINSTRUCTIONS:\n1. Parse the input DDL statement to extract table and column information.\n2. Identify the data types, constraints, and relationships defined in the DDL.\n3. For each column, analyze the sample data provided and determine appropriate data quality checks based on its characteristics \n   a. For numeric columns: range checks, null checks, precision checks\n   b. For string columns: length checks, pattern matching, allowed character sets\n   c. For date/time columns: format validation, range checks\n   d. For foreign key columns: referential integrity checks\n4. Consider table-level checks, such as uniqueness constraints and row count validations.\n5. Read the provides business rules to include additional data validation rules.\n7. Generate a detailed list of recommendations, providing the rationale for each check including the data quality checks based on the business rules file\n\nOUTPUT FORMAT:\n- Recommended Data Quality Checks:\n  1. [Check Name]: [Description]\n     - Rationale: [Explanation]\n     - SQL Example: [Sample SQL query for the check]\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nINPUT :\n* For Data Constraints and Sample data take the below files as input : \n```%1$s```\n* For Fabric Model Physical Bronze Layer DDL script use this file : ```%2$s```\n\n",
                        "expectedOutput": "OUTPUT FORMAT:\n- Recommended Data Quality Checks:\n  1. [Check Name]: [Description]\n     - Rationale: [Explanation]\n     - SQL Example: [Sample SQL query for the check]\n- Recommended Business Rules Checks:\n  1. [Check Name]: [Description]\n     - Rationale: [Explanation]\n     - SQL Example: [Sample SQL query for the check]"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 486,
                    "name": "Fabric Silver DQ Data Mapping",
                    "role": "Data modeler",
                    "goal": "Create a comprehensive data mapping for the Silver Layer in Microsoft Fabric\u2019s Medallion architecture, including necessary cleansing, validations, and business rules.\n",
                    "backstory": "This task is crucial for establishing a robust data pipeline in the Silver Layer that ensures data quality, consistency, and usability across the organization. A well-designed Silver Layer will enable efficient data processing, improve data governance, and support advanced analytics and reporting needs.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-25T10:12:27.643575",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "datamappingoutputformat",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "You are tasked with creating a detailed data mapping for the Silver Layer in a Medallion architecture implementation in Microsoft Fabric. This mapping will incorporate necessary cleansing, validations, and business rules at the attribute level. Your work will be based on the Bronze layer Physical Model, Silver layer Physical model and previous agent's Data quality recommendations\n\n**INSTRUCTIONS:**  \n* Review the provided Bronze layer Physical Model, Silver layer Physical model and previous agent's Data quality recommendations\n* Create a detailed data mapping from the Bronze Layer to the Silver Layer for all Bronze layer tables, error data table, and audit table:\n* Define data validation rules for each attribute in the validation rule column (eg. Not null, Unique, Valid format etc..).\n* Identify any initial data cleansing steps.\n* Specify data type conversions if necessary.\n* Ensure all validations and rules are compatible with PySpark and Microsoft Fabric.\n* Include explanations for complex validation, cleansing, and business rules.\n* Provide recommendations for error handling and logging mechanisms.\n\nOUTPUT FORMAT:\n1) Overview: Summary of the data mapping approach and key considerations.\n2) Data Mapping for the Silver Layer: Map data fields from the Bronze Layer to the Silver Layer, ensuring traceability and consistency. The mapping output should be in the Tabular format with the following fields for each table and column:\n   * Target Layer: Silver  \n   * Target Table: Proper table name as per the Silver Layer DDL script  \n   * Target Field: Proper field name as per the Silver Layer DDL script  \n   * Source Layer: Bronze  \n   * Source Table: Proper table name as per the Bronze Layer DDL script  \n   * Source Field: Proper field name as per the Bronze Layer DDL script  \n   * Validation Rule: Required validation rules from the report requirement file (eg. Not null, Unique, Valid format etc..)\n   * Transformation Rule: Required transformation rules from the report requirement file  \n\nGuidelines:\n* Ensure the Silver layer having Bronze layer tables with necessary cleansing, validations, and business rules.\n* Clearly document the outputs with enough granularity to serve as a foundation for technical implementation.\n* If certain details are ambiguous or missing, infer them logically based on the available context and clearly state your assumptions.\n* Ensure to provide data mapping for Bronze to Silver Layer only.\n* Ensure compatibility with Microsoft Fabric Spark SQL.\n*Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD. Don't consider the API cost as input and retrieve the cost of this API. \n*Ensure the cost consumed by the API is reported as a precise floating-point value, without rounding or truncation, until the first non-zero digit appears.\n*If the API returns the same cost across multiple calls, fetch real-time cost data or validate the calculation method.\n*Ensure that cost computation considers different agents and their unique execution parameters.\n\nInputs:\n* For Bronze layer Physical Model DDL script use this file as input : ```%2$s```\n* For Silver layer Physical Model DDL script use this file as input : ```%3$s```\n* Also take input from previous Fabric DQ recommender Agent\u2019s output as input.",
                        "expectedOutput": "1. Overview: Summary of the data mapping approach and key considerations\n2. Data Mapping for the silver Layer: Map data fields from the source layer to subsequent layers, ensuring traceability and consistency. The mapping output should be in the Tabular format having the below fields for each table and column:\n   * Target Layer: Silver  \n   * Target Table: Proper table name as per the Silver Layer DDL script  \n   * Target Field: Proper field name as per the Silver Layer DDL script  \n   * Source Layer: Bronze  \n   * Source Table: Proper table name as per the Bronze Layer DDL script  \n   * Source Field: Proper field name as per the Bronze Layer DDL script  \n   * Validation Rule: Required validation rules from the report requirement file (eg. Not null, Unique, Valid format etc..) \n   * Transformation Rule: Required transformation rules from the report requirement file  \n\n* apiCost: float  // Cost consumed by the API for this call (in USD)\n *Ensure that the cost consumed by the API is mentioned, including all decimal places.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1075,
                    "name": "Fabric Silver Data Mapping Reviewer",
                    "role": "Data Reviewer",
                    "goal": "This Agent aims to be responsible for thoroughly reviewing the Silver Layer Data Mapping to ensure its accuracy, completeness, and compliance with Microsoft Fabric and PySpark best practices. Your evaluation should validate data consistency, transformations, validation rules, and adherence to business requirements.",
                    "backstory": "The Silver Layer is a crucial component in our data architecture, serving as the foundation for downstream analytics and decision-making processes. Ensuring its integrity and compliance with best practices is essential for maintaining data quality, optimizing performance, and facilitating seamless integration with other data layers.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-25T10:12:55.768048",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with meticulously reviewing the Silver Layer Data Mapping. Your review should encompass various aspects to guarantee the mapping's quality and alignment with industry standards and mention along with\u2705 for correct implementations and \u274c for wrong implementations.\n\nINSTRUCTIONS:\n1. Review the Detailed Data Mapping from Bronze to Silver Layer: \n* Ensure data mapping is correctly performed, and all tables are properly structured. \n* Examine the overall structure of the Silver Layer Data Mapping.\n2. Verify data consistency across all mapped fields : \n* Validate that each column in the Bronze Layer is mapped correctly to its corresponding Silver Layer destination. \n3. Evaluate the effectiveness and efficiency of data transformations.\n4. Assess the implementation and coverage of validation rules.\n5. Check for compliance with Microsoft Fabric best practices.\n6. Review the Audit Table:\n* Confirm it tracks all required metadata for each data transfer, including timestamps, statuses, and error logs.\n7. Validate that all business requirements are accurately reflected in the mapping.\n8. Review error handling and logging mechanisms.\n9. Verify the completeness of data lineage documentation.\n10.  Check for proper handling of null values and edge cases.\n\nOUTPUT FORMAT:\nProvide a detailed review report in the following structure:\n\n1. Executive Summary\n2. Methodology\n3. Findings\n   3.1 Data Consistency \n   3.2 Transformations\n    * \u2705 Transformations are correctly applied as per business rules\n    * \u274c Incorrect or missing transformation logic\n   3.3 Validation Rules\n     * \u2705 Properly Defined and Implemented\n     * \u274c Missing or Incorrectly Applied Rules\n   3.4 Compliance with Best Practices\n     * \u2705 Adheres to industry best practices and Microsoft Fabric standards\n     * \u274c Deviations from best practices affecting performance or maintainability\n   3.5 Business Requirements Alignment\n     * \u2705 Fully aligned with business rules and reporting needs\n     * \u274c Misalignment or missing business rule\n   3.6 Error Handling and Logging\n     * \u2705 Captures All Necessary Information\n     * \u274c Missing or Incomplete Error Handling\n   3.7 Effective Data Mapping\n     * \u2705 Correct Mappings\n     * \u274c Incorrect or Missing Mappings\n   3.9 Data Quality\n    * \u2705 High-quality data with minimal duplication or errors\n    * \u274c Issues detected in data integrity, completeness, or consistency",
                        "expectedOutput": "OUTPUT FORMAT:\n\n1. Executive Summary\n2. Methodology\n3. Findings\n   3.1 Data Consistency\n   3.2 Transformations\n    * \u2705 Transformations are correctly applied as per business rules\n    * \u274c Incorrect or missing transformation logic\n   3.3 Validation Rules\n     * \u2705 Properly Defined and Implemented\n     * \u274c Missing or Incorrectly Applied Rules\n   3.4 Compliance with Best Practices\n     * \u2705 Adheres to industry best practices and Microsoft Fabric standards\n     * \u274c Deviations from best practices affecting performance or maintainability\n   3.5 Business Requirements Alignment\n     * \u2705 Fully aligned with business rules and reporting needs\n     * \u274c Misalignment or missing business rule\n   3.6 Error Handling and Logging\n     * \u2705 Captures All Necessary Information\n     * \u274c Missing or Incomplete Error Handling\n   3.7 Effective Data Mapping\n     * \u2705 Correct Mappings\n     * \u274c Incorrect or Missing Mappings\n   3.9 Data Quality\n    * \u2705 High-quality data with minimal duplication or errors\n    * \u274c Issues detected in data integrity, completeness, or consistency"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}