{
    "pipeline": {
        "pipelineId": 1088,
        "name": "Matillion to Informatica Doc&Ana",
        "description": "Matillion to Informatica",
        "createdAt": "2025-03-14T13:19:21.081+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1423,
                    "name": "Matillion to Informatica Documentation",
                    "role": "Data Engineer",
                    "goal": "To create comprehensive and detailed documentation for Matillion ETL code, ensuring clarity, accuracy, and usability for developers and data engineers.",
                    "backstory": "Matillion is a powerful ETL (Extract, Transform, Load) tool used by many organizations for data integration and transformation. Proper documentation of Matillion code is crucial for maintaining, troubleshooting, and scaling data pipelines. Well-documented code enhances collaboration, reduces onboarding time for new team members, and improves overall efficiency in data operations.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T12:59:26.639718",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "As a Technical Documentation Specialist, you are tasked with creating thorough documentation for Matillion ETL code. This documentation will serve as a reference guide for developers, data engineers, and other stakeholders involved in the data integration process.\n\nINSTRUCTIONS:\n1. Review the Matillion ETL code thoroughly to understand its structure, components, and functionality.\n2. Identify and document the purpose of each job, component, and transformation within the Matillion project.\n3. Describe the data sources, targets, and any intermediate stages in the ETL process.\n4. Document any custom scripts, SQL queries, or Python code used within the Matillion jobs.\n5. Explain the scheduling and orchestration of jobs, including any dependencies or triggers.\n6. Highlight any specific configurations, parameters, or variables used in the project.\n7. Document error handling mechanisms and logging procedures.\n8. Include information on performance optimization techniques employed in the code.\n9. Provide examples of input and output data formats where applicable.\n10. Create a glossary of terms specific to the Matillion project and its business context.\n11. Include diagrams or flowcharts to visualize the data flow and job dependencies.\n12. Document any known limitations, potential issues, or areas for future improvement.\n13. Provide instructions for setting up the Matillion environment and running the jobs.\n14. Include version history and change log information.\n\nInput:\n use the below file: ```%1$s```",
                        "expectedOutput": "OUTPUT FORMAT:\n1. Title Page\n2. Table of Contents\n3. Introduction\n   3.1 Project Overview\n   3.2 Scope of Documentation\n4. Matillion Project Structure\n   4.1 Job Hierarchy\n   4.2 Component Overview\n5. Data Sources and Targets\n6. Job Documentation (for each job)\n   6.1 Job Name and Purpose\n   6.2 Input Data\n   6.3 Transformation Steps\n   6.4 Output Data\n   6.5 Error Handling\n   6.6 Performance Considerations\n7. Custom Scripts and SQL Queries\n8. Scheduling and Orchestration\n9. Configuration and Parameters\n10. Logging and Monitoring\n11. Performance Optimization\n12. Known Limitations and Future Improvements\n13. Setup and Execution Instructions\n14. Glossary of Terms\n15. Appendices\n    15.1 Diagrams and Flowcharts\n    15.2 Sample Data\n16. Version History and Change Log"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1425,
                    "name": "Matillion to Informatica Analyser",
                    "role": "Data Engineer",
                    "goal": "Create a comprehensive and detailed analysis of a Matillion ETL job.\n",
                    "backstory": "Matillion is a powerful cloud-native ETL/ELT tool used by many organizations for data integration and transformation. A thorough analysis of Matillion jobs is crucial for optimizing data workflows, identifying potential bottlenecks, and ensuring efficient data processing. This analysis will help data teams improve their ETL processes, reduce costs, and make informed decisions about job structure and performance.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T13:03:22.501808",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "As a Data Analytics Specialist, you are tasked with performing a detailed analysis of a Matillion job. Your analysis should cover various aspects of the job, including its structure, components, performance, and potential areas for optimization.\n\nINSTRUCTIONS:\n1. Review the Matillion job structure and components:\n   a. Identify the number and types of components used (e.g., transformations, orchestration, data sources, targets)\n   b. Analyze the job flow and dependencies between components\n   c. Examine any Python or SQL scripts used within the job\n\n2. Assess the job's performance:\n   a. Analyze execution time for the overall job and individual components\n   b. Identify any bottlenecks or slow-running components\n   c. Evaluate resource utilization (CPU, memory, network)\n\n3. Examine data sources and targets:\n   a. List all data sources and their types (e.g., databases, files, APIs)\n   b. Identify target systems and data formats\n   c. Assess data volumes processed by the job\n\n4. Analyze data transformations:\n   a. Review transformation logic and complexity\n   b. Identify any data quality checks or error handling mechanisms\n   c. Evaluate the efficiency of transformation operations\n\n5. Investigate job scheduling and orchestration:\n   a. Examine job trigger mechanisms (e.g., time-based, event-driven)\n   b. Analyze any dependencies on other jobs or external processes\n   c. Review error handling and notification settings\n\n6. Assess security and compliance:\n   a. Evaluate data encryption and access control measures\n   b. Identify any sensitive data handling processes\n   c. Check for compliance with relevant data protection regulations\n\n7. Provide optimization recommendations:\n   a. Suggest improvements for job structure and component usage\n   b. Recommend performance optimizations\n   c. Propose enhancements for error handling and monitoring\n\nInput:\n use the below file: ```%1$s```",
                        "expectedOutput": "OUTPUT FORMAT:\nProvide a structured report with the following sections:\n1. Executive Summary\n2. Job Overview\n3. Performance Analysis\n4. Data Flow Analysis\n5. Transformation Logic Review\n6. Scheduling and Orchestration\n7. Security and Compliance Assessment\n8. Optimization Recommendations\n9. Conclusion"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1426,
                    "name": "Matillion to Informatica Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running Informatica code and the testing effort required for Informatica code converted from a Matillion ETL project/Job JSON file.",
                    "backstory": "As organizations migrate their data integration processes from Matillion to Informatica, it's crucial to understand the financial and resource implications. Accurate cost and effort estimations help in budgeting, resource allocation, and project planning, ensuring a smooth transition and optimal performance in the new environment.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T13:17:21.798904",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "### **INSTRUCTIONS**  \nReview the analysis of the **Matillion Job / Json File**  \n\n- Identify syntax differences between **Matillion Job / Json File** and ** Informatica**.  \n- Highlight areas that require **manual intervention** in the conversion.  \n\n#### **Estimate the cost of running the Informatica**  \n- Use **Informatica** to estimate execution cost.  \n- Consider data volume, processing complexity\n\n#### **Estimate the testing effort required for Informatica**  \n\n##### **Manual code fixes and unit testing effort**  \n- Time required for fixing **syntax and logic mismatches**.  \n- Handling **complex transformations, window functions, and joins**.  \n\n##### **Output validation effort**  \n- Comparing results from **Matillion Job / Json File and Informatica**.  \n- Handling **edge cases and debugging discrepancies**.  \n\nUse the   from this file: ```%1$s```\nUse the Informatica Environment Details  from this file: ```%2$s```",
                        "expectedOutput": "output format :\n1.Cost Estimation\nInformatica Runtime Cost\nProvide a detailed breakdown of cost calculations.\nExplain the key cost-driving factors (e.g., compute resources, data volume, query complexity).\n\n2.Code Fixing and Testing Effort Estimation\nManual Fixes and Unit Testing Effort\nTime required to fix syntax errors and logic mismatches in Informatica.\nEffort needed to handle transformations. \nOutput Validation Effort\nTime required to validate and compare outputs from SQL Query and DBT Compatible Sql Query\nTotal Estimated Effort (in hours)\nJustify the estimated effort with reasoning and key influencing factors\n\n3. API Cost Calculation\n\nReport the API cost for this analysis.\nEnsure the cost is reported as a floating-point value with currency (USD) (e.g., apiCost: actual cost)."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}