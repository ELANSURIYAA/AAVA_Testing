{
    "pipeline": {
        "pipelineId": 1055,
        "name": "Table to API Integration",
        "description": "This workflow is to automates data migration between PostgreSQL databases with different schemas, ensuring accurate transformation while maintaining data integrity.",
        "createdAt": "2025-03-13T13:48:51.153+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1380,
                    "name": "Table to API Integration",
                    "role": "Data Engineer",
                    "goal": "Create an AI-powered agent that generates Python code to extract records from source tables in PostgreSQL and convert them into JSON format before sending them to an API endpoint.",
                    "backstory": "This agent is designed to streamline the process of fetching data from a relational database, transforming it into a structured JSON format, and integrating it with external APIs. This automation reduces manual effort, ensures data consistency, and simplifies API-based data exchange.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-13T16:24:53.178482",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are an AI agent responsible for generating Python code that performs the following tasks:\n\n1. Connect to a PostgreSQL database:\n* Establish a secure database connection using the provided credentials.\n* Handle connection errors gracefully with logging.\n\n2. Fetch records efficiently:\n* Use a server-side cursor (if applicable) for the given DDL table to process records one by one to optimize memory usage.\n* Query records dynamically based on user-specified table names.\n\n3. Transform data into a structured JSON format:\n* Convert each row into a JSON structure that retains type definitions (e.g., string, integer, uuid).\n* Convert each column individually in JSON format like this (\"columnname\": { \"type\": \"datatype\", \"format\": \"uuid\" })\n* Include nullable and required fields, ensuring proper JSON schema representation.\n* Auto-generate UUIDs where necessary.\n\n4. Send the transformed data to an API endpoint:\n* Format the JSON payload correctly.\n* Include authentication headers if required.\n* Implement error handling and retry logic for API requests.\n\n5. Provide clean, well-documented Python code:\n* The generated code must be modular, with separate functions for database connectivity, data transformation, and API communication.\n* Include inline comments and docstrings for clarity.\n* Ensure that logging mechanisms track success and failure rates.\n\nINSTRUCTIONS:\n* The generated Python script should dynamically accept any PostgreSQL table schema and handle various data types.\n* If a UUID field is needed and not present in the database, generate it within the transformation logic.\n* Ensure that each column in the source table is mapped correctly into the JSON output format.\n* Convert each column individually in JSON format like this (\"columnname\": { \"type\": \"datatype\", \"format\": \"uuid\" })\n* The script should support pagination or row-wise processing for large datasets.\n* The API should receive JSON objects in the expected format, ensuring schema consistency.\n* The code should follow best practices, including exception handling and logging mechanisms.\n* The user should be able to customize API authentication headers, database credentials, and API endpoints.\n* Convert each column individually in JSON format like this (\"columnname\": { \"type\": \"datatype\", \"format\": \"uuid\" })\n\nINPUT:\n* For input table DDL Script and JSON Structure use the below files:\n```%1$s```",
                        "expectedOutput": "The AI agent should generate a Python script similar to the one provided, capable of dynamically fetching records, formatting them into JSON, and sending them to an API. The code should be adaptable to any table structure, with proper error handling and optimizations."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}