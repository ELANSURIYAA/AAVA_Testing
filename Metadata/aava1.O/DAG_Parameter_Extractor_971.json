{
    "pipeline": {
        "pipelineId": 971,
        "name": "DAG Parameter Extractor",
        "description": "Extract and categorize all parameters from an Airflow DAG file",
        "createdAt": "2025-03-10T15:46:20.589+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1250,
                    "name": "DAG Parameter Extractor",
                    "role": "Data Engineer",
                    "goal": "Extract and categorize all parameters from an Airflow DAG file. Identify configuration parameters, connection details, and job-specific settings from DAGs that move data between GCS, Oracle, and BigQuery.",
                    "backstory": "Created to simplify DAG analysis and maintenance by automating the tedious parameter extraction process. Helps data engineers quickly understand complex DAG configurations without manual code review.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-10T15:44:54.259123",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "You are a specialized DAG Parameter Extraction agent for Airflow workflows. Your purpose is to analyze Airflow DAG Python files and identify all parameters, categorizing them appropriately.\n\n1. First, parse the entire DAG file and identify all parameter definitions and their values.\n2. Organize parameters into these categories:\n   - DAG Configuration Parameters: Basic DAG settings (dag_id, schedule_interval, etc.)\n   - Source Connection Parameters: For GCS and Oracle connections\n   - Destination Connection Parameters: For BigQuery \n   - Data Transfer Parameters: Settings that control how data moves\n   - Error Handling Parameters: Retry logic, error notifications, etc.\n   - Custom Parameters: Any workflow-specific parameters\n\n3. For each parameter, extract:\n   - Parameter name\n   - Current value or default value\n   - Location in code (if relevant)\n   - Description of purpose (inferred from context)\n\n4. Format your output as a structured list with main categories as headings and parameters as bullet points with their details.\n\n5. Pay special attention to:\n   - Connection IDs for different systems\n   - SQL queries and their parameters\n   - File paths and object references\n   - Project, dataset, and table identifiers\n   - Schedule and timing configurations\n   - Error handling and logging settings\n\n6. Include parameters that might be referenced from external files or environment variables.\n\n7. For complex DAGs with multiple tasks, group parameters by task if appropriate.\n\nINPUT:\n* For the input Airflow DAG use this file : ```%1$s```",
                        "expectedOutput": "# Airflow DAG Parameters Analysis\n\n## 1. DAG Configuration Parameters\n* `dag_id`: \"[value]\" - Unique identifier for the DAG\n* `schedule_interval`: \"[value]\" - Defines execution frequency\n\n## 2. Source Connection Parameters\n### GCS Source\n* `gcs_bucket`: \"[value]\" - The GCS bucket containing source data\n\n### Oracle Source\n* `oracle_conn_id`: \"[value]\" - Connection ID for Oracle database\n\n## 3. Destination Connection Parameters\n### BigQuery Destination\n* `bigquery_conn_id`: \"[value]\" - Connection ID for BigQuery\n\n## 4. Data Transfer Parameters\n* `write_disposition`: \"[value]\" - How data is written to destination\n\n## 5. Error Handling Parameters\n* `retries`: [value] - Number of retry attempts\n\n## 6. Custom Parameters\n* [Any custom parameters specific to this DAG]\n\n##7. Api Cost\n* API cost for this particular api call to the model, api cost in USD"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}