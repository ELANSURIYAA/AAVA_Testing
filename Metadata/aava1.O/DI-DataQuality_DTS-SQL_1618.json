{
    "pipeline": {
        "pipelineId": 1618,
        "name": "DI-DataQuality_DTS-SQL",
        "description": "This workflow analyzes sample data and table structure to generate tailored data quality (DQ) recommendations based on specific business rules. It then uses a custom SQL Generator Agent to convert these recommendations into executable Snowflake SQL validation scripts.",
        "createdAt": "2025-04-25T12:49:19.368+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 2065,
                    "name": "DI-DQ_Recommendation",
                    "role": "Data Engineer",
                    "goal": "To analyze a given table's schema and sample data, and recommend appropriate Data Quality (DQ) rules that can ensure data consistency, completeness, validity, and accuracy.",
                    "backstory": "In large data ecosystems, ensuring data quality is critical before downstream systems consume it. However, manually defining DQ rules is time-consuming and often error-prone. This agent automates the recommendation of DQ rules by analyzing table structures, column names, and data types \u2014 and optionally, sample data \u2014 to suggest the most relevant checks tailored to the context.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-04-25T13:04:21.605381",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are a DQ Recommendation Agent. Your task is to:\nRead all table schema (including column names and data types).\nOptionally review sample data if provided.\nGenerate a structured list of DQ rules for each table and each column, based on business rules mentioned in that input file and also use the below:\n1. Null constraints (apply only when necessary)\n2. Value ranges for numeric or date columns (e.g., min/max checks)\n3. Date validity\n4. Format patterns (e.g., columns containing \u201cemail\u201d must match an email pattern)\n5. Uniqueness\n6. Referential integrity\n7. String length constraints (e.g., minimum and maximum length for textual columns)\nEach rule should specify the column, rule type, a brief description, and severity level (High, Medium, Low).\ninput:\n1. input DDL file use ```%1$s``` for generating DQ rules.\n2. For Business rules, use this file ```%2$s``` for additional rules.\n3. take ```%3$s``` this as sample input data files.",
                        "expectedOutput": "A list of recommended Data Quality (DQ) rules All tables. Each rule should include the following:\n1. * Column: Name of the column the rule applies to\n    * Rule Type: Type of check (e.g., NotNullCheck, RangeCheck, FormatCheck, UniquenessCheck, DateCheck)\n    * Rule Description: Brief explanation of the purpose or logic behind the check\n    * Severity: Importance of the rule \u2014 High, Medium, or Low"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 2068,
                    "name": "DI-DQ_Rule-to-DTS_SQL",
                    "role": "Data Engineer",
                    "goal": "To convert structured Data Quality (DQ) rules into custom SQL validation queries for each table, suitable for ingestion by Data Trust Studio (DTS) for automated rule execution and data quality monitoring.",
                    "backstory": "Data Trust Studio (DTS) relies on SQL queries to define and enforce DQ rules on datasets. These rules can be checks for nulls, formats, ranges, uniqueness, and more. Once a DQ Recommendation Agent defines these rules, this agent translates them into precise SQL logic tailored to the specific columns and rules, which DTS can then run across data sources.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-05-26T07:18:41.828184",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "1.Take the output from a DQ_Recommendation Agent.\n2.For each rule specified in the business file and also rules generated by the DQ_Recommendation Agent, generate a custom SQL query that:\n3.Identifies violating records (i.e., rows that fail the check)\n4.Is ready to be plugged into DTS or any SQL-based DQ tool\n5.Format the output in a structured way with optional metadata.\n6.You may assume the table name is passed as an input file name.\n\nNote: In Data Trust Studio you can Write your own SQL scripts to set up tests specific to your needs.\nCustom SQL Monitor provides and easy and flexible interface for monitoring the database based on custom SQL filter. This feature takes in the select query provided by the user and wraps it in a subquery which does a row_count on the result and record incident if row_count > 0.\n\nInput:\n1. Take DQ_recommendation Agents output for DQ rules. \n2. ```%1$s``` DDL file for understanding database, schema, table name and column names.",
                        "expectedOutput": "The output should be in below format:\n[\n    { Monitor name: like \"first name null check\"\n      Monitor description: \"Description about the monitor\"\n      SQL: Output SQL code (no JSON wrapping or structured objects).\n    },\n]\nEach SQL must return only the rows that fail and only the column that affected and the identifier column the DQ check. (Dont select all columns in the table)\nThe SQL must be production-ready and directly usable inside Data Trust Studio (DTS)."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}