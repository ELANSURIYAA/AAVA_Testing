{
    "pipeline": {
        "pipelineId": 1619,
        "name": "DI-DataQuality_SF-INFA",
        "description": "This workflow takes DQ recommendations as input and transforms them into actionable outputs. One agent generates a Snowflake stored procedure for automated validation, while another produces an Informatica mapping for ETL integration.",
        "createdAt": "2025-04-25T13:34:30.985+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 2066,
                    "name": "DI-DQ_Rule-to-Snowflake_SP",
                    "role": "Data Engineer",
                    "goal": "To automatically convert Data Quality (DQ) rules into a Snowflake Stored Procedure that can validate a target table against the recommended data quality checks.",
                    "backstory": "Once DQ rules are defined by the DQ Recommendation Agent, they must be translated into executable logic to run checks directly in Snowflake. Manually writing these procedures for each table is time-consuming. This agent automates the generation of Snowflake-compatible stored procedures based on structured DQ rule definitions.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-04-30T09:05:55.670322",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Your task is to generate a complete, executable Snowflake stored procedure that performs data quality (DQ) validation checks based on the given DQ rules(```%1$s```).\n1. Validation Execution\n   - Loop through each DQ rule from the DQ recommendation file.\n   - Loop through each parsed DQ rule.\n   - Construct validation queries dynamically based on snowflake.createStatement({...}).execute() for each DQ rule.\n   - Proper use of escaped SQL literals (e.g., use `.replace(/'/g, \"''\")` for safety).\n   - Valid Snowflake regex syntax (avoid invalid ranges like `0-9-+()`; place `-` safely in brackets).\n   -ove the hyphen - to the end or start of the character class\n   - Handle both single-column and multi-column rules.\n   - Use valid regex syntax; avoid invalid character class ranges like 0-9-+(). Position hyphens safely in character classes. !~ is not valid in Snowflake SQL instead use RLIKE or REGEXP. Ensure your regex is valid:\n   - Parse the DQ_ISSUE_LOG DDL (```%3$s```) to determine columns to populate.\n\n2. Batch ID Generation\n   - Create a Snowflake sequence named SEQ_BATCH_ID if it doesn\u2019t exist.\n   - Use SEQ_BATCH_ID.NEXTVAL to generate a unique BATCH_ID each time the procedure runs.\n\n3. Issue Logging\n   - Insert failed records into DQ_ISSUE_LOG with required columns mentioned in the Issue log DDL.\n\n4. Code Quality\n   - Use valid and executable Snowflake JavaScript.\n   - Avoid procedural SQL embedded inside SQL strings.\n   - Log meaningful error messages on failure.\n   - Ensure the code is structured, modular, and readable.\n\nFor input: \n1. Take ```%1$s``` as DQ Recommendation.\n2. input DDL file use ```%2$s```.\n3. For DQ_ISSUE_LOG table use ```%3$s``` DDL file.\nExpected Output Format:\n\n-- =============================================\n-- Author:       Ascendion AVA+\n-- Created on:   <today\u2019s date> (auto-filled from web)\n-- Description:  Executes DQ validation checks on source table and logs violations\n-- =============================================\n\n-- Valid Snowflake JavaScript Stored Procedure:\n-- Procedure name: sp_run_dq_checks (or similar)\n-- Implements dynamic DQ checks\n-- Inserts failed checks into DQ_ISSUE_LOG table",
                        "expectedOutput": "The output should be in the below format:\n-- =============================================\n-- Author:        Ascendion AVA+\n-- Created on:  put today date.\n-- Description:   write one line description of the procedure\n-- =============================================\n\nA valid Snowflake Stored Procedure:\nProcedure name (e.g., sp_run_dq_checks)\nDQ checks implemented via SQL queries\nInsertion of any failed checks into a results table like DQ_ISSUE_LOG"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 2067,
                    "name": "DI-DQ_Rule-to-Informatica_Mapping",
                    "role": "Data Engineer",
                    "goal": "To convert structured Data Quality (DQ) rules into a complete and executable Informatica mapping in XML format that performs the specified DQ validations using appropriate transformations for each table.",
                    "backstory": "After DQ rules are generated from a table's metadata and sample data, implementing them manually in Informatica is tedious and prone to human error. This agent automates the translation of machine-readable DQ recommendations into Informatica mappings, for deployment in ETL pipelines.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-04-25T13:32:00.295378",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "1. Read the source all table schema from the provided DDL file.\n2. Take as input a list of DQ rules in structured format (typically generated by a DQ Recommendation Agent).\n3. For each rule, generate corresponding Informatica logic that includes:\n 3.1 Expression transformations for checks such as format, length, or range validations\n 3.2 Filter transformations to isolate passing or failing rows\n 3.3 Router transformations for conditional routing based on DQ checks\n4. Optionally route rejected rows to a logging table or error output for further analysis.\n5 Generate a complete Informatica XML mapping that includes:\n 5.1 Source definition (based on input DDL)\n 5.2 Transformation logic for each DQ rule\n 5.3 Target definitions (for valid and rejected rows)\n 5.4 Full mapping flow and metadata\n\nFor input: \n1. take DQ_Recommendation from ```%1$s```.\n2. input DDL file use ```%2$s```.",
                        "expectedOutput": "A complete Informatica mapping in XML format, including:\n1 Source definition\n2 Transformations implementing the DQ rules\n3 Target definition(s)\n4 Connections and flow\nOr, optionally, a high-level Informatica XML template representing the full mapping flow."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}