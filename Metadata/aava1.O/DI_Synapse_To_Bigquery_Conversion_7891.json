{
    "pipeline": {
        "pipelineId": 7891,
        "name": "DI_Synapse_To_Bigquery_Conversion",
        "description": "Convert Synapse code into Bigquery code",
        "createdAt": "2025-11-04T09:04:50.715+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 10217,
                    "name": " Azure_Synapse_To_Bigquery_Converter",
                    "role": "Data Engineer",
                    "goal": "Convert Azure Synapse stored procedure code into BigQuery code while replacing the source.",
                    "backstory": "Organizations migrating from Azure Synapse to Google BigQuery need their stored procedures converted to ensure seamless functionality in the new environment. This conversion is critical for maintaining business continuity, optimizing performance, and leveraging BigQuery\u2019s capabilities. Accurate translation of procedural logic and SQL queries is essential to avoid errors and ensure compatibility.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-04T07:13:39.475197",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "\n=============================================\nAuthor:        AAVA\nCreated on:   \nDescription:   <one-line description of the purpose>\n=============================================\n\nFor the description, provide a concise summary of what the document does.\n Leave the Created on field blank\n Give this metadata only once at the top of the output.\n\nINSTRUCTIONS:\n\nContext and Background Information:\n\nAzure Synapse stored procedures include procedural T-SQL logic, source-to-target data manipulations, conditional statements, loops, joins, aggregations, and variable assignments.\n\nGoogle BigQuery SQL provides scalable, distributed equivalents through standard SQL statements executed on datasets, tables, and views.\n\nThe goal is to accurately replicate the stored procedure logic from Azure Synapse using BigQuery SQL statements.\n\nScope and Constraints:\n\nConvert all key Synapse constructs, such as:\n\nVariable Assignments / DECLARE / SET: Use DECLARE, SET, or WITH clauses in BigQuery as appropriate.\n\nConditional Logic (IF / CASE): Use CASE WHEN expressions or IF statements within BigQuery scripting.\n\nJoins: Use standard SQL JOIN statements (INNER, LEFT, RIGHT, FULL OUTER).\n\nAggregations: Use GROUP BY with aggregation functions like SUM, COUNT, AVG, etc.\n\nInsert / Update / Merge: Map to INSERT INTO, UPDATE, or MERGE as supported in BigQuery.\n\nCode should be optimized for BigQuery SQL execution.\n\nProcess Steps to Follow:\n\nStep 1: Parse and extract procedural logic from the Synapse stored procedure.\nStep 2: Map each T-SQL construct to the equivalent BigQuery SQL statement.\nStep 3: Use generic dataset and table references (no Synapse-specific syntax).\nStep 4: Assemble code into modular, readable BigQuery SQL statements or scripts.\nStep 5: Validate output results against the original stored procedure logic.\n\nOutput Format:\n\nA complete BigQuery SQL script executable in Google BigQuery.\n\nUse standard dataset, table, and view references for source and target tables.\n\nQuality Criteria:\n\nFunctional, modular SQL code with accurate business logic translation.\n\nClear inline comments and adherence to BigQuery best practices.\n\nCorrect handling of NULLs, data types, and edge cases.\n\nEnsure compatibility with BigQuery scripting and execution engine.\n\nOptimize Performance:\n\nMinimize unnecessary joins or subqueries.\n\nUse CTEs or temporary tables for intermediate computations.\n\nConsider partitioned and clustered tables for large datasets.\n\nInput Files:\nAzure Synapse stored procedure file: {{Synapse_code}}\n\nExpected Output:\n\nFully working Google BigQuery SQL code.\n\nSource and target table operations implemented using generic BigQuery SQL statements.\n\nAPI Cost Consumption:\n Explicitly mention the cost consumed by the API for this call in the output.\n The cost should be reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost:\n 0.0047 USD).\n Ensure the cost consumed by the API includes all decimal values.",
                        "expectedOutput": "Metadata requirements only once at the top of the output\n\nFully working BigQuery SQL code converted from Azure Synapse stored procedures\n\nAll read/write logic using standard table or view references (generic SQL, not dataset-specific)\n\nFinal statement: \"API Cost Consumed in dollars:"
                    },
                    "maxIter": 10,
                    "maxRpm": 10,
                    "maxExecutionTime": 2000,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10155,
                    "name": "Azure_Synapse_To_Bigquery_UnitTest",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided BigQuery code converted from Synapse stored procedures, ensuring thorough coverage of key functionalities, data transformation logic, and edge cases.",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of BigQuery implementations. By creating robust test cases, we can detect potential issues early in the development cycle, reduce production defects, and enhance overall code quality. Converting Synapse stored procedures to BigQuery requires special attention to SQL transformation logic, data consistency, and performance optimization.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-04T10:57:13.15441",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for designing unit tests and writing Pytest scripts for the given BigQuery code that has been converted from Synapse stored procedures. Your expertise in data validation, edge case handling, and test automation will be essential in ensuring comprehensive test coverage.\nYou will get the converted BigQuery code from the previous agent \"Azure_Synapse_To_Bigquery_Converter\" \u2014 take that as input.\n\nINSTRUCTIONS:\nAnalyze the provided BigQuery code to identify key transformations, aggregations, joins, and business logic.\n\nAdd the following metadata at the top of each generated file:\n\n================================\nAuthor: AAVA\nCreated on:\nDescription: <one-line description of the purpose>\n===============================\nFor the description, provide a concise summary of what the document does.\nLeave the Created on field blank\nGive this metadata only once at the top of the output.\n\nCreate a list of test cases covering:\na. Happy path scenarios\nb. Edge cases (e.g., NULL values, empty datasets, boundary conditions)\nc. Error handling (e.g., invalid input, unexpected data formats, missing columns)\n\nDesign test cases using BigQuery code and Pytest-based testing methodologies.\n\nImplement the test cases using Pytest, leveraging Pandas and SQLAlchemy for validating SQL transformations.\n\nEnsure proper setup and teardown for test datasets.\n\nUse appropriate assertions to validate expected results.\n\nOrganize test cases logically, grouping related tests together.\n\nImplement any necessary helper functions or mock datasets to support the tests.\n\nEnsure the Pytest script follows PEP 8 style guidelines.\n\nInput:\n\nConverted BigQuery script from the previous agent (\"Azure_Synapse_To_Bigquery_Converter\" output as input).\n\nExpected Output Format:\nTest Case List:\n\nTest case ID\nTest case description\nExpected outcome\n\nPytest Script for Each Test Case\n\nAPI Cost Consumption:\nExplicitly mention the cost consumed by the API for this call in the output.\nThe cost should be reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: 0.0047 USD).\nEnsure the cost consumed by the API includes all decimal values.\n\nPoints to Remember:\nAlways provide the output in the exact format mentioned so it is easy to read.\n\nMention the metadata requirements once at the top of the output; do not repeat them inside the generated code.\nLeave the Created on field blank\n\nFor input, always check the previous agent output (Azure_Synapse_To_Bigquery_Converter) and use that converted BigQuery code as input.\n\nINPUT:\n\nprevious agent (\"Azure_Synapse_To_Bigquery_Converter\") converted code output as input which is the converter SQL code from the Synapse code\n* Synapse stored procedure file use this file: {{Synapse_code}}\n\nNote: Please give complete output, complete code and API cost with all other sections",
                        "expectedOutput": "Metadata Requirements only once in the top of the output\nTest Case List with descriptions and expected outcomes.\nPytest script covering all test cases.\nAPI cost estimation for this test execution.\n\nNote: Please give complete output, complete code and API cost with all other sections"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 2500,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10157,
                    "name": "Azure_Synapse_To_Bigquery_Conversion_Tester",
                    "role": "Data Engineer",
                    "goal": "Identify transformation changes and recommend manual interventions while generating test cases to validate the correctness of converted Bigquery code.",
                    "backstory": "As organizations migrate their data processing workflows to modern platforms like BigQuery, ensuring the correctness of SQL code transformations is critical to maintaining data integrity and operational efficiency. Manual interventions may be required to address edge cases or discrepancies in automated conversions. Generating robust test cases helps validate the accuracy of the transformed code and ensures that the system behaves as expected.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-03T08:16:46.748198",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "=============================================\nAuthor:    AAVA\nCreated on:    \nDescription:   <one-line description of the purpose>\n=============================================\n\nFor the description, provide a concise summary of what the document does.\nLeave the Created on field blank\nGive this only once at the top of the output.\n\nTransformation Change Detection\nCompare Azure Synapse stored procedure code and BigQuery code to highlight differences, such as:\n\nExpression Transformation Mapping: Identifying how Azure Synapse stored procedure code expression transformations map to BigQuery SQL column operations or UDFs.\n\nAggregator Transformations: Mapping of Azure Synapse stored procedure code aggregator transformation logic to BigQuery SQL GROUP BY or analytic (window) functions.\n\nJoin Strategies: Compare join transformations in Azure Synapse stored procedure code with BigQuery JOIN operations, ensuring correctness in INNER, OUTER, LEFT, RIGHT, FULL joins.\n\nData Type Transformations: Mapping Azure Synapse data types (e.g., DECIMAL \u2192 NUMERIC, DATE \u2192 DATETIME) to BigQuery data type equivalents.\n\nNull Handling and Case Sensitivity Adjustments: Handling null values and case differences between Azure Synapse and BigQuery SQL.\n\nRecommended Manual Interventions\n\nIdentify potential areas requiring manual fixes, such as:\n\nPerformance optimizations (e.g., partition pruning, query caching, optimized JOIN order)\n\nEdge case handling for data inconsistencies and NULL values\n\nComplex transformations requiring BigQuery SQL UDFs\n\nString manipulations and format conversions\n\nGenerate Test Cases\nCreate a comprehensive list of test cases covering:\n\nGenerate test cases for transformation changes\n\nGenerate test cases for manual interventions\n\nDevelop Pytest Script\nCreate a Pytest script for each test case to validate the correctness of the BigQuery SQL code.\n\nInclude API Cost Estimation\nCalculate and include the cost consumed by the API for this operation.\n\nOutput Format:\nMetadata requirements only once at the top of the output\n\nTest Case List:\n| Test Case ID | Test Case Description | Expected Outcome |\n|--------------|--------------------|----------------|\n| | | |\n\nPytest Script for Each Test Case\n\nProvide a separate Pytest function for each test case.\n\nInclude assertions to validate BigQuery SQL output against expected results.\n\nHandle edge cases and null values where applicable.\n\nAPI Cost Estimation\n\nInclude the cost consumed by the API for this operation as a floating-point value in USD.\n\nFormat example: apiCost: 0.0523 USD\n\nInput:\nPrevious agent (Azure_Synapse_To_Bigquery_Converter) output as input. \n\nFor Azure Synapse code stored in file: {{Synapse_code}}\n\nAzure_Synapse_To_Bigquery_Analyzer agent generated file: {{Analyzer_Output}}\n\nNote: Please give complete output, complete code and API cost with all other sections",
                        "expectedOutput": "Metadata requirements only once in the top of the output\n1. Test Case List:\nTest case ID\nTest case description\nExpected outcome\n2. Pytest Script for Each Test Case\n3. API Cost Estimation\n\nNote: Please give complete output, complete code and API cost with all other sections"
                    },
                    "maxIter": 3,
                    "maxRpm": 300,
                    "maxExecutionTime": 1800,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 10198,
                    "name": " Azure_Synapse_To_Bigquery_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "Automate the reconciliation process between Synapse stored procedures (original SQL logic) and BigQuery SQL (converted implementation) by generating test cases and BigQuery-based reconciliation scripts. This ensures that the converted BigQuery SQL produces results consistent with the original Synapse procedures, validating correctness, data consistency, and completeness at scale.",
                    "backstory": "As enterprises transition from Synapse stored procedures to BigQuery SQL for enhanced scalability, performance, and cost efficiency, ensuring the accuracy of SQL transformations becomes a critical challenge. Manual validation is often time-consuming, error-prone, and inefficient for large datasets. Automating this validation ensures data consistency and correctness without requiring manual intervention.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-03T08:25:06.946981",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "A complete, executable Python script that:\n- Metadata Requirements only once in the top of the output\n- Takes Synapse procedure logic and Bigquery sql code as inputs.\n\n- Performs all conversion and validation steps automatically.\n\n- Generates a clear reconciliation report showing the match status for each table.\n\n- Follows best practices for performance, security, and error handling.\n\n- Includes detailed comments explaining each section's purpose.\n\n- Can be run in an automated environment.\n\n- Returns structured results that can be easily parsed by other systems.\n\nAdditional Considerations:\n- The script must handle all edge cases, including:\n\n  - Data type mismatches\n\n  - NULL values\n\n  - String-case inconsistencies\n\n  - Large dataset performance\n\n- It should provide real-time status updates and generate comprehensive logs for troubleshooting.\n\nAPI Cost Estimation:\n- The script should also include the cost consumed by the API for this execution.\n\nNote: Please generate complete python script along with all section without fail.\n"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 2700,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 10162,
                    "name": " Azure_Synapse_To_Bigquery_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure the accuracy, completeness, and efficiency of the Synapse stored procedure code to Bigquery code conversion while maintaining consistency in data processing, business logic, and performance.",
                    "backstory": "As organizations convert traditional Synapse stored procedures into modern BigQuery SQL, it is crucial to ensure that the converted logic preserves the original functionality while taking advantage of BigQuery\u2019s scalability, performance, and integration capabilities. This validation is essential for maintaining business continuity, optimizing query performance, and enabling future scalability across data workloads.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-03T13:50:44.401675",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "=============================================\nAuthor:  AAVA\nCreated on:\nDescription:   <one-line description of the purpose>\n=============================================\nFor the description, provide a concise summary of what the document does.\nLeave the Created on field blank\n\n As a Senior Data Engineer, you will review the converted BigQuery code that was generated from Synapse stored procedures. Your objective is to ensure that the converted BigQuery code accurately replicates the logic and intent of the original stored procedures while leveraging BigQuery\u2019s distributed processing, integration, and performance features.\n\nINSTRUCTIONS:\n\nAnalyze the original Synapse stored procedure structure and data flow.\n\nReview the corresponding BigQuery code for each stored procedure.\n\nVerify that all data sources, joins, and destinations are correctly mapped in BigQuery.\n\nEnsure that all SQL transformations, aggregations, and business logic are accurately implemented in the BigQuery code (including any UDFs, scripting logic, or dataflow equivalents).\n\nCheck for proper error handling, exception management, and logging mechanisms in the BigQuery implementation.\n\nValidate that the BigQuery code follows best practices for query optimization and performance (e.g., appropriate use of partitioned/clustered tables, caching, materialized views, and optimized UDF usage).\n\nIdentify any potential improvements or optimization opportunities in the converted BigQuery logic.\n\nTest the BigQuery code with representative sample datasets to validate correctness.\n\nCompare the output of the BigQuery implementation with the original Synapse stored procedure output.\n\nAPI cost for this section\n\nINPUT:\n\nFor the input Synapse stored procedure file, use: {{Synapse_code}}\nAlso take the output of the \"Azure_Synapse_To_Bigquery_Converter\" agent\u2019s converted BigQuery code as input.",
                        "expectedOutput": "Metadata requirements only once in the top of the output\n1. Summary\n2. Conversion Accuracy\n3. Optimization Suggestions\n4. API Cost Estimation\n\nNote: Please add all mentioned sections or points without fail and don't include '*' and \"#'"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}