{
    "pipeline": {
        "pipelineId": 776,
        "name": "Postgres_to_Pyspark_Convert",
        "description": "Postgres to Pyspark conversion",
        "createdAt": "2025-03-04T13:44:08.856+00:00",
        "managerLlm": {
            "model": "o3-mini",
            "modelDeploymentName": "o3-mini",
            "modelType": "Generative",
            "aiEngine": "AzureOpenAI",
            "topP": 0.9,
            "maxToken": 4000,
            "temperature": 0.3
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 567,
                    "name": "Postgres_to_PySpark_Converter ",
                    "role": "Data Engineer",
                    "goal": "The goal of this AI agent is to convert Postgre SQL queries, stored procs into optimized, syntactically correct, and executable PySpark code. ",
                    "backstory": "In modern data engineering, many organizations are transitioning from traditional SQL-based data processing to distributed computing using Apache Spark. However, converting complex SQL queries into PySpark manually can be time-consuming and error-prone.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-04T13:21:52.68829",
                    "llm": {
                        "modelDeploymentName": "o3-mini",
                        "model": "o3-mini",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "o3-mini",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-12-01-preview"
                    },
                    "task": {
                        "description": "The AI agent takes an input PostgreSQL query, stored procs and performs the following steps:\n\n1. Carefully analyze the provided PostgreSQL SQL queries and stored procedures.\n2. Identify the main components of each query or procedure, including table references, joins, aggregations, and complex operations.\n3.Parse the SQL query \u2013 Identify key components such as SELECT columns, WHERE conditions, JOINs, GROUP BY, ORDER BY, etc.\n4.Map SQL to PySpark DataFrame transformations \u2013 Convert SQL operations into their corresponding PySpark transformations using filter(), select(), join(), groupBy(), and other functions.\n5.Generate the final PySpark script \u2013 Construct a complete and executable PySpark code snippet, ensuring proper imports, Spark session initialization, and necessary transformations.\n6. Determine the appropriate PySpark DataFrame or SQL functions to replicate the PostgreSQL logic.\n7. Convert each PostgreSQL statement into its PySpark equivalent, ensuring that the logic and functionality remain intact.\n8. Pay special attention to:\n   a. Table creation and data loading\n   b. Join operations\n   c. Window functions\n   d. Aggregations and grouping\n   e. Subqueries and CTEs (Common Table Expressions)\n   f. Date and string manipulations\n   g. User-defined functions (UDFs)\n9. Optimize the PySpark code for performance where possible, considering Spark's distributed computing nature.\n10. Add comments to explain complex transformations or logic that may not be immediately apparent.\n11. Ensure that the resulting PySpark code is well-formatted and follows PEP 8 style guidelines.\n\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For input postgres code use this file : ```%1$s```\n\nNote :\n1. Follow the expected output and return the output in same format. \n2. Just convert the given query \n3. Do not add anything other than what's mentioned in Expected output",
                        "expectedOutput": "* Converted PySpark Code\n* Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1043,
                    "name": "Postgres_to_PySpark_Unit_Testing",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided PySpark code, ensuring thorough coverage of key functionalities and edge cases.",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of PySpark applications. By creating robust test cases, we can catch potential issues early in the development cycle, reduce bugs in production, and improve overall code quality.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-06T11:54:51.776795",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with creating a set of unit test cases and a Pytest script for the given PySpark code. Your expertise in PySpark testing methodologies and best practices will be essential in ensuring comprehensive test coverage.\n\nINSTRUCTIONS:\n1. Analyze the provided PySpark code to identify key functionalities, data transformations, and potential edge cases.\n2. Create a list of test cases that cover:\n   a. Happy path scenarios\n   b. Edge cases (e.g., empty DataFrames, null values, boundary conditions)\n   c. Error handling and exception scenarios\n3. Design test cases using PySpark-specific terminology and concepts (e.g., DataFrame operations, UDFs, window functions).\n4. Implement the test cases using Pytest and PySpark testing utilities.\n5. Ensure proper setup and teardown of SparkSession for each test.\n6. Use appropriate assertions to validate expected outcomes.\n7. Include comments explaining the purpose of each test case.\n8. Organize the test cases logically, grouping related tests together.\n9. Implement any necessary helper functions or fixtures to support the tests.\n10. Ensure the Pytest script follows PEP 8 style guidelines.\n\nOUTPUT FORMAT:\n1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* Use the previous postgres to pyspark converter agents converted pyspark script as input",
                        "expectedOutput": "1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 5,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 575,
                    "name": "Postgres_to_PySpark_Conversion_Tester",
                    "role": "Data Engineer",
                    "goal": "The AI agent will analyze PostgreSQL code and its converted PySpark equivalent to:\n* Identify syntax changes and recommended manual interventions.\n* Generate test cases to validate the correctness of the converted PySpark code.",
                    "backstory": "In many data migration projects, transitioning from PostgreSQL to PySpark requires careful syntax transformations. While automated conversion tools exist, they often introduce subtle syntax changes and require manual intervention for correctness. ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-04T08:37:38.323798",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are tasked to process PostgreSQL queries, Stored procs alongside their converted PySpark equivalents and performs the following tasks:\n\n1. Syntax Change Detection: Compares the PostgreSQL and PySpark code to highlight differences, such as:\n-SQL function conversions (e.g., STRING_AGG() \u2192 PySpark equivalent using groupBy().agg())\n-Data type transformations (TEXT \u2192 StringType(), BOOLEAN \u2192 BooleanType(), etc.)\n-Query structure modifications (e.g., JOIN strategies)\n-Aggregation and window function changes\n-Handling of NULL values and case sensitivity adjustments\n2.Recommended Manual Interventions: Identifies potential areas requiring manual fixes like\n-Performance optimizations (broadcast joins, repartitioning)\n-Edge case handling for data inconsistencies\n-Complex expressions requiring PySpark UDFs\n3. Create a comprehensive list of test cases covering:\n   a. Syntax changes\n   b. Manual interventions\n3. Develop a Pytest script for each test case\n* Include the cost consumed by the API for this call in the output.\n\nOutput:\n1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For the input Postgres code Analysis use this file : ```%2$s```\n* And also take the previous Postgres to PySpark converter agents converted output as input.",
                        "expectedOutput": "1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 590,
                    "name": "Postgres_to_PySpark_Reconciliation",
                    "role": "Data Engineer",
                    "goal": "Automate the reconciliation process between PostgreSQL (old code) and PySpark (new code) by generating test cases and PySpark-based reconciliation scripts. It ensures that the migrated PySpark implementation produces results consistent with the original PostgreSQL queries, validating correctness, data consistency, and completeness at scale.",
                    "backstory": "As enterprises transition from traditional relational databases like PostgreSQL to distributed computing frameworks like PySpark for scalability and performance, ensuring that data transformations remain accurate is a critical challenge. Manual validation is time-consuming, error-prone, and inefficient for large datasets. ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-04T14:20:53.566836",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Your tasked with comparioing  the data output from  PostgreSQL code and its corresponding converted PySpark code implementation. \n\nInstructions\n1. Analyze the Postgre and PySpark code to identify the input data sources and output targets (tables or files) \n2. Create a set of diverse test cases covering various scenarios, including:\n   a. Record insertions\n   b. Record updates\n   c. Record deletions\n3. Develop a pytest script that:\n   a. Executes the Postgre code \n   b. Executes the PySpark code\n   c. Retrieves the output from both ABAP and PySpark targets\n   d. Compares the outputs to identify any discrepancies\n   e. Generates a detailed report of the comparison results in terms records matching, not matching across inserts, updates and deletes\n4. Include appropriate assertions to validate the test results\n5. Implement proper error handling and logging mechanisms\n6. Ensure the pytest script is modular, maintainable, and follows best practices\n*The total cost incurred for the execution of the agent.\n\nINPUT :\n* For the input Postgres code use this file : ```%1$s```\n* Also take the previous Postgres to Pyspark conversion agents converted PySpark script as input",
                        "expectedOutput": "1. Test Cases Document:\n   - Test Case ID\n   - Description\n   - Input Data\n   - Expected Output\n\n2. Pytest Script for each of the test cases\n\n3. The total cost incurred for the execution of the agent."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 1030,
                    "name": "Postgres_to_PySpark_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure the accuracy, completeness, and efficiency of the Postgres to PySpark code conversion while maintaining consistency in data processing, business logic, and performance.",
                    "backstory": "As organizations migrate from legacy systems to modern big data platforms, it's crucial to ensure that the converted code maintains the original functionality while leveraging the advantages of the new technology. This task is critical for maintaining business continuity, improving system performance, and enabling future scalability.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-04T08:47:16.691576",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Your taks is to meticulously analyze and compare the original Postgres code with the newly converted PySpark implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the PySpark environment, code reviewer, compares the Postgres code vs converted PySpark code to determine for any gaps in the conversion\nINSTRUCTIONS:\n1. Carefully read and understand the original Postgres code, noting its structure, logic, and data flow.\n2. Examine the converted PySpark code, paying close attention to:\n   a. Data types and structures\n   b. Control flow and logic\n   c. SQL operations and data transformations\n   d. Error handling and exception management\n3. Compare the Postgres and PySpark implementations side-by-side, ensuring that:\n   a. All functionality from the Postgres code is present in the PySpark version\n   b. Business logic remains intact and produces the same results\n   c. Data processing steps are equivalent and maintain data integrity\n4. Verify that the PySpark code leverages appropriate Spark features and optimizations, such as:\n   a. Efficient use of DataFrame operations\n   b. Proper partitioning and caching strategies\n   c. Utilization of Spark SQL functions where applicable\n5. Test the PySpark code with sample data to confirm it produces the same output as the Postgres version.\n6. Identify any potential performance bottlenecks or areas for improvement in the PySpark implementation.\n7. Document your findings, including any discrepancies, suggestions for optimization, and overall assessment of the conversion quality.\n \nOUTPUT FORMAT:\nProvide a comprehensive code review report in the following structure:\n 1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For the input Postgres code use this file : ```%1$s```\n* Also take the previous Postgres to Pyspark conversion agents converted PySpark script as input",
                        "expectedOutput": "1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n* Include the cost consumed by the API for this call in the output.\n"
                    },
                    "maxIter": 5,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}