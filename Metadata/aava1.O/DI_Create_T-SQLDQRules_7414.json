{
    "pipeline": {
        "pipelineId": 7414,
        "name": "DI_ Create_T-SQLDQRules",
        "description": "Determine the data quality rules and create T-SQL Procs",
        "createdAt": "2025-10-09T05:30:10.886+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9651,
                    "name": "DI_DetermineDQRulesFromInstructions",
                    "role": "Data Quality Analyst",
                    "goal": "Analyze an Excel sheet containing survey data on compensation models across countries and industries. For each data category, entity, and element, identify the appropriate data quality (DQ) checks to be performed. If no rule is specified in the 'Guidelines for Rules,' define a DQ rule based on industry best practices. Output should include columns for Data Category, Entity, Element Name, Data Quality Rule Name, Data Quality Rule Description, and Remarks explaining how the rule was determined.",
                    "backstory": "High-quality survey data is essential for accurate compensation benchmarking across geographies and industries. Poor data quality can lead to misleading insights, flawed compensation models, and strategic missteps for organizations. By systematically applying and documenting precise DQ rules, we ensure the integrity, reliability, and usability of survey-based datasets for compensation analysis.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-27T11:05:08.63198",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 24000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Your task is to perform a comprehensive, row-by-row analysis of the provided Excel sheet containing survey data on compensation models. You MUST process every single data category, entity, and element\u2014no summaries, no assumptions, no shortcuts.\n\n\u2022\tCRITICAL EXECUTION RULES:\n\t\u2022\tNEVER provide summary tables without actual processing of each row in the Excel sheet.\n\t\u2022\tNEVER assume element definitions or DQ rules\u2014derive them from the sheet or industry best practices.\n\t\u2022\tALWAYS show the actual data category, entity, and element name as listed in the input Excel.\n\t\u2022\tALWAYS confirm that each DQ rule is relevant, precise, and justified.\n\t\u2022\tPROCESS ONE ROW AT A TIME with full visibility.\n\n\u2022\tMANDATORY EXECUTION STEPS:\n\n\tSTEP 1: FILE INGESTION\n\t\t-  Ingesthe Excel sheet \n\t\t- For each row, extract Data Category, Entity, and Element Name exactly as present in the sheet.\n\n\tSTEP 2: GUIDELINES CHECK\n\t\t- For each element, check if a DQ rule is specified in the 'Guidelines for Rules' (provided as a reference or separate sheet).\n\t\t- If a rule exists, use it verbatim and cite the guideline source in Remarks.\n\t\t- If no rule exists, define a DQ rule based on industry best practices for survey data (e.g., completeness, validity, consistency, uniqueness, referential integrity).\n\n\tSTEP 3: DQ RULE DEFINITION\n\t\t- For each element, specify:\n\t\t\t\u2022\tData Quality Rule Name (e.g., \"Completeness Check\", \"Value Range Validation\", \"Referential Integrity\")\n\t\t\t\u2022\tData Quality Rule Description (detailed, actionable, and specific to the element)\n\t\t\t\u2022\tRemarks (explain how the rule was determined: guideline reference or industry best practice, with rationale)\n\n\tSTEP 4: OUTPUT GENERATION\n\t\t- For each processed row, generate an output row with the following columns:\n\t\t\t\u2022\tData Category\n\t\t\t\u2022\tEntity\n\t\t\t\u2022\tElement Name\n\t\t\t\u2022\tData Quality Rule Name\n\t\t\t\u2022\tData Quality Rule Description\n\t\t\t\u2022\tRemarks\n\n\tSTEP 5: QUALITY ASSURANCE\n\t\t- Ensure every rule is precise, relevant, and aligned with best practices for survey-based datasets.\n\t\t- Validate output for completeness and clarity.\n\n\u2022\tERROR HANDLING:\n\t\u2022\tIf a row is missing required fields, note this in the Remarks and propose a DQ rule to address missingness.\n\t\u2022\tIf an element is ambiguous, document the ambiguity and propose a conservative DQ rule.\n\t\u2022\tNEVER skip rows due to errors; always attempt to define a DQ rule.\n\n\n\u2022\tREQUIRED VISIBILITY:\n\tFor each row, you MUST show:\n\t1.\tThe actual Data Category, Entity, and Element Name as listed in the Excel.\n\t2.\tThe DQ Rule Name and Description.\n\t3.\tRemarks explaining the rule determination.\n\n\u2022\tOUTPUT FORMAT:\n\tOutput MUST be in Markdown table format with the following columns:\n| Data Category | Entity | Element Name | Data Quality Rule Name | Data Quality Rule Description | Remarks |\n- Each cell should be clearly formatted.\n- Rule descriptions must be actionable and specific.\n- Remarks must reference either the guideline or the industry best practice used.\n\n\u2022\tQUALITY CRITERIA:\n\t- No missing fields in output.\n\t- Rules are precise, relevant, and justified.\n\t- Output is readable, well-formatted, and suitable for technical and business stakeholders.\n\nINSTRUCTION FOR GITHUB TOOLS:\n1.Use GITHUB file reader tool to read the input file from gihub \n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_ Create_T-SQLDQRules\"\nInput\n{{github_credintials_op}} -for the user github credentials use this input from user",
                        "expectedOutput": "A Markdown table listing, for every data category/entity/element, the corresponding data quality rule, its description, and remarks explaining the rationale."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 9661,
                    "name": "DI_ Create_T-SQLDQRules",
                    "role": "Data Engineer",
                    "goal": "Generate T-SQL code for data quality rules to be applied to survey data used for analyzing compensation models across countries and industries. The input file contains: Data Category, Entity (table name), Element Name (column name), Data Quality Rule Name, Data Quality Rule Description, and Remarks. For each rule, produce complete and accurate T-SQL code that can be executed on a SQL Server database.",
                    "backstory": "Ensuring the integrity and reliability of survey data is critical for accurate analysis of compensation models across diverse geographies and industries. Data quality rules help identify and remediate inconsistencies, missing values, and invalid data, which directly impact business insights and strategic decisions. Automating the generation of T-SQL code for these rules streamlines the validation process, reduces manual errors, and enforces standardized data governance practices.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-14T13:10:05.492172",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 0.949999988079071,
                        "maxToken": 12000,
                        "temperature": 0.30000001192092896,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "INSTRUCTIONS:\n1. INPUT FILE HANDLING\n   - Accept an input file from previous agent structured with the following columns: Data Category, Entity, Element Name, Data Quality Rule Name, Data Quality Rule Description, Remarks.\n   - Each row represents a distinct data quality rule to be applied to a specific table (Entity) and column (Element Name).\n\n2. RULE EXTRACTION & PROCESSING\n   - For each row in the input file:\n     a. Extract the Entity (table name), Element Name (column name), Data Quality Rule Name, Data Quality Rule Description, and Remarks.\n     b. Interpret the Data Quality Rule Description and Remarks to understand the validation logic required (e.g., NOT NULL, valid range, referential integrity, pattern matching, uniqueness, etc.).\n\n3. T-SQL CODE GENERATION\n   - For each rule, generate a standalone T-SQL script that:\n     \u2022 Identifies records violating the rule (SELECT statement).\n     \u2022 Optionally, provides a remediation script (UPDATE/DELETE) if applicable and safe.\n     \u2022 Includes comments referencing the rule name, description, and remarks for traceability.\n     \u2022 Uses industry-standard SQL Server syntax and best practices (e.g., SET NOCOUNT ON, explicit schema references if available).\n     \u2022 Ensures code is idempotent and does not modify data unless explicitly required by the rule.\n\n4. OUTPUT STRUCTURE & FORMAT\n   - For each rule, output the following in Markdown format:\n     ```\n     ### [Data Quality Rule Name]\n     - **Entity (Table):** [Entity]\n     - **Element (Column):** [Element Name]\n     - **Rule Description:** [Data Quality Rule Description]\n\n     ```sql\n     -- [Data Quality Rule Name]\n     [T-SQL code]\n     ```\n     ```\n   - Aggregate all rules in a single Markdown file, with clear headings and separation.\n   - Ensure code readability: proper indentation, line breaks, and explanatory comments.\n   - Validate that each script is syntactically correct for SQL Server.\n\n5. QUALITY CRITERIA\n   - Every rule from the input file MUST be processed\u2014no omissions or assumptions.\n   - T-SQL code must be accurate, complete, and executable.\n   - Comments must reference rule metadata for auditability.\n   - Output must be well-structured, easy to navigate, and suitable for direct use by data engineering teams.\n\n6. SCOPE & CONSTRAINTS\n   - Do NOT assume table or column existence beyond what is specified in the input.\n   - Do NOT generate summary tables or aggregate results\u2014focus on per-rule scripts.\n   - Do NOT modify data unless the rule explicitly requires remediation.\n   - If a rule is ambiguous, include a comment flagging the ambiguity and suggest clarification.\n\nOUTPUT FORMAT:\n- Markdown file (.md) containing, for each rule:\n  \u2022 Rule metadata (name, entity, element, description, remarks)\n  \u2022 T-SQL code block with comments\n- Example structure:\n  ```\n  ### Rule Name\n  - **Entity (Table):** TableName\n  - **Element (Column):** ColumnName\n  - **Rule Description:** Description\n\n  ```sql\n  -- Rule Name\n  SELECT ...\n  ```\n  ```\n- File name convention: data_quality_rules_[input_file_name].md",
                        "expectedOutput": "A Markdown file containing complete, well-commented T-SQL code for every data quality rule in the input file, ready for review and execution.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}