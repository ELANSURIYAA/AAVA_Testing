{
    "pipeline": {
        "pipelineId": 6304,
        "name": "DI_Databricks_Fact_Data_Mapping_Gold_Layer",
        "description": "This workflow is for recommending and creating the gold Fact data mapping",
        "createdAt": "2025-08-25T06:24:26.662+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 8165,
                    "name": "DI_Databricks_Gold_Fact_Transformation_Recommender",
                    "role": "Data modeler",
                    "goal": "Analyze the Model Conceptual, Data Constraints, Silver Layer Physical DDL script, Gold Layer Physical DDL script, and Sample Data to generate comprehensive transformation rules specifically for Fact tables in the Gold layer.",
                    "backstory": "Data transformation for Fact tables is critical for maintaining accuracy, consistency, and completeness before they are used in analytical reporting. By automatically generating transformation rules for Fact tables, we ensure that key metrics, aggregations, and relationships are structured correctly, enriched with necessary data points, and aligned with reporting and performance optimization needs.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-21T06:16:41.747827",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks Gold Fact Transformation Recommender Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks Gold Fact Transformation Recommender followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks Gold Fact Transformation Recommender followed by a number), proceed to create the  Databricks Gold Fact Transformation Recommender  for the input file from the input directory. The Databricks Gold Fact Transformation Recommender instructions and structure are given below. Once generated, store the Databricks Gold Fact Transformation Recommender in the output directory with the file name  Databricks_Gold_Fact_Transformation_Recommender_1.md.\n \nThe agent must:\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks Gold Fact Transformation Recommender containing the sections listed in **Databricks Gold Fact Transformation Recommender Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Gold_Fact_Transformation_Recommenderr_1.md.\n*The output file should properly in the md format including md formatted Tables and headings\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Gold Fact Transformation Recommender Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks Gold Fact Transformation Recommender file in GitHub output directory with the Databricks_Gold_Fact_Transformation_Recommender_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Gold_Fact_Transformation_Recommender_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Gold_Fact_Transformation_Recommender}}\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Gold_Fact_Transformation_Recommender_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Gold Fact Transformation Recommender Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\nYou will read the Model Conceptual, Data Constraints, Silver Layer Physical DDL script, Gold Layer Physical DDL script, and Sample Data and generate transformation rules only for Fact tables.\n\nINSTRUCTIONS:\n\n1. Parse the Silver Layer DDL script to extract only Fact tables and their column definitions.\n2. Analyze the Model Conceptual and Data Constraints to identify business-critical metrics, aggregations, and necessary transformations for Fact tables.\n3. Inspect Sample Data to detect patterns, outliers, and potential standardization requirements for Fact metrics.\n4. Generate transformation rules for Fact tables, including:\n* Metric Standardization: Ensure key measures are formatted correctly and align with business KPIs.\n* Fact-Dimension Mapping: Ensure foreign keys link correctly to Dimension tables (e.g., surrogate keys, natural key mapping).\n* Data Aggregation Rules: Define pre-aggregations (e.g., monthly/quarterly summaries) where applicable.\n* Normalization and Standardization: Ensure consistency in numeric values (e.g., currency conversions, percentage calculations, rounding rules).\n* Handling Missing or Invalid Data: Define strategies for handling NULL values, missing records, or outlier data points.\n5. Provide SQL transformations for each rule, ensuring alignment with the Silver Layer schema.\n6. Ensure traceability of transformations by linking each rule back to its source from the Model Conceptual, Data Constraints, and Silver Layer schema to Gold Layer.\n\nOUTPUT FORMAT:\n1. Transformation Rules for Fact Tables:\n* [Rule Name]: [Description]\n    - Rationale: [Explanation]\n    - SQL Example: [Sample SQL transformation]\n\n2. Ensure API cost consumption is included in the output, explicitly reporting the cost as a floating-point value in USD (e.g., apiCost: actual cost).",
                        "expectedOutput": "\n**Mode 1 Output**:\n* Display the Databricks Gold Fact Transformation Recommender output\n* And store the Databricks_Gold_Fact_Transformation_Recommender in the GitHub output directory with the file name as `Databricks_Gold_Fact_Transformation_Recommender_<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Gold Fact Transformation Recommender output\n* And store the updated Databricks Gold Fact Transformation Recommender output in the GitHub output directory with the file name as `Databricks_Gold_Fact_Transformation_Recommender_<next_version>.md` \u2014 Updated Databricks Gold Fact Transformation Recommender with requested changes applied, preserving structure and formatting.\n"
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 8169,
                    "name": "DI_Databricks_Gold_Fact_Transformation_Data_Mapping",
                    "role": "Data modeler",
                    "goal": "Create a comprehensive data mapping for Fact tables in the Gold Layer, incorporating necessary transformations, validations, and aggregation rules.",
                    "backstory": "This task ensures that Fact tables in the Gold Layer are correctly structured, linked to their corresponding Dimension tables, and validated for high data quality. A well-defined Fact model supports accurate analytics, optimized performance, and consistency in business intelligence applications.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-21T07:01:11.535974",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "datamappingoutputformat",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks Gold Fact Transformation Data Mapping Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks Gold Fact Transformation Data Mapping underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks Gold Fact Transformation Data Mapping followed by a number , proceed to create the  Databricks Gold Fact Transformation Data Mapping  for the input file from the input directory. The Databricks Gold Fact Transformation Data Mapping instructions and structure are given below. Once generated, store the Databricks Gold Fact Transformation Data Mapping in the output directory with the file name  Databricks_Gold_Fact_Transformation_Data_Mapping_1.md.\n \nThe agent must:\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks Databricks Gold Fact Transformation Data Mapping containing the sections listed in **Databricks Gold Fact Transformation Data Mapping Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Gold_Fact_Transformation_Data_Mapping_1.md.\n*The output file should properly in the md format including md formatted Tables and headings\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Gold Fact Transformation Data Mapping Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks Gold Fact Transformation Data Mapping file in GitHub output directory with the Databricks_Gold_Fact_Transformation_Data_Mapping_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks Gold Fact Transformation Data Mapping_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Gold_Fact_Transformation_Data_Mapping}}\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Gold_Fact_Transformation_Data_Mapping_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Databricks Gold Fact Transformation Data Mapping Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n \nYou are tasked with creating a detailed data mapping specifically for Fact tables in the Gold Layer. This mapping will incorporate necessary transformations, aggregations, validations, and cleansing rules at the attribute level.\nYour work will be based on the Silver and Gold Layer physical model DDL scripts and the previous Fabric Gold Fact Transformation Recommender Agents' recommendations.\n\nINSTRUCTIONS:\n\n1. Review the provided Silver and Gold Layer physical model DDL scripts.\n2. Create a detailed data mapping for Fact tables from the Silver to Gold Layer, ensuring:\n* Fact-Dimension Relationships: Define foreign key mappings to Dimension tables.\n* Metric Calculations and Aggregations: Define SUM, AVG, COUNT, MAX, MIN, etc., where applicable.\n* Data Validation Rules: Ensure consistency in numeric and date fields (e.g., rounding rules, null handling).\n* Cleansing Logic: Handle missing values, duplicates, currency conversions, and ensure unit standardization (e.g., grams to kilograms, meters to kilometers).\n3. Ensure all transformations and rules are compatible with PySpark and Databricks.\n4. Include explanations for complex calculations, transformations, and business rules.\n\nInputs:\n* Also take input from previous Databricks Gold Fact Transformation Recommender Agent\u2019s output recommendations as input\n\n\nExpected Output\n1. Overview: Summary of the data mapping approach and key considerations.\n2. Data Mapping for Fact Tables:\nThe mapping output should be in tabular format with the following fields for each Fact table and its columns:\n* Target Layer: Gold\n* Target Table: Proper table name as per the Gold Layer DDL script\n* Target Field: Proper field name as per the Gold Layer DDL script\n* Source Layer: Silver\n* Source Table: Proper table name as per the Silver Layer DDL script\n* Source Field: Proper field name as per the Silver Layer DDL script\n* Validation Rule: Required validation rules from the Data constrains file\n* Transformation Rule: Required transformation rules from the previous Databricks Gold Fact Transformation Recommender Agents' output recommendations (e.g., metric calculations, normalization, aggregation).\n3. Ensure API cost consumption is included in the output, explicitly reporting the cost as a floating-point value in USD (e.g., apiCost: actual cost).",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Databricks Gold Fact Transformation Data Mapping output\n* And store the Databricks_Gold_Fact_Transformation_Data_Mapping in the GitHub output directory with the file name as `Databricks_Gold_Fact_Transformation_Data_Mapping_<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Gold Fact Transformation Data Mapping output\n* And store the updated Databricks Gold Fact Transformation Data Mapping output in the GitHub output directory with the file name as `Databricks_Gold_Fact_Transformation_Data_Mapping_next_version>.md` \u2014 Updated Databricks Gold Fact Transformation Data Mapping with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 8125,
                    "name": "DI_Databricks_Gold_Model_Reviewer",
                    "role": "Data modeler",
                    "goal": "This agent is to evaluate and validate the physical data model for alignment with reporting requirement and compatibility with Databricks and PySpark.\n",
                    "backstory": "Evaluate and validate the physical data model for alignment with reporting requirements , source data structure, and compatibility with Databricks and PySpark.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-25T04:54:56.12927",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "The agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the input data.\n* Identify the Reviewer file in GitHub output directory with the actual input file name Databricks_Gold_Model_Reviewer_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).if file is already exist in the output directory with some version number then generate the newer output and Save the updated file to the same GitHub output directory with the with the actual input file name Databricks_Gold_Model_Reviewer_next incremented version number (e.g., `_4`).\nif the file is not exist then save the output file name should be the actual input file name, followed by _Reviewer_1.md.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Reviewer containing the sections listed in **Reviewer Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and File present in the github input directory: `{{GitHub_Details_For_Databricks_Gold_Model_Reviewer}}`\n \n## **Reviewer Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n \n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the Input or workflow does.\n \n---\n\nYou are tasked with thoroughly evaluating the physical data model and associated DDL scripts. Give a green tick mark \u2705 if it\u2019s correctly implemented and a red tick mark \u274c for missing or incorrectly implemented. Your evaluation should cover multiple aspects to ensure the model's quality, completeness, and compatibility.\n\nINSTRUCTIONS:\n1.\tReview the provided physical data model and DDL scripts.\n2.\tCompare the model against the reporting requirements or model conceptual:\na. Identify all required data elements.\nb. Confirm all required tables and columns from the Gold Layer are present and correctly structured.\nc. Check for appropriate data types and sizes.\nd. Verify correct categorization of Facts, Dimensions, and Code tables, ensuring accurate data modeling.\n3.\tAnalyze the model's alignment with the source data structure:\na. Ensure all source data elements are accounted for.\nb. Verify that data transformations are correctly represented.\nc. Validate all aggregations, calculations, and business rules for accuracy and PySpark compatibility.\n4.\tAssess the model for adherence to best practices:\na. Check for proper normalization.\nb. Evaluate indexing strategies.\nc. Review naming conventions and consistency.\nd. Confirm inclusion of load_date, update_date, source_system columns and verify audit/error tables for tracking data issues.\n5.\tIdentify any missing requirements or inconsistencies in the model.\n6.\tEvaluate the DDL scripts for compatibility with Microsoft Fabric and Spark:\na. Verify syntax compatibility.\nb. Check for any unsupported data types or features.\n7.\tDocument any deviations from best practices or potential optimizations.\n8.\tProvide recommendations for addressing identified issues or improvements.\n9.\tAttached knowledge base file containing all the unsupported features in Microsoft Fabric. You need to verify that the output DDL script does not include any unsupported features mentioned in the knowledge base file.\n\nOUTPUT FORMAT:\nProvide a comprehensive evaluation report in the following structure:\n1.\tAlignment with Conceptual Data Model\n1.1 \u2705 Green Tick: Covered Requirements\n1.2 \u274c Red Tick: Missing Requirements\n2.\tSource Data Structure Compatibility\n2.1 \u2705 Green Tick: Aligned Elements\n2.2 \u274c Red Tick: Misaligned or Missing Elements\n3.\tBest Practices Assessment\n3.1 \u2705 Green Tick: Adherence to Best Practices\n3.2 \u274c Red Tick: Deviations from Best Practices\n4.\tDDL Script Compatibility\n4.1 Microsoft Fabric Compatibility\n4.2 Spark Compatibility\n4.3 Used any unsupported features in Microsoft Fabric\n5.\tIdentified Issues and Recommendations\n6.\tapiCost: float // Cost consumed by the API for this call (in USD)\nExpected Output:\nProvide a comprehensive evaluation report in the following structure:\n1.\tAlignment with Conceptual Data Model\n1.1 \u2705 Green Tick: Covered Requirements\n1.2 \u274c Red Tick: Missing Requirements\n2.\tSource Data Structure Compatibility\n2.1 \u2705 Green Tick: Aligned Elements\n2.2 \u274c Red Tick: Misaligned or Missing Elements\n3.\tBest Practices Assessment\n3.1 \u2705 Green Tick: Adherence to Best Practices\n3.2 \u274c Red Tick: Deviations from Best Practices\n4.\tDDL Script Compatibility\n4.1 Microsoft Fabric Compatibility\n4.2 Spark Compatibility\n4.3 Used any unsupported features in Microsoft Fabric\n5.\tIdentified Issues and Recommendations\n6.\tapiCost: float // Cost consumed by the API for this call (in USD)\n\n\n",
                        "expectedOutput": "\n**Mode 1 Output**:\n\n* Display the Databricks_Gold_Model_Reviewer output\n\n* And store the Databricks_Gold_Model_Reviewer output in the GitHub output directory with the file name as `Databricks_Gold_Model_Reviewer<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n\n* Display the updated Databricks_Gold_Model_Reviewer output\n\n* And store the updated Reviewer output in the GitHub output directory with the file name as Databricks_Gold_Model_Reviewer_<next_version>.md` \u2014 Updated Reviewer with requested changes applied, preserving structure and formatting.\n\n\n"
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}