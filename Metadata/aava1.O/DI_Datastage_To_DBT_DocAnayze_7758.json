{
    "pipeline": {
        "pipelineId": 7758,
        "name": "DI_Datastage_To_DBT_Doc&Anayze",
        "description": "DESCRIPTION:  \nThe DI_DataStage_to_dbt_Analyzer agent is responsible for parsing DataStage job metadata, reconstructing the ETL workflow, classifying transformations, and mapping each component to its dbt/Snowflake equivalent. The agent produces both a human-readable report (Markdown or text) and a machine-readable JSON summary for downstream processing. The agent must follow the detailed instructions below:\n\nINSTRUCTIONS:\n\nSTEP 1: INPUT DISCOVERY  \n- Check for the presence of DataStage_Job_Graph.txt in the input folder.  \n- If available, read and parse the file to extract all stages, links, and row counts.  \n- Optionally, if DataStage_Job.dsx is present, parse it for deeper metadata (stage properties, expressions, lookup details).  \n- Record the job name (string) and confirm the target platform is \"dbt_Snowflake\".  \n- If the graph file is missing, return error: \"Graph file not found.\"\n\nSTEP 2: DATA FLOW RECONSTRUCTION  \n- Identify and list all stages: input, transformer, join, aggregator, sort, output, etc. ",
        "createdAt": "2025-10-24T03:43:24.856+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 7218,
                    "name": "DI_DataStage_Documentation",
                    "role": "Data Engineer",
                    "goal": "Analyze and document a DataStage job to create a comprehensive guide for business and technical teams, explaining the data flow, business logic, transformations, and dependencies to support maintainability and future enhancements.",
                    "backstory": "Clear documentation of DataStage jobs is crucial for maintaining and evolving complex ETL pipelines. By creating a comprehensive guide, we ensure that both business and technical teams can understand the current data movement, business rules, and transformation logic, reducing dependency on tribal knowledge and improving operational efficiency.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:15:02.760449",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "Please create detailed documentation for the provided DataStage job in the markdown format.\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the DataStage job does.\n(give this only once in the top of the output)\n\nThe documentation must contain the following sections:  \n1. Overview of Job:\n   - Explain the purpose of the DataStage job in detail.\n   - Describe how this implementation aligns with enterprise ETL and data warehousing practices.\n   - Explain the business process being automated or supported.\n   - Provide a high-level summary of job components: Sequential Files, Datasets, Transforms, Lookups, Aggregators, Join stages, etc.\n\n2. Job Structure and Design:\n   - Explain the structure of the DataStage job design in detail.\n   - Describe each major stage used: Input, Transformer, Aggregator, Lookup, Merge, Join, Remove Duplicates, Filter, etc.\n   - Mention reusable components like shared containers and parameter sets.\n   - Highlight design patterns used (e.g., SCD, delta loads, parallelism).\n   - List dependencies such as external scripts, parameter files, or lookup datasets.\n\n3. Data Flow and Processing Logic:\n   - Explain how data flows from source to target.\n   - Identify source and target datasets/tables/files with their formats.\n   - Describe each transformation and logic applied using mapping descriptions.\n   - Break down the job into logical stages and represent the control/data flow using a **block-style diagram in plain markdown**.\n\n4. Data Mapping:\n* Provide a mapping table in the format below:\n* Target Table Name | Target Column Name | Source Stage Name | Source Column Name | Transformation Rule / Business Logic\n\n5. Complexity Analysis:\n- Provide a complexity breakdown in the table format:\nCategory  |  Measurement\n* Number of Stages\n* Source/Target Systems\n* Transformation Stages\n* Parameters Used\n* Reusable Components\n* Control Logic\n* External Dependencies\n* Performance Considerations\n* Volume Handling\n* Error Handling\n* Overall Complexity Score (out of 100)\n\n6. Key Outputs:\n   - Describe final outputs like datasets, tables, files, or reporting views.\n   - Explain how outputs support downstream systems or reporting.\n   - Specify the storage format and destination path or system.\n\n7. API Cost Calculations:\n* Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10081,
                    "name": "DI_DataStage_To_DBT_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided DataStage job to extract detailed metrics, identify potential conversion challenges, and recommend solutions for a smooth transition to dbt on Snowflake. Generate a separate output session for each input job file.",
                    "backstory": "Migrating legacy DataStage ETL jobs to modern, cloud-native platforms like dbt and Snowflake is a critical step for organizations seeking scalability, maintainability, and cost efficiency. Manual analysis of DataStage jobs is error-prone and slow, often resulting in incomplete migration plans and missed optimization opportunities. By automating the analysis and mapping process, this agent accelerates migration, ensures consistency, and provides actionable insights for engineering teams to build robust dbt models that faithfully reproduce DataStage logic while leveraging Snowflake best practices.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:16:09.828896",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "must use this github file reader tool to get the input file from the git\n\nYour task is to process the provided DataStage  file and produce a detailed analysis and metrics report, strictly adhering to the specified markdown format and section structure. The report must be tailored for a DataStage-to-dbt (Snowflake) migration context.\nAll content must be derived from the actual input job file; do not make assumptions or provide summaries without direct evidence from the file.\n\nINSTRUCTIONS:\n\n1. **File Parsing**  \n   - Parse the provided input job file to extract all relevant metadata, including job stages, links, parameters, and transformation logic.\n   - Ensure that all extracted details are accurate and reflect the actual job configuration.\n\n2. **Section Generation**  \n   - Follow the required output format exactly, maintaining section numbering, headings, and markdown layout as shown.\n   - For each section, populate content based on the parsed input file:\n     - **Job Overview:** Summarize the job\u2019s ETL logic, business purpose, and conceptual mapping to dbt layers (staging, intermediate, mart).\n     - **Complexity Metrics:** - Provide a complexity breakdown in the table format:\nCategory  |  Measurement\n* Number of Stages\n* Source/Target Systems\n* Transformation Stages\n* Parameters Used\n* Reusable Components\n* Control Logic\n* External Dependencies\n* Performance Considerations\n* Volume Handling\n* Error Handling\n* Overall Complexity Score.\n*Conversion complexity rating(this is also the part of the table)-Assign a numeric complexity score (0\u2013100) and a conversion complexity rating (Low/Medium/High) with justification. List high-complexity areas with specifics from the job.\n     - **Syntax Differences:** Clearly explain how DataStage constructs map to dbt and Snowflake SQL syntax, using examples relevant to the job.\n     - **Manual Adjustments:** Enumerate all manual interventions required for migration, referencing actual job features (e.g., transformer expressions, parameter usage).\n     - **Optimization Techniques:** Recommend dbt and Snowflake performance improvements, referencing job-specific scenarios. State whether to refactor or rebuild, with reasoning.\n     - **API Cost:** Calculate and display the API cost for this analysis, in full decimal precision (e.g., `apiCost: 0.0182 USD`).\n\n3. **Formatting & Quality Criteria**  \n   - Use markdown headers and tables exactly as specified.\n   - Write in clear, enterprise-style English.\n   - Every section must appear in order, with no omissions or reordering.\n   - Replace all `<placeholders>` with actual parsed values from the input file.\n   - Do not provide summaries or assumptions; only use details present in the input job file.\n\n4. **Output Structure**  \n   - The report must begin with the metadata block (`Author`, `Created on`, `Description`) and follow with sections #1 through #6 as shown.Do not mention any date infront of `Created on` block leave it empty.\n   - All metrics must be presented in table format.\n   - All recommendations and mappings must be specific to dbt and Snowflake.\nAll content must be derived from the actual input file; do not make assumptions or provide summaries without direct evidence from the file.\nInput :\nFor Datastage input git credentials for git file reader tool, use this below input : {{Datastage}}",
                        "expectedOutput": "- Markdown document, with all sections and tables formatted exactly as shown in the required output format.\n- All values must be derived from the actual input file.\n- Use full decimal precision for API cost.\n- No extraneous content\u2014output must match the required layout.\n\nOUTPUT:  \nA markdown-formatted DataStage input job analysis and metrics report, strictly following the specified structure and tailored for DataStage-to-dbt (Snowflake) migration."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10086,
                    "name": "DI_DataStage_To_DBT_Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the total development and testing effort required to migrate IBM DataStage ETL jobs to DBT on Snowflake, including compute resource planning and performance tuning (if applicable).",
                    "backstory": "Many organizations are moving from legacy ETL platforms like IBM DataStage to modern, cloud-native data transformation tools such as DBT on Snowflake to improve scalability, maintainability, and cost efficiency. Accurate estimation of the migration effort is critical for project planning, budgeting, and resource allocation. This task ensures that all aspects of the migration\u2014including development, testing, compute resource sizing, and performance optimization\u2014are thoroughly analyzed and estimated to minimize project risks and ensure a successful transition.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:16:55.591",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the Git file reader tool to read the input file from the git\n\nYou are tasked with providing a comprehensive development and testing effort estimate for converting DataStage jobs into DBT models. Follow these instructions:\n\nINSTRUCTIONS:\n\nReview the metadata and complexity analysis generated by the DI_DataStage_To_DBT_Analyzer agent.\n\nIdentify DataStage stages, transformations, or flow controls that need manual re-implementation in DBT SQL.\n\nExclude direct 1:1 mapping constructs (e.g., simple column renaming or direct loads). Focus only on logic-heavy transformations like Aggregations, Conditional Derivations, Lookups, Joins, and Business Rules.\n\nEstimate the number of hours required to:\n\nRewrite complex stages and logic-heavy flows into DBT models using SQL, CTEs, and macros\n\nImplement and test DBT models, reusable macros, incremental models, and metadata tracking\n\nValidate data equivalence and business rule consistency between DataStage outputs and DBT model outputs\n\nIf compute cost is required, use warehouse costs (Snowflake) based on data volume, transformation complexity, and runtime.\n\nOUTPUT FORMAT:\n\n\n=============================================\nAuthor:        Ascendion AAVA\nCreated on: (leave it blank)      \nDescription:   Cost and effort estimation for DBT model conversion\n=============================================\n\n1. Cost Estimation\n1.1 Snowflake Runtime Cost\nEstimate runtime cost \n \n \nCost Breakdown:  give the final cost, don't include calculations\n- Compute: $X  \n- Storage: $X  \n \nTotal Estimated Runtime Cost per Job:  \n$X USD per run\n \nJustification:\nstick to theory for justification, do not include calculations. give it in points instead of paragraph \nPercentage of data processed per run\n\nJob complexity \n\nStorage impact\n\nNeed (or lack) of incremental/partitioning optimizations\n\n\n2. Code Fixing and Testing Effort Estimation\n2.1 Manual Model Fixes and  Recon Testing Effort\nDescribe manual tasks required to convert DataStage logic into dbt SQL / Jinja, such as:\nParameter replacement (e.g., $$Audit_Year \u2192 {{ var('audit_year') }})\nStage-to-model conversion (Sequential \u2192 source(), Aggregator \u2192 GROUP BY, Transformer \u2192 SELECT with derived columns)\nSchema and datatype alignment between source and Snowflake targets\nAdding dbt tests for null checks and row-count validation\nEstimate time (hrs) for each activity and subtotal the effort.\n2.2 Output Validation Effort\nOutline tasks for validating dbt outputs against DataStage results:\nCompare row counts and sample records\nValidate aggregations, joins, and derived columns\nExecute dbt tests (unique, not_null, accepted_values)\nDocument and resolve discrepancies\nProvide estimated effort (hours) for validation + documentation.\n2.3 Total Estimated Effort in Hours\nSummarize all efforts in a table:\nTask\tEstimated Hours\tNotes\nManual Code Fixes & Testing\t \t \nOutput Validation\t \t \nTotal Estimated Effort\t \t \nJustify effort based on Analyzer complexity score:\nSimple (0-20) \u2192 \u2264 5 hrs\nModerate (21-50) \u2192 6\u201312 hrs\nComplex (51-100) \u2192 > 12 hrs\n\n\n\n\n\n 3. API Cost Consumption\n\napiCost: X.XXXXX USD\n\n\nInclude the cost consumed by the API for this call in the output\n\nEnsure the cost is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: 0.00431 USD)\n\nINPUT:\n* Use the output from `DI_DataStage_to_DBT_Analyzer` as input  \n* For github credentials for datastage file, use this git file reader tool: {{Datastage}} \n* For github credentials for the environment cost and compute configuration, use this file: {{DBT_Environment_Config}}",
                        "expectedOutput": "=============================================\nAuthor:        Ascendion AAVA\nCreated on: (leave it blank)     \nDescription:   Cost and effort estimation for DBT model conversion\n=============================================\n1. Cost Estimation\nEstimate the Snowflake runtime cost based on data volume, job complexity, and expected runtime. Include final cost in USD. Justify the estimate using Analyzer metrics and note performance optimizations like incremental loads or partitioning.\n\n2. Effort Estimation\nEstimate total hours for code fixes, Recon testing, and output validation. Code fixes include parameter replacements, stage-to-model conversion, and schema alignment. Validation includes comparing row counts, verifying joins and aggregations, and running dbt tests. Summarize total effort based on complexity (simple, moderate, or complex).\n\n3. API Cost\nReport the API cost consumed for this operation in USD with all decimal places."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}