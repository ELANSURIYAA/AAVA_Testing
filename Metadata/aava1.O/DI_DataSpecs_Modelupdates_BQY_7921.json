{
    "pipeline": {
        "pipelineId": 7921,
        "name": "DI_DataSpecs_Modelupdates_BQY",
        "description": "Includes tech spec creation, functional testing with SQL scripts, and DDL-based model updates.",
        "createdAt": "2025-10-31T17:40:00.303+00:00",
        "managerLlm": {
            "model": "gemini-2.5-pro",
            "modelDeploymentName": "gemini-2.5-pro",
            "modelType": "Generative",
            "aiEngine": "GoogleAI",
            "topP": 0.95,
            "maxToken": 32000,
            "temperature": 0.2,
            "gcpProjectId": "genai-platform-431215",
            "gcpLocation": "us-central1"
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 10295,
                    "name": "DI_Data_Technical_Specification_BQY",
                    "role": "Data Engineer",
                    "goal": "Develop a detailed technical specification document based on the provided inputs, outlining code changes, data model updates, and source-to-target mapping with transformation rules.  ",
                    "backstory": "This task is critical for ensuring seamless integration of the new source table into the existing data pipeline and data models. A well-defined technical specification will serve as the blueprint for developers and stakeholders, reducing ambiguity, ensuring alignment, and maintaining data integrity across the system. The specification will also facilitate efficient implementation of the enhancement and help prevent downstream issues during deployment.  ",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-31T17:38:12.710559",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "The agent must create a comprehensive technical specification document based on the provided inputs:  \n- JIRA story file  \n- Confluence context file  \n- DDL file for the new source table  \n- Existing source data model  \n- Existing target data model  \n\nThe specification should include:  \n1. **Code Changes Required for the Enhancement:**  \n   - Identify the specific areas in the codebase that need modification.  \n   - Detail the logic and functionality changes required to incorporate the new source table.  \n   - Include pseudocode or code snippets where applicable.  \n\n2. **Updates to the Data Models:**  \n   - Analyze the existing source and target data models.  \n   - Define the updates required to integrate the new source table into the models.  \n   - Ensure consistency and alignment between the source and target models.  \n\n3. **Source-to-Target Mapping:**  \n   - Provide a detailed mapping of fields from the new source table to the target data model.  \n   - Include any transformation rules or business logic required for the mapping.  \n\n**INSTRUCTIONS:**  \n1. **Context and Background Information:**  \n   - Review the JIRA story file to understand the business requirements and objectives.  \n   - Refer to the Confluence context file for additional project details and constraints.  \n   - Analyze the DDL file to understand the structure and schema of the new source table.  \n   - Examine the existing source and target data models to identify dependencies and relationships.  \n\n2. **Scope and Constraints:**  \n   - Ensure the specification aligns with the business requirements outlined in the JIRA story.  \n   - Maintain compatibility with existing systems and processes.  \n   - Adhere to data governance and security standards.  \n\n3. **Process Steps to Follow:**  \n   - Step 1: Extract relevant details from the provided files.  \n   - Step 2: Identify code changes required for the enhancement, including impacted modules and functions.  \n   - Step 3: Define updates to the source and target data models, ensuring logical consistency.  \n   - Step 4: Create a detailed source-to-target mapping, including transformation rules.  \n   - Step 5: Format the technical specification document as per industry standards.  \n4. **OUTPUT FORMAT:**  \n   - **Format:** Markdown  \n   - **Structure Requirements:**  \n- **Metadata Requirements:**\n-=============================================\n-Author: Ascendion AVA+\n-Date: <Leave it blank>\n-Description: <one-line description of the purpose >\n-============================================= \n     - Title: Technical Specification for [Enhancement Name]  \n     - Sections:  \n       - Introduction  \n       - Code Changes  \n       - Data Model Updates  \n       - Source-to-Target Mapping  \n       - Assumptions and Constraints  \n       - References  \n     - Use headings, subheadings, and bullet points for clarity.  \n   - **Quality Criteria:**  \n     - Clear and concise language.  \n     - Logical flow and organization.  \n     - Accurate and complete mapping and transformation rules.  \n   - **Formatting Needs:**  \n     - Use tables for source-to-target mapping.  \n     - Include diagrams for data model updates (if applicable).  \nPoints to Remember:\nRemember Must use the Github File Writer Tool to upload the File in the Github Repo for the Environment Details for github take that from the input\nRemember for the branch input use it as \"main\" and conent is what you give as output save the file as .md format\n\n\nAll the Inputs (Jira Stories, Confluence Documentation, Source Data Model, Target Data Model) are available in the zip folder that is uploaded. *****The input file names are provided in {{Technical_Specifications}}\n* GitHub repo details : {{GitHub_Repo_Details}}\n",
                        "expectedOutput": "****MASKED****"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10296,
                    "name": "DI_Model_Changes_BQY",
                    "role": "Data Engineer",
                    "goal": "To automate and systematize the detection, generation, and documentation of schema changes between the current BigQuery datasets/tables and updated technical specifications.\nThe BigQuery Data Model Evolution Automation (DMEA) framework ensures that schema evolution is:\n\n**Traceable:\nEach schema change (additions, modifications, deletions) is versioned and logged with change metadata in a centralized BigQuery audit dataset.\n\n**Auditable:\nChange history is fully recorded, enabling governance teams to track who initiated changes, when, and why \u2014 leveraging Cloud Audit Logs and metadata tables.\n\n**Minimally Disruptive:\nSchema migrations are simulated and validated against staging datasets before deployment, ensuring compatibility and zero data loss.\n\n**Backed with DDL and Rollback Scripts:\nAutomated generation of BigQuery DDL (e.g., ALTER TABLE, CREATE TABLE) and rollback SQL scripts for reversible schema updates.\n\n**Fully Documented:\nAll detected changes and resulting schemas are automatically published to a centralized Data Catalog or documentation repository (e.g., in Markdown, JSON, or Dataform format) for transparency across development and governance teams.",
                    "backstory": "Evolving BigQuery datasets and table schemas to accommodate new product features, regulatory updates, or refactored data pipelines is often a manual and error-prone process. This can lead to:\n\n**Data quality degradation from unvalidated schema changes\n**Broken downstream dependencies in Looker Studio, Dataform, or dbt models\n**Inconsistent or outdated schema documentation across environments\n**Missed audit and compliance requirements for data governance\n\nTo address these challenges, the BigQuery Data Model Evolution Agent (DMEA) was developed as an intelligent intermediary between existing BigQuery schemas and new technical specifications.\nDMEA functions as a schema diffing engine, validator, and DDL generator, automating the detection and safe implementation of schema changes. It keeps BigQuery data models agile, auditable, and governed\u2014ensuring every schema evolution is consistent, reversible, and fully documented.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-31T17:36:57.798548",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "DMEA accepts two primary inputs:\n**The existing data model (ER diagrams, DDLs, JSON schemas, etc.)\n**The new technical specifications (manually entered or taken as input from upstream agents like TSA - Technical Specification Agent)\nIt performs the following stages:\n1. **Model Ingestion\n**Parse and map existing model: tables, fields, constraints, relationships\n**Build internal representations for current schema using graph/tree formats\n**Supports relational (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB), and modern lakehouse (Delta Lake, Snowflake, BigQuery) models\n2. **Spec Parsing & Mapping\n    Normalize inputs from tech specs to structural requirements\n    Detect:\n         **Additions (new tables/columns/indexes)\n         **Modifications (type changes, nullable, constraints)\n         **Deprecations (dropping columns, soft deletes)\n    Infer indirect changes (e.g., changed business rule implies a column constraint update)\n3. **Delta Computation\n    Compare existing vs desired model\n    Categorize deltas:\n        **New tables/fields\n        **Changed column types/nullability\n        **Added/removed constraints\n        **Modified indexes/PKs\n    Compute version bump impact (patch/minor/major)\n4. **Impact Assessment\n    --Downstream break detection (views, ETL jobs, APIs)\n    --Data loss risk (e.g., narrowing column types, dropping constraints)\n    --Foreign key ripple effects\n    --Platform-specific caveats (e.g., PostgreSQL vs MySQL)\n5. **DDL/Alter Statement Generation\n    --Forward DDLs:\n        CREATE TABLE, ALTER TABLE, ADD CONSTRAINT, DROP COLUMN\n    --Rollback support\n    --Optional zero-downtime deployment (via COPY, rename strategies)\n    --Index rebalancing if necessary\n    --Optional data migration scripts (e.g., populate new tables from old ones)\n6. **Documentation\n    --Side-by-side diff of model (before vs after)\n    --Full DDL logs with change reasons\n    --Visual diagrams (ERD updates)\n    --Change traceability matrix (tech spec section \u2192 DDL line)\n\nThe inputs for this agent which is technical specification and existing DDL's :\n--existing DDL's file names are mentioned in {{Data_Model_Delta}}\n--Take the technical specification requirement from the first agent named ------\"\"\"\"DI_Data_Technical_Specification_BQY\"\"\"\" ",
                        "expectedOutput": "****MASKED****"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10298,
                    "name": "DI_Functional_Test_Cases_BQY",
                    "role": "Data Validation Specialist",
                    "goal": " Create detailed and specific functional test cases based on technical specifications derived from Jira, ensuring comprehensive coverage of all requirements and edge cases. ",
                    "backstory": "Functional test cases are critical for ensuring the quality and reliability of the software product. By deriving test cases from Jira technical specifications, the team can validate that the application meets all requirements and performs as expected in various scenarios, including edge cases. Comprehensive test coverage reduces the risk of defects slipping into production, improves user satisfaction, and ensures compliance with project deliverables.  ",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-31T17:35:59.347714",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "****MASKED****"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}