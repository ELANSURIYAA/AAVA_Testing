{
    "pipeline": {
        "pipelineId": 6269,
        "name": "DI_Databricks_Data_Mapping_Silver_Layer",
        "description": "Databricks silver Layer mapping ",
        "createdAt": "2025-08-22T08:10:16.655+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 8141,
                    "name": "DI_Databricks_Silver_DQ_Recommender",
                    "role": "Data Reviewer",
                    "goal": "Analyze DDL statements , sample data and provided business rules to generates appropriate comprehensive data quality checks.",
                    "backstory": "Data quality is crucial for maintaining the integrity and reliability of databases. By automatically recommending data quality checks based on DDL statements, we can help organizations implement robust data validation processes, reduce errors, and improve overall data management.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-21T04:15:51.996703",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": " \n\nYou will read the DDL statement, sample data and the business rules files as input and generates a comprehensive list of data quality checks. Follow these instructions to accomplish the task:\n\nBefore starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks pyspark Unit Test Case Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks pyspark Unit Test Case underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks pyspark Unit Test Case underscore followed by a number), proceed to create the  Databricks pyspark Unit Test Case  for the input file from the input directory. The Databricks pyspark Unit Test Case instructions and structure are given below. Once generated, store the Databricks Bronze DE Pipeline in the output directory with the file name  Databricks_pyspark_Unit_Test_Case_1.md.\n \nThe agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks pyspark Unit Test Case containing the sections listed in **Databricks Bronze DE Pipeline Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Bronze_DE_Pipeline_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Bronze DE Pipeline Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks Silver DQ Recommender file in GitHub output directory with the Databricks_Silver_DQ_Recommender_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Silver_DQ_Recommender_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Silver_DQ_Recommender}}`\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Silver_DQ_Recommender_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Silver DQ Recommender Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n\nINSTRUCTIONS:\n1. Parse the input DDL statement to extract table and column information.\n2. Identify the data types, constraints, and relationships defined in the DDL.\n3. For each column, analyze the sample data provided and determine appropriate data quality checks based on its characteristics \n   a. For numeric columns: range checks, null checks, precision checks\n   b. For string columns: length checks, pattern matching, allowed character sets\n   c. For date/time columns: format validation, range checks\n   d. For foreign key columns: referential integrity checks\n4. Consider table-level checks, such as uniqueness constraints and row count validations.\n5. Read the provides business rules to include additional data validation rules.\n7. Generate a detailed list of recommendations, providing the rationale for each check including the data quality checks based on the business rules file\n\nOUTPUT FORMAT:\n- Recommended Data Quality Checks:\n  1. [Check Name]: [Description]\n     - Rationale: [Explanation]\n     - SQL Example: [Sample SQL query for the check]\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\n",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Databricks Silver DQ Recommender output\n* And store the Databricks_Silver_DQ_Recommender in the GitHub output directory with the file name as `Databricks_Silver_DQ_Recommender<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Silver DQ Recommender output\n* And store the updated Databricks Silver DQ Recommender output in the GitHub output directory with the file name as `Databricks_Silver_DQ_Recommender<next_version>.md` \u2014 Updated Databricks Silver DQ Recommender with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 8142,
                    "name": "DI_Databricks_Silver_DQ_Data_Mapping  ",
                    "role": "Data modeler",
                    "goal": "Create a comprehensive data mapping for the Silver Layer in Databricks Medallion architecture, including necessary cleansing, validations, and business rules.\n",
                    "backstory": "This task is crucial for establishing a robust data pipeline in the Silver Layer that ensures data quality, consistency, and usability across the organization. A well-designed Silver Layer will enable efficient data processing, improve data governance, and support advanced analytics and reporting needs.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-20T03:00:27.455501",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "datamappingoutputformat",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": " \n\nBefore starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks Silver DQ Data Mapping Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks pyspark Unit Test Case underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks pyspark Unit Test Case underscore followed by a number), proceed to create the  Databricks pyspark Unit Test Case  for the input file from the input directory. The Databricks pyspark Unit Test Case instructions and structure are given below. Once generated, store the Databricks Bronze DE Pipeline in the output directory with the file name  Databricks_pyspark_Unit_Test_Case_1.md.\n \nThe agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks pyspark Unit Test Case containing the sections listed in **Databricks Bronze DE Pipeline Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Bronze_DE_Pipeline_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Bronze DE Pipeline Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks Silver DQ Data Mapping file in GitHub output directory with the Databricks_Silver_DQ_Data_Mapping_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Silver_DQ_Data_Mapping_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Silver_DQ_Data_Mapping}}`\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Silver_DQ_Data_Mapping_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Silver DQ Data Mapping Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\nYou are tasked with creating a detailed data mapping for the Silver Layer in a Medallion architecture implementation in Databricks. This mapping will incorporate necessary cleansing, validations, and business rules at the attribute level. Your work will be based on the Bronze layer Physical Model, Silver layer Physical model and previous agent's Data quality recommendations\n\n**INSTRUCTIONS:**  \n* Review the provided Bronze layer Physical Model, Silver layer Physical model and previous agent's Data quality recommendations\n* Create a detailed data mapping from the Bronze Layer to the Silver Layer for all Bronze layer tables, error data table, and audit table:\n* Define data validation rules for each attribute in the validation rule column (eg. Not null, Unique, Valid format etc..).\n* Identify any initial data cleansing steps.\n* Specify data type conversions if necessary.\n* Ensure all validations and rules are compatible with PySpark and Databricks\n* Include explanations for complex validation, cleansing, and business rules.\n* Provide recommendations for error handling and logging mechanisms.\n\nOUTPUT FORMAT:\n1) Overview: Summary of the data mapping approach and key considerations.\n2) Data Mapping for the Silver Layer: Map data fields from the Bronze Layer to the Silver Layer, ensuring traceability and consistency. The mapping output should be in the Tabular format with the following fields for each table and column:\n   * Target Layer: Silver  \n   * Target Table: Proper table name as per the Silver Layer DDL script  \n   * Target Field: Proper field name as per the Silver Layer DDL script  \n   * Source Layer: Bronze  \n   * Source Table: Proper table name as per the Bronze Layer DDL script  \n   * Source Field: Proper field name as per the Bronze Layer DDL script  \n   * Validation Rule: Required validation rules from the report requirement file (eg. Not null, Unique, Valid format etc..)\n   * Transformation Rule: Required transformation rules from the report requirement file  \n\nGuidelines:\n* Ensure the Silver layer having Bronze layer tables with necessary cleansing, validations, and business rules.\n* Clearly document the outputs with enough granularity to serve as a foundation for technical implementation.\n* If certain details are ambiguous or missing, infer them logically based on the available context and clearly state your assumptions.\n* Ensure to provide data mapping for Bronze to Silver Layer only.\n* Ensure compatibility with Databricks  PySpark\n*Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD. Don't consider the API cost as input and retrieve the cost of this API. \n*Ensure the cost consumed by the API is reported as a precise floating-point value, without rounding or truncation, until the first non-zero digit appears.\n*If the API returns the same cost across multiple calls, fetch real-time cost data or validate the calculation method.\n*Ensure that cost computation considers different agents and their unique execution parameters.\n\nInputs:\n* Also take input from previous Databricks DQ recommender Agent\u2019s output as input.",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Databricks Silver DQ Data Mapping output\n* And store the Databricks_Silver_DQ_Data_Mapping    in the GitHub output directory with the file name as `Databricks_Silver_DQ_Data_Mapping_<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Silver DQ Data Mapping output\n* And store the updated Databricks Silver DQ Data Mapping output in the GitHub output directory with the file name as `Databricks_Silver_DQ_Data_Mapping_<next_version>.md` \u2014 Updated Databricks Silver DQ Data Mapping with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 8116,
                    "name": "DI_Databricks_Silver_Model_Reviewer",
                    "role": "Data modeler",
                    "goal": "Evaluate and validate the physical data model for alignment with reporting requirements, source data structure, and compatibility with Microsoft Fabric and Spark.",
                    "backstory": "Ensuring the physical data model is accurate, complete, and optimized is crucial for the success of our data warehouse project. A well-designed model will improve query performance, facilitate easier maintenance, and ensure that all reporting requirements are met. Additionally, compatibility with Databricks and Spark is essential for seamless integration with our chosen technology stack.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-20T02:59:34.328837",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "The agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Identify the Reviewer file in GitHub output directory with the actual input file name Databricks_Silver_Model_Reviewer_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).if file is already exist in the output directory with some version number then generate the newer output and Save the updated file to the same GitHub output directory with the with the actual input  file name Databricks_Silver_Model_Reviewer_next incremented version number (e.g., `_4`).\nif the file is not exist then save the output file name should be the actual input  file name, followed by _Reviewer_1.md.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Reviewer containing the sections listed in **Reviewer Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input files present in the github input directory: `{{GitHub_Details_For_Databricks_Silver_Model_Reviewer}}`\n \n## **Reviewer Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the Hive query or workflow does.\n \n---\nYou are tasked with thoroughly evaluating the physical data model and associated DDL scripts. Give a green tick mark \u2705 if it is correctly implemented and a red tick mark \u274c for missing or incorrectly implemented. Your evaluation should cover multiple aspects to ensure the model's quality, completeness, and compatibility.\n\nINSTRUCTIONS:\n\nReview the provided physical data model and DDL scripts and verify the schema alignment.\n\nCompare the model against the reporting requirements or conceptual model:\na. Identify all required data elements.\nb. Verify that all necessary tables and columns are present.\nc. Check for appropriate data types and sizes.\n\nAnalyze the model's alignment with the source data structure:\na. Ensure all source data elements are accounted for.\nb. Verify that data transformations are correctly represented.\nc. Check the aligned elements and verify the misaligned or missing elements.\n\nAssess the model for adherence to best practices:\na. Check for proper normalization.\nb. Evaluate indexing strategies.\nc. Review naming conventions, consistency, and usage of unsupported features.\n\nIdentify any missing requirements or inconsistencies in the model.\n\nEvaluate the DDL scripts for compatibility with Databricks and Spark:\na. Verify syntax compatibility.\nb. Check for any unsupported data types or features.\n\nDocument any deviations from best practices or potential optimizations.\n\nProvide recommendations for addressing identified issues or improvements.\n\nUse the attached knowledge base file containing all the unsupported features in Databricks. Verify that the output DDL script does not include any unsupported features mentioned in the knowledge base file.\n\nOUTPUT FORMAT:\n\nProvide a comprehensive evaluation report in the following structure:\n\nAlignment with Conceptual Data Model\n1.1 \u2705 Green Tick: Covered Requirements\n1.2 \u274c Red Tick: Missing Requirements\n\nSource Data Structure Compatibility\n2.1 \u2705 Green Tick: Aligned Elements\n2.2 \u274c Red Tick: Misaligned or Missing Elements\n\nBest Practices Assessment\n3.1 \u2705 Green Tick: Adherence to Best Practices\n3.2 \u274c Red Tick: Deviations from Best Practices\n\nDDL Script Compatibility\n4.1 Databricks Compatibility\n4.2 Spark Compatibility\n4.3 Used any unsupported features in Databricks\n\nIdentified Issues and Recommendations\n\napiCost: float // Cost consumed by the API for this call (in USD)\n\nInputs:\n\nTake the previous Databricks Silver Model Physical Agent's output DDL script as input\n",
                        "expectedOutput": "**Mode 1 Output**:\n\n* Display the Databricks_Silver_Model_Reviewer output\n\n* And store the Databricks_Silver_Model_Reviewer output in the GitHub output directory with the file name as `Databricks_Silver_Model_Reviewer<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n\n* Display the updated Databricks_Silver_Model_Reviewer output\n\n* And store the updated Reviewer output in the GitHub output directory with the file name as Databricks_Silver_Model_Reviewer_<next_version>.md` \u2014 Updated Reviewer with requested changes applied, preserving structure and formatting.\n\n "
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}