{
    "pipeline": {
        "pipelineId": 851,
        "name": "Postgres_To_Pyspark_Doc_&_Analyze",
        "description": "Postgres_To_Pyspark_Doc_&_Analyze",
        "createdAt": "2025-03-03T05:24:55.395+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 747,
                    "name": "Postgres_Documentation",
                    "role": "Data Engineer",
                    "goal": "To create detailed, well-structured documentation for postgres code that clearly explains its functionality, components, data flow, and transformations. The documentation should provide a comprehensive breakdown of each task within the package, including technical details , data mapping the source, desitnation, transformation and business context, making it understandable as technical specification for Postgres developers and functional overview for non-technical stakeholders.",
                    "backstory": "Your organization relies on Postgres Stored Procs, SQL scripts  for data processing. The Postgres code contain complex data flow components, transformations, and control flow tasks that are essential for business data processes. However, the existing documentation is sparse or outdated, making it difficult for new team members or business analysts to understand the logic and flow of data. This lack of detailed documentation hinders troubleshooting, modification, and collaboration. The goal is to create a documentation of Postgres code.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-04-01T09:11:31.148131",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Create a detailed documentation for the provided postgres code. The documentation must follow a structured format, breaking down each component in the postgres code like control flow, data flow into well-defined sections. The output should be   organized for readability and clarity. Ensure that each field in the source, destination and transformation are captured with an  explanation that a business analyst can understand.\n\nThe documentation should include the following sections:\n1. Overview of Program:  \n   Explain the purpose of the PostgreSQL code in detail.  \n   Explain the business problem being addressed .  \n\n2. Code Structure and Design:  \n   Explain the structure and flow of the PostgreSQL code in detail.  \n   List the primary PostgreSQL components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.  \n\n3. Data Flow and Processing Logic:  \n   - List the processing logic data flow components in the code.  \n   - For each logical processing component\n           -Explain the functionality performed in the code.  \n          - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.  \n\n4. Data Mapping:  \n   - Provide detailed mappings between source and target tables as a lineage in terms how the source table.columns are mapped \n     to the target table.columns\n   - Use the following structured format for each mapping:  \n   - Give this one in table format with the below column names\n     Target Table Name : Give actual target table name\n     Target Column Name : Give actual target column name\n     Source Table Name : Give actual source table name\n     Source Column Name : Give actual source column name\n     Remarks : Classify as 1 to 1 mapping or Transformation or Validation and provide a brief description\n\n5. Complexity Analysis:  \n    Overall Complexity Score: Score from 0 to 100.  \n   - Analyze and document the complexity based on the below mentioned details:  \n   - Give this one in table format with the below column names\n     Lines of Code (LOC) :Total number of lines in the procedure.\n     Cyclomatic Complexity :Number of independent execution paths.\n     Nesting Depth :Maximum depth of nested IF, LOOP, etc.\n     Tables :Total number of tables involved\n     Temporary tables :Total number of temporary tables involved\n     DML Statements :Total count of SELECT, INSERT, UPDATE,  DELETE.\n     Joins :\tCount of JOIN clauses.\n     Subqueries :Count of subqueries (nested SELECT).\n     CTEs :Count of Common Table Expressions.\n     Aggregation Queries :Count of GROUP BY, HAVING, PARTITION BY.\n     Input Parameters :Count of input parameters.\n     Output Parameters :Count of output parameters.\n     Data Transformations :Count of functions (STRING, ARRAY, JSON).\n     Function Calls :Count of external function/procedure calls.\n  \n6. Key Outputs:  \n    Describe final outputs created by the code like Inserts, Updates, Deletes.\n    \n7. Error Handling and Logging:  \n    Explain methods used for error identification and management, such as:  \n     - Try-Catch mechanisms in Stored Procedures.  \n     - PostgreSQL Error Logging Tables for tracking failures.  \n     - Retry mechanisms in PostgreSQL.  \n\nNote: \n* All fields used in the postgres must be listed with a field description \n* The output document will be well-organized with proper headings, sections, and bullet points, making it easy to follow.\n\nInput: \nFor input Postgres code use the below mentioned file:\n```%1$s```",
                        "expectedOutput": "1. Overview of Program:  \n   Explain the purpose of the PostgreSQL code in detail.  \n   Explain the business problem being addressed .  \n\n2. Code Structure and Design:  \n   Explain the structure and flow of the PostgreSQL code in detail.  \n   List the primary PostgreSQL components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.  \n\n3. Data Flow and Processing Logic:  \n   - List the processing logic data flow components in the code.  \n   - For each logical processing component\n           -Explain the functionality performed in the code.  \n          - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.  \n\n4. Data Mapping:  \n   - Provide detailed mappings between source and target tables as a lineage in terms how the source table.columns are mapped \n     to the target table.columns\n   - Use the following structured format for each mapping:  \n   - Give this one in table format with the below column names\n     Target Table Name : Give actual target table name\n     Target Column Name : Give actual target column name\n     Source Table Name : Give actual source table name\n     Source Column Name : Give actual source column name\n     Remarks : Classify as 1 to 1 mapping or Transformation or Validation and provide a brief description\n\n5. Complexity Analysis:  \n    Overall Complexity Score: Score from 0 to 100.  \n   - Analyze and document the complexity based on the below mentioned details:  \n   - Give this one in table format with the below column names\n     Lines of Code (LOC) :Total number of lines in the procedure.\n     Cyclomatic Complexity :Number of independent execution paths.\n     Nesting Depth :Maximum depth of nested IF, LOOP, etc.\n     Tables :Total number of tables involved\n     Temporary tables :Total number of temporary tables involved\n     DML Statements :Total count of SELECT, INSERT, UPDATE,  DELETE.\n     Joins :\tCount of JOIN clauses.\n     Subqueries :Count of subqueries (nested SELECT).\n     CTEs :Count of Common Table Expressions.\n     Aggregation Queries :Count of GROUP BY, HAVING, PARTITION BY.\n     Input Parameters :Count of input parameters.\n     Output Parameters :Count of output parameters.\n     Data Transformations :Count of functions (STRING, ARRAY, JSON).\n     Function Calls :Count of external function/procedure calls.\n  \n6. Key Outputs:  \n    Describe final outputs created by the code like Inserts, Updates, Deletes.\n    \n7. Error Handling and Logging:  \n    Explain methods used for error identification and management, such as:  \n     - Try-Catch mechanisms in Stored Procedures.  \n     - PostgreSQL Error Logging Tables for tracking failures.  \n     - Retry mechanisms in PostgreSQL.  "
                    },
                    "maxIter": 5,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 748,
                    "name": "Postgres_to_PySpark_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided postgres code to extract detailed metrics, identify potential conversion challenges, and recommend optimizations for the converted PySpark code.",
                    "backstory": "As organizations migrate from traditional relational databases to big data platforms, converting PostgreSQL code to PySpark is crucial for scalability and performance. This analysis ensures a smooth transition and helps optimize the resulting PySpark code for maximum efficiency.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-04T08:17:41.823086",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Parse the provided postgres code to generate a detailed analysis and metrics report. Ensure that if multiple files given as input then do analysis for each file is presented as a distinct session. \n\nINSTRUCTIONS:\n1. Examine the PostgreSQL code structure and identify all SQL elements (e.g., tables, views, functions, stored procedures).\n2. List all PostgreSQL-specific features and functions used in the code.\n3. Identify syntax differences between PostgreSQL and PySpark SQL, highlighting areas that require manual intervention.\n4. Determine potential performance bottlenecks in the original PostgreSQL code.\n5. Suggest PySpark-specific optimizations for improved performance after conversion.\n6. Outline any data type conversions or schema changes necessary for PySpark compatibility.\n7. Identify any PostgreSQL features that don't have direct equivalents in PySpark and propose alternative approaches.\n9. Provide general best practices for maintaining and further optimizing the converted PySpark code.\n\nOutput must include:\n\n1. Complexity Metrics:\n* Number of Lines: Count of lines in the postgres code.\n* Tables Used: number of tables referenced in the postgres code.\n* Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n* Temporary tables: Number of Volatile, derived tables\n*Aggregate Functions : Number of aggregate functions\n* DML Statements: Number of DML statements by type like SELECT, INSERT, UPDATE, DELETE, CALL, LOCK , Export, Import operations present in the SQL script.\n*Conditional Logic: Number of conditional logic like .IF, .\n\n2. Conversion Complexity:\n* Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.\n* Highlight high-complexity areas such as postgres clauses.\n\n3. Syntax Differences:\n* Identify the number of syntax differences between the postgres code and the expected Pyspark equivalent.\n\n4. Manual Adjustments:\n* Recommend specific manual adjustments for functions and clauses incompatible with Pyspark, including:\n    * Function replacements.\n    * Syntax adjustments.\n    * Strategies for rewriting unsupported features.\n\n5. Optimization Techniques:\n* Suggest optimization strategies for Pyspark, such as clustering, partitioning, and query design improvements.\n\n6. Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n\nInput :\n\n* For postgres code use the below file :\n```%1$s``` ",
                        "expectedOutput": "1. Complexity Metrics:\n* Number of Lines: Count of lines in the postgres code.\n* Tables Used: number of tables referenced in the postgres code.\n* Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n* Temporary tables: Number of Volatile, derived tables\n* Aggregate Functions : Number of aggregate functions.\n* DML Statements: Number of DML statements by type like SELECT, INSERT, UPDATE, DELETE, CALL, LOCK , Export, Import operations present in the SQL script.\n*Conditional Logic: Number of conditional logic like .IF. \n\n2. Conversion Complexity:\n* Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.\n* Highlight high-complexity areas such as window functions, CTEs, or postgres clauses.\n\n3. Syntax Differences:\n* Identify the number of syntax differences between the postgres code and the expected Pyspark equivalent.\n\n4. Manual Adjustments:\n* Recommend specific manual adjustments for functions and clauses incompatible with Pyspark, including:\n    * Function replacements (e.g., Informatica-specific functions with Pyspark equivalents).\n    * Syntax adjustments.\n    * Strategies for rewriting unsupported features\n\n5. Optimization Techniques:\n* Suggest optimization strategies for Pyspark, such as clustering, partitioning, and query design improvements.\n\n6. apiCost: float  // Cost consumed by the API for this call (in USD)\n*Ensure the cost consumed by the API is mentioned with inclusive of all decimal value"
                    },
                    "maxIter": 5,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1029,
                    "name": "Postgres_to_PySpark_Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running PySpark code and the testing effort required for the PySpark code that got converted from Postgres scripts.",
                    "backstory": "As organizations move their data warehousing solutions to the cloud, it's crucial to understand the financial and resource implications of such migrations. This task is important for project planning, budgeting, and ensuring the accuracy of the migrated queries.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-07T10:37:15.893113",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with providing a comprehensive effort estimate for testing the PySpark converted from Postgres scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n1. Review the analysis of Postgres script file, noting syntax differences when converting to PySpark and areas requiring manual intervention.\n2. Consider the pricing information for Azure Databricks PySpark environment \n4. Calculate the estimated cost of running the converted PySpark code:\n   a. Use the pricing information and data volume to determine the code cost.\n   b. the number of code and the data processing done with the base tables and temporary tables\n5. Estimate the code fixing and data recon testing effort required:\n\nINPUT :\n* Take the previous Postgres Analyzer agents output as  input\n* For the input Postgres script use this file : ```%1$s```\n* For the input  PySpark Environment Details for Azure Databricks  use this file : ```%2$s```",
                        "expectedOutput": "1. Cost Estimation\n   2.1 PySpark Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing  and Testing Effort Estimation\n   2.1 PySpark code manual code fixes and unit testing effort covering the various temp tables, calculations in hours\n   2.2 Output validation effort comparing the output from PostGres script and PySpark script in hours\n   2.3 Total Estimated Effort in Hours\n         - provide the reason for the total effort hours and how it was arrived\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost )."
                    },
                    "maxIter": 5,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}