{
    "pipeline": {
        "pipelineId": 6090,
        "name": "DI_PySpark_Doc&Analyse_Json",
        "description": "PySpark Asset Riontionaliser Workflow",
        "createdAt": "2025-12-16T14:06:11.085+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 7999,
                    "name": "DI_PySpark_Documentation_Json",
                    "role": "Data Engineer",
                    "goal": "Analyze and document a PySpark script to generate comprehensive, structured documentation in JSON format that serves both business and technical teams. This documentation should explain the logic, data transformations, dependencies, complexity, and outputs in a machine-readable, sectioned JSON format.",
                    "backstory": "PySpark scripts can be highly optimized yet complex to understand. There's often a gap between how distributed data transformations are implemented in PySpark and how the business understands them. This prompt ensures documentation captures the complete technical logic and business relevance \u2014 usable for onboarding, refactoring, automation, and governance.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-14T03:02:40.905268",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Description\nAnalyze the provided PySpark script to generate detailed, script-specific documentation. This should be accurate, readable, and context-rich JSON suitable for both technical and non-technical audiences.\n\nStructure:\n{\n  \"1. Overview of Program\": \"<5-line paragraph + business context and system integration summary>\",\n  \"2. Business Purpose of Outputs\": \"<How the outputs support business needs, integration, monitoring, validation>\",\n  \"3. Code Structure and Design\": \"<5-line intro + detailed description of structure, components, transformations, RDD/DataFrame operations>\",\n  \"4. Data Flow and Processing Logic\": {\n    \"Processed Datasets\": [\"<list all DataFrame or RDD names used>\"],\n    \"Data Flow\": \"<5-line intro + data journey from input to output, including key PySpark transformations and actions>\"\n  },\n  \"5. Data Mapping\": [\n    {\n      \"Target Dataset Name\": \"<dataset>\",\n      \"Target Field Name\": \"<field>\",\n      \"Source Dataset Name\": \"<dataset>\",\n      \"Source Field Name\": \"<field>\",\n      \"Data Type\": \"<Data Type>\",\n      \"Transformation Logic\": \"<PySpark transformation or function applied>\",\n      \"Business Purpose\": \"<explanation>\"\n    }\n  ],\n  \"6. Complexity Analysis\": {\n    \"Number of Lines\": <integer>,\n    \"Datasets Used\": <integer>,\n    \"Joins Used\": \"<Join types used or 'None'>\",\n    \"PySpark Transformations\": \"<count of transformations>\",\n    \"User-Defined Functions (UDFs)\": \"<count>\",\n    \"OUTPUT Targets\": <count>,\n    \"Conditional Logic\": <count>,\n    \"Filters or Aggregations\": <count>,\n    \"Function Calls or Reusable Modules\": \"<count>\",\n    \"Performance Controls\": \"<list if any (e.g., caching, partitioning)>\",\n    \"External Dependencies\": \"<list external data sources or APIs>\",\n    \"Overall Complexity Score\": <0\u2013100 integer>\n  },\n  \"7. Performance Optimization Strategies\": \"<5-line intro + paragraph on performance techniques (caching, broadcast joins, filter pushdown, partitioning, etc.)>\",\n  \"8. Technical Elements and Best Practices\": \"<5-line intro + paragraph on modular design, naming conventions, logging, and error handling in PySpark>\",\n  \"9. Key Outputs\": [\n    \"<brief description of each key output DataFrame, file, or table>\"\n  ]\n}\n\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}\n\n",
                        "expectedOutput": "Expected Output\n\nOutput must be strict JSON, starting with { and ending with }\n\nNo markdown, no triple backticks\n\nKeep file names with extensions, do not include folder paths\n\nEvery dataset or component must be explicitly listed\n\nAll sections in markdown-style headings within the JSON keys (e.g., 1. Overview of Program)\n\nNo generic text \u2014 tie all insights directly to the provided PySpark logic\n\nThis format is designed to be UI/automation ready"
                    },
                    "maxIter": 15,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 8000,
                    "name": "DI_PySpark_Analyser_Json",
                    "role": "Data Engineer",
                    "goal": "Analyze an Informatica Cloud mapping file to extract detailed transformation metrics, assess logical complexity, identify migration barriers, and recommend precise solutions for a successful migration to PySpark. Output should be a structured and detailed JSON suitable for engineering analysis, tooling, and project planning.",
                    "backstory": "Informatica Cloud is a powerful ETL platform widely used for orchestrating business-critical data pipelines. However, with the shift toward open-source, scalable, and cloud-native platforms like PySpark, organizations increasingly seek to migrate their ETL workloads for cost reduction, better control, and performance at scale.\n\nThese migrations are not trivial \u2014 Informatica's drag-and-drop transformations, reusable logic, implicit typing, and UI-bound behavior do not directly translate to PySpark. Therefore, there\u2019s a strong need for a deep pre-migration analysis that evaluates compatibility, quantifies migration complexity, and maps out all required refactoring efforts.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-14T03:04:53.899036",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "This agent analyzes an Informatica Cloud mapping definition (JSON/XML format or exported .txt). It parses components, transformation flows, reusable logic, and output structure, then compiles a migration readiness report. The goal is to surface:\n* Which features are incompatible with PySpark\n* How to manually adjust/refactor them\n* What level of effort and logic redesign is required\n* What optimization strategies are applicable in PySpark\nIt outputs a detailed JSON document with 7 well-defined sections, enabling engineering teams to plan the migration, assign effort, and de-risk execution.\n\nFinal Output:\nStructured JSON (no markdown, no backticks), strictly conforming to:\n{\n  \"1. Script Overview\": { ... },\n  \"2. Complexity Metrics\": { ... },\n  \"3. Feature Compatibility Check\": { ... },\n  \"4. Manual Adjustments for PySpark Migration\": { ... },\n  \"5. Optimization Techniques in PySpark\": { ... },\n  \"6. Cost Estimation\": { ... },\n  \"7. Code Fixing and Data Recon Testing Effort Estimation\": { ... },\n  \"8. API Cost\": { \"apiCost\": \"<float in USD>\" }\n}\n\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}",
                        "expectedOutput": "Detailed analyzed report in JSON format"
                    },
                    "maxIter": 15,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 8001,
                    "name": "DI_PySpark_Plan_Json",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running PySpark jobs and the testing effort required for PySpark code, and produce the output in a structured JSON format.",
                    "backstory": "When developing or maintaining large-scale PySpark pipelines, understanding both the runtime infrastructure cost and the testing effort is critical. This includes estimating cloud resource usage for production runs and evaluating the effort required for validating functional correctness, performance, and data accuracy.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-14T03:09:03.058274",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with providing a comprehensive effort and cost estimate for executing and testing a PySpark script. Use any provided PySpark performance metrics, code review notes, or profiling outputs to guide your estimation.\n\nINSTRUCTIONS:\n\nReview the provided PySpark analysis output to:\n\nIdentify complex constructs (e.g., UDFs, nested transformations, multi-stage joins, wide aggregations).\n\nDetermine sections requiring logic validation or performance refactoring.\n\nFocus especially on shuffle-heavy operations, wide transformations (GROUP BY, ROLLUP), and output writes.\n\nEstimate the effort hours for:\n\nManual intervention and optimization of complex constructs.\n\nData reconciliation and validation testing (pre/post execution output comparison, intermediate dataset checks).\n\nSyntax differences when adapting to different Spark environments.\n\nApplying performance optimization techniques.\n\nDo not count effort for simple syntax or formatting-level edits.\n\nEstimate the PySpark runtime cost by:\n\nCalculating resource requirements (executors, memory, vCPU usage, shuffle size).\n\nMapping execution profiles to cloud pricing models (e.g., Dataproc, EMR, Spark on Kubernetes).\n\nConsidering both temporary and final datasets and their read/write overheads.\n\nInclude the API cost separately if applicable.\n\nJSON Output Template\n\n{\n  \"1. Cost Estimation\": {\n    \"1.1 PySpark Runtime Cost\": {\n      \"Cluster Configuration\": {\n        \"Number of Executors\": \"<number>\",\n        \"Executor Memory\": \"<size in GB>\",\n        \"Driver Memory\": \"<size in GB>\"\n      },\n      \"Approximate Data Volume Processed\": {\n        \"Input Data\": \"<estimated size and notes>\",\n        \"Output Data\": \"<estimated size and notes>\"\n      },\n      \"Time Taken for Each Phase\": {\n        \"Shuffle-heavy JOINs\": \"<time duration>\",\n        \"Wide Transforms (e.g., GROUP BY, ROLLUP)\": \"<time duration>\",\n        \"Output Writes\": \"<time duration>\"\n      },\n      \"Cost Model\": {\n        \"Pricing Model (e.g., DBU, vCPU Hour)\": \"<pricing description>\",\n        \"Total Runtime Cost\": \"<calculated cost and method>\"\n      },\n      \"Justification\": [\n        \"<reason 1>\",\n        \"<reason 2>\",\n        \"... add more if needed\"\n      ]\n    }\n  },\n  \"2. Code Fixing and Data Recon Testing Effort Estimation\": {\n    \"2.1 Estimated Effort in Hours\": {\n      \"Manual intervention and solutions of complex constructs\": \"<hours>\",\n      \"Data recon and pipeline testing, including test case creation, validation of intermediate datasets, and output comparison\": \"<hours>\",\n      \"Syntax Differences\": \"<hours>\",\n      \"Optimization Techniques\": \"<hours>\"\n    },\n    \"Major Contributors\": {\n      \"Rewriting nested transformations or window functions\": \"<hours>\",\n      \"Refactoring output writes for Spark APIs\": \"<hours>\",\n      \"Managing schema consistency across distributed stages\": \"<hours>\"\n    }\n  },\n  \"3. API Cost\": {\n    \"apiCost\": \"<float in USD>\"\n  }\n}\n\n\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}\n\nFor the PySpark analysis report, use the output from the previous PySpark_Analyser_JSON agent output.\n\nFor the Spark environment resource and pricing reference, use: {{Env_Variable}}.",
                        "expectedOutput": "{\n  \"1. Cost Estimation\": {\n    \"1.1 PySpark Runtime Cost\": {\n      \"Cluster Configuration\": {\n        \"Number of Executors\": \"<number>\",\n        \"Executor Memory\": \"<size in GB>\",\n        \"Driver Memory\": \"<size in GB>\"\n      },\n      \"Approximate Data Volume Processed\": {\n        \"Input Data\": \"<estimated size and notes>\",\n        \"Output Data\": \"<estimated size and notes>\"\n      },\n      \"Time Taken for Each Phase\": {\n        \"Shuffle-heavy JOINs\": \"<time duration>\",\n        \"Wide Transforms (e.g., GROUP BY, ROLLUP)\": \"<time duration>\",\n        \"Output Writes\": \"<time duration>\"\n      },\n      \"Cost Model\": {\n        \"Pricing Model (e.g., DBU, vCPU Hour)\": \"<pricing description>\",\n        \"Total Runtime Cost\": \"<calculated cost and method>\"\n      },\n      \"Justification\": [\n        \"<reason 1>\",\n        \"<reason 2>\",\n        \"... add more if needed\"\n      ]\n    }\n  },\n  \"2. Code Fixing and Data Recon Testing Effort Estimation\": {\n    \"2.1 Estimated Effort in Hours\": {\n      \"Manual intervention and solutions of complex constructs during Hive to Spark translation\": \"<hours>\",\n      \"Data recon and pipeline testing, including test case creation, validation of intermediate datasets, and output comparison\": \"<hours>\",\n      \"Syntax Differences\": \"<hours>\",\n      \"Optimization Techniques\": \"<hours>\"\n    },\n    \"Major Contributors\": {\n      \"Rewriting nested queries or window functions\": \"<hours>\",\n      \"Refactoring INSERT/OUTPUT statements for Spark write APIs\": \"<hours>\",\n      \"Managing schema consistency across distributed stages\": \"<hours>\"\n    }\n  },\n  \"3. API Cost\": {\n    \"apiCost\": \"<float in USD>\"\n  }\n}"
                    },
                    "maxIter": 20,
                    "maxRpm": 20,
                    "maxExecutionTime": 180,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}