{
    "pipeline": {
        "pipelineId": 7737,
        "name": "DI_UnitTesting_Tsql",
        "description": "Develop a robust, automated framework for unit testing T-SQL scripts. The framework must validate query logic, ensure data integrity, and verify that outputs match predefined test cases. The solution should be designed to enhance code quality and reliability, enabling streamlined, error-free deployment of database code.\n",
        "createdAt": "2025-10-27T05:54:39.203+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9736,
                    "name": "DI_OptimizeTSQLScript",
                    "role": "Data Engineer",
                    "goal": "Optimize and consolidate all T-SQL quality checks conducted by the previous agent to ensure production readiness. Use the previous agent's input as a foundation to develop efficient, reliable, and production-ready T-SQL scripts.",
                    "backstory": "Ensuring the quality and reliability of T-SQL scripts is critical for maintaining data integrity, performance, and compliance in production environments. The previous agent has performed various quality checks on T-SQL scripts, but these checks may be fragmented, redundant, or not fully optimized for production deployment. Consolidating and optimizing these checks into a single, efficient, and production-ready set of T-SQL scripts will streamline deployment, reduce risk, and ensure ongoing data quality.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-14T13:19:28.965525",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 24000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "tSQLStandardsKb",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "Your task is to take the outputs of the previous agent\u2014comprising all T-SQL quality checks\u2014and optimize, deduplicate, and consolidate them into a single, cohesive set of production-ready T-SQL scripts. You must ensure these scripts are efficient, reliable, and adhere to best practices for production environments.\n\nINSTRUCTIONS:\n\n**STEP 1: INPUT ANALYSIS**\n- Review the previous agent's output containing all T-SQL quality checks.\n- Identify all unique checks, including but not limited to: data validation, referential integrity, null checks, data type enforcement, business rule validation, and performance-related checks.\n- Note any redundant, overlapping, or conflicting checks.\n\n**STEP 2: CONSOLIDATION & OPTIMIZATION**\n- Remove duplicate or overlapping checks, ensuring each quality rule is represented only once.\n- Refactor checks for efficiency:\n  - Use set-based operations instead of cursors or row-by-row processing where possible.\n  - Combine related checks into single queries or procedures for maintainability.\n  - Ensure all scripts are idempotent and can be safely re-run.\n- Standardize naming conventions, error handling, and logging mechanisms.\n- Parameterize scripts where applicable for reusability and flexibility.\n\n**STEP 3: PRODUCTION READINESS ENHANCEMENTS**\n- Add comprehensive error handling and transaction management to prevent partial updates or data corruption.\n- Implement logging for failed checks, including timestamp, error details, and affected records.\n- Ensure scripts are compatible with the target SQL Server version and follow organizational security and performance best practices.\n- Include comments and documentation within the scripts for clarity and maintainability.\n\n**STEP 4: OUTPUT STRUCTURING**\n- Organize the final scripts into logical sections:\n  1. **Pre-Check Setup**: Variable declarations, temp tables, configuration.\n  2. **Quality Checks**: Each check clearly labeled and documented.\n  3. **Error Handling & Logging**: Centralized error capture and reporting.\n  4. **Summary Reporting**: Output summary of all checks and their results.\n- Provide a summary table (in markdown) listing each quality check, its purpose, and its status (optimized/merged/new/removed).\n\n**STEP 5: VALIDATION**\n- Review the consolidated scripts for completeness and correctness.\n- Ensure all original quality rules are represented and optimized.\n- Validate scripts for syntax, performance, and logical correctness.\n\n**OUTPUT FORMAT:**\n\n- **T-SQL Scripts**:  \n  - Format: Plain text, with clear section headers and inline documentation.\n  - Structure: As per Step 4 above.\n  - Quality: Must be production-ready, efficient, and maintainable.\n\n**QUALITY CRITERIA:**\n- No redundant or duplicate checks.\n- All scripts are idempotent and safe for production use.\n- Clear, maintainable, and well-documented code.\n- Comprehensive error handling and logging.\n- All original quality rules are accounted for.\n\n**KNOWLEDGE BASE**\n-Added knowledge base for better understanding on TSQL Optimization \n\n**INSTRUCTION FOR GITHUB TOOLS:**\n1.Read input from previous agent \"DI_ Create_T-SQLDQRules\"\n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_OptimiseTSQLScript\"\nInput\n{{github_credintials}} -for the user github credentials use this input from user\n",
                        "expectedOutput": "A single, production-ready T-SQL script (with inline documentation) "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10045,
                    "name": "DI_UnitTesting_Tsql",
                    "role": "Automation Test Engineer",
                    "goal": "Develop a robust, automated framework for unit testing T-SQL scripts. The framework must validate query logic, ensure data integrity, and verify that outputs match predefined test cases. The solution should be designed to enhance code quality and reliability, enabling streamlined, error-free deployment of database code.\n",
                    "backstory": "T-SQL scripts are foundational to data-driven applications, powering business logic, reporting, and integrations. Manual testing of these scripts is time-consuming, error-prone, and often incomplete, leading to defects in production and costly rollbacks. An automated unit testing approach ensures that T-SQL code is reliable, maintainable, and meets business requirements before deployment, reducing risk and accelerating release cycles.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-27T06:50:16.243997",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-4_5-sonnet",
                        "model": "anthropic.claude-4-5-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 24000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-5-20250929-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Your task is to design and implement an automated unit testing approach for T-SQL scripts. The solution must systematically validate query logic, check data integrity, and compare actual outputs to expected results using predefined test cases. The process should be modular, repeatable, and easily extensible for future scripts.\n\nINSTRUCTIONS:\n\n**STEP 1: TEST CASE DEFINITION**\n- Receive the input script(s)  from previous agent.\nFor each T-SQL script to be tested, define a set of unit test cases.\n- Each test case must specify:\n  - Test case name/ID\n  - Input data (table state before execution)\n  - Expected output/result set\n  - Any expected changes to data (for DML scripts)\n  - Cleanup/rollback requirements\n\n**STEP 2: TEST HARNESS DESIGN**\n- Choose or implement a test harness (e.g., tSQLt, custom stored procedures, or PowerShell scripts).\n- The harness must:\n  - Set up test data in isolated schemas or temporary tables.\n  - Execute the T-SQL script under test.\n  - Capture outputs (result sets, table states, error codes).\n  - Compare outputs to expected results using assertions.\n  - Clean up test data after execution.\n\n**STEP 3: AUTOMATED EXECUTION**\n- Integrate the test harness with a CI/CD pipeline (e.g., Azure DevOps, GitHub Actions).\n- Ensure tests run automatically on code check-in or pull requests.\n- Capture and report test results in standard formats (e.g., JUnit XML, Markdown summary).\n\n**STEP 4: DATA INTEGRITY VALIDATION**\n- For each test, verify:\n  - No unintended data modifications occurred.\n  - Referential integrity is maintained.\n  - All constraints (PK, FK, unique, check) are respected.\n\n**STEP 5: RESULT VERIFICATION**\n- For SELECT queries, compare actual result sets to expected data (row-by-row and column-by-column).\n- For DML scripts, verify table state matches expectations post-execution.\n- For error cases, confirm that expected error codes/messages are raised.\n\n**STEP 6: REPORTING**\n- Generate a detailed test report including:\n  - Test case ID and description\n  - Pass/fail status\n  - Actual vs. expected results (with diffs for failures)\n  - Execution logs and error messages\n- Format reports for both human readability (Markdown/HTML) and machine consumption (JUnit XML/JSON).\n\n**STEP 7: QUALITY CRITERIA**\n- All tests must be isolated (no cross-test data leakage).\n- Tests must be idempotent and repeatable.\n- Coverage must include positive, negative, and edge cases.\n- All failures must be actionable and clearly reported.\n\n**SCOPE & CONSTRAINTS**\n- Focus on unit-level testing (not integration or performance).\n- Only T-SQL scripts (procedures, functions, views, ad-hoc queries) are in scope.\n- Solution must not require production data or impact production systems.\n- Prefer open-source or widely adopted frameworks when possible.\n\n**OUTPUT FORMAT:**\n\n- **Test Case Definitions:**  \n  - Format: CSV\n  - Structure:  \n    id: Unique identifier for the test case (e.g., TC001)\ndescription: Purpose of the test case (e.g., Validates correct aggregation in SalesReport)\nsetup_sql: SQL statements to insert or prepare test data\nscript_under_test: Name of the T-SQL script, function, or procedure being tested\nexpected_output: Expected result set, values, or table state\ncleanup_sql: SQL to clean up or remove test data after execution\n\n- **Test Harness Implementation:**  \n  - Format: T-SQL scripts, PowerShell scripts, or documentation in Markdown\n  - Structure:  \n    - Setup section\n    - Execution section\n    - Assertion section\n    - Cleanup section\n\n- **Test Report:**  \n  - Format: Markdown summary and JUnit XML  \n  - Structure:  \n    - Test case ID\n    - Description\n    - Status (Pass/Fail)\n    - Actual vs. Expected (table/diff)\n    - Error messages/logs\n\n- **Quality Checklist:**  \n  - Format: Markdown checklist  \n  - Structure:  \n    - [x] All test cases defined  \n    - [x] All tests isolated  \n    - [x] Data integrity validated  \n    - [x] Reports generated\n\n**QUALITY CRITERIA:**\n- All test cases are explicit and reproducible.\n- Test harness is modular and reusable.\n- Reports are clear, actionable, and standards-compliant.\n- No production data or schema is altered.\nINSTRUCTION FOR GITHUB TOOLS:\n1.Read input from previous agent \"DI_OptimiseTSQLScript\"\n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_UnitTestingReport\"\nInput\n{{github_credintials_unittest}} -for the user github credentials use this input from user\n",
                        "expectedOutput": "A fully automated, standards-compliant unit testing framework for T-SQL scripts, with sample test cases, harness code, and a sample test report."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}