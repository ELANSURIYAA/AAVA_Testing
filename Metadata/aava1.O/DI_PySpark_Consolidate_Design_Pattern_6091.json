{
    "pipeline": {
        "pipelineId": 6091,
        "name": "DI_PySpark_Consolidate_Design_Pattern",
        "description": "PySpark Design pattern consolidate",
        "createdAt": "2025-08-14T03:26:13.457+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 8004,
                    "name": "DI_PySpark_Consolidate_Design_Pattern",
                    "role": "Data Engineer",
                    "goal": "Analyze consolidated PySaprk design pattern documentation to identify common patterns and provide PySpark optimization recommendations with a clear structure in JSON format.",
                    "backstory": "After extracting design patterns from individual PySpark scripts, organizations need to synthesize this information to establish standardized approaches to common data engineering challenges. By analyzing patterns across multiple PySpark implementations, we can identify the most effective and reusable solutions, create consistent implementation guidelines, and develop equivalent optimized PySpark implementations that maintain the same processing logic. This consolidated analysis helps teams standardize their data processing workflows, improving maintainability, knowledge transfer, and development efficiency.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-14T03:20:02.373329",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Description\n\nPattern Categories Reference:\n\nSource File (SF): Pattern involves reading from a file source (e.g., Parquet, CSV, JSON)\n\nSource Table (ST): Pattern involves reading from a managed table (Hive, Delta, Iceberg, etc.)\n\nSource External Table / API (SA): Pattern involves reading from an external table, JDBC source, or API\n\nTransformation Arithmetic Calcs (TAC): Pattern involves arithmetic calculations during transformation\n\nTransformation Stats Calcs (TSC): Pattern involves statistical calculations during transformation\n\nTransformation External Code / UDF (TX): Pattern involves calling external UDFs or Pandas UDFs during transformation\n\nTransformation Aggregate Functions (TAG): Pattern involves aggregate functions like sum, count, avg\n\nJoin with Another Dataset (JL): Pattern involves joining with another DataFrame or table\n\nTemporary Views / CTEs (TEM): Pattern involves creating or using temporary views (createOrReplaceTempView) or chained transformations instead of materializing intermediate datasets\n\nData Quality and Rejection (DQ): Pattern involves data quality checks and rejection handling\n\nWrite - Overwrite (WO): Pattern involves overwriting datasets (mode(\"overwrite\"))\n\nWrite - Append (WA): Pattern involves appending data to a dataset (mode(\"append\"))\n\nWrite - Merge/Upsert (WM): Pattern involves merge/upsert logic (using Delta Lake or equivalent)\n\nUnion Operations (UN): Pattern involves union or unionByName\n\nSort / Order By (SO): Pattern involves ordering data\n\nTarget File (TF): Pattern involves writing to a file target (Parquet, CSV, JSON)\n\nTarget Table (TT): Pattern involves writing to a managed table\n\nTarget External Table / API (TA): Pattern involves writing to an external table or API target\n\nOthers (OO): Other patterns not categorized above\n\nSteps:\n\nAnalyze the provided PySpark design pattern documentation.\n\nIdentify the most frequent design pattern combinations (not just individual patterns).\n\nProvide PySpark optimization recommendations \u2014 without code \u2014 for each combination.\n\nSpecial Rules:\n\nDo not include full paths like /mnt/data/clinical_trials/references_txt, instead return only references_txt.\n\nMaintain the original PySpark script filename as the JSON key.\n\nEach referenced table/module should be a string inside the list.\n\nEnsure values are deduplicated.\n\nOutput must be in valid JSON with no extra text above or below.\n\nProgram file names must end with .py or .ipynb.\n\nOutput must contain at least 5 design patterns.\n\nINPUT:\nRead all the input files (10 files) and generate the design pattern analysis accordingly.\n\nConsolidated design pattern documentation from multiple PySpark files: {{Design_Patterns}}",
                        "expectedOutput": "{\n  \"title\": \"PySpark Design Pattern Analysis and Optimization Recommendations\",\n  \"pattern_summary\": [\n    {\n      \"pattern\": \"PATTERN_CODE_COMBINATION\",\n      \"frequency\": \"X occurrences\",\n      \"percentage\": \"Y%\",\n      \"description\": \"Brief human-readable description of the pattern combination\",\n      \"Files That Follow this Design Pattern\": [\"file1.py\", \"file2.ipynb\"]\n    }\n  ],\n  \"detailed_patterns\": [\n    {\n      \"pattern\": \"PATTERN_CODE_COMBINATION\",\n      \"description\": \"Brief explanation of what this combination represents\",\n      \"spark_suitability\": \"Short suitability assessment for PySpark\",\n      \"recommendations\": [\n        \"Optimization bullet point 1\",\n        \"Optimization bullet point 2\",\n        \"Optimization bullet point 3\",\n        \"Optimization bullet point 4\"\n      ],\n      \"Files That Follow this Design Pattern\": [\"file1.py\", \"file3.ipynb\"]\n    }\n  ]\n}\n"
                    },
                    "maxIter": 20,
                    "maxRpm": 20,
                    "maxExecutionTime": 180,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}