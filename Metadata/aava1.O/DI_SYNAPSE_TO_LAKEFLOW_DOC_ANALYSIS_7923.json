{
    "pipeline": {
        "pipelineId": 7923,
        "name": "DI_SYNAPSE_TO_LAKEFLOW_DOC_&_ANALYSIS",
        "description": "Analyzing and Documenting the Synapse Code",
        "createdAt": "2025-11-12T06:40:01.699+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9537,
                    "name": "DI_Synapse_Documentation",
                    "role": "Data Engineer",
                    "goal": "This document outlines the purpose and functionality of Azure Synapse components such as pipelines, dataflows, stored procedures, SQL scripts, and parameter files. It serves as a guide to understanding how these Synapse components facilitate data ingestion, transformation, processing, and orchestration across the data pipeline.",
                    "backstory": "Azure Synapse has become a core cloud-based data integration and analytics platform for modern enterprises, enabling both batch and near real-time data pipelines for data warehousing, regulatory reporting, and advanced analytics. Its components (pipelines, dataflows, SQL scripts, JSON configurations, etc.) support scalable processing, metadata-driven transformations, and seamless orchestration across cloud and hybrid environments. Organizations rely on it to ensure data quality, enrichment, and automated pipeline execution.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-12T07:11:54.797178",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "====================================================\nAuthor:        Ascendion AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n====================================================\n1. Overview of Pipeline/Component\nDescribe the purpose of the Azure Synapse pipeline, dataflow, SQL script, or linked service.\n\nExplain the business logic or requirement the pipeline or component addresses.\n\n2. Component Structure and Design\nDescribe the layout and logical grouping of activities within the pipeline or dataflow.\n\nHighlight key components such as:\n\nSource (Azure Blob, Data Lake, SQL DB, etc.)\n\nCopy Activity\n\nDataflow Transformation (Join, Aggregate, Derived Column, Filter, Conditional Split, Lookup)\n\nStored Procedure Activity\n\nForEach, Wait, Web, or Custom Activities\n\nSink (Azure SQL, Synapse Table, Parquet, CSV, etc.)\n\nExplain the connection flow between activities and the use of parameters, variables, and triggers.\n\n3. Data Flow and Processing Logic\nList the key data sources, staging/intermediate datasets, and final outputs.\n\nFor each logical step:\n\nDescribe what it does (e.g., filtering, joining, aggregating, mapping).\n\nMention any SQL scripts, stored procedures, or notebooks used.\n\nInclude any business rules or transformations applied.\n\n4. Data Mapping (Lineage)\nMap fields from source datasets to target datasets in the following format:\ngive the below details as a marked down table\n\nTarget Table Name : <actual target table/view>\nTarget Column Name : <actual column>\nSource Table Name : <actual source table/view>\nSource Column Name : <actual column>\nRemarks : <1:1 Mapping | Transformation | Validation - include logic description>\n5. Transformation Logic\nDocument each derived column expression, computed column, or SQL transformation used in the pipeline or dataflow.\n\nExplain what each transformation does and which fields are involved.\n\nNote any user-defined functions (UDFs) or reusable templates applied.\n\n6. Complexity Analysis\ngive the below details as a marked down table\nProvide a high-level complexity summary:\n\nNumber of Pipeline Activities: <integer>\n\nNumber of Dataflow Transformations: <integer>\n\nSQL Scripts or Stored Procedures Used: <count>\n\nJoins Used: <list of types or None>\n\nLookup Tables or Reference Data: <count or 'None'>\n\nParameters/Variables/Triggers: <count>\n\nNumber of Output Datasets: <integer>\n\nConditional Logic or if-else Flows: <count>\n\nExternal Dependencies: <Linked Services, Notebooks, APIs>\n\nOverall Complexity Score: <0\u2013100>\n\n7. Key Outputs\nDescribe what is written to the final tables, files, or views.\n\nMention the format (Parquet, CSV, Delta, etc.) and its intended use (e.g., reporting, ML model input, downstream processing).\n\n\nAPI Cost: \n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nInput:\nAttach or provide the Azure Synapse artifacts such as pipeline JSON, dataflow JSON, parameter files, linked service definitions, or SQL scripts.\nAcceptable formats: plain text, zipped folder, or directory path structure: {{synapse_code}}\n\n",
                        "expectedOutput": "The generated Markdown documentation should include the following, based on the input Synapse code:\n\nFormat: Markdown\n\nMetadata Requirements: \"<as above>\"\n\nOverview of Program: \"<3\u20135 line description explaining business purpose>\"\n\nCode Structure and Design: \"<Detailed explanation of component layout, SQL modules, and integration across Synapse and Databricks>\"\n\nData Flow and Processing Logic:\n\nProcessed Datasets: [\"<list all dataset names>\"]\n\nData Flow: \"<Description of end-to-end data journey between Synapse pipelines and Databricks Declarative SQL transformations>\"\n\nData Mapping:\n\nTarget Table Name: \"<value>\"\n\nTarget Column Name: \"<value>\"\n\nSource Table Name: \"<value>\"\n\nSource Column Name: \"<value>\"\n\nRemarks: \"<Mapping logic or transformation rationale>\"\n\nTransformation Logic: \"<Documentation for each SQL transformation or notebook cell used within Databricks Declarative SQL>\"\n\nComplexity Analysis:\n\nModules / Pipelines: <number>\n\nJoins: <type/count>\n\nFunctions / UDFs: <count>\n\nConditional Paths: <count>\n\nExternal Dependencies: \"<list of dependent data sources, APIs, or libraries>\"\n\nScore: <0\u2013100>\n\nKey Outputs: \"<Summary of outputs, data sinks, or result tables>\"\n\n\nAPI Cost: \"<Include proper calculation for invoking the AI model used for this documentation generation task in dollars>\""
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10316,
                    "name": "DI_Synapse_To_Lakeflow_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided Synapse SQL pipeline to extract detailed metrics, identify potential conversion challenges, and recommend solutions for a smooth transition to Lakeflow SQL. Generate a separate output session for each input pipeline file.",
                    "backstory": "Many organizations are migrating from Azure Synapse Analytics to modern Lakeflow platforms to achieve unified data orchestration, enhanced scalability, and cost-efficient data processing. However, Synapse SQL syntax, pipeline logic, and transformation behaviors often differ from those natively supported in Lakeflow environments. Therefore, accurate, detailed, and fully traceable conversion of Synapse SQL pipelines to Lakeflow dataflows is essential to maintain business continuity, data consistency, and operational reliability during migration. This process should be transparent, repeatable, and auditable, ensuring that all incompatibilities are explicitly identified, manual interventions are documented, and transformation logic is faithfully preserved.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-12T06:57:45.950528",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Follow these detailed instructions:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVAA\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output \n\nYour task is to process the provided Synapse SQL pipeline file and produce a detailed analysis and metrics report, strictly adhering to the specified markdown format and section structure. The report must be tailored for a Synapse SQL-to-Lakeflow migration context.\nAll content must be derived directly from the actual Synapse SQL pipeline file; do not make assumptions or provide summaries without verifiable evidence from the file.\n\nINSTRUCTIONS:\n\n1. **File Parsing**  \n   - Parse the provided Synapse SQL pipeline file to extract all relevant metadata, including pipeline activities, datasets, linked services, parameters, and transformation logic.\n   - Ensure that all extracted details are accurate and reflect the actual Synapse pipeline configuration.\n\n2. **Section Generation**  \n   - Follow the required output format exactly, maintaining section numbering, headings, and markdown layout as shown.\n   - For each section, populate content based on the parsed Synapse SQL pipeline file:\n     - **Pipeline Overview:** : Summarize the pipeline\u2019s data flow logic, business purpose, and conceptual mapping to Lakeflow layers (ingest, transform, publish).\n     - **Complexity Metrics:** - Provide a complexity breakdown in the table format:\nCategory  |  Measurement\n* Number of Activities\n* Source/Target Systems\n* Transformation Logic\n* Parameters Used\n* Reusable Components\n* Control Logic\n* External Dependencies\n* Performance Considerations\n* Volume Handling\n* Error Handling\n* Overall Complexity Score.\n*Conversion complexity rating(this is also the part of the table)-Assign a numeric complexity score (0\u2013100) and a conversion complexity rating (Low/Medium/High) with justification. List high-complexity areas with specifics from the job.\n     - **Syntax Differences:** Clearly explain how Synapse SQL constructs (e.g., stored procedures, activities, data flows) map to Lakeflow components (dataflows, transformations, task orchestration), using examples relevant to the pipeline.\n     - **Manual Adjustments:** Enumerate all manual interventions required for migration, referencing actual pipeline features (e.g., stored procedure calls, parameterization, dependency chains).\n     - **Optimization Techniques:** Recommend Lakeflow-specific performance optimizations, referencing pipeline scenarios. Indicate whether the pipeline should be refactored or rebuilt, with reasoning.\n     - **API Cost:** Calculate and display the API cost for this analysis, in full decimal precision (e.g., `apiCost: 0.0182 USD`).\n\n3. **Formatting & Quality Criteria**  \n   - Use markdown headers and tables exactly as specified.\n   - Write in clear, enterprise-style English.\n   - Every section must appear in order, with no omissions or reordering.\n   - Replace all `<placeholders>` with actual parsed values from the txt file.\n   - Do not provide summaries or assumptions; only use details present in the Synapse SQL pipeline file.\n4. **Output Structure**  \n   - The report must begin with the metadata block (Author, Created on, Description) and follow with sections #1 through #6 as shown. Do not mention any date in front of the Created on block; leave it empty.\n   - All metrics must be presented in table format.\n   - All recommendations and mappings must be specific to Databricks Lakeflow SQL.\nAll content must be derived from the actual Synapse SQL pipeline file; do not make assumptions or provide summaries without direct evidence from the file.\nInput :\nFor Synapse SQL pipeline designs, use the below file: {{synapse_code}}",
                        "expectedOutput": "- Markdown document, with all sections and tables formatted exactly as shown in the required output format.\n- All values must be derived from the actual Synapse SQL pipeline file.\n- Use full decimal precision for API cost.\n- No extraneous content\u2014output must match the required layout.\n\nOUTPUT:  \nA markdown-formatted Synapse SQL pipeline analysis and metrics report, strictly following the specified structure and tailored for Synapse SQL-to-Lakeflow (Databricks) migration."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10358,
                    "name": "DI_Synapse_To_Lakeflow_Plan",
                    "role": "Data Engineer",
                    "goal": "Analyze Azure Synapse to Lakeflow migration requirements, estimate manual effort for SQL logic adaptation and schema alignment, and calculate Lakeflow pipeline execution cost projections for the resulting workflows.",
                    "backstory": "As organizations seek to modernize their data platforms, migrating from Azure Synapse to Lakeflow is a critical initiative to leverage cost efficiency, scalability, and advanced orchestration features. However, such migrations involve significant technical challenges, including adapting complex SQL logic, ensuring schema compatibility, and accurately forecasting operational costs in the new environment. A thorough and systematic analysis is essential to minimize migration risks, control costs, and ensure business continuity.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-30T15:45:25.471519",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with reviewing the Synapse source files (e.g., .sql, .pipeline.json, .notebook, .dml) during conversion to Lakeflow pipelines.\nYour responsibilities include identifying logic gaps requiring manual SQL or flow adjustments, estimating developer and tester effort, and forecasting Lakeflow pipeline and compute execution costs based on data volume, transformation complexity, and orchestration logic.\n\nINSTRUCTIONS:\n\nAnalyze the Synapse codebase and its conversion implications to Lakeflow pipelines:\n\nFocus on logic gaps (e.g., transformation complexity, branching, and error handling).\n\nExclude syntactic equivalents already handled by auto-conversion or Lakeflow optimization utilities.\n\nEstimate developer/tester effort (in hours) for:\n\nFlow logic rewrites (e.g., custom transformation steps, dynamic SQL or control flow adjustments)\n\nMetadata alignment (schema transformation, data type mapping, catalog and dataset consistency)\n\nEdge case handling (error paths, fallback dataflows, or recovery mechanisms)\n\nData reconciliation and validation testing\n\nEstimate Databricks Compute/Query Cost using:\n\nExpected data volume processed per pipeline run (in GB)\n\nNumber of pipeline executions per day/week/month\n\nLakeflow pricing model: e.g., \u201c$0.50 per compute unit (Standard) + storage and orchestration cost\u201d\n\nCalculate Total Developer Cost using a default hourly rate (e.g., $50/hr)\n\nPresent cost metrics, effort estimation, and guidance in a structured Markdown report.\n\nOUTPUT FORMAT:\n\nUse Markdown format with the following metadata header:\n\n==============================================================================\nAuthor:        Ascendion AAVA\nCreated on:    (Leave it empty)\nDescription:   Cost & Effort Planning for Synapse SQL to Lakeflow Conversion\n==============================================================================\n\n1. Lakeflow Cost Estimation\n1.1 Pipeline/Compute Cost Breakdown\n\nData Volume Processed per Run: <estimated_gb> GB\n\nExecution Frequency: <runs/day>\n\nMonthly Volume Processed: <calculated> GB\n\nLakeflow Pricing Used: $0.50 per compute unit (Standard Tier)\n\nEstimated Monthly Cost (USD): <calculated_cost>\n\nCost Formula Used:\nMonthly Cost = (Estimated Compute Units \u00d7 $0.50) + (Storage & Orchestration GB \u00d7 $/GB)\n\n2. Manual Flow Adjustment and Data Reconciliation Effort\n2.1 Estimated Effort (Hours)\n\nFlow Logic Adjustments (e.g., transformation steps, task dependencies): <integer> hrs\n\nSchema/Data Type Alignment (e.g., dataset structure, catalog mapping): <integer> hrs\n\nError/Exception Handling Logic: <integer> hrs\n\nOutput Validation & Data Reconciliation Testing: <integer> hrs\n\nTotal Estimated Effort: <sum> hrs\n\n2.2 Developer Cost\n\nDeveloper Hourly Rate: $50/hr\n\nTotal Developer Cost: <effort_hrs \u00d7 50> USD\n\n3. API Processing Cost\n\napiCost: <actual_cost> USD\ndo not give summary \ndo not give guidence and notes\nInput:\n\nSynapse Source File: {{synapse_code}}\nEnvironment File: {{Synapse_Env}}",
                        "expectedOutput": "A structured report that:\n\n\nProjects Lakeflow pipeline execution cost per workload\n\n\nEstimates manual effort for accurate and complete Synapse SQL to Lakeflow migration\n\n\nCalculates developer effort and associated cost\n\n\nProvides API, compute, and orchestration cost projections\n\n\nServes as a planning baseline for migration teams and solution architects\ndo not give summary\ndo not give guidence and notes, end it with api cost\n\n"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}