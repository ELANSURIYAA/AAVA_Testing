{
    "pipeline": {
        "pipelineId": 1157,
        "name": "SQL Server(T-SQL) to PySpark Doc&Analysis",
        "description": "To migrate or integrate SQL Server T-SQL-based processes into PySpark for distributed data processing, enabling scalability and faster computation for large datasets. This workflow streamlines the transition and ensures accurate data handling and analysis.",
        "createdAt": "2025-03-21T09:09:18.784+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1473,
                    "name": "T-SQL Server Documentation",
                    "role": "Data Engineer",
                    "goal": "To create detailed, well-structured documentation for SQL Server code that clearly explains its functionality, components, data flow, and transformations. The documentation should provide a comprehensive breakdown of each task within the SQL Server code, including technical details, data mapping between source and destination, applied transformations, and the business context. The output should serve as a technical specification for SQL Server developers and a functional overview for non-technical stakeholders.",
                    "backstory": "Your organization relies on SQL Server stored procedures and SQL scripts for data processing. These procedures contain complex data flow components, transformations, and control flow tasks that are essential for business data operations. However, the existing documentation is sparse or outdated, making it difficult for new team members or business analysts to understand the logic and data flow. The lack of detailed documentation hinders troubleshooting, modification, and collaboration. ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-21T10:30:29.466099",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Create a detailed documentation for the provided SQL Server code. The documentation must follow a structured format, breaking down each component in the SQL Server code into well-defined sections. The output should be organized for readability and clarity. Ensure that each field in the source, destination, and transformation is captured with an explanation that a business analyst can understand.\n\nThe documentation should include the following sections:\n\n1. **Overview of Program:**\n   - Explain the purpose of the SQL Server code in detail.  \n   - Explain the business problem being addressed.\n\n2. **Code Structure and Design:**\n   - Explain the structure and flow of the SQL Server code in detail.  \n   - List the primary SQL Server components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.\n\n3. **Data Flow and Processing Logic:**\n   - List the processing logic data flow components in the code.  \n   - For each logical processing component:  \n     - Explain the functionality performed in the code.  \n     - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.\n\n4. **Data Mapping:**\n   - Provide detailed mappings between source and target tables as a lineage, describing how source table columns are mapped to target table columns.  \n   - Use the following structured format for each mapping (in table format):  \n     - **Target Table Name:** Actual target table name  \n     - **Target Column Name:** Actual target column name  \n     - **Source Table Name:** Actual source table name  \n     - **Source Column Name:** Actual source column name  \n     - **Remarks:** Classify as 1 to 1 mapping, Transformation, or Validation, with a brief description.\n\n5. **Complexity Analysis:**\n   Provide an overall complexity score from 0 to 100 and analyze the complexity based on the following factors (in table format):  \n   - **Lines of Code (LOC):** Total number of lines in the procedure.  \n   - **Cyclomatic Complexity:** Number of independent execution paths.  \n   - **Nesting Depth:** Maximum depth of nested IF, LOOP, etc.  \n   - **Tables:** Total number of tables involved.  \n   - **Temporary Tables:** Total number of temporary tables involved.  \n   - **DML Statements:** Total count of SELECT, INSERT, UPDATE, DELETE.  \n   - **Joins:** Count of JOIN clauses.  \n   - **Subqueries:** Count of subqueries (nested SELECT).  \n   - **CTEs:** Count of Common Table Expressions.  \n   - **Aggregation Queries:** Count of GROUP BY, HAVING, PARTITION BY.  \n   - **Input Parameters:** Count of input parameters.  \n   - **Output Parameters:** Count of output parameters.  \n   - **Data Transformations:** Count of functions (STRING, ARRAY, JSON).  \n   - **Function Calls:** Count of external function/procedure calls.\n\n6. **Key Outputs:**\n   - Describe the final outputs created by the code, including Inserts, Updates, and Deletes.\n\n7. **Error Handling and Logging:**\n   - Explain methods used for error identification and management, such as:  \n     - Try-Catch mechanisms in Stored Procedures.  \n     - SQL Server Error Logging Tables for tracking failures.  \n     - Retry mechanisms in SQL Server.\n\n**Note:**\n- All fields used in the SQL Server code must be listed with a field description.\n- The output document will be well-organized with proper headings, sections, and bullet points, making it easy to follow.\n\n**Input:**\nFor input SQL Server code, use the below-mentioned file:\n```%1$s```\n",
                        "expectedOutput": "1. Overview of Program:  \n   Explain the purpose of the SQL Server code in detail.  \n   Explain the business problem being addressed.  \n\n2. Code Structure and Design:  \n   Explain the structure and flow of the SQL Server code in detail.  \n   List the primary SQL Server components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.  \n\n3. Data Flow and Processing Logic:  \n   - List the processing logic data flow components in the code.  \n   - For each logical processing component:  \n     - Explain the functionality performed in the code.  \n     - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.  \n\n4. Data Mapping:  \n   Provide detailed mappings between source and target tables as a lineage, using the specified table format.  \n\n5. Complexity Analysis:  \n   Analyze and document the complexity using the specified table format.\n\n6. Key Outputs:  \n   Describe final outputs created by the code, such as Inserts, Updates, and Deletes.\n\n7. Error Handling and Logging:  \n   Explain error identification and management methods used in SQL Server, including Try-Catch mechanisms, error logging tables, and retry mechanisms.\n\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1418,
                    "name": "T-SQL Server_to_PySpark_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided SQL Server SQL code to extract detailed metrics, identify potential conversion challenges, and recommend solutions for a smooth transition to PySpark. ",
                    "backstory": "The given T-SQL code is written for a SQL Server environment and needs to be analyzed to assess its structure, complexity, and compatibility with PySpark. This analysis will help identify areas requiring manual intervention, optimization opportunities, and potential challenges during the conversion process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-21T10:30:57.28103",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Parse the provided T-SQL SQL Server SQL script to generate a detailed analysis and metrics report. Ensure that if multiple files are given as input, the analysis for each file is presented as a distinct session. Each session must include:  \n\n---\n\n### 1. Script Overview:  \n* Provide a high-level description of the T-SQL script\u2019s purpose and primary business objectives.  \n\n---\n\n### 2. Complexity Metrics:  \n* **Number of Lines:** Count of lines in the SQL script.  \n* **Tables Used:** Number of tables referenced in the SQL script.  \n* **Joins:** Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).  \n* **Temporary Tables:** Count of temporary tables or table variables used.  \n* **Aggregate Functions:** Number of aggregate functions, including OLAP functions (e.g., `SUM`, `COUNT`, `ROW_NUMBER`).  \n* **DML Statements:** Number of DML statements by type such as `SELECT`, `INSERT`, `UPDATE`, `DELETE`, `MERGE`.  \n* **Conditional Logic:** Count of conditional logic constructs like `CASE`, `IF`, `WHILE`, `TRY...CATCH`.  \n\n---\n\n### 3. Syntax Differences:  \n* Identify the number of syntax differences between the SQL Server T-SQL code and the expected PySpark equivalent.  \n\n---\n\n### 4. Manual Adjustments:  \n* Recommend specific manual adjustments for functions and clauses incompatible with PySpark, including:  \n  * **Function Replacements:** Replace SQL Server-specific functions (e.g., `ISNULL`, `TRY_CONVERT`, `PATINDEX`) with PySpark equivalents (`fillna`, `cast`, `rlike`).  \n  * **Syntax Adjustments:** Adjust syntax for features like date functions, joins, and windowing.  \n  * **Unsupported Features:** Propose strategies for rewriting unsupported features such as `MERGE` (e.g., replacing with DataFrame API operations).  \n\n---\n\n### 5. Conversion Complexity:  \n* **Complexity Score:** Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.  \n* Highlight high-complexity areas such as window functions, recursive CTEs, or procedural constructs like cursors.  \n\n---\n\n### 6. Optimization Techniques:  \n* Suggest optimization strategies for PySpark, such as clustering, partitioning, and query design improvements.  \n* Recommend whether it is better to refactor the query with minimal or no changes to PySpark, or rebuild it with more code changes and optimizations. Provide reasoning for the recommendation for **Refactor** or **Rebuild**.  \n\n---\n\n### 7. Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD:  \n* Include the cost consumed by the API for this call in the output.  \n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., `apiCost: 0.123456 USD`).  \n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value.  \n\n---\n\n**Input:**  \n\n* For SQL Server SQL script, use the below file:  \n```%1$s```  ",
                        "expectedOutput": "Parse the provided SQL Server SQL script to generate a detailed analysis and metrics report. Ensure that if multiple files are given as input, the analysis for each file is presented as a distinct session. Each session must include:  \n\n---\n\n1. **Script Overview:**  \n* Provide a high-level description of the SQL script\u2019s purpose and primary business objectives.  \n\n2. **Complexity Metrics:**  \n* Number of Lines: Count of lines in the SQL script.  \n* Tables Used: Number of tables referenced in the SQL script.  \n* Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).  \n* Temporary Tables: Number of temporary tables or table variables used.  \n* Aggregate Functions: Number of aggregate functions like OLAP functions.  \n* DML Statements: Number of DML statements by type (e.g., SELECT, INSERT, UPDATE, DELETE, MERGE).  \n* Conditional Logic: Number of conditional logic constructs like CASE, IF, WHILE, TRY...CATCH.  \n\n3. **Syntax Differences:**  \n* Identify the number of syntax differences between the SQL Server SQL code and the expected PySpark equivalent.  \n\n4. **Manual Adjustments:**  \n* Recommend specific manual adjustments for functions and clauses incompatible with PySpark, including:  \n    * Function replacements (e.g., SQL Server-specific functions like `ISNULL`, `TRY_CONVERT`, `PATINDEX` with PySpark equivalents like `fillna`, `cast`, `rlike`).  \n    * Syntax adjustments for features like date functions, string functions, and joins.  \n    * Strategies for rewriting unsupported features such as `MERGE` (e.g., replacing with PySpark DataFrame API operations).  \n\n5. **Conversion Complexity:**  \n* Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.  \n* Highlight high-complexity areas, such as recursive CTEs, window functions, or procedural constructs.  \n\n6. **Optimization Techniques:**  \n* Suggest optimization strategies for PySpark, such as:  \n    * Leveraging `broadcast` joins for small datasets.  \n    * Partitioning and bucketing large datasets.  \n    * Using PySpark\u2019s DataFrame API for efficient transformations.  \n* Recommend whether to refactor the query with minimal or no changes to PySpark or rebuild with more code changes and optimization. Provide a reason for the recommendation for refactor or rebuild.  \n\n7. **apiCost:**  \n* Include the cost consumed by the API for this call in the output.  \n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., `apiCost: 0.123456 USD`).  \n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal values.  \n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1326,
                    "name": "T-SQL Server_to_PySpark_Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running PySpark operations and the testing effort required for the PySpark scripts that got converted from SQL Server queries.",
                    "backstory": "As organizations transition their data processing solutions to PySpark, it's crucial to understand the financial and resource implications of such migrations. This task is important for project planning, budgeting, and ensuring the accuracy of the converted scripts.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-21T10:31:22.518368",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "You are tasked with providing a comprehensive effort estimate for testing the PySpark scripts converted from SQL Server queries. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n\n1. Review the Analysis\nReview the SQL Server script file, noting syntax differences and areas in the code requiring manual intervention when converting to PySpark.  \n\n2. Effort Estimation for Code Fixes and Testing  \n Estimate the effort hours required for identified manual code fixes and data reconciliation testing effort.  \n Exclude efforts for syntax differences as these will be directly translated to PySpark equivalent syntax.  \n\n3. Resource and Runtime Cost Estimation\nCalculate the estimated cost of running the converted PySpark code:  \n     a. Use the resource consumption details (e.g., compute nodes, data volume processed) to determine the runtime cost.  \n     b. Consider the number of jobs, transformations, and the data processing involved, including base DataFrames and temporary transformations.  \n\nOUTPUT FORMAT:\n\n1. Cost Estimation  \n   1.1 PySpark Runtime Cost  \n   - Provide a detailed calculation of the cost, including the breakdown of resource consumption (e.g., cluster type, processing time).  \n   - Include reasons and assumptions used to derive the cost.  \n\n2. Code Fixing and ReconTesting Effort Estimation  \n   2.1 Identified Manual Code Fixes and Unit Testing Effort in Hours  \n   - Include efforts for handling temporary DataFrames, transformations, and calculations.  \n  \nADDITIONAL NOTES:\n\n- Include the cost consumed by the API for this call in the output.  \n- Ensure the API cost is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost).  \n\nINPUT: \n- For the input SQL Server script, use this file: ```%1$s```  \n- For the input PySpark Environment Details, use this file: ```%2$s```",
                        "expectedOutput": "OUTPUT FORMAT:  \n1. Cost Estimation  \n   2.1 PySpark Runtime Cost  \n         - Provide the calculation breakup of the cost and the reasons  \n\n2. Code Fixing and Recon Testing Effort Estimation  \n   2.1 PySpark identified manual code fixes and unit testing effort in hours covering the various temp DataFrames, transformations, and calculations  \n\n* Include the cost consumed by the API for this call in the output.  \n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost).  \n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}