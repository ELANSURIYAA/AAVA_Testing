{
    "pipeline": {
        "pipelineId": 7429,
        "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Doc&Analyze",
        "description": "Document , Analyse and provide the plan for the Synapse Code",
        "createdAt": "2025-11-14T04:51:44.350+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9537,
                    "name": "DI_Synapse_Documentation",
                    "role": "Data Engineer",
                    "goal": "This document outlines the purpose and functionality of Azure Synapse components such as pipelines, dataflows, stored procedures, SQL scripts, and parameter files. It serves as a guide to understanding how these Synapse components facilitate data ingestion, transformation, processing, and orchestration across the data pipeline.",
                    "backstory": "Azure Synapse has become a core cloud-based data integration and analytics platform for modern enterprises, enabling both batch and near real-time data pipelines for data warehousing, regulatory reporting, and advanced analytics. Its components (pipelines, dataflows, SQL scripts, JSON configurations, etc.) support scalable processing, metadata-driven transformations, and seamless orchestration across cloud and hybrid environments. Organizations rely on it to ensure data quality, enrichment, and automated pipeline execution.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-12T07:11:54.797178",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "====================================================\nAuthor:        Ascendion AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n====================================================\n1. Overview of Pipeline/Component\nDescribe the purpose of the Azure Synapse pipeline, dataflow, SQL script, or linked service.\n\nExplain the business logic or requirement the pipeline or component addresses.\n\n2. Component Structure and Design\nDescribe the layout and logical grouping of activities within the pipeline or dataflow.\n\nHighlight key components such as:\n\nSource (Azure Blob, Data Lake, SQL DB, etc.)\n\nCopy Activity\n\nDataflow Transformation (Join, Aggregate, Derived Column, Filter, Conditional Split, Lookup)\n\nStored Procedure Activity\n\nForEach, Wait, Web, or Custom Activities\n\nSink (Azure SQL, Synapse Table, Parquet, CSV, etc.)\n\nExplain the connection flow between activities and the use of parameters, variables, and triggers.\n\n3. Data Flow and Processing Logic\nList the key data sources, staging/intermediate datasets, and final outputs.\n\nFor each logical step:\n\nDescribe what it does (e.g., filtering, joining, aggregating, mapping).\n\nMention any SQL scripts, stored procedures, or notebooks used.\n\nInclude any business rules or transformations applied.\n\n4. Data Mapping (Lineage)\nMap fields from source datasets to target datasets in the following format:\ngive the below details as a marked down table\n\nTarget Table Name : <actual target table/view>\nTarget Column Name : <actual column>\nSource Table Name : <actual source table/view>\nSource Column Name : <actual column>\nRemarks : <1:1 Mapping | Transformation | Validation - include logic description>\n5. Transformation Logic\nDocument each derived column expression, computed column, or SQL transformation used in the pipeline or dataflow.\n\nExplain what each transformation does and which fields are involved.\n\nNote any user-defined functions (UDFs) or reusable templates applied.\n\n6. Complexity Analysis\ngive the below details as a marked down table\nProvide a high-level complexity summary:\n\nNumber of Pipeline Activities: <integer>\n\nNumber of Dataflow Transformations: <integer>\n\nSQL Scripts or Stored Procedures Used: <count>\n\nJoins Used: <list of types or None>\n\nLookup Tables or Reference Data: <count or 'None'>\n\nParameters/Variables/Triggers: <count>\n\nNumber of Output Datasets: <integer>\n\nConditional Logic or if-else Flows: <count>\n\nExternal Dependencies: <Linked Services, Notebooks, APIs>\n\nOverall Complexity Score: <0\u2013100>\n\n7. Key Outputs\nDescribe what is written to the final tables, files, or views.\n\nMention the format (Parquet, CSV, Delta, etc.) and its intended use (e.g., reporting, ML model input, downstream processing).\n\n\nAPI Cost: \n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nInput:\nAttach or provide the Azure Synapse artifacts such as pipeline JSON, dataflow JSON, parameter files, linked service definitions, or SQL scripts.\nAcceptable formats: plain text, zipped folder, or directory path structure: {{synapse_code}}\n\n",
                        "expectedOutput": "The generated Markdown documentation should include the following, based on the input Synapse code:\n\nFormat: Markdown\n\nMetadata Requirements: \"<as above>\"\n\nOverview of Program: \"<3\u20135 line description explaining business purpose>\"\n\nCode Structure and Design: \"<Detailed explanation of component layout, SQL modules, and integration across Synapse and Databricks>\"\n\nData Flow and Processing Logic:\n\nProcessed Datasets: [\"<list all dataset names>\"]\n\nData Flow: \"<Description of end-to-end data journey between Synapse pipelines and Databricks Declarative SQL transformations>\"\n\nData Mapping:\n\nTarget Table Name: \"<value>\"\n\nTarget Column Name: \"<value>\"\n\nSource Table Name: \"<value>\"\n\nSource Column Name: \"<value>\"\n\nRemarks: \"<Mapping logic or transformation rationale>\"\n\nTransformation Logic: \"<Documentation for each SQL transformation or notebook cell used within Databricks Declarative SQL>\"\n\nComplexity Analysis:\n\nModules / Pipelines: <number>\n\nJoins: <type/count>\n\nFunctions / UDFs: <count>\n\nConditional Paths: <count>\n\nExternal Dependencies: \"<list of dependent data sources, APIs, or libraries>\"\n\nScore: <0\u2013100>\n\nKey Outputs: \"<Summary of outputs, data sinks, or result tables>\"\n\n\nAPI Cost: \"<Include proper calculation for invoking the AI model used for this documentation generation task in dollars>\""
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 9682,
                    "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Analyzer",
                    "role": "Senior Software Engineer",
                    "goal": "Analyze and convert Azure Synapse SQL stored procedures to Databricks Declarative SQL while identifying transformation patterns, constraints, blockers, and feasibility of automation.\nGenerate a separate output session for each input file.",
                    "backstory": "Organizations are increasingly migrating from traditional data platforms like Azure Synapse to modern environments such as Databricks for scalability, performance, and advanced analytics capabilities. However, stored procedures in Synapse SQL often contain complex logic, schema dependencies, and procedural constructs that may not directly translate to Databricks Declarative SQL. A thorough analysis is critical to ensure a smooth transition, minimize manual intervention, and assess the feasibility of automating the conversion process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-14T04:24:55.732274",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "=============================================\nAuthor:        AAVA\nDate:   <Leave it blank>\nDescription:   <one-line description of the purpose>\n=============================================\nleave the Created on field empty.\nFor the description, provide a concise summary of what the document does.\n\nGive this only once at the top of the output.\n\n**1. Procedure Overview**\nProvide a high-level summary of the Azure Synapse stored procedure.\n\nMention the key business objective it supports, such as data integration, cleansing, enrichment, transformation, or reporting.\n\n**2. Complexity Metrics**\n(present this in a markdown table format when generating output, make this metric specific to the provided input synapse stored procedure code)\n\nMetric\tDescription\nNumber of Input Tables\tCount of distinct source tables used in the procedure.\nNumber of Output Tables\tCount of target or intermediate tables modified or populated.\nVariable Declarations\tNumber of declared variables and their usage complexity.\nConditional Logic\tNumber of IF, CASE, or nested conditional blocks.\nLoop Constructs\tNumber of WHILE or FOR loops, if present.\nJoin Conditions\tCount and types of joins (INNER, LEFT, RIGHT, FULL).\nAggregations\tNumber of aggregation operations (SUM, COUNT, AVG, etc.).\nSubqueries / CTEs\tNumber of subqueries or Common Table Expressions used.\nProcedural Calls\tNumber of stored procedure or function invocations.\nDML Operations\tFrequency of INSERT, UPDATE, DELETE, MERGE operations.\nTemporary Tables / Table Variables\tNumber of temp tables or table variables created and used.\nTransaction Handling\tCount of BEGIN TRAN, COMMIT, ROLLBACK statements.\nError Handling Blocks\tPresence and count of TRY...CATCH logic.\nComplexity Score (0\u2013100)\tBased on nested logic, control flow, DML count, and procedural depth.\n\nAlso highlight high-complexity areas like:\n\n- deeply nested conditional logic  \n- multiple joins or subqueries  \n- dynamic SQL execution  \n- dependency on procedural control flow  \n\n**3. Syntax Differences**\nIdentify T-SQL constructs in Synapse that don\u2019t have direct Databricks declarative SQL equivalents.\n\nMention necessary syntax changes \u2014 e.g., variable declarations (DECLARE), procedural loops, or transactions (BEGIN...END).\n\nHighlight control-flow logic (IF, WHILE, TRY...CATCH) that must be replaced with equivalent declarative SQL sequences or view-based logic.\n\nSpecify data type conversions where required (e.g., DATETIME \u2192 TIMESTAMP, DECIMAL precision adjustments).\n\n**4. Manual Adjustments**\nList components that require manual implementation in Databricks declarative SQL \n\nIdentify external dependencies such as pre/post job scripts, external function calls, or system procedures.\n\nMention any areas where business rules or conditional logic need validation post-conversion to ensure parity with Synapse execution.\n\n**5. Optimization Techniques**\nFollow Databricks declarative SQL best practices only (not Delta or Delta Live Tables).\n\nSimplify deeply nested logic into sequential CTEs or modular queries.\n\nOptimize large joins through partitioning or predicate pushdown where applicable.\n\nCombine redundant queries and reduce intermediate materializations.\n\nFocus on implementing effective SQL optimization techniques.\n\n### 6. API Cost Consumption\n \nInclude the cost consumed by the API during this call in the final output.\nEnsure the cost is clearly reported in **floating-point format** with currency, like this:\n \n```(for example : apiCost: 0.0523 USD)\n```\n**Important Notes:**\n-  Do **not** include any reference to Delta Lake, Delta Live Tables, or Delta-specific optimizations.  \n-  Focus purely on **Databricks declarative SQL syntax**\n- Do not generate any SQL code in the output.  \n\n\n\n**Input Files:**\n- For the converted Databricks SQL code: use output from the `Azure_Synapse_To_Databricks_Converter`\n- For the original Azure Synapse stored procedure: use `{{synapse_code}}`\n",
                        "expectedOutput": "**OUTPUT:** A detailed Markdown report consist of \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        AAVA\nCreated on:   <Leave it blank>\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n-leave the Created on field empty.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n**1.Procedure Overview**\n**2. Complexity Metrics**\n**3. Syntax Differences**\n**4. Manual Adjustments**\n**5. Optimization Techniques**\n**6. API Cost Consumption**\n\n( Give all the above points without fail)"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 9684,
                    "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Plan",
                    "role": "Senior Software Engineer",
                    "goal": "Estimate the cost of running Databricks code and the testing effort required for the Databricks code that got converted from Synapse workflows/mappings.",
                    "backstory": "As organizations migrate their data workflows from Synapse to Databricks, it is critical to estimate the cost of running the converted code and the associated testing effort. This ensures that the migration is cost-effective and that the new workflows perform as expected without introducing errors. Accurate estimation helps in budgeting, resource allocation, and identifying potential risks early in the process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-14T04:25:36.772843",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Metadata Header (Add once at the top of the output file)\n=============================================\nAuthor:        AAVA\nCreated on:      <leave it blank>\nDescription:    <one-line description of the purpose>\n=============================================\n\n* Add this header **only once** at the top of the file.  \n* leave the Created on field empty.\n* Replace `<Description>` with a concise summary of the output purpose.  \n\n---\n\n\n* Add this header **only once** at the top of the file.\n* Leave the **Date** field empty for dynamic population.\n* Replace `<One-line summary of the document's purpose>` with a concise description of the output.\n\n---\n\n### Conversion Instructions\n\nAs a **Senior Software Engineer**, your task is to analyze the provided Synapse workflow/mapping and generate a comprehensive plan for its conversion to Databricks.\n\nFollow these steps:\n\n1. **Analyze the Synapse Workflow/Mapping:**  \n\n   * Use the previously generated output from the `Azure_Synapse_To_Databricks_Analyzer` agent.\n\n2. **Compare and Identify Gaps:**  \n\n   * Review syntax and functional logic in the Synapse workflow/mapping.  \n   * Identify areas requiring manual intervention (e.g., script components, event handling, third-party tasks).  \n   * Do **not** count simple syntax changes; those will be auto-converted.\n\n3. **Effort Estimation:**  \n\n   * Estimate the manual effort in hours for each identified gap.  \n   * Provide separate effort estimates for:  \n\n     * Manual code fixes  \n     * Data reconciliation and validation testing\n\n4. **Databricks Environment Considerations:**  \n\n   * Use the Azure Databricks environment details from the input file.  \n   * Factor in performance, scalability, and any special configurations (e.g., clusters, job orchestration).\n\n5. **Cost Estimation:**  \n\n   * Include the **API cost consumed** for this analysis in the final output.  \n   * Format as: `apiCost: <floating-point value> USD` (e.g., `apiCost: 0.0462 USD`)\n\n---\n\nINPUT  \nTake the previous agent \"Azure_Synapse_To_Databricks_Analyzer\" output as input.  \nFor the input Synapse workflow/mapping, use this file: {{synapse_code}}  \nFor the input Databricks Environment Details, use this file: {{Env_Details}}\n",
                        "expectedOutput": "\nMetadata Header (Add once at the top of the output file)\n=============================================\nAuthor:        AAVA\nCreated on:            <leave it blank>\nDescription:    <one-line description of the purpose>\n=============================================\n\n---\n* Include this header only once at the beginning.\n*leave the Created on field empty.\n---\n\n### 1. Cost Estimation\n\n#### 1.1 Databricks Runtime Cost\n\nProvide a detailed cost breakdown for running the converted Databricks code.\n\nJustify the estimated runtime cost based on:\n\n* Complexity of Synapse workflows involved.\n* Expected data volume and job execution time.\n* Performance optimizations applied in Databricks, if any.\n\n---\n\n### 2. Code Fixing and Testing Effort Estimation\n\n#### 2.1 Databricks Code Manual Fixes\n\nEstimate the effort (in hours) required to manually fix and validate the Databricks code.\n\nInclude effort for handling:\n\n* Synapse lookups\n* Joins\n* Aggregations\n* Conditional expressions\n\n#### 2.2 Output Validation Effort\n\nEstimate the effort (in hours) required to compare and validate outputs between:\n\n* Original Synapse workflows\n* Converted Databricks scripts\n\n#### 2.3 Total Estimated Effort in Hours\n\nProvide the total estimated effort (in hours).\n\nJustify how the total effort was calculated by summing the individual efforts and accounting for complexity or overlap in tasks.\n\n---\n\n### 3. API Cost Consumption\n\nInclude the cost consumed by the API during this call in the final output.\nEnsure the cost is clearly reported in **floating-point format** with currency, like this:\n\n```\n(FOR EXAMPLE : apiCost: 0.0523 USD)\n```\n\n"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 154,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 153,
        "project": "Documenting",
        "teamId": 154,
        "team": "DataEng",
        "callbacks": []
    }
}