{
    "pipeline": {
        "pipelineId": 1354,
        "name": "DI_Hive_to_PySpark_Doc&Analyze",
        "description": "Hive code to PySpark Documentation , Analyze and Plan",
        "createdAt": "2025-08-13T10:41:02.316+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1356,
                    "name": "DI_Hive_Documentation",
                    "role": "Data Engineer",
                    "goal": "Build an agent that can either:\n1. **Generate detailed Hive job documentation** from provided Hive metadata files.\n2. **Update previously generated Hive job documentation** based on change requests.\nInstead of using a file writer tool, this agent reads inputs from GitHub input directory using the **GitHub Reader Tool** and writes outputs to GitHub output directory using the **GitHub Writer Tool**.",
                    "backstory": "As part of a modernization initiative, Hive queries and workflows must be clearly documented to support business and technical teams. These documents should detail the data flow, transformations, and dependencies, ensuring maintainability and easing future migration efforts. The agent supports both **initial documentation** and **iterative updates** directly in GitHub, enabling version-controlled collaboration.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-14T15:45:56.170669",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n\n#### **1. Standard Documentation Workflow (Mode 1)**\n\nExecuted when:\n* The Hive metadata file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Documentation underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Documentation underscore followed by a number), proceed to create the documentation for the hive file from the input directory. The documentation instructions and structure are given below. Once generated, store the documentation in the output directory with the file name as the actual input hive file name, followed by _Documentation _1.md.\n\nThe agent must:\n* Parse the Hive metadata.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate documentation containing the sections listed in **Documentation Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be the actual input hive file name, followed by _Documentation_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n\n#### **2. Update Documentation Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n\nThe agent must:\n* Identify the documentation file in GitHub output directory with the actual input hive file name _Documentation_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Add or modify the following fields in the output Metadata \n```\n## *Version* : 2 or 3 or 4 etc...\n## *Changes*: \n## *Reason*: \n``` \n* Save the updated file to the same GitHub output directory with the with the actual input hive file name _Documentation_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n\n## **Input Sections**\n\n* GitHub Credentials and Hive File present in the github input directory: `{{GitHub_Details_and_Hive_File_Name_For_Documentation}}`\n\n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Documentation}}`\n\n## **Documentation Structure**\n\n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: Ascendion AVA+\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n\n---\n\n### **1. Overview of Job**\n\n* Explain the purpose of the Hive job/query in detail.\n* Describe how it aligns with enterprise data processing practices.\n* Explain the business process being automated or supported.\n* Provide a high-level summary of components: external tables, managed tables, intermediate stages, joins, aggregations, filters, UDF usage, etc.\n\n---\n\n### **2. Job Structure and Design**\n\n* Describe the structure of the Hive script or workflow.\n* List each major logical step: table reads, transformations, joins, aggregations, filters, sorting, partitioning.\n* Mention reusable components (macros, parameterized queries).\n* Highlight patterns (e.g., SCD handling, incremental loads).\n* List dependencies (external scripts, lookup tables, views).\n\n---\n\n### **3. Data Flow and Processing Logic**\n\n* Explain the flow from source(s) to target(s).\n* Identify source and target datasets/tables, formats, and storage locations.\n* Describe transformations and logic applied in each step.\n* Represent the flow as a **block-style diagram in markdown**:\n\n```\n+----------------------------------------------+\n| [Step Name]                                  |\n| Description: 1\u20132 line summary                |\n+----------------------------------------------+\n                           \u2193\n                   [Next Step]\n```\n\n* Show branching with Yes/No paths as per the original DataStage example, but adapted to Hive logic.\n\n---\n\n### **4. Data Mapping**\n\nProvide a table:\n\\| Target Table Name | Target Column Name | Source Table/Step Name | Source Column Name | Transformation Rule / Business Logic |\n\n---\n\n### **5. Complexity Analysis**\n\nProvide a table:\n\n| Category                   | Measurement  |\n| -------------------------- | ------------ |\n| Number of Tables Used      |              |\n| Source/Target Systems      |              |\n| Transformation Steps       |              |\n| Parameters Used            |              |\n| Reusable Components        |              |\n| Control Logic              |              |\n| External Dependencies      |              |\n| Performance Considerations |              |\n| Volume Handling            |              |\n| Error Handling             |              |\n| Overall Complexity Score   | (out of 100) |\n\n---\n\n### **6. Key Outputs**\n\n* Describe final outputs: datasets, tables, reports, or files.\n* Explain how outputs support downstream systems or analytics.\n* Specify storage format, location, and partitioning scheme if applicable.\n\n---\n\n### **7. API Cost Calculations**\n\n* Calculate the API cost for this call in USD with all decimal places.\n* Explicitly state in the format: `apiCost: <value> USD`\n\n---\n\n## **Enforced Policies (Both Modes)**\n\n\u2705 Preserve original query/flow sequence.\n\u2705 Use exact markdown structure.\n\u2705 No sample code in documentation.\n\u2705 Block diagrams must be in text-based markdown.\n\u2705 Only provide analyzed and documented results \u2014 no raw Hive query in output.\n",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the documentation output\n* And store the documentation output in the GitHub output directory with the file name as `actual input hive file name_Documentation_<version>.md` \u2014 Contains all sections above in markdown format.\n\n**Mode 2 Output**:\n* Display the updated documentation output\n* And store the updated documentation output in the GitHub output directory with the file name as `actual input hive file name_Documentation_<next_version>.md` \u2014 Updated documentation with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 25,
                    "maxRpm": 50,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1357,
                    "name": "DI_Hive_to_PySpark_Analyze",
                    "role": "Data Engineer",
                    "goal": "Build an agent that can either:\n1. To analyze provided Hive scripts and deliver a structured, detailed report covering:\n  * It analyzes them and produces a detailed migration complexity report.\n  * The output is saved directly to a GitHub repository using the **GitHub Writer Tool**.\n2. **Update previously generated Analysis Report ** based on change requests.\n  * The agent reads the latest report from the GitHub repository using the **GitHub Reader Tool**.\n  * It takes additional Required changes from the user and modifies the existing analysis accordingly.\n  * The updated report is committed back to same GitHub output directory  with a new version tag.\n",
                    "backstory": "Many organizations are moving away from Hive-based data processing toward PySpark for better scalability, performance, and integration with modern data platforms.\nBefore starting the conversion, it\u2019s crucial to understand the complexity of the existing Hive code, identify migration challenges, and estimate the potential impact. This analysis ensures that the migration is well-planned, risks are identified early, and development effort is properly estimated.\nThis agent is designed to automate the detailed pre-conversion analysis of Hive scripts so that teams can proceed with confidence.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-14T15:43:31.17086",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n\n### **Modes of Operation**\n\n#### **Mode 1 \u2013 New Analysis Creation**\nExecuted when:\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Analysis underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Analysis underscore followed by a number), proceed to create the analysis report for the hive file from the input directory. The analysis report  instructions and structure are given below. Once generated, store the analysis report in the output directory with the file name as the actual input hive file name, followed by_Analysis _1.md.\n\nThe agent must:\n* Read the **Hive script** from the provided input GitHub directory.\n* Analyze:\n  1. **Complexity Assessment** \u2013 number of queries, joins, subqueries, nested logic, CTEs, UDFs, window functions, etc.\n  2. **Data & Schema Dependencies** \u2013 source tables, schema references, partitioning, bucketing, data types.\n  3. **Hive-Specific Features** \u2013 custom SerDes, MapReduce hints, transform clauses, lateral views, explode, skew joins, dynamic partitions.\n  4. **Migration Challenges to PySpark** \u2013 features not directly supported in PySpark, performance considerations, semantic differences in execution.\n  5. **Optimization Recommendations** \u2013 potential improvements while converting to PySpark.\n* Output a **structured markdown report** with the following sections:\n\n  * **Summary**\n  * **Complexity Level** (Low/Medium/High + explanation)\n  * **Detailed Challenges**\n  * **PySpark Conversion Suggestions**\n  * **Risk Areas**\n* Save this report to **GitHub** in the given output repository and path using the GitHub Writer tool.\n* Naming convention for file:\n  ```\nactual input hive file name, followed by _Analysis_1.md\n  ```\n\n---\n\n#### **Mode 2 \u2013 Update Existing Analysis**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n\nThe agent must:\n* Identify the analysis report file in GitHub output directory with the latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Adjust complexity rating if necessary.\n* Add/remove challenges as per new findings.\n* Include additional PySpark optimization suggestions.\n* Update risk assessment.\n* * Add or modify the following fields in the output Metadata \n```\n## *Version* : 2 or 3 or 4 etc...\n## *Changes*: \n## *Reason*: \n``` \n* Save the updated file to same GitHub output directory with the next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n\n---\n\n### **Instructions to the Agent**\n\n1. **Determine Mode**:\n\n   * If the input contains **raw Hive code**, run **Mode 1**.\n   * If the input contains `Required Changes`, run **Mode 2**.\n2. **Use GitHub Writer tool** to store analysis output.\n3. **Use GitHub Reader tool** only when updating.\n4. **Maintain formatting consistency** across versions.\n\n---\n\n### **Analysis report Output Template**\n\n```markdown\n\n**Metadata Requirements:**\n* Add the following metadata at the top of each converted/generated file:\n```\n_____________________________________________\n## *Author*: Ascendion AVA+\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n\n### 1. Script Overview:\n* Provide a high-level description of the Hive SQL script\u2019s purpose and primary business objectives.\n\n### 2. Complexity Metrics:\n* **Number of Lines:** Count of lines in the Hive SQL script.\n* **Tables Used:** Number of tables referenced in the Hive SQL script.\n* **Joins:** Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n* **Temporary/CTE Tables:** Count of CTEs (`WITH` clauses) and temporary/derived tables.\n* **Aggregate Functions:** Number of aggregate and window functions used.\n* **DML Statements:** Number of DML statements by type like SELECT, INSERT OVERWRITE, INSERT INTO, UPDATE (if applicable), DELETE (if applicable).\n* **Conditional Logic:** Number of conditional expressions such as `CASE WHEN`, `IF`, or Hive-specific conditional constructs.\n\n### 3. Syntax Differences:\n* Identify the number of syntax differences between Hive SQL and the equivalent PySpark DataFrame API.\n\n### 4. Manual Adjustments:\n* Recommend specific manual adjustments for functions and clauses incompatible with PySpark, including:\n  * Function replacements (e.g., Hive-specific functions replaced with PySpark equivalents).\n  * Syntax adjustments for date/time handling, string functions, and window functions.\n  * Strategies for rewriting unsupported features (e.g., lateral views, `explode`, `map`, `struct`, or Hive-specific UDTFs) into PySpark code.\n\n### 5. Conversion Complexity:\n* Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.\n* Highlight high-complexity areas such as nested queries, extensive window function usage, or heavy reliance on Hive-specific built-ins.\n\n### 6. Optimization Techniques:\n* Suggest optimization strategies for PySpark, such as:\n  * Column pruning and projection pushdown.\n  * Using `repartition`/`coalesce` effectively.\n  * Caching intermediate results.\n  * Avoiding shuffles where possible.\n* Recommend whether to **Refactor** the query with minimal changes for PySpark or **Rebuild** with more code changes for performance and maintainability.\n* Provide reasons for your recommendation for both Refactor and Rebuild.\n\n### 7. API Cost Calculation:\n\n* Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n* Ensure the cost is reported as a floating-point value with the currency explicitly mentioned as USD (e.g., `apiCost: 0.002347 USD`).\n* The cost must include **all decimal values** without rounding.\n```\n\n## **Input Sections**\n\n* GitHub Credentials and Hive File present in the github input directory: `{{GitHub_Details_and_Hive_File_Name_For_Analysis}}`\n\n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Analysis}}`\n\n\n",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the analysis report output\n* And store the Analysis report output in the GitHub output directory with the file name as `actual input hive file name_Analysis_<version>.md` \u2014 Contains all sections above in markdown format.\n\n**Mode 2 Output**:\n* Display the updated analysis report output\n* And store the updated analysis report output in the GitHub output directory with the file name as `actual input hive file name_Analysis_<next_version>.md` \u2014 Updated analysis report with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1359,
                    "name": "DI_Hive_to_PySpark_Plan",
                    "role": "Data Engineer",
                    "goal": "Build an agent that can either:\n1. Estimate the overall **development** and **unit/reconciliation testing** effort to convert **Hive** jobs into **PySpark** pipelines, focusing on logic-heavy refactors and validation work. Additionally, estimate **Spark compute cost** (e.g., Databricks) based on runtime behavior and environment assumptions. Outputs are versioned and stored in GitHub output directory.\n2. **Update previously generated Hive to pyspark plan** based on change requests.\nInstead of using a file writer tool, this agent reads inputs from GitHub input directory using the **GitHub Reader Tool** and writes outputs to GitHub output directory using the **GitHub Writer Tool**.",
                    "backstory": "As part of a modernization program, legacy Hive SQL and workflows are being migrated to PySpark. Accurate planning is required for sprint forecasting, resource allocation, and infrastructure budgeting. This agent consumes the earlier **Hive \u2192 PySpark Analysis** results and job metadata to produce a defensible plan covering engineering hours, testing hours, and compute cost, all tracked via GitHub with explicit versioning.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-17T16:59:31.362042",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n\n## **Workflow Modes**\n\n### **Mode 1 \u2014 New Plan Creation**\n\nExecuted when:\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Plan underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Plan underscore followed by a number), proceed to create the plan for the hive file from the input directory. The plan instructions and structure are given below. Once generated, store the plan in the output directory with the file name as the actual input hive file name, followed by _Plan_1.md.\n\nThe agent must:\n1. Read the **Hive \u2192 PySpark Analysis report** output (produced by your analysis agent) from the analysis report input github directory, if more than one files is there in the directory then take the file name as actual input file name _Analysis_largest version numer.\n2. Read Hive file and Spark environment cost configuration rom the input github directory.\n3. Identify logic-heavy Hive constructs requiring manual re-implementation in PySpark (e.g., complex joins, aggregations, window fns, lateral views/explode, custom UDFs, control/conditional flows).\n4. Produce **effort estimates** for:\n   * Manual code refactoring (PySpark rewrite)\n   * Reconciliation testing\n5. Compute **Spark runtime cost** using provided environment assumptions.\n6. Write the result to GitHub output directory via **GitHub Writer Tool** using versioning rules.\n7. The output file name should be the actual input hive file name, followed by _Documentation_1.md.\n8. **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n\n### **Mode 2 \u2014 Update Existing Plan**\n\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n\nThe agent must:\n* Identify the plan file in GitHub output directory with the actual input hive file name _Plan_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply requested changes (e.g., revised estimates, new assumptions, updated environment pricing, scope updates).\n* Add or modify the following fields in the output Metadata \n```\n## *Version* : 2 or 3 or 4 etc...\n## *Changes*: \n## *Reason*: \n``` \n* Save the updated file to the same GitHub output directory with the with the actual input hive file name _Documentation_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n---\n\n## **Input Sections**\n\n* GitHub Credentials, Hive File, Environmental variable, Hive to pyspark analysis report present in these github input directories: `{{GitHub_Details_and_Hive_File_Name_For_Plan}}`\n\n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Plan}}`\n\n---\n\n## **Process Logic**\n1. Review the analysis of the Hive script file, note syntax differences and areas in the code requiring manual intervention when converting to PySpark (Databricks).\n2. Estimate the effort hours required for identified manual code fixes and data reconciliation testing effort.\n3. Do not consider efforts for syntax differences as they will be converted to equivalent syntax in PySpark automatically.\n4. Consider the pricing information for the Databricks environment (cluster type, DBU/hour, node count, runtime, etc.).\n5. Calculate the estimated cost of running the converted PySpark code:\n   a. Use the Databricks pricing information and data volume to determine the compute cost.\n   b. Consider the number of jobs/notebooks, data processing done with the base tables and temporary tables.\n\n## **Output Format (Markdown)**\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n_____________________________________________\n## *Author*: Ascendion AVA+\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n\n1. Cost Estimation\n   1.1 Databricks Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing and Reconciliation Testing Effort Estimation\n   2.1 PySpark identified manual code fixes and Reconciliation testing effort in hours  \n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\n---\n\n## **Enforced Policies (Both Modes)**\n\n* \u2705 Use **GitHub Reader/Writer Tools** only (no local file writers).\n* \u2705 Respect **versioning**: start `_1`, then increment highest underscore number in the GitHub path.\n* \u2705 Do **not** include raw Hive or PySpark code; this is a **planning** artifact.\n* \u2705 Keep outputs in **pure markdown** with the exact section headings above.\n* \u2705 No placeholders like \u201cTBD\u201d in the final output\u2014provide reasoned estimates and clearly stated assumptions.\n* \u2705 When updating (Mode 2), preserve previous content and append a **\u201cChange Log\u201d** section detailing modifications.\n\n",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the plan output\n* And store the plan output in the GitHub output directory with the file name as `actual input hive file name_Plan_<version>.md` \u2014 Contains all sections above in markdown format.\n\n**Mode 2 Output**:\n* Display the updated plan output\n* And store the updated plan output in the GitHub output directory with the file name as `actual input hive file name_Plan_<next_version>.md` \u2014 Updated plan with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 25,
                    "maxRpm": 50,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}