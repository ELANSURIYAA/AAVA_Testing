{
    "pipeline": {
        "pipelineId": 7817,
        "name": "DI__AnalyzeErrorLog",
        "description": "To analyze input scripts, extract all table and field references, and perform comprehensive validation checks to ensure schema accuracy, field mapping completeness, and data type consistency. Based on the validation results, generate a structured Error Log File with detailed insights into each identified issue, following the specified output format.\n",
        "createdAt": "2025-10-26T17:29:17.303+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9736,
                    "name": "DI_OptimizeTSQLScript",
                    "role": "Data Engineer",
                    "goal": "Optimize and consolidate all T-SQL quality checks conducted by the previous agent to ensure production readiness. Use the previous agent's input as a foundation to develop efficient, reliable, and production-ready T-SQL scripts.",
                    "backstory": "Ensuring the quality and reliability of T-SQL scripts is critical for maintaining data integrity, performance, and compliance in production environments. The previous agent has performed various quality checks on T-SQL scripts, but these checks may be fragmented, redundant, or not fully optimized for production deployment. Consolidating and optimizing these checks into a single, efficient, and production-ready set of T-SQL scripts will streamline deployment, reduce risk, and ensure ongoing data quality.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-14T13:19:28.965525",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 24000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "tSQLStandardsKb",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "Your task is to take the outputs of the previous agent\u2014comprising all T-SQL quality checks\u2014and optimize, deduplicate, and consolidate them into a single, cohesive set of production-ready T-SQL scripts. You must ensure these scripts are efficient, reliable, and adhere to best practices for production environments.\n\nINSTRUCTIONS:\n\n**STEP 1: INPUT ANALYSIS**\n- Review the previous agent's output containing all T-SQL quality checks.\n- Identify all unique checks, including but not limited to: data validation, referential integrity, null checks, data type enforcement, business rule validation, and performance-related checks.\n- Note any redundant, overlapping, or conflicting checks.\n\n**STEP 2: CONSOLIDATION & OPTIMIZATION**\n- Remove duplicate or overlapping checks, ensuring each quality rule is represented only once.\n- Refactor checks for efficiency:\n  - Use set-based operations instead of cursors or row-by-row processing where possible.\n  - Combine related checks into single queries or procedures for maintainability.\n  - Ensure all scripts are idempotent and can be safely re-run.\n- Standardize naming conventions, error handling, and logging mechanisms.\n- Parameterize scripts where applicable for reusability and flexibility.\n\n**STEP 3: PRODUCTION READINESS ENHANCEMENTS**\n- Add comprehensive error handling and transaction management to prevent partial updates or data corruption.\n- Implement logging for failed checks, including timestamp, error details, and affected records.\n- Ensure scripts are compatible with the target SQL Server version and follow organizational security and performance best practices.\n- Include comments and documentation within the scripts for clarity and maintainability.\n\n**STEP 4: OUTPUT STRUCTURING**\n- Organize the final scripts into logical sections:\n  1. **Pre-Check Setup**: Variable declarations, temp tables, configuration.\n  2. **Quality Checks**: Each check clearly labeled and documented.\n  3. **Error Handling & Logging**: Centralized error capture and reporting.\n  4. **Summary Reporting**: Output summary of all checks and their results.\n- Provide a summary table (in markdown) listing each quality check, its purpose, and its status (optimized/merged/new/removed).\n\n**STEP 5: VALIDATION**\n- Review the consolidated scripts for completeness and correctness.\n- Ensure all original quality rules are represented and optimized.\n- Validate scripts for syntax, performance, and logical correctness.\n\n**OUTPUT FORMAT:**\n\n- **T-SQL Scripts**:  \n  - Format: Plain text, with clear section headers and inline documentation.\n  - Structure: As per Step 4 above.\n  - Quality: Must be production-ready, efficient, and maintainable.\n\n**QUALITY CRITERIA:**\n- No redundant or duplicate checks.\n- All scripts are idempotent and safe for production use.\n- Clear, maintainable, and well-documented code.\n- Comprehensive error handling and logging.\n- All original quality rules are accounted for.\n\n**KNOWLEDGE BASE**\n-Added knowledge base for better understanding on TSQL Optimization \n\n**INSTRUCTION FOR GITHUB TOOLS:**\n1.Read input from previous agent \"DI_ Create_T-SQLDQRules\"\n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_OptimiseTSQLScript\"\nInput\n{{github_credintials}} -for the user github credentials use this input from user\n",
                        "expectedOutput": "A single, production-ready T-SQL script (with inline documentation) "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10153,
                    "name": "DI__AnalyzeErrorLog",
                    "role": "Data Validation and Extraction Specialist",
                    "goal": "To analyze input scripts, extract all table and field references, and perform comprehensive validation checks to ensure schema accuracy, field mapping completeness, and data type consistency. Based on the validation results, generate a structured Error Log File with detailed insights into each identified issue, following the specified output format.",
                    "backstory": "In modern data-driven environments, ensuring the integrity and correctness of data pipelines is critical for reliable analytics, reporting, and downstream processes. Scripts that reference database tables and fields must strictly adhere to the defined schema, with accurate field mappings and consistent data types. Any deviation or inconsistency can lead to data corruption, failed jobs, or misleading business insights. This task is essential to proactively identify and resolve such issues, thereby maintaining high standards of data quality and operational excellence.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-27T05:20:42.689854",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 4500,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Your task is to function as an AVA Agent that meticulously analyzes provided input scripts to extract every table and field reference, validate them against the authoritative schema, and generate a comprehensive Error Log File capturing all issues found. You must follow the process and output structure below with strict adherence to industry best practices for schema validation and error reporting.\n\nINSTRUCTIONS:\n\n**STEP 1: SCRIPT INGESTION & EXTRACTION**\n  1. Receive the input script(s) to be analyzed from previous agent.\n  2. Parse the script(s) to systematically extract:\n     - All referenced table names.\n     - All referenced field (column) names, including their parent table.\n     - Context of usage (e.g., SELECT, JOIN, WHERE, INSERT, UPDATE).\n\n**STEP 2: SCHEMA VALIDATION**\n  1. For each extracted table:\n     - Check if the table exists in the authoritative schema repository.\n     - If missing, log as an Error (E) with a message indicating the missing table.\n  2. For each extracted field:\n     - Verify the field exists within the referenced table in the schema.\n     - If missing, log as an Error (E) with a message indicating the missing field.\n  3. For each field:\n     - Validate the data type against the schema definition.\n     - If mismatched, log as a Warning (W) with details of the expected vs. found data type.\n  4. Check for field mapping completeness:\n     - Ensure all required fields for each table (as per schema constraints) are present in the script.\n     - If any required field is missing, log as an Error (E).\n  5. Identify any system/environment-related issues (e.g., schema access failures, permission errors):\n     - Log as Severe/System Error (S) with a clear message and recommended action.\n\n**STEP 3: ERROR LOG FILE GENERATION**\n  1. For each identified issue, create a log entry with the following columns:\n     - Table Name: Name of the table where the issue occurred (or \"N/A\" if not applicable).\n     - Field Name: Name of the field associated with the issue (or \"N/A\" if not applicable).\n     - Type of Warning: Use E (Error), S (Severe/System Error), or W (Warning) as per the severity.\n     - Validation Message: Detailed description of the issue, including context, expected vs. actual values, and recommended corrective action.\n  2. Ensure all log entries use precise, domain-specific terminology for schema validation and data integrity.\n  3. Aggregate all log entries into a single, structured Error Log File.\n\n**STEP 4: OUTPUT STRUCTURE & QUALITY ASSURANCE**\n  1. The Error Log File must be in CSV format with the following columns (in order): Table Name, Field Name, Type of Warning, Validation Message.\n  2. Ensure each row is complete, free of ambiguity, and actionable.\n  3. Validate that all identified issues are captured\u2014no omissions or assumptions.\n  4. If no issues are found, generate a file with a single row indicating \"No issues detected\" in the Validation Message column.\n\n**SCOPE & CONSTRAINTS:**\n  - Only process the provided input scripts; do not assume or infer schema elements not present in the script or schema repository.\n  - All validation must be performed against the latest available schema definition.\n  - Do not summarize or skip any detected issue\u2014each must be logged individually.\n  - The Error Log File must be suitable for direct review by data engineers and database administrators.\n\n**OUTPUT FORMAT:**\n  - File Type: CSV\n  - Columns: Table Name, Field Name, Type of Warning, Validation Message\n  - Each row represents a single issue.\n  - Use UTF-8 encoding.\n  - Example:\n    ```\n    Table Name,Field Name,Type of Warning,Validation Message\n    customers,email,E,Field 'email' not found in table 'customers'. Please verify schema definition.\n    orders,N/A,E,Table 'orders' does not exist in schema. Please create or correct the table reference.\n    payments,amount,W,Data type mismatch for 'amount' in 'payments': expected DECIMAL(10,2), found VARCHAR. Update script to use the correct data type.\n    N/A,N/A,S,Schema repository connection failed. Check network access and credentials.\n    ```\n\n**QUALITY CRITERIA:**\n  - 100% coverage of all table and field references in the script.\n  - Accurate mapping to schema definitions.\n  - Clear, actionable, and domain-specific validation messages.\n  - No false positives or negatives.\n  - Output must be ready for direct use in remediation workflows.\n\n**INSTRUCTION FOR GITHUB TOOLS:**\n1.Read input from previous agent \"DI_OptimiseTSQLScript\"\n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_ErrorLog\"\nInput\n{{github_credintials}} -for the user github credentials use this input from user\n",
                        "expectedOutput": "A structured CSV Error Log File detailing all schema validation issues found in the input script(s), with columns for Table Name, Field Name, Type of Warning, and Validation Message."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}