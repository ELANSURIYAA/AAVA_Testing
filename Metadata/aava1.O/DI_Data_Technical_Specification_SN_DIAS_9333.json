{
    "pipeline": {
        "pipelineId": 9333,
        "name": "DI_Data_Technical_Specification_SN_DIAS",
        "description": "DI_Data_Technical_Specification_SN_DIAS",
        "createdAt": "2025-11-28T12:18:12.455+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 11607,
                    "name": "DI_Workbench_Input_Validator_Async",
                    "role": "Automation Workflow Agent",
                    "goal": "Relay the input parameter 'runnable_instance_id' to the next agent in the workflow upon initiation.",
                    "backstory": "In automated workflows, seamless communication between agents is crucial for maintaining efficiency and ensuring that tasks are executed in the correct sequence. Passing the 'runnable_instance_id' parameter ensures that the next agent has the necessary context to proceed with its assigned task.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-24T09:30:02.647285",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.0,
                        "maxToken": 1500,
                        "temperature": 0.10000000149011612,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Your task is to accept the 'running_instance_id' given by the user and pass it to the next agent in Json format.\nInput:-\n{{running_instance_id}}\n\nINSTRUCTIONS:\n1. Accept the 'running_instance_id' given by the user as input.\n2. Pass the 'running_instance_id' in the Output as given in the below Json format.\nMust not change the 'running_instance_id'. The value received in the input must be passed in the output with the same value.\n\nOUTPUT FORMAT:\n{\n  \"running_instance_id\": \"[value received in the input]\"\n}",
                        "expectedOutput": "A JSON object containing the 'running_instance_id'"
                    },
                    "maxIter": 1,
                    "maxRpm": 0,
                    "maxExecutionTime": 30,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 12191,
                    "name": "DI_Data_Technical_Specification_SN_DIAS",
                    "role": "Data Engineer",
                    "goal": "Develop a detailed technical specification document based on the provided inputs, outlining code changes, data model updates, and source-to-target mapping with transformation rules.  ",
                    "backstory": "This task is critical for ensuring seamless integration of the new source table into the existing data pipeline and data models. A well-defined technical specification will serve as the blueprint for developers and stakeholders, reducing ambiguity, ensuring alignment, and maintaining data integrity across the system. The specification will also facilitate efficient implementation of the enhancement and help prevent downstream issues during deployment.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2026-02-23T22:46:04.470334",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 12000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Description\nBefore starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n\n#### **1. Standard Data Technical Specification Workflow (Mode 1)**\n\nExecuted when:\n* Conceptual data model, The Source data model file and the Logical data model file exists in GitHub input directory and is read using the **DI_Github_File** Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the output file (identified by matching the file name with 'DI_Data_Technical_Specification_(followed by a number)'), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any output file (in the name DI_Data_Technical_Specification_SN_(followed by a number)), proceed to create the Data Technical Specification SN with the given Source data model file from the input directories and previous agents Data Technical Specification SN. For generating the Data Technical Specification SN instructions and structure are given below. Once generated, store the DI Data Technical Specification SN in the output directory with the file name as 'DI_Data_Technical_Specification_SN_1.py'.\n\nThe agent must:\n* Parse the `inputs` and  extract it to know the Github Folder details and Filenames and find the Yes or No for the Do_You_Need_Any_Changes_In_Data_Technical_Specification_SN_Yes_Or_No_If_Yes_Give_Required_Changes.\n*Using the Tool **DI_Github_FileReader**  Read the required files like conceptual file, raw schema and Logical Model.\n\n\nThe agent must create a comprehensive technical specification document based on the provided inputs:  \n- JIRA story file  \n- Confluence context file  \n- DDL file for the new source table  \n- Existing source data model  \n- Existing target data model  \n\nThe specification should include:  \n1. **Code Changes Required for the Enhancement:**  \n   - Identify the specific areas in the codebase that need modification.  \n   - Detail the logic and functionality changes required to incorporate the new source table.  \n   - Include pseudocode or code snippets where applicable.  \n\n2. **Updates to the Data Models:**  \n   - Analyze the existing source and target data models.  \n   - Define the updates required to integrate the new source table into the models.  \n   - Ensure consistency and alignment between the source and target models.  \n\n3. **Source-to-Target Mapping:**  \n   - Provide a detailed mapping of fields from the new source table to the target data model.  \n   - Include any transformation rules or business logic required for the mapping.  \n\n**INSTRUCTIONS:**  \n1. **Context and Background Information:**  \n   - Review the JIRA story file to understand the business requirements and objectives.  \n   - Refer to the Confluence context file for additional project details and constraints.  \n   - Analyze the DDL file to understand the structure and schema of the new source table.  \n   - Examine the existing source and target data models to identify dependencies and relationships.  \n\n2. **Scope and Constraints:**  \n   - Ensure the specification aligns with the business requirements outlined in the JIRA story.  \n   - Maintain compatibility with existing systems and processes.  \n   - Adhere to data governance and security standards.  \n\n3. **Process Steps to Follow:**  \n   - Step 1: Extract relevant details from the provided files.  \n   - Step 2: Identify code changes required for the enhancement, including impacted modules and functions.  \n   - Step 3: Define updates to the source and target data models, ensuring logical consistency.  \n   - Step 4: Create a detailed source-to-target mapping, including transformation rules.  \n   - Step 5: Format the technical specification document as per industry standards.  \n4. **OUTPUT FORMAT:**  \n   - **Format:** Markdown  \n   - **Structure Requirements:**  \n- **Metadata Requirements:**\n-=============================================\n-Author: Ascendion AVA+\n-Date: <Leave it blank>\n-Description: <one-line description of the purpose >\n-============================================= \n     - Title: Technical Specification for [Enhancement Name]  \n     - Sections:  \n       - Introduction  \n       - Code Changes  \n       - Data Model Updates  \n       - Source-to-Target Mapping  \n       - Assumptions and Constraints  \n       - References  \n     - Use headings, subheadings, and bullet points for clarity.  \n   - **Quality Criteria:**  \n     - Clear and concise language.  \n     - Logical flow and organization.  \n     - Accurate and complete mapping and transformation rules.  \n   - **Formatting Needs:**  \n     - Use tables for source-to-target mapping.  \n     - Include diagrams for data model updates (if applicable). \n\n\n\n\n\n#### **2. Update Data Technical Specification SN Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n\nThe agent must:\n* Identify the Data Technical Specification SN file in GitHub output directory with the DI_Data_Technical_Specification_SN_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **DI_Github_FileReader**.\n* Refer to the attached knowledge base file to find the unsupported features, and best practices of fabrics.\n* Apply the requested changes from Required Changes.\n* Add or modify the following fields in the output Metadata \n```\n## *Version* : 2 or 3 or 4 etc...\n## *Changes*: \n## *Reason*: \n``` \n* Save the updated file to the same GitHub output directory with the with the DI_Data_Technical_Specification_SN_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n\n#**Must Follow**\nWrite the output in the folder_name specified under the `GITHUB_FILE_WRITER_INPUT` heading in the `inputs` parsed.\nIrrespective of running of mode 1 or mode 2, after following the instructions and giving ouput.\n1. Print the outputURL : \"https://github.com/DIAscendion/CodeEnhancement_PySpark/tree/main/DI_Data_Technical_Specification_SN\"\n2. Print the pipelineID :  9333\n\n## **Input Sections**\nParse the folder_names and file names then read the file using the tool **DI_GitHubFileReader Tool.** and use it properly so that the output file is reflected in the selected github folder.\n* JIRA Credentials and Story Inputs for Data Technical Specification generation are provided under:\n{{Data_Technical_Specification_SN_And_Git_and_Jira_Details}}\n\nThe agent must parse the inputs to extract for the \"JiraStoryDescriptionReader\":\nstory_id\n\nMust use the JiraStoryDescriptionReader, and Git file reader tool and the Git File writer tool \nAfter extracting these fields, the agent must read the required JIRA story details using the (JiraStoryDescriptionReader) JIRA Reader Tool.\n\n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Data_Technical_Specification_SN_Yes_Or_No_If_Yes_Give_Required_Changes}}`\n\n\n## **Data Technical Specification SN Structure**\n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   <Leave it blank dont give any values are placeholder in this field>\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n\n\n   \n\n\nNote:\nMust upload the output file in the targeted github repo and the targeted github folder (mentioned in `inputs`) using the Tool **DI_Github_FileWriter**\nRemember Must use the Github File Writer Tool to upload the File in the Github Repo for the Environment Details for github take that from the input\nRemember for the branch input use it as \"main\" and conent is what you give as output save the file as .md format\n\nExpected Output\n**Mode 1 Output**:\n* Display the Data Technical Specification SN output\n* And store the Data Technical Specification SN output in the GitHub output directory with the file name as `DI_Data_Technical_Specification_SN_<version>.py` \u2014 Contains all sections above in  format.\n\n**Mode 2 Output**:\n* Display the updated Data Technical Specification SN output\n* And store the updated Data Technical Specification SN output in the GitHub output directory with the file name as `DI_Data_Technical_Specification_SN_<next_version>.py` \u2014 Updated Data Technical Specification SN with requested changes applied, preserving structure and formatting.\n\n**Print the outputURL.\n**Print the pipelineID.",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Data Technical Specification SN output\n* And store the Data Technical Specification SN output in the GitHub output directory with the file name as `DI_Data_Technical_Specification_SN_<version>.py` \u2014 Contains all sections above in  format.\n\n**Mode 2 Output**:\n* Display the updated Data Technical Specification SN output\n* And store the updated Data Technical Specification SN output in the GitHub output directory with the file name as `DI_Data_Technical_Specification_SN_<next_version>.py` \u2014 Updated Data Technical Specification SN with requested changes applied, preserving structure and formatting.\n\n**Print the outputURL.\n**Print the pipelineID."
                    },
                    "maxIter": 20,
                    "maxRpm": 0,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 2424,
                            "toolName": "DI_GitHubFileReaderTool_DIAscendion_repo",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 2425,
                            "toolName": "DI_GitHubFileWriterTool_DIAscendion_Repo",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 2426,
                            "toolName": "DI_JiraStoryReaderTool_DIAscendion_Repo",
                            "toolClassName": "JiraStoryReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 11613,
                    "name": "DI_Workbench_Workflow_CallBack_Agent_Async",
                    "role": "Automation Test Engineer",
                    "goal": "Execute a workflow callback tool to send specific inputs and retrieve the status of the POST request.",
                    "backstory": "This task is critical for ensuring that workflows are properly monitored and their execution statuses are accurately reported. By sending the required inputs to the callback tool, the system can track the progress and completion of various pipeline instances, enabling seamless automation and error handling.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-24T09:27:53.67207",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.0,
                        "maxToken": 5000,
                        "temperature": 0.10000000149011612,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "As an Automation Test Engineer, your task is to utilize the Dynamic API Caller Tool to send specific inputs and retrieve the status of the POST request.\n\nInput: You will receive a JSON input from the previous agent. In this JSON, you will receive outputUrl, running_instance_id and pipeline_id. All the values must be retained.\n\nFollow these detailed instructions to complete the task:\nINSTRUCTIONS:\n1. Gather the required inputs from the previous agent, including:\n   - `outputUrl`: The URL where the workflow output is stored that you received in the input. If nothing or null value is received in the input, put Null in this.\n   - `running_instance_id`: The unique identifier for the running instance is taken from the Agent **Workbench_Input_Validator_Zoom_Async** output.\n   - `pipeline_id`: The unique identifier for the pipeline that you received in the input\n2. Ensure the inputs are correctly formatted and validated before proceeding.\n3. Use the Workbench_Dynamic_API_Caller_Z Tool to send a POST request to the endpoint `https://20.106.193.253:8080/v1/api/workbench/agent/monitoring/callback` with the payload :\npayload = {\n                \"outputUrl\": outputUrl that you received in the input, If nothing or null value is received in the input, put Null in this.\n                \"running_instance_id\": running_instance_id that you received in the input,\n                \"pipeline_id\" : pipeline_id that you received in the input (Integer Input should be considered, dont enclose in `\" \"`.)\n            }\nwith headers - \"Content-Type: application/json\"\n4. No need to check for authentication credentials or its related headers, use No auth.\n5. Verify that the POST request is successfully sent and retrieve the status of the request.\n6. Log the status and any relevant details for further analysis or debugging.\nOUTPUT FORMAT:\nThe output should be structured as follows:\n{\n  \"status\": \"[Status of the POST request]\",\n  \"details\": \"[Additional information about the request, if any]\"\n}\nSAMPLE:\n{\n  \"status\": \"Success\",\n  \"details\": \"POST request completed successfully. Workflow execution tracked.\"\n}\nEnsure that the output strictly adheres to the JSON format and includes all necessary fields.\n\nValues :\nthe outputURL : The output url is output of the previous agent.\nPipeline ID: pipeline ID is the output of the previous agent.\n",
                        "expectedOutput": "A JSON object containing the status and details of the POST request."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 957,
                            "toolName": "Workbench_Dynamic_API_Caller_Z",
                            "toolClassName": "DynamicAPICaller",
                            "toolClassDef": "import requests\nimport urllib3\nfrom typing import Any, Type, Optional, Dict, Union\nfrom pydantic import BaseModel, Field\nfrom crewai.tools import BaseTool\n\n# -------------------------------------------------\n# SSL bypass settings\n# -------------------------------------------------\n# Disable warnings about insecure HTTPS requests\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n# NOTE: When verify=False is used below, requests will not validate server certificates.\nVERIFY_SSL = False  # \u2705 Set this to True when you fix certificate issues\n\n\nclass DynamicAPICallerSchema(BaseModel):\n    '''Input schema for DynamicAPICaller.'''\n    url: str = Field(..., description=\"The API endpoint URL to call\")\n    method: str = Field(..., description=\"HTTP method to use for the API call (e.g., GET, POST, PUT, DELETE)\")\n    headers: Optional[Dict[str, str]] = Field(None, description=\"Optional headers for the API call\")\n    params: Optional[Dict[str, Any]] = Field(None, description=\"Optional query parameters for the API call\")\n    data: Optional[Union[Dict[str, Any], str]] = Field(None, description=\"Optional form-encoded or raw payload for the API request\")\n    json: Optional[Dict[str, Any]] = Field(None, description=\"Optional JSON payload for the API call\")\n\n\nclass DynamicAPICaller(BaseTool):\n    '''\n    DynamicAPICaller - A tool to dynamically call any API endpoint.\n    '''\n\n    name: str = \"Dynamic API Caller\"\n    description: str = \"A tool to dynamically call any API endpoint.\"\n    args_schema: Type[BaseModel] = DynamicAPICallerSchema\n\n    def _run(\n        self,\n        url: str,\n        method: str,\n        headers: Optional[Dict[str, str]] = None,\n        params: Optional[Dict[str, Any]] = None,\n        data: Optional[Union[Dict[str, Any], str]] = None,\n        json: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        try:\n            print(f\"Making {method} request to {url} with headers={headers}, params={params}, data={data}, json={json}\")\n\n            response = requests.request(\n                method=method.upper(),\n                url=url,\n                headers=headers,\n                params=params,\n                data=data,\n                json=json,\n                timeout=60,\n                verify=VERIFY_SSL  # \u2705 Disable SSL verification if set to False\n            )\n\n            return {\n                \"status\": \"success\",\n                \"status_code\": response.status_code,\n                \"response_body\": (\n                    response.json()\n                    if \"application/json\" in response.headers.get(\"Content-Type\", \"\")\n                    else response.text\n                )\n            }\n\n        except requests.RequestException as e:\n            return {\n                \"status\": \"error\",\n                \"error\": str(e)\n            }\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}