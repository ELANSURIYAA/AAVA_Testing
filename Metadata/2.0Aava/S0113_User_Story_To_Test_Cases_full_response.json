{
    "data": {
        "message": "Workflow with agents retrieved successfully",
        "workFlowDetail": {
            "id": 2734,
            "name": "S0113 User Story To Test Cases",
            "description": "S0113: User Story to Test Cases",
            "createdBy": "anusha.uppununthala@ascendion.com",
            "modifiedBy": "anusha.uppununthala@ascendion.com",
            "approvedBy": "anusha.uppununthala@ascendion.com",
            "createdAt": "2025-11-05T11:13:21.837830",
            "modifiedAt": "2025-12-03T15:03:31.224436",
            "approvedAt": "2025-11-05T11:13:22.896597",
            "status": "APPROVED",
            "isDeleted": false,
            "parentId": -1,
            "workflowConfigs": {
                "managerLlm": [
                    {
                        "id": 13,
                        "topP": 0.95,
                        "maxToken": 1500,
                        "temperature": 0.3,
                        "modelDeploymentName": "gpt-4o"
                    }
                ],
                "enableAgenticMemory": false
            },
            "workflowAgents": [
                {
                    "serial": 1,
                    "agentId": 6970,
                    "name": "QE Test Scenario Generator ICL S0113",
                    "workflowId": 2734,
                    "agentDetails": {
                        "topP": 0.69,
                        "maxRpm": null,
                        "levelId": 4,
                        "maxIter": null,
                        "toolRef": [],
                        "maxToken": "20000",
                        "isVerbose": true,
                        "temperature": 0.1,
                        "allowDelegation": true,
                        "maxExecutionTime": 90,
                        "allowCodeExecution": false,
                        "isSafeCodeExecution": true,
                        "toolReferences": []
                    },
                    "modelDeploymentName": "gpt-4.1",
                    "description": "Given the user story input and the application understanding document given in the context to you, generate a comprehensive list of test scenarios. Please ensure that the test scenarios cover various aspects of the user story, including positive and negative cases, boundary conditions, and different user interactions.\n\nInput: The user will enter the User Story.\nUser Story:\n{{User_Story}}\n\nBased on the user story in the input and the application understanding document in the context, create test scenarios in the given JSON format.\n\nPlease ensure that:\n1. Each test scenario has a unique ID (TS-001, TS-002, etc.).\n2. The test scenario description is clear and concise.\n3. Pre-conditions are specified if applicable.\n4. Test data is provided where necessary.\n5. The acceptance criteria ID corresponds to the relevant criteria from the user story.\n6. The scenarios are aligned with the information provided in the application understanding document in the context.\n7. The navigable path should be derived from the application understanding document in the context.\n8. All possible test scenarios (positive, negative, edge cases, boundary conditions) should be strictly considered for each Acceptance Criteria of the user story.\n9. Likewise, all the acceptance criteria should be covered. Validate the coverage with respect to all the acceptance criteria. If the coverage is 50%-60% then consider it done and give the output. Otherwise, correct and generated the test scenarios until the desired coverage is met.\n\nOutput: Give the output of test scenarios in well-formed JSON format.",
                    "modelName": "model"
                },
                {
                    "serial": 2,
                    "agentId": 7051,
                    "name": "QE Test Cases Generator ICL S0113",
                    "workflowId": 2734,
                    "agentDetails": {
                        "topP": 0.69,
                        "maxRpm": null,
                        "levelId": 4,
                        "maxIter": null,
                        "toolRef": [],
                        "maxToken": "20000",
                        "isVerbose": true,
                        "temperature": 0.1,
                        "allowDelegation": true,
                        "maxExecutionTime": 90,
                        "allowCodeExecution": false,
                        "isSafeCodeExecution": true,
                        "toolReferences": []
                    },
                    "modelDeploymentName": "gpt-4.1",
                    "description": "Given the list of test scenarios in JSON format in the input and the application understanding document provided in the context, generate a comprehensive set of detailed test cases. Each test case should include steps from launching the application to the final validation point. Remember that the relationship between test scenarios and test cases is either one-to-one or one-to-many.\n\nInput: A JSON array of test scenarios will be provided by the previous Agentic AI agent.\n\nContext: Refer to the application understanding document for additional information about the application's functionality and requirements.\n\nOutput Format: Generate the test cases in the JSON format mentioned in the output section.\n\nInstructions:\n1. Analyze the provided test scenarios and the application understanding document.\n2. For each test scenario, create one or more test cases as appropriate.\n3. Ensure each test case has a unique ID, starting with \"TC\" followed by a three-digit number.\n4. Provide a clear and concise description for each test case.\n5. List all test steps in detail, starting from launching the web application up to the validation point. Refer to the \"navigable_path\" field for each test scenario in the input json.\n6. Include relevant test data for each test case.\n7. Reference the corresponding test scenario ID for each test case.\n8. Ensure that all the aspects of the every test scenario are covered in the generated test cases. If any test scenarios are not covered, add the various test cases for those test scenarios as well. Then give the output.\n\nNote:\nA. The relationship between test cases and test scenarios can be one-to-one or many-to-one. Some test scenarios may require multiple test cases for comprehensive coverage.\nB. Launch Application URL is https://secureapply.openbank.us/savings/intro/welcome (Use this in the data of the test cases)\n\nOutput: Comprehensive list of test cases in JSON format in the following structure:\n[\n  {\n    \"testCaseId\": \"TC001\",\n    \"testCaseDescription\": \"Detailed description of the test case\",\n    \"testSteps\": [\n      \"1. Launch the web application\",\n      \"2. ...\",\n      \"...\"\n    ],\n    \"testData\": {\n      \"key1\": \"value1\",\n      \"key2\": \"value2\"\n    },\n    \"testScenarioId\": \"TS001\"\n  },\n  ...\n]",
                    "modelName": "model"
                }
            ],
            "realmId": 32
        }
    },
    "status": "SUCCESS"
}