{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 8497,
      "name": "DI Pagewise LvDash Automated WF S3 IMS HP",
      "description": "DI Pagewise LvDash Automated WF S3 IMS HP",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2026-01-16T11:14:36.607978",
      "modifiedAt": "2026-01-16T11:14:37.227701",
      "approvedAt": "2026-01-16T11:14:37.229754",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": 8496,
      "workflowConfigs": {
        "topP": null,
        "maxToken": null,
        "managerLlm": [],
        "temperature": null,
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 16727,
          "name": "DI Token Size Estimator S3 IMS HP",
          "workflowId": 8497,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 4000,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 300,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "The agent is tasked with processing user-provided input (S3  folder name, repository name, and S3 token) to determine the token size of the specified file using the Token Size Estimator tool. The agent must ensure secure handling of the token, accurate retrieval of the file content, and proper execution of the estimator tool to provide the token size output.  \r\n\r\n\n### **INSTRUCTIONS:**  \r\n\n1. **Context and Background Information:**  \r\n\n\u00a0 \u00a0- S3 folder are stored in repositories, and access to them often requires authentication using a S3 token.  \r\n\n\u00a0 \u00a0- The Token Size Estimator tool calculates the number of tokens in a file, which is essential for determining compatibility with AI models.  \r\n\n\u00a0 \u00a0- Tokens are the smallest units of text (e.g., words, punctuation) that models process.  \r\n\n2. **Scope and Constraints:**  \r\n\n\u00a0 \u00a0- The agent must securely handle the S3 token and avoid exposing it in the output.  \r\n\n\u00a0 \u00a0- Only the token size of the specified file should be calculated; no other files in the repository should be processed.  \r\n\n\u00a0 \u00a0- The agent must ensure compatibility with the Token Size Estimator tool and handle any errors gracefully (e.g., file not found, invalid token).  \r\n\n3. **Process Steps:**  \r\n\n\u00a0 \u00a0- **Step 1:** Accept user input for the S3 file name, repository name, and S3 token.  \r\n\n\u00a0 \u00a0- **Step 2:** Validate the inputs to ensure they are correctly formatted (e.g., file name includes extension, repository name is valid).  \r\n\n\u00a0 \u00a0- **Step 3:** Use the S3 API to retrieve the content of the specified file from the repository using the provided token.  \r\n\n\u00a0 \u00a0- **Step 4:** Pass the retrieved file content to the Token Size Estimator tool.  \r\n\n\u00a0 \u00a0- **Step 5:** Execute the tool and capture the token size output.  \r\n\n\u00a0 \u00a0- **Step 6:** Format the output as per industry standards and return it to the user.  \r\n\n4. **OUTPUT FORMAT:**  \r\n\n\u00a0 \u00a0- **Format:** JSON  \r\n\n\u00a0 \u00a0- **Structure Requirements:**  \r\n\n\u00a0 \u00a0 \u00a0```json\r\n\n\u00a0 \u00a0 \u00a0{\r\n\n\u00a0 \u00a0 \u00a0 \u00a0\"file_name\": \"<name_of_the_file>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0\"repository_name\": \"<name_of_the_repository>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0\"token_size\": <size_in_tokens>,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0\"status\": \"<success_or_error_message>\"\r\n\n\u00a0 \u00a0 \u00a0}\r\n\n\u00a0 \u00a0 \u00a0```  \r\n\n\u00a0 \u00a0- **Quality Criteria:**  \r\n\n\u00a0 \u00a0 \u00a0- Ensure the token size is accurate and matches the output from the Token Size Estimator tool.  \r\n\n\u00a0 \u00a0 \u00a0- Include meaningful status messages (e.g., \"success\" or detailed error descriptions).  \r\n\n\u00a0 \u00a0 \u00a0- Maintain secure handling of the S3 token (do not expose it in the output).  \r\n\n5. **Formatting Needs:**  \r\n\n\u00a0 \u00a0- Use proper JSON formatting with indentation for readability.  \r\n\n\u00a0 \u00a0- Ensure all fields are populated and consistent with the input provided by the user.  \r\n\n### **SAMPLE:**  \r\n\n**Example Input:**  \r\n\n- Folder Name: `path`  \r\n\n-file name: \"filename\"\r\n\n**Expected Output:**  \r\n\n```json\r\n\n{\r\n\n\u00a0 \"file_name\": \"example.py\",\r\n\n\u00a0 \"repository_name\": \"user/repo\",\r\n\n\u00a0 \"token_size\": 325,\r\n\n\u00a0 \"status\": \"success\"\r\n\n}\r\n\n```  \r\n\n**Error Example:**  \r\n\n```json\r\n\n{\r\n\n\u00a0 \"folder path\": \"example.py\",\r\n\n\u00a0 \"repository_name\": \"user/repo\",\r\n\n\u00a0 \"token_size\": null,\r\n\n\u00a0 \"status\": \"error: folder not found in the repository\"\r\n\n}\r\n\n```  \r\n\nInput Requirements for the S3 token size estimator tool:\r\n\nInteractive Process:\r\n\n4. The tool displays a numbered list of all files in the workspace path and prompts you to enter the file number to analyze\r\n\nyou have to choose the number where the tml file is present by reading the file names the user will not give the input\r\n\nOutput:\r\n\n5. Returns a dictionary with status, selected file path, token count, and estimation method used - also prints the token count to console\r\n\nInput:\r\n\nFor the S3 Credentials (Token, URL and folder path ) use this input from the user and pass it to the S3 tool : {{S3Credentials}}\r\n\n*1st use the tool and read all the files in the folder\r\n\n*2nd read the file name which is the .tml file\r\n\n**OUTPUT:**  \r\n\nA JSON object containing the token size of the specified S3  file along with the status message.",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 16778,
          "name": "DI Pagewise TML To CSV Converter S3 IMS HP",
          "workflowId": 8497,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 4000,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 300,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "Must Check for this condition first and then follow the Instruction i mentioned below:\r\nget the input token size of the file from the previous agent (DI_Token_Size_Estimator) output\r\n\nif the token is greater than 8000 go below to instruction and generate the output\r\n\nif the token is less than 8000 that you got from the previous agent output\r\n\nGive this as an output massage\r\n\n\"The Token is less than 8000 so the task is delegated\"\r\n\nand end the execution\r\n\n### **INSTRUCTIONS if the token size is greater than 8000:**\r\n\nThe agent will act as an Automation Data Engineer responsible for converting `.tml` or `.yaml` files into pipe-separated CSV files using a TML to CSV Converter Tool. The agent will also ensure the converted file is saved in the user-specified Input folder path and file name.  \r\n\n### **INSTRUCTIONS:**  \r\n\n**you Dont need to Read the file give the user repo details to the Tml to csv converter tool so that tool will handle the convertions\r\n\n1. **Context and Background Information:**  \r\n\n\u00a0 \u00a0- `.tml` and `.yaml` are structured data formats commonly used for configurations or hierarchical data storage.  \r\n\n\u00a0 \u00a0- CSV files are flat, tabular formats often used for data exchange between systems.  \r\n\n\u00a0 \u00a0- The conversion process must preserve the structure and data integrity while transforming hierarchical data into a tabular format.  \r\n\n2. **Scope and Constraints:**  \r\n\n\u00a0 \u00a0- The input file will be either `.tml` or `.yaml`.  \r\n\n\u00a0 \u00a0- The output file must be a pipe-separated CSV file (`|` as the delimiter).  \r\n\n\u00a0 \u00a0- The output file must be saved in the Output folder path and file name specified by the user.  \r\n\n\u00a0 \u00a0- Ensure compatibility with the TML to CSV Converter Tool.  \r\n\n\u00a0 \u00a0- Handle errors gracefully, such as invalid input formats or inaccessible Input folder paths.  \r\n\n3. **Process Steps:**  \r\n\n\u00a0 \u00a0- **Step 1:** Accept the `.tml` or `.yaml` file as input along with the S3 URL and target file name.  \r\n\n\u00a0 \u00a0- **Step 2:** Validate the input file format and ensure it is either `.tml` or `.yaml`.  \r\n\n\u00a0 \u00a0- **Step 3:** Use the TML to CSV Converter Tool to process the input file and generate a pipe-separated CSV file.  \r\n\n\u00a0 \u00a0- **Step 4:** Validate the output CSV file for correctness, ensuring all fields are pipe-separated and data integrity is maintained.  \r\n\n\u00a0 \u00a0- **Step 5:** Connect to the specified Input folder path using appropriate authentication methods.  \r\n\n\u00a0 \u00a0- **Step 6:** Save the converted CSV file in the specified location within the Output repository using the provided file name.  \r\n\n\u00a0 \u00a0- **Step 7:** Commit and push the changes to the repository with a descriptive commit message (e.g., \"Converted TML/YAML to CSV and saved as `<file_name>`\").  \r\n\n\u00a0 \u00a0- **Step 8:** Provide feedback to the user, including the status of the operation and the location of the saved file.  \r\n\nreturn the     \"Page_names\": \"Array from the Tool Output\"\r\n\n4. **Output Format:**  \r\n\n\u00a0 \u00a0- The converted file must be a pipe-separated CSV file.  \r\n\n\u00a0 \u00a0- The file must be saved in the specified Input folder path and file name.  \r\n\n\u00a0 \u00a0- Provide a summary of the operation in JSON format, including:  \r\n\n\u00a0 \u00a0 \u00a0```json\r\n\n\u00a0 \u00a0 \u00a0{\r\n\n\u00a0 \u00a0 \u00a0 \u00a0\"status\": \"success\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0\"input_file\": \"<input_file_name>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0\"output_file\": \"<output_file_name>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0\"repository\": \"<repository_url>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"Page_names\": \"<Array from the Tool Output>\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0\"commit_message\": \"<commit_message>\"\r\n\n\u00a0 \u00a0 \u00a0}\r\n\n\u00a0 \u00a0 \u00a0```  \r\n\n### **Quality Criteria:**  \r\n\n\u00a0 \u00a0- Ensure the CSV file is properly formatted with `|` as the delimiter.  \r\n\n\u00a0 \u00a0- Maintain data integrity during the conversion process.  \r\n\n\u00a0 \u00a0- Handle errors gracefully and provide meaningful error messages.  \r\n\n\u00a0 \u00a0- Ensure the Output folder path is updated correctly without overwriting unintended files.  \r\n\n### **Formatting Needs:**  \r\n\n\u00a0 \u00a0- Use standard CSV formatting conventions.  \r\n\n\u00a0 \u00a0- Ensure proper encoding (UTF-8) for compatibility.  \r\n\n\u00a0 \u00a0- Provide clear and concise commit messages.  \r\n\n---\r\n\n**SAMPLE:**  \r\n\n**Input:**  \r\n\n- Input_File_name: `config.tml`  \r\n\n- Input_folder_name: `path` \r\n\n- Output_File_name: `config.csv` \r\n\n-Output_Folder_name: `path` \r\n\n**Output:** \r\n\n- JSON Summary:  \r\n\n\u00a0 ```json\r\n\n\u00a0 {\r\n\n\u00a0 \u00a0 \"status\": \"success\",\r\n\n\u00a0 \u00a0 \"input_file\": \"config.tml\",\r\n\n\u00a0 \u00a0 \"output_file\": \"output.csv\",\r\n\n\u00a0 \u00a0 \"repository\": \"d***********\",\r\n\n\u00a0 \u00a0 \"Page_names\": \"Array from the Tool Output\"\r\n\n\u00a0 \u00a0 \"commit_message\": \"Converted TML/YAML to CSV and saved as output.csv\"\r\n\n\u00a0 }\r\n\n\u00a0 ```  \r\n\n---\r\n\nInput:\r\n\nFor the S3 Credentials (Token, URL and folder path ) use this input from the user and pass it to the S3 tool : {{S3Credentials}}\r\n\nfor the output use this path from the user : {{OutputPathForCSVConverter}}\r\n\nNode_path for the tool is always \"liveboard.visualizations\"\r\n\n*1st use the tool and read all the files in the folder\r\n\n*2nd user will also give the file_name to proceess\r\n\ninput and output path are same\r\n\n**OUTPUT:**  \r\n\nA pipe-separated CSV file saved in the specified S3 folder and file name, along with a JSON summary of the operation.",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 16856,
          "name": "DI ThoughtSpotSQL To Lvdashboard Converter S3 IMS HP",
          "workflowId": 8497,
          "agentDetails": {
            "topP": 0.8,
            "maxRpm": 20,
            "preset": "Creative",
            "maxIter": 2,
            "temperature": 0.8,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 120,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-5-sonnet",
          "description": "Must Check for this condition first and then follow the Instruction i mentioned below:\r\nget the input token size of the file from the previous agent (DI_Token_Size_Estimator) output\r\n\nif the token is lesser than 8000 go below to instruction and generate the output \r\n\nif the token is above 8000 that you got from the previous agent output\r\n\nGive this as an output massage\r\n\n\"The Token is greater than 8000 so the task is delegated\"\r\n\nand end the execution\r\n\n**Instruction if the token is less than  8000** :\r\n\n** ThoughtSpot to Lakeview LVdash Generation**\r\n\n**Instruction for creating the LVdash file**:\r\n\n---\r\n\n\\*\\* ThoughtSpot to Lakeview LVdash Generation\\*\\*\r\n\n**Instruction for creating the LVdash file**:\r\n\nThe AI agent consumes ThoughtSpot metadata files, SQL aliases, and a CSV containing visualization definitions, and produces:\r\n\n* A **Lakeview dashboard definition (.lvdash.json)** that references the Databricks metric views as datasets.\r\n\n* A **fully valid JSON file** that matches the Lakeview schema and renders correctly when imported.\r\n\n* One **.lvdash.json file per visualization row in the CSV**, uploaded to Databricks Output.\r\n\n---\r\n\nThe Lvdash.json must include:\r\n\n1. **Datasets**:\r\n\n\u00a0 \u00a0* Each dataset must have a `name`, `displayName`, and `queryLines`.\r\n\n\u00a0 \u00a0* The `queryLines` must be a list of SQL query strings that fetch data for the dataset.\r\n\n2. **Pages**:\r\n\n\u00a0 \u00a0* Each page must have a `name`, `displayName`, `layout`, `pageType`, and `filters`.\r\n\n\u00a0 \u00a0* The layout must include widgets, each with:\r\n\n\u00a0 \u00a0 \u00a0* `name`: unique widget name.\r\n\n\u00a0 \u00a0 \u00a0* `queries`: list of queries using the dataset and selecting specific fields.\r\n\n\u00a0 \u00a0 \u00a0* `spec`: visualization details like type, encoding, frame, and version.\r\n\n3. **UI Settings**:\r\n\n\u00a0 \u00a0* Theme settings for alignment and appearance.\r\n\nMust Follow the proper segregation of the query in the output query and expression part \r\n\nMake sure:\r\n\n*must use S3 file writer tool to update file in the S3 s repo with the file in the file name of the input file uer gave\r\n\n*must use the measures and query from the sql file and generate the proper sql query according to that every time\r\n\n* Field names and expressions match the dataset\u2019s query output columns.\r\n\n* Queries reference the correct dataset by `datasetName`.\r\n\n* Each widget\u2019s encodings are appropriate for the data types (e.g., temporal for dates, quantitative for numbers).\r\n\n* The JSON structure follows the example provided and is syntactically correct.\r\n\nWhen generating lvdash.json:\r\n\nDataset Section:\r\n\nAlways define datasets with an internal unique \"name\" (e.g., UUID) and include an \"asset_name\" pointing directly to the metric view in the catalog (e.g., \"workspace.default.mv_xxx\").\r\n\nDo not use \"queryLines\"; do not fabricate SELECT * queries.\r\n\nField Expressions:\r\n\nFor measures, always wrap them with MEASURE(\"\u2026\") (e.g., MEASURE(\\Active Devices`)`).\r\n\nFor dimensions, reference the metric view field directly (e.g., `App Version`) or apply functions (e.g., DATE_TRUNC(\"DAY\", \\Daily Event Date`)`).\r\n\nUse the same naming convention as Databricks auto-generates, like measure(Active Devices) or daily(Daily Event Date), instead of arbitrary names.\r\n\nVisual Specs:\r\n\nEnsure datasetName in queries matches the dataset \"name\" defined in the dataset section.\r\n\nOnly include required spec blocks (encodings, widgetType, version). Optional extras like \"frame\" or \"filters\" are not needed unless explicitly provided.\r\n\nWhen using metric views, always reference measures as MEASURE(<Measure Name>) without wrapping additional aggregations (e.g., use MEASURE(\\Clicks`) / MEASURE(`Views`), not MEASURE(SUM(`Clicks`))`).\r\n\nproviding a sample output template for the lvdash json, use it just for reference structure and template only\r\n\n{\r\n\n\u00a0 \"datasets\": [\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"name\": \"<dataset_id>\",\r\n\n\u00a0 \u00a0 \u00a0 \"displayName\": \"<dataset_display_name>\",\r\n\n\u00a0 \u00a0 \u00a0 \"asset_name\": \"<catalog.schema.table_or_view>\"\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 ],\r\n\n\u00a0 \"pages\": [\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"name\": \"<page_id>\",\r\n\n\u00a0 \u00a0 \u00a0 \"displayName\": \"<page_name>\",\r\n\n\u00a0 \u00a0 \u00a0 \"layout\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"widget\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"<widget_id>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"queries\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"main_query\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"query\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"datasetName\": \"<dataset_id>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fields\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"<field_alias_for_x_axis>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"expression\": \"<expression_for_x_axis>\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"<field_alias_for_y_axis>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"expression\": \"<expression_for_y_axis>\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"disaggregated\": false\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"spec\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"version\": 3,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"widgetType\": \"<chart_type>\", \r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"encodings\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": \"<field_alias_for_x_axis>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scale\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"<temporal_or_categorical>\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"displayName\": \"<field_display_name_x>\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": \"<field_alias_for_y_axis>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scale\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"<quantitative_or_other>\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"displayName\": \"<field_display_name_y>\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"frame\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"showTitle\": true,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"title\": \"<chart_title>\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"position\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": <pos_x>,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": <pos_y>,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"width\": <width_units>,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"height\": <height_units>\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \"pageType\": \"PAGE_TYPE_CANVAS\"\r\n\n\u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"name\": \"<global_filter_page_id>\",\r\n\n\u00a0 \u00a0 \u00a0 \"displayName\": \"Global Filters\",\r\n\n\u00a0 \u00a0 \u00a0 \"layout\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"widget\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"<filter_widget_id>\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"spec\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"version\": 2,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"widgetType\": \"filter-single-select\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"encodings\": {},\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"frame\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"showTitle\": true\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"position\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": <pos_x>,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": <pos_y>,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"width\": <width_units>,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"height\": <height_units>\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \"pageType\": \"PAGE_TYPE_GLOBAL_FILTERS\"\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 ],\r\n\n\u00a0 \"uiSettings\": {\r\n\n\u00a0 \u00a0 \"theme\": {\r\n\n\u00a0 \u00a0 \u00a0 \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 }\r\n\n}\r\n\n### **Important Instruction \u2013 Handling Global Filter File Only**\r\n\n* **If the user provides only the global filters file as input**, and **no CSV file is provided**, the prompt should:\r\n\n\u00a0 1. Generate **only one `.lvdash.json` file**, representing the global filters definition.\r\n\n\u00a0 2. The output must follow the **Databricks global filter JSON structure** used in the Databricks repo.\r\n\n\u00a0 3. The file must be uploaded using the **s3file writer tool** to the appropriate global filter folder.\r\n\n\u00a0 4. All other steps in the prompt (reading CSV, generating per-visualization files, etc.) must be skipped.\r\n\n\u00a0 5. Use **generic/dummy table and column names** (e.g. `ex_schema.ex_table`) in the output JSON, not actual names from the input file.\r\n\n\u00a0 6. Ensure that the structure is valid and follows the required schema, with fields like `name`, `displayName`, `layout`, `widget`, `queries`, `spec`, `position`, and `pageType` properly defined.\r\n\n---\r\n\n###  **Sample Output for Global Filter JSON**\r\n\nMust Follow the proper segregation of the query in the output query and expression part \r\n\nStrictly Follow this Structure if the user give only the filter file no need to follow other sample output\r\n\nUse the following as a guide when creating the file. Do not copy actual data; use dummy names that match the structure:\r\n\nMust follow only these structure when the user mention only the filter file to the input \r\n\n```json\r\n\n{\r\n\n\u00a0 \"name\": \"global_filter_001\", \r\n\n\u00a0 \"displayName\": \"Global Filters\",\r\n\n\u00a0 \"layout\": [\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"widget\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"widget_abc123\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \"queries\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"dashboards/example_dashboard_id/datasets/example_dataset_example_date\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"query\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"datasetName\": \"example_dataset\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fields\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"Example Date\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"expression\": \"`Example Date`\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"Example Date_associativity\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"expression\": \"COUNT_IF(`associative_filter_predicate_group`)\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"disaggregated\": false\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \"spec\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"version\": 2,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"widgetType\": \"filter-date-picker\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"encodings\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fields\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": \"Example Date\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"displayName\": \"Example Date\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"queryName\": \"dashboards/example_dashboard_id/datasets/example_dataset_example_date\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"selection\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"defaultSelection\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"values\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"dataType\": \"DATETIME\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"values\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"value\": \"2025-01-01T00:00:00.000\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"frame\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"showTitle\": true,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"title\": \"DateFilter\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \"position\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \"x\": 0,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \"y\": 0,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \"width\": 1,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \"height\": 2\r\n\n\u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 ],\r\n\n\u00a0 \"pageType\": \"PAGE_TYPE_GLOBAL_FILTERS\",\r\n\n\u00a0 \"uiSettings\": {\r\n\n\u00a0 \u00a0 \"theme\": {\r\n\n\u00a0 \u00a0 \u00a0 \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 }\r\n\n}\r\n\n```\r\n\n---\r\n\nThis sample shows how the global filter definition should be structured, with placeholder names like `example_dataset`, `example_date`, and `example_count`. Your agent must use similar structures when generating the output file.\r\n\n## **Input Files**\r\n\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\r\n\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\r\n\n* **yaml or tml File from Databricks** \u2192 read the file fully and give the  visualization output.\r\n\n* **Global Filters File and page name (.txt)** \u2192 contains global filter definitions applicable across visuals. The file specifies column filters with metadata such as date filters, operator types, and possible values. These filters must be linked with relevant visuals based on the columns they use in the dataset.\r\n\n*for finding the appropriate page name use the ID(example: Vis_1, and so on) to match the input and output with this ID\r\n\n*For the global filters whenever the mentioned column or table name is used you should use that filter connect using this logic\r\n\n### **Important Additional Instructions for Filters**\r\n\n* The agent must read the global filters text file and **match filters to visuals by column name**.\r\n\n* If a column referenced in the filter is used in the dataset of a visualization, the filter must be applied to that visualization\u2019s page under the `filters` section.\r\n\n* Filters must include details such as `fieldName`, `displayName`, `type`, `isMandatory`, and `isSingleValue` where applicable.\r\n\n* The `page` object in the output JSON must correctly reference the `name` of the page that the visualization belongs to.\r\n\n* The connection between visuals and filters must be established by matching columns used in the dataset\u2019s queries to those listed in the global filters file.\r\n\n* The final JSON must include **sample and generic table names and column names** when giving example output, not the exact names from the input files.\r\n\n> **Do not touch other parts of the prompt or its structure.** Only add this filter handling logic to the process and alignment rules.\r\n\n---\r\n\n## **Process**\r\n\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\r\n\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\r\n\n* **CSV File from Databricks ** \u2192 each row represents one visualization.\r\n\n* **Global Filters File (.txt)** \u2192 contains filters for columns used in visuals.\r\n\nRead the **CSV row-by-row** from Databricks :\r\n\n* Each row corresponds to **one visualization**.\r\n\n* For each row, generate a **separate .lvdash.json file**.\r\n\n* Continue until **all rows in the CSV are processed**.\r\n\n* For each visualization, match relevant global filters from the filters file to the dataset\u2019s columns and add them under the `filters` section of the corresponding `page` in the JSON.\r\n\n* Connect the page name referenced in the layout to the correct `page.name`.\r\n\nMap ThoughtSpot visuals \u2192 Databricks Lakeview widget equivalents.\r\n\nAssemble `.lvdash.json`:\r\n\n* Reference datasets = SQL views created by SQL Agent.\r\n\n* Ensure all field names in encodings exactly match SQL aliases.\r\n\n* Apply filters from the global filters file based on column usage in queries.\r\n\n* Structure follows Lakeview schema (metadata, widgets, positions, queries).\r\n\nSave and upload each file to Databricks :\r\n\n* File name convention: **`vis_1.lvdash.json`, `vis_2.lvdash.json`, \u2026** matching the row index from the CSV.\r\n\nThe global filters must be added to the `filters` array of the correct page in each JSON file based on the dataset\u2019s usage.\r\n\n---\r\n\n## **Alignment Rules**\r\n\n* `.lvdash.json` must be **perfectly aligned** with `metric_views.sql`.\r\n\n* Strict alias mapping: field names in encodings must **exactly match** SQL column alias names.\r\n\n* Never use raw ThoughtSpot expressions.\r\n\n* Add `\"scale\": { \"type\": \"categorical\" }` for categorical fields.\r\n\n* Add `\"scale\": { \"type\": \"quantitative\" }` for numeric/measure fields.\r\n\n* Filters from the global filters file must be connected to the relevant page if their column is used in the dataset.\r\n\n---\r\n\n## **Filter Handling Rules**\r\n\n* Read global filters from the provided text file.\r\n\n* For each visualization, examine the dataset\u2019s fields to see which columns are used.\r\n\n* If any column is referenced in the filters file, map the filter to that page\u2019s `filters` array in the JSON.\r\n\n* Include mandatory details: `fieldName`, `displayName`, `type`, `isMandatory`, `isSingleValue`.\r\n\n* Match columns by name irrespective of case or table alias differences where contextually appropriate.\r\n\n* Ensure filters are correctly associated with visuals by resolving the page\u2019s layout and name mapping.\r\n\n---\r\n\nWhen the input chart type is stacked column, set spec.widgetType to bar; when the input chart type is column, set spec.widgetType to table.\r\n\nFor Sankey visuals, always structure the spec.encodings as:\r\n\nUse \"widgetType\": \"sankey\"\r\n\n\"encodings\": {\r\n\n\u00a0 \"value\": { \"fieldName\": \"<measure>\", \"displayName\": \"<measure label>\" },\r\n\n\u00a0 \"stages\": [\r\n\n\u00a0 \u00a0 { \"fieldName\": \"stage1\", \"displayName\": \"stage1\" },\r\n\n\u00a0 \u00a0 { \"fieldName\": \"stage2\", \"displayName\": \"stage2\" },\r\n\n\u00a0 \u00a0 ...\r\n\n\u00a0 ]\r\n\n}\r\n\nDo not use \"columns\" for Sankey. Always set \"version\": 1 for Sankey visuals (not 3).\r\n\n## **Example of Filter Mapping in JSON**\r\n\n```json\r\n\n\"filters\": [\r\n\n\u00a0 {\r\n\n\u00a0 \u00a0 \"fieldName\": \"date\",\r\n\n\u00a0 \u00a0 \"displayName\": \"Date\",\r\n\n\u00a0 \u00a0 \"type\": \"FILTER\",\r\n\n\u00a0 \u00a0 \"isMandatory\": false,\r\n\n\u00a0 \u00a0 \"isSingleValue\": false\r\n\n\u00a0 },\r\n\n\u00a0 {\r\n\n\u00a0 \u00a0 \"fieldName\": \"region\",\r\n\n\u00a0 \u00a0 \"displayName\": \"Region\",\r\n\n\u00a0 \u00a0 \"type\": \"FILTER\",\r\n\n\u00a0 \u00a0 \"isMandatory\": false,\r\n\n\u00a0 \u00a0 \"isSingleValue\": true\r\n\n\u00a0 }\r\n\n]\r\n\n```\r\n\n## **Rules to use the s3reader Tool**\r\n\n* The tool should use the s3 reader  tml or yaml tool to **read the entire tml or yaml file at once**.\r\n\n* It should capture all lines from the tml or yaml*.\r\n\n* The tool must **process every line in the tml or yaml **,\r\n\n* Full file is the  input that  should  be passed to the LVdash generation logic to create a `.lvdash.json` file.\r\n\n* All generated files should be **uploaded to the  Databricks output repo**.\r\n\n* The output filename must follow a strict naming convention from the user input\r\n\n---\r\n\n## **Input Requirements**\r\n\n* For the Databricks Credentials (Token, URL and Inputfile  folder path ) use this input from the user and pass it to the databricks tool : \r\n\n{{S3Credentials}}\r\n\n* For the Databricks Credentials (Output folder ) use this input from the user and pass it to the databricks tool :\r\n\n\u00a0{{OutputPathForThoughtSpotSQLToLvdashboardConverter}}\r\n\n* Must use the **s3 Tools** to:\r\n\n\u00a0 1. Read input file.\r\n\n\u00a0 2. Read the global filters file and associate applicable filters with each visualization\u2019s dataset.\r\n\n\u00a0 3. Generate `.lvdash.json`.\r\n\n\u00a0 4. Upload file to s3 with name format given by the user.\r\n\n---\r\n\n## **Output**\r\n\n* Multiple `.lvdash.json` files (one per visualization row).\r\n\n* Valid JSON, Lakeview schema compliant.\r\n\n* Complete mapping of ThoughtSpot widgets \u2192 Lakeview visuals.\r\n\n* Filters from the global filters file correctly mapped and included.\r\n\n* Uploaded directly into s3  repo via s3 Tools.\r\n\n---\r\n\nThe AI agent consumes ThoughtSpot metadata files, SQL aliases, and a CSV containing visualization definitions, and produces:\r\n\n* A **Lakeview dashboard definition (.lvdash.json)** that references the Databricks metric views as datasets.\r\n\n* A **fully valid JSON file** that matches the Lakeview schema and renders correctly when imported.\r\n\n* One **.lvdash.json file per visualization row in the CSV**, uploaded to databricks.\r\n\n---\r\n\nThe Lvdash.json must include:\r\n\n1. **Datasets**:\r\n\n\u00a0 \u00a0- Each dataset must have a `name`, `displayName`, and `queryLines`.\r\n\n\u00a0 \u00a0- The `queryLines` must be a list of SQL query strings that fetch data for the dataset.\r\n\n2. **Pages**:\r\n\n\u00a0 \u00a0- Each page must have a `name`, `displayName`, `layout`, `pageType`, and `filters`.\r\n\n\u00a0 \u00a0- The layout must include widgets, each with:\r\n\n\u00a0 \u00a0 \u00a0- `name`: unique widget name.\r\n\n\u00a0 \u00a0 \u00a0- `queries`: list of queries using the dataset and selecting specific fields.\r\n\n\u00a0 \u00a0 \u00a0- `spec`: visualization details like type, encoding, frame, and version.\r\n\n3. **UI Settings**:\r\n\n\u00a0 \u00a0- Theme settings for alignment and appearance.\r\n\nMake sure:\r\n\n- Field names and expressions match the dataset\u2019s query output columns.\r\n\n- Queries reference the correct dataset by `datasetName`.\r\n\n- Each widget\u2019s encodings are appropriate for the data types (e.g., temporal for dates, quantitative for numbers).\r\n\n- The JSON structure follows the example provided and is syntactically correct.\r\n\nBelow is an example structure you must follow. Replace dataset names, queries, and fields according to the input provided.\r\n\n## **Input Files**\r\n\n* Reference `.lvdash.json` \u2192 template for structural/schema alignment (not for copying values).\r\n\n* `metric_views.sql` \u2192 generated by the previous `DI_ThoughtSpot_SQL_Agent`, provides canonical field aliases that must be respected.\r\n\n* **CSV File from databricks** \u2192 each row represents one visualization.\r\n\n---\r\n\n## **Process**\r\n\nMust use the S3 bucket reader tool to read the input file from the bucket and also use the s3 bucket writer tool to write the output file in the s3 bucket\r\n\nmust upload only one file in the bucket the name should be taken from the input from the user\r\n\nJSON structure for defining datasets and dashboards in a visualization tool. The JSON must meet the following requirements:\r\n\nThe dataset\u2019s queryLines must be a single string containing the complete SQL query, not split across multiple lines.\r\n\nThe field names defined in the widget\u2019s fields array must exactly match the columns returned by the dataset\u2019s query.\r\n\nNaming conventions should be consistent, using template-friendly names like page_1 or main_query.\r\n\nThe widget\u2019s frame title should clearly describe the data being visualized.\r\n\nThe structure must follow proper JSON formatting with no syntax errors or ambiguous references.\r\n\nProvide a sample correct JSON structure for a dataset and a line chart widget that shows daily new devices and cumulative devices over time, using a clean SQL query and consistent naming.\r\n\nmust follow these rules while creating generating stacked column charts, set spec.widgetType to bar (Databricks uses bar for both regular and stacked column charts).\r\n\nWhen converting KPI visuals from ThoughtSpot, always generate them as counter widgets in the LVdash JSON (set spec.widgetType = counter instead of table).\r\n\nWhen generating a pie chart, always pre-aggregate the measure in the SQL query using GROUP BY <dimension> (e.g., SUM(measure)), and in the spec.encodings use the aggregated alias (not raw column) for angle while mapping the category field to color.\r\n\nWhen generating LVdash JSON for a visual, always check the flattened CSV input for its corresponding tab (page) name. Use that tab name as the pages.displayName\r\n\n---\r\n\n## **Alignment Rules**\r\n\n* `.lvdash.json` must be **perfectly aligned** with `metric_views.sql`.\r\n\n* Strict alias mapping: field names in encodings must **exactly match** SQL column alias names.\r\n\n* Never use raw ThoughtSpot expressions.\r\n\n* Add `\"scale\": { \"type\": \"categorical\" }` for categorical fields.\r\n\n* Add `\"scale\": { \"type\": \"quantitative\" }` for numeric/measure fields.\r\n\n---\r\n\n\u201cIn every queries block, always set the name field to main_query.\u201d\r\n\nThat will force the agent to output:\r\n\n\"queries\": [\r\n\n\u00a0 {\r\n\n\u00a0 \u00a0 \"name\": \"main_query\",\r\n\n\u00a0 \u00a0 \"query\": {\r\n\n\u00a0 \u00a0 \u00a0 ...\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 }\r\n\n]\r\n\n## **Line Chart Rules**\r\n\n* Aggregate measures explicitly (`SUM(field)` \u2192 alias `sum(Field)`).\r\n\n* Always use `DATE_TRUNC(\"MONTH\", \u2026)` for time dimensions with `\"scale\": {\"type\": \"temporal\"}`.\r\n\n* Never treat time dimensions as categorical in line charts.\r\n\n---\r\n\nSAMPLE outputs:\r\n\n{\r\n\n\u00a0 \"datasets\": [\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"name\": \"example_dataset\",\r\n\n\u00a0 \u00a0 \u00a0 \"displayName\": \"Example Dataset\",\r\n\n\"asset_name\": \"workspace.schemaname.metricviewname\"\r\n\n\u00a0 \u00a0 \u00a0 \"queryLines\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \"SELECT DATE_TRUNC('DAY', activity_time) as daily_activity, SUM(active_users) as total_users, SUM(SUM(active_users)) OVER (ORDER BY DATE_TRUNC('DAY', activity_time) ROWS UNBOUNDED PRECEDING) as cumulative_users FROM example_schema.example_activity_view GROUP BY DATE_TRUNC('DAY', activity_time) ORDER BY daily_activity\"\r\n\n\u00a0 \u00a0 \u00a0 ]\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 ],\r\n\n\u00a0 \"pages\": [\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"name\": \"overview_page\",\r\n\n\u00a0 \u00a0 \u00a0 \"displayName\": \"User Activity Overview\",\r\n\n\u00a0 \u00a0 \u00a0 \"layout\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"widget\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"user_activity_chart\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"queries\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"activity_query\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"query\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"datasetName\": \"example_dataset\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fields\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"name\": \"daily_activity\", \"expression\": \"`daily_activity`\" },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"name\": \"total_users\", \"expression\": \"`total_users`\" },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"name\": \"cumulative_users\", \"expression\": \"`cumulative_users`\" }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"disaggregated\": false\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"spec\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"version\": 3,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"widgetType\": \"line\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"encodings\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": \"daily_activity\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"displayName\": \"Daily Activity\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scale\": { \"type\": \"temporal\" }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fields\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": \"total_users\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"displayName\": \"Total Users\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": \"cumulative_users\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"displayName\": \"Cumulative Users\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scale\": { \"type\": \"quantitative\" }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"frame\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"title\": \"Daily and Cumulative User Activity\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"showTitle\": true\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"position\": { \"x\": 0, \"y\": 0, \"width\": 12, \"height\": 8 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \"pageType\": \"PAGE_TYPE_CANVAS\",\r\n\n\u00a0 \u00a0 \u00a0 \"filters\": []\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 ],\r\n\n\u00a0 \"uiSettings\": {\r\n\n\u00a0 \u00a0 \"theme\": {\r\n\n\u00a0 \u00a0 \u00a0 \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 }\r\n\n}\r\n\nSample output for Single visuals:\r\n\n{\r\n\n\u00a0 \"datasets\": [\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"name\": \"your_dataset_name\",\r\n\n\u00a0 \u00a0 \u00a0 \"displayName\": \"Your Dataset Display Name\",\r\n\n\u00a0 \u00a0 \u00a0 \"queryLines\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \"SELECT column1 AS field1, column2 AS field2, SUM(column3) OVER (ORDER BY column1 ROWS UNBOUNDED PRECEDING) AS cumulative_field FROM your_database.your_table GROUP BY column1, column2 ORDER BY column1\"\r\n\n\u00a0 \u00a0 \u00a0 ]\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 ],\r\n\n\u00a0 \"pages\": [\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"name\": \"page_1\",\r\n\n\u00a0 \u00a0 \u00a0 \"displayName\": \"Your Page Title\",\r\n\n\u00a0 \u00a0 \u00a0 \"layout\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"widget\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"line_chart_widget\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"queries\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"main_query\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"query\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"datasetName\": \"your_dataset_name\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fields\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"name\": \"field1\", \"expression\": \"`field1`\" },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"name\": \"field2\", \"expression\": \"`field2`\" },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"name\": \"cumulative_field\", \"expression\": \"`cumulative_field`\" }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"disaggregated\": false\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"spec\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"version\": 3,\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"widgetType\": \"line\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"encodings\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": \"field1\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"displayName\": \"Field 1 Label\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scale\": { \"type\": \"temporal\" }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fields\": [\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": \"field2\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"displayName\": \"Field 2 Label\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": \"cumulative_field\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"displayName\": \"Cumulative Field Label\"\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scale\": { \"type\": \"quantitative\" }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"frame\": {\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"title\": \"Your Chart Title\",\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"showTitle\": true\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"position\": { \"x\": 0, \"y\": 0, \"width\": 12, \"height\": 8 }\r\n\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\n\u00a0 \u00a0 \u00a0 ],\r\n\n\u00a0 \u00a0 \u00a0 \"pageType\": \"PAGE_TYPE\",\r\n\n\u00a0 \u00a0 \u00a0 \"filters\": []\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 ],\r\n\n\u00a0 \"uiSettings\": {\r\n\n\u00a0 \u00a0 \"theme\": {\r\n\n\u00a0 \u00a0 \u00a0 \"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 }\r\n\n}\r\n\n## **Encoding Rules**\r\n\n* **Single field axis** \u2192\r\n\n```json\r\n\n\"y\": {\r\n\n\u00a0 \"fieldName\": \"Sales\",\r\n\n\u00a0 \"displayName\": \"Sales\",\r\n\n\u00a0 \"scale\": { \"type\": \"quantitative\" }\r\n\n}\r\n\n```\r\n\n* **Multi-measure axis** \u2192\r\n\n```json\r\n\n\"y\": {\r\n\n\u00a0 \"fields\": [\r\n\n\u00a0 \u00a0 { \"fieldName\": \"Processed\", \"displayName\": \"Processed\" },\r\n\n\u00a0 \u00a0 { \"fieldName\": \"Shipped\", \"displayName\": \"Shipped\" }\r\n\n\u00a0 ],\r\n\n\u00a0 \"scale\": { \"type\": \"quantitative\" }\r\n\n}\r\n\n```\r\n\n---\r\n\n## **Dataset Mapping Rules**\r\n\n* Every dataset in `.lvdash.json` must directly map to a SQL SELECT statement from `metric_views.sql`.\r\n\n* Column names must exactly match aliases.\r\n\n* Never alias an aggregated field with the same name as its raw column.\r\n\n---\r\n\n---",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 16855,
          "name": "DI PageWise ThoughtSpotSQL To Individual Lvdashboard Creator S3 IMS HP Clone",
          "workflowId": 8497,
          "agentDetails": {
            "topP": 0.7,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 800,
            "temperature": 0.1,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 300,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "must follow the below instruction:\r\nMust Check for this condition first and then follow the Instruction i mentioned below:\r\n\nget the input token size of the file from the previous agent (DI_Token_Size_Estimator) output\r\n\nif the token is greater than 8000 go below to instruction and generate the output \r\n\nif the token is less than 8000 that you got from the previous agent output\r\n\nGive this as an output massage\r\n\n\"The Token is less than 8000 so the task is delegated\" \r\n\nand end the execution\r\n\n### **INSTRUCTIONS if the token size is greater than 8000:**  \r\n\nMust note that once the you have to run the tool only once, means the array ends means you can end the tool no need to run the tool more than one time\r\n\nThe agent is responsible for executing the **`DI_Workflow_Executor_HP`** tool to process workflows page_names List by page_names List from a CSV file stored in a Databricks repository. The agent will simulate API calls for each page_names List, generate output JSON files, and return them in a structured format.\r\n\n---\r\n\n#### **Context and Background Information:**\r\n\n* The tool reads input data from a CSV file hosted in a Databricks repository.\r\n\n* Each page_names List in the CSV file represents a unique workflow execution.\r\n\n* The tool simulates API calls for processing workflows and generates output JSON files for each page_names List.\r\n\n* Outputs must be structured but the agent itself should **not create or update anything in Databricks**.\r\n\n---\r\n\n#### **Scope and Constraints:**\r\n\n* Ensure that all page_names List in the CSV file are processed sequentially.\r\n\n* Simulate API calls for each page_names List without actual external API interaction.\r\n\n* Generate output JSON files in memory, but do not upload or commit them to Databricks.\r\n\n* Handle errors gracefully, such as malformed CSV page_names List , and log meaningful messages for debugging.\r\n\n---\r\n\n#### **Process Steps:**\r\n\n1. **page_names List in CSV File:**\r\n\nFor thepage_names List use the previous agent output which is tml to csv where u can find the page_names List of the csv file and then u can pass those information to the workflow executor tool\r\n\n2. **Iterate Through page_names List:**\r\n\n\u00a0 \u00a0* Loop through each value in the CSV file (starting from value1).\r\n\n\u00a0 \u00a0* For each value:\r\n\n\u00a0 \u00a0 \u00a0* Construct a folder path and response file name for storing the output.\r\n\n\u00a0 \u00a0 \u00a0* Create a dummy payload simulating the API call for workflow execution.\r\n\n\u00a0 \u00a0 \u00a0* Generate a sample output JSON file containing the simulated results.\r\n\n3. **Log Progress:**\r\n\n\u00a0 \u00a0* Print progress logs for each value processed, including  and simulated status.\r\n\n4. **Return Summary:**\r\n\n\u00a0 \u00a0* Return a summary message indicating the total number of page_names List processed and that the workflow execution was simulated successfully.\r\n\n---\r\n\n#### **OUTPUT FORMAT:**\r\n\na message if the tool executed successfully\r\n\nA summary message indicating successful execution of the workflow for all page_names Listand confirmation of outputs uploaded to the Databricks repository.\r\n\n* **Quality Criteria:**\r\n\n\u00a0 * Include accurate value and workflow names in the output.\r\n\n\u00a0 * Do not attempt to commit or upload files to Databricks.\r\n\n---\r\n\n#### **INPUT PARAMETERS:**\r\n\n* For the Databricks Credentials (Token, URL and Inputfile  folder path ) use this input from the user and pass it to the databricks tool : {{S3Credentials}}\r\n\nFor the input values  internal ava api key, mail`, `csv_file_path`, ` and internal ava `, `csv file name `, and `metric file name` and the ''page file name , use these directly from the user input:\r\n\n```\r\n\n{{AVACredentials}}\r\n\n```\r\n\n* For the Databricks Credentials (Output folder ) use this input from the user and pass it to the databricks tool : {{OutputPathForDIPageWiseThoughtSpotSQLToIndividualLvdashboardCreator}}\r\n\nMust for the page_names List for the tool alone use the previous agent (DI_TML_To_CSV_Converter) output as input where u can find the page_names List in the csv \r\n\n---\r\n\nInput FIlenames:\r\n\nInput folder name: Extract/ThoughtSpotTestingP2/HPX Executive Summary UC/Input\r\n\ninput file name: New_MetricViewsOnModelFile.sql\r\n\nCSV File name: YAML_To_CSV_With_PageName.csv\r\n\nfilter value: \"any values from the user\"\r\n\nOutput Folder:Extract/ThoughtSpotTestingP2/HPX Executive Summary UC/Output\r\n\n#### **OUTPUT:**\r\n\nA summary message indicating successful execution of the workflow for all page_names List, and confirmation that the outputs were generated (but not uploaded to Databricks).\r\n\n---",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 16758,
          "name": "DI Pagewise Thoughspot Json Merger S3 IMS HP Clone",
          "workflowId": 8497,
          "agentDetails": {
            "topP": 0.7,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 800,
            "temperature": 0.1,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 300,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "The agent is tasked with automating the JSON merging process using the Lvadash JSON merger tool. Follow the detailed instructions below to ensure the task is completed successfully.  \r\nThis ensures this TOOL flow is strict and sequential:\r\n\nFirst Use S3DatabricksMergerTool to merge JSONs \r\n\nSecond Use S3DatabricksMergerTool  to upload the merged .lvdash.json into S3.\r\n\n---\r\n\n### **INSTRUCTIONS:**  \r\n\n#### **Context and Background Information:**  \r\n\nThe Lvadash JSON merger tool is a command-line utility designed to merge multiple JSON files into a single consolidated \r\n\n#### **Scope and Constraints:**  \r\n\n- Ensure all JSON files in the repository are merged without data loss or corruption.  \r\n\n- The merged file must adhere to JSON formatting standards.  \r\n\n- The process must be repeatable and scalable for future use.  \r\n\n#### **Process Steps to Follow:**  \r\n\n1. **Retrieve S3 Folder Credentials:**  \r\n\n\u00a0 \u00a0- Validate the credentials by attempting to clone or pull the repository.  \r\n\n2. **Clone the Repository:**  \r\n\n\u00a0 \u00a0- Use the S3 Folder credentials to clone the repository locally.  \r\n\n\u00a0 \u00a0- Ensure the repository contains the JSON files to be merged.  \r\n\n3. **Run the Lvadash JSON Merger Tool:**  \r\n\n\u00a0 \u00a0- Navigate to the directory containing the JSON files.  \r\n\n\u00a0 \u00a0- Execute the Lvadash JSON merger tool to merge all JSON files into a single consolidated file.  \r\n\n\u00a0 \u00a0- Verify the merged output for correctness and adherence to JSON standards.  \r\n\n4. **Commit the Merged Output to s3 Folder:**  \r\n\n\u00a0 \u00a0- Replace or add the merged JSON file to the repository.  \r\n\n\u00a0 \u00a0- Stage the changes using `s3Folder add`.  \r\n\n\u00a0 \u00a0- Commit the changes with a descriptive message (e.g., \"Merged JSON files using Lvadash JSON merger tool\").  \r\n\n\u00a0 \u00a0- Push the changes back to the remote repository.  \r\n\n5. **Error Handling:**  \r\n\n\u00a0 \u00a0- Implement error handling for invalid s3 Folder credentials, tool execution failures, and commit/push errors.  \r\n\n\u00a0 \u00a0- Log all errors and provide actionable feedback to the user.  \r\n\n---\r\n\n### **OUTPUT FORMAT:**  \r\n\n#### **Format Specifications:**  \r\n\n- The merged JSON file must be formatted according to industry-standard JSON conventions.  \r\n\n- The Databricks Foldercommit message should be concise and descriptive.  \r\n\n#### **Structure Requirements:**  \r\n\n- JSON file structure should be hierarchical and well-organized.  \r\n\n- Ensure no duplicate keys or conflicting data in the merged output.  \r\n\n#### **Quality Criteria:**  \r\n\n- The merged JSON file must pass JSON validation checks.  \r\n\n- The s3 Folder repository must reflect the changes accurately after the push operation.  \r\n\n#### **Formatting Needs:**  \r\n\n- Use proper indentation (2 spaces) for the JSON file.  \r\n\n- Commit messages should follow s3 Folder best practices (e.g., \"feat: Merged JSON files for data consolidation\").  \r\n\n---\r\n\n### **SAMPLE:**  \r\n\n#### **Example of Merged JSON Output:**  \r\n\n```json\r\n\n{\r\n\n\u00a0 \"users\": [\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"id\": 1,\r\n\n\u00a0 \u00a0 \u00a0 \"name\": \"Alice\"\r\n\n\u00a0 \u00a0 },\r\n\n\u00a0 \u00a0 {\r\n\n\u00a0 \u00a0 \u00a0 \"id\": 2,\r\n\n\u00a0 \u00a0 \u00a0 \"name\": \"Bob\"\r\n\n\u00a0 \u00a0 }\r\n\n\u00a0 ],\r\n\n\u00a0 \"settings\": {\r\n\n\u00a0 \u00a0 \"theme\": \"dark\",\r\n\n\u00a0 \u00a0 \"notifications\": true\r\n\n\u00a0 }\r\n\n}\r\n\n---\r\n\nMust use the S3DatabricksMergerTool the tool only once and upload the file name that is exactly matching with the user input\r\n\nInput:\r\n\n* For the Databricks Credentials (Output folder ) use this input from the user and pass it to the S3databricks tool : {{OutputPathForJsonMerger}}\r\n\n*For the output file name and folder for the S3  use this inputfrom the user: \r\n\n{{S3Credentials}}\r\n\n**OUTPUT:**  \r\n\nA merged JSON file committed to the S3 repository using the Lvadash JSON merger tool.",
          "modelName": "model"
        }
      ],
      "realmId": 79,
      "tags": [
        1,
        2
      ],
      "practiceArea": 4
    }
  },
  "status": "SUCCESS"
}