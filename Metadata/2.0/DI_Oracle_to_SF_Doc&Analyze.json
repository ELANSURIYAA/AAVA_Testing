{
  "workFlowDetail": {
    "id": 2786,
    "name": "DI Oracle to SF Doc&Analyze"
  },
  "workflowAgents": [
    {
      "agentId": 7059,
      "name": "DI Oracle Documentation",
      "modelDeploymentName": "gpt-4.1",
      "description": "Please create detailed documentation for the provided Oracle stored procedure.\n\nThe documentation must contain the following sections:  \n\n1. Metadata Requirements:**\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it Empty)\nDescription:   <one-line description of the generated document>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n\n2. Overview of Program:  \n   - Explain the purpose of the Oracle stored procedure in detail.  \n   - Describe how this implementation aligns with enterprise data warehousing and analytics.  \n   - Explain the business problem being addressed and its benefits.  \n   - Provide a high-level summary of Oracle stored procedure components like PL/SQL blocks, Packages, Procedures, Functions, Views, and Tables.  \n\n3. Code Structure and Design:  \n   - Explain the structure of the Oracle stored procedure code in detail.  \n   - Describe key components like DDL, DML, Joins, Indexing, and PL/SQL Blocks.  \n   - List the primary Oracle stored procedure components such as Tables, Views, Stored Procedures, Functions, Packages, Joins, Aggregations, and Subqueries.  \n   - Highlight dependencies on Oracle objects, performance tuning techniques, or third-party integrations.  \n\n4. Data Flow and Processing Logic:  \n   - Explain how data flows within the Oracle stored procedure implementation.  \n   - List the source and destination tables, fields, and data types.  \n   - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.  \n\n5. Data Mapping:  \n* Provide data mapping details, including transformations applied to the data in the below format:  \n* Target Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks\n* Mapping column will have the details whether its 1 to 1 mapping or the transformation rule or the validation rule  \n\n6. Complexity Analysis:  \n   - Analyze and document the complexity based on the following:  \n   - Give this one in the table format with below two columns for the below data\nCategory  |  Measurement\n* Number of Lines: Count of lines in the stored procedure.\n* Tables Used: number of tables referenced in the stored procedure.\n* Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n* Temporary tables: Number of Global Temporary Tables, derived tables\n* Aggregate Functions: Number of aggregate functions like OLAP functions\n* DML Statements: Number of DML statements by type like SELECT, INSERT, UPDATE, DELETE, MERGE, EXECUTE, LOCK operations present in the stored procedure.\n* Conditional Logic: Number of conditional logic like IF-THEN-ELSE, CASE, DECODE\n* stored procedure code Complexity: Number of joins, subqueries, and stored procedures.  \n* Performance Considerations: Query execution time, temporary tablespace usage, and memory consumption.  \n* Data Volume Handling: Number of records processed.  \n* Dependency Complexity: External dependencies such as Packages, Procedures, Functions, or External Scripts.  \n* Overall Complexity Score: Score from 0 to 100. \n\n7. Key Outputs:  \n   - Describe final outputs such as Aggregated Reports, Tables, Views, or Data Exports.  \n   - Explain how outputs align with business goals and reporting needs.  \n   - Specify the storage format (e.g., Staging Tables, Production Tables, Flat Files, External Data Sources).  \n\n8. Error Handling and Logging:  \n   - Explain methods used for error identification and management, such as:  \n     - Try-Catch-Exception mechanisms in PL/SQL.  \n     - Oracle Error Logging Tables for tracking failures.  \n     - Retry mechanisms in SQL*Loader and Data Pump.  \n     - Automated alerts and monitoring dashboards.  \n\n9. apiCost: float  // Cost consumed by the API for this call (in USD)\n- Calculate the total **number of input tokens** used (including this prompt + the input SQL).\n- Calculate the total **number of output tokens** used (including the converted SQL + explanation).\n- Automatically detect the model used for processing this prompt.\n- Retrieve the current pricing for that model (for both input and output tokens) from the system or environment, if available.\n- Use that pricing to compute cost:\n- Input Cost = `input_tokens * [input_cost_per_token]`\n- Output Cost = `output_tokens * [output_cost_per_token]`\n- Present the full formula and breakdown clearly:\n\nInput:\n* For Oracle stored procedure use below file: \n{{OracleFile}}\n",
      "toolReferences": []
    },
    {
      "agentId": 5557,
      "name": "DI Oracle to Snowflake Analyzer",
      "modelDeploymentName": "gpt-4.1",
      "description": "Parse the Oracle stored procedure code to generate a comprehensive migration-readiness report. If multiple procedures are provided, each file\u2019s analysis must be reported in a separate session. Each session should include:\n\nINSTRUCTIONS:\n\n1. Metadata Requirements:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the generated analysis report>\n=============================================\n```\n- For the description, provide a concise summary of what the analysis does.\n\n2. Script Overview:\nProvide a concise summary of the stored procedure\u2019s purpose and business logic and Highlight functional sections like DML operations, PL/SQL blocks, exception handling, temporary table usage, and data transformation logic. Briefly describe the nature of the data workflow or operation (e.g., ETL logic, report generation, data sync).\n\n3. Complexity Metrics:\nPresent the following metrics in a **tabular format** with the specified column names:\n\n| Metric                | Count / Type                                                                 |\n|-----------------------|------------------------------------------------------------------------------|\n| Number of Lines       | Total lines of code in the procedure.                                        |\n| Tables Used           | Total number of referenced tables.                                           |\n| Joins                 | Count and types of joins (e.g., INNER, LEFT, CROSS).                         |\n| Temporary Tables      | Count of global temporary and derived tables.                                |\n| Aggregate Functions   | Count of aggregate/OLAP functions used.                                      |\n| DML Statements        | Count by type: SELECT, INSERT, UPDATE, DELETE, MERGE, EXECUTE, LOCK.         |\n| Conditional Logic     | Count of conditionals: IF-THEN-ELSE, CASE, DECODE, etc.                      |\nand also give Suggest Snowflake-native performance enhancements such as:\n- Use of micro-partitioning, clustering keys.\n- Query refactoring using CTEs or lateral flattening.\n- Handling semi-structured data (if any).\n- Recommendations on whether to **Refactor** with minimal changes or **Rebuild** with better design. Justify the choice based on performance, maintainability, and compatibility.\n\n4. Syntax & Feature Compatibility Check:\nIdentify Oracle features or constructs that require changes for Snowflake compatibility.\n\nHighlight elements like:\n- PL/SQL procedural blocks.\n- CURSOR usage.\n- Package and function dependencies.\n- Oracle-specific analytical or hierarchical queries.\n- Usage of sequences, synonyms, and indexes.\n\n5. Manual Adjustments for Snowflake Migration:\nProvide detailed recommendations for:\n- Function and feature replacements (e.g., `SYSDATE` \u2192 `CURRENT_TIMESTAMP`).\n- Syntax changes (e.g., `:=` assignment, exception handling blocks).\n- Workarounds for unsupported features like autonomous transactions or DBMS_OUTPUT.\n- Translating Oracle procedural constructs to Snowflake Scripting.\n\n6. Conversion Complexity Score:\nAssign a migration complexity score (0\u2013100) based on:\n- Number of incompatible features.\n- Amount of procedural logic vs SQL logic.\n- Manual intervention points.\n- Overall complexity and modularity of the stored procedure.\n\nAlso, highlight particularly complex blocks or areas (e.g., nested loops, dynamic SQL, BULK COLLECT/FORALL usage).\n\n 7. Optimization Techniques:\n* Suggest optimization strategies for snowflake, such as partitioning, clustering, and query performance enhancements (e.g., materialized views, WITH clauses).\n\n8. API Cost Estimation and Justification\nCost Section Instructions:\n- Calculate the total **number of input tokens** used (including this prompt + the input SQL).\n- Calculate the total **number of output tokens** used (including the converted SQL + explanation).\n- Automatically detect the model used for processing this prompt.\n- Retrieve the current pricing for that model (for both input and output tokens) from the system or environment, if available.\n- Use that pricing to compute cost:\n- Input Cost = `input_tokens * [input_cost_per_token]`\n- Output Cost = `output_tokens * [output_cost_per_token]`\n- Present the full formula and breakdown clearly:\n\nNote:\nStrictly followed the expected output\n\nInput:\nFor Oracle stored procedure(s), use the below file(s) or text file(s) which contain Oracle PL/SQL code:\n{{OracleFile}}",
      "toolReferences": []
    },
    {
      "agentId": 4841,
      "name": "DI Oracle to Snowflake Plan",
      "modelDeploymentName": "gpt-4.1",
      "description": "You are tasked with providing a detailed cost and effort estimation for executing and testing Snowflake stored procedures that were converted from Oracle PL/SQL scripts. Use the prior Oracle analysis report to drive this estimation.\n\nINSTRUCTIONS:\n1. Metadata Requirements:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate: (Leave it empty)\nDescription:   <one-line description of the generated plan>\n=============================================\n```\n- For the description, provide a concise summary of what the plan does.\n\nYou are tasked with providing a comprehensive effort estimate for testing the Snowflake code converted from Oracle PL/SQL scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n\nReview the analysis of the Oracle PL/SQL script file, noting logic differences and areas in the code requiring manual intervention when converting to Snowflake.\n\nEstimate the effort hours required for identified manual code fixes and data reconciliation testing efforts.\n\nDo not consider effort for pure syntax-level differences as they will be handled automatically by conversion tools.\n\nConsider the pricing information for the Snowflake environment.\n\nCalculate the estimated cost of running the converted Snowflake code:\na. Use Snowflake's pricing model and data volume to determine the query cost.\nb. Include the number of queries executed and the data processed using both temporary and permanent tables.\n- Strictly follow the ecpected output format\nOUTPUT FORMAT:\n Metadata Requirements:\n\n\n1. Cost Estimation\n   1.1 Snowflake Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n2. Code Fixing  and Testing Effort Estimation\n   2.1 Snowflake identified manual code fixes and unit testing effort in hours covering the various temp tables, calculations \n   2.2 Optimization and performance tuning of Snowflake queries.\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\n-follow strictly the expected output format\nINPUT\n\nFor the input Oracle analysis report and code reference, use the file:\n{{OracleFile}}\n\nFor the Snowflake environment resource and pricing reference, use the file:\n{{EnvVariable}}",
      "toolReferences": []
    }
  ]
}