{
    "workflowId": 8573,
    "workflowName": "DI_AbInitio_To_PySpark_EMR_Glue_Doc&Analyze",
    "nodes": [
        {
            "agentName": "DI_AbInitio_Documentation",
            "model": "anthropic.claude-4-sonnet",
            "tools": [],
            "task": {
                "description": "The documentation should include the following sections:\n\n- Format: Markdown\n- Add the following metadata at the top of each generated file:\n====================================================\nAuthor:        AAVA\nDate:          \nDescription:   <one-line description of the purpose>\n====================================================\n\n1. Overview of Graph/Component\n   - Describe the purpose of the Ab Initio graph (.mp), transformation (.xfr), data definition (.dml), plan (.plan), or pset (.pset).\n   - Explain the business logic or requirement the graph or component addresses.\n\n2. Component Structure and Design\n   - Describe the layout and logical grouping of components inside the graph or plan.\n   - Highlight key components such as Input File, Reformat, Join, Sort, Dedup, Rollup, Output File, Run Program, and others.\n   - Mention the connection flow between components and the use of parameters or variables.\n\n3. Data Flow and Processing Logic\n   - List the key data sources, intermediate files, and final outputs.\n   - For each logical step:\n     - Describe what it does (filtering, joining, reformatting, aggregation, etc.).\n     - Mention any .xfr or .dml files used.\n     - Include any business rules or transformations applied.\n\n4. Data Mapping (Lineage)\n   - Map fields from input datasets to output datasets.\n   - ***IMPORTANT: The Data Mapping must be generated in TABLE format.***\n   - The table should have the following columns:\n     | Target Table | Target Column | Source Table | Source Column | Remarks |\n   - Remarks should include: 1:1 Mapping | Transformation | Validation (with logic description).\n\n5. Transformation Logic\n   - Document each .xfr function used or called in the flow.\n   - Explain what each function does and what fields are involved.\n   - Note any external function calls or reusable components.\n\n6. Complexity Analysis\n   - Number of Graph Components: <integer>\n   - Number of Lines of Code (in .xfr or .plan): <integer>\n   - Transform Functions Used: <count>\n   - Joins Used: <list of types or None>\n   - Lookup Files or Datasets: <count or None>\n   - Parameter Sets (.pset) or Plan Files Used: <count>\n   - Number of Output Datasets: <integer>\n   - Conditional Logic or if-else flows: <count>\n   - External Dependencies: <JDBC, shell scripts, other tools>\n   - Overall Complexity Score: <0\u2013100>\n\n7. Key Outputs\n   - Describe what is written to final datasets or passed to the next stages.\n   - Mention the format (Delimited, Fixed Width, etc.) and intended use (report, downstream system).\n\n8. Error Handling and Logging\n   - Document any Reject, Error, or Log components used.\n   - Mention .xfr-based error tagging, reject thresholds, or control file usage.\n   - Describe how errors are handled (auto-abort, reject files, alerting, etc.).\n\n9. API Cost (LLM Cost ONLY)\n   - ***You must calculate and print ONLY the LLM API cost consumed for THIS PARTICULAR CALL.***\n   - Do NOT calculate any job-related cost or cloud compute cost.\n   - The cost must be based purely on tokens used in this API call.\n   - Output format:\n     - Tokens Used (Prompt + Completion)\n     - Cost per 1K tokens\n     - Final Cost in USD for this single documentation run\n\nInput:\nAttach or provide the Ab Initio files (.mp, .xfr, .dml, .plan, .pset). Acceptable formats: plain text, zipped folder, or directory path structure: {{AbInitio_Code}}\n",
                "expectedOutput": "The generated Markdown documentation should include the following, based on the input Ab Initio code:\n- **Format:** Markdown  \n- Metadata Requirements: \"<as above>\"  \n- Overview of Program: \"<3\u20135 line description explaining business purpose>\"  \n- Code Structure and Design: \"<Detailed explanation of component layout and connection>\"  \n- Data Flow and Processing Logic:  \n  - Processed Datasets: [\"<list all dataset names>\"]  \n  - Data Flow: \"<Description of end-to-end data journey>\"  \n- Data Mapping:  \n  - Target Table Name: \"<value>\"  \n  - Target Column Name: \"<value>\"  \n  - Source Table Name: \"<value>\"  \n  - Source Column Name: \"<value>\"  \n  - Remarks: \"<Mapping logic>\"  \n- Transformation Logic: \"<Documentation for each .xfr used>\"  \n- Complexity Analysis:  \n  - Components: <number>  \n  - Joins: <type/count>  \n  - Functions: <count>  \n  - Conditional Paths: <count>  \n  - External Dependencies: \"<list>\"  \n  - Score: <0\u2013100>  \n- Key Outputs: \"<Summary of outputs>\"  \n- Error Handling and Logging: \"<How errors are managed>\"  \n- API Cost: \"<With Proper calculation for call the ai model for this particular task>\""
            }
        },
        {
            "agentName": "DI_AbInitio_To_PySpark_EMR_Glue_Analyzer",
            "model": "anthropic.claude-4-sonnet",
            "tools": [],
            "task": {
                "description": "This agent performs pre-conversion analysis of Ab Initio code to evaluate readiness for migration to PySpark on AWS EMR + Glue. It will:\n-Break down components, logic, and metadata in the Ab Initio graphs.\n-Identify translation challenges such as .xfr logic, rollups, reject flows, lookups, or .pset usage.\n-Score the complexity of converting the flow to EMR/Glue PySpark.\n-Recommend high-level PySpark design approaches suited for EMR and Glue.\n-Highlight any manual tasks future developers will need to address.\n\n###**INSTRUCTIONS:**\n1.Process Steps to Follow:\n     Step 1: Parse and interpret the Ab Initio .mp, .xfr, .dml files. Identify ETL components, flows, schemas, lookup structures, and transformation logic.\n     Step 2:Highlight potential challenges in converting Ab Initio components to PySpark for EMR/Glue, including: complex .xfr logic\n,reject or fallback flows, rollups, lookups, multi-input joins, .pset or dynamic parameter usage, schema conversion issues when mapping .dml \u2192 Glue Catalog, S3 partitioning or file format considerations\n     Step 3:Assign a conversion complexity score (0\u2013100) based on graph size, component types, .xfr density, branching, and schema complexity.\n     Step 4:Recommend high-level PySpark + EMR/Glue design strategies, such as modular code structure, Glue Catalog usage, S3 folder layout, and handling of joins or partitions.\n     Step 5:Suggest performance strategies that should be implemented in the PySpark design for EMR/Glue (e.g., caching, broadcast joins, repartitioning, avoiding unnecessary shuffles, preferring native PySpark functions over UDFs).\n\n### **Output Format:**  \nUse **Markdown formatting**. Include the following metadata header:\n\n```\n==================================================================================\nAuthor:        AAVA\nCreated on:    (Leave it empty)\nDescription:   Pre-conversion analysis of Ab Initio ETL flow for PySpark EMR/Glue migration\n==================================================================================\n```\n###**Sections to Include**\n\n####Syntax & Logical Structure Analysis:\n-Fill the table with each detected component (e.g., Input, Reformat, Rollup, Join, Broadcast, Filter, Sort, Output, etc.).Each row should represent one component.no bullet lists allowed. Return this section only as a table with the following columns:\n| Component | Description of Behavior | Likely PySpark Equivalent | Notes (Rejects, branching, conditions) |\n- Detail how each component behaves and the likely PySpark equivalent.\n- Mention any chained or conditional flows (e.g., reject or fallback branches).\n\n####Anticipated Manual Interventions:\n- Custom logic embedded in `.xfr` that requires manual PySpark function writing.\n-.dml types needing manual schema rewriting for Glue Catalog\n-Parameter sets (`.pset`) and dynamic inputs that need parsing.\n-Ab Initio-specific patterns without direct Spark equivalents\n\n####Complexity Evaluation:\n-Complexity Score (0\u2013100)\n-Justification based on:\ncomponent count\n     -.xfr density\n     -joins/lookups\n     -iterative or feedback loops\n     -schema and file format complexity\n\n####Performance & Scalability Recommendations:\n-Where to use broadcast joins\n-Caching or checkpointing guidance\n-S3 partitioning recommendations\n-Avoiding UDFs\n-Reducing shuffle volume\n\n####Refactor vs. Rebuild Recommendation:\nPick one:\n-**Refactor:** Mostly direct translation.\n-**Rebuild:** Logic should be redesigned for clarity or EMR performance.\n\n####API Cost:\nInclude the API cost for this call in USD (floating-point with full decimal precision).\napiCost: <actual_value> USD\n\n### **Input:**  \n- Ab Initio Source Files: {{AbInitio_Code}}  ",
                "expectedOutput": "A structured Markdown report providing detailed pre-conversion analysis, complexity scoring, migration risks, and recommendations for PySpark implementation on AWS EMR and Glue."
            }
        },
        {
            "agentName": "DI_AbInitio_To_PySpark_EMR_Glue_Plan",
            "model": "anthropic.claude-4-sonnet",
            "tools": [],
            "task": {
                "description": "You are tasked with reviewing while converting the Ab Initio source files (e.g., .mp, .xfr, .dml, .pset) into PySpark code, identifying logic gaps requiring manual resolution, estimating developer/tester effort, and calculating estimated cloud runtime costs using AWS EMR or Spark on AWS infrastructure.\n\n### **INSTRUCTIONS:**  \n1.Analyze the Ab Initio source while converting to PySpark code:\n- Focus on logical inconsistencies, incomplete transformation rules, metadata misalignment, and downstream output correctness.\n- Exclude pure syntax translation differences that are already handled by automated conversion.\n2.Estimate developer/tester effort in hours for:\n- Manual fixes in PySpark (e.g., transformation functions, joins, rejects, lookups)\n- Metadata/schema reconciliation\n- Data validation and functional testing\n\n3.Estimate AWS EMR Glue Cost using:\n- Assumed cluster configuration (e.g., m5.xlarge, 4 workers, 1 master)\n- Estimated PySpark job duration (in minutes)\n- AWS pricing model: e.g., $0.192 per instance-hour, $0.023 per GB storage/month\n\n4.Calculate Total Developer Cost using a default hourly rate (e.g., $50/hr)\n5.Present cost metrics and effort details in a clear, structured format.\n\n### **OUTPUT FORMAT:**  \nUse **Markdown** format and include the following metadata header:\n\nHeader\n```\n========================================================\nAuthor:        AAVA\nCreated on:    (Leave it empty)\nDescription:   \\<one-line summary of the code\u2019s purpose>\n========================================================\n```\n\n####1. AWS EMR Glue Runtime Cost Estimation\n#####1.1 EMR/Spark Job Cost Breakdown\n\n- **Cluster Configuration**: \n- Master Node: <e.g., m5.xlarge, 1 node>\n- Worker Nodes: <e.g., m5.xlarge, 4 nodes>\n- Total vCPUs & Memory\n\n- **Job Duration Estimate**: <minutes>  \n- **AWS Pricing**:  \n-Compute (per instance-hour)\n- Storage (per GB-hour equivalent)\n\n- **Cost Formula Used**:\n\nTotal Cost = (Total Instances \u00d7 Duration in hours \u00d7 Compute (per instance-hour))\n+ (Storage GB \u00d7 Duration in hours \u00d7 Storage (per GB-hour))\n\n- **Estimated Runtime Cost (USD)**: `<calculated_value>`\n\n#### 2. Manual Code Fixing and Data Reconciliation Effort  \n##### 2.1 Estimated Effort (Hours)  \n\n- Logic Corrections (e.g., .xfr transformations): <integer> hrs\n- Metadata Alignment (e.g., .dml type fixes): <integer> hrs\n- Rejected Row Handling / Edge Case Logic: <integer> hrs\n- Data Reconciliation & Output Validation: <integer> hrs\n- **Total Effort**: <sum> hrs\n\n##### 2.2 Developer Cost \n- Developer Rate: `$50/hr`  \n- **Total Developer Cost**: `<effort_hrs \u00d7 50>` USD\n\n\n#### 3. API Cost  \napiCost: <actual_cost> (in USD)\n\n### **Input:**  \n* AbInitio Source File(s): {{AbInitio_Code}}\n* Environmental variable file : {{Env_Variable}}",
                "expectedOutput": "A structured Markdown report with the following:\n- Header\n- AWS EMR Glue job cost analysis\n- Developer effort estimation\n- Total projected PySpark execution and validation cost\n- API processing cost"
            }
        }
    ]
}