{
    "workflowId": 148,
    "workflowName": "DI_INFA_To_PySpark_Doc&Analyse",
    "nodes": [
        {
            "agentName": "DI_Informatica_Documentation",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "The documentation should include the following sections:  \n\n- **Format:** Markdown  \n- Add the following metadata at the top of generated file:  \n====================================================\nAuthor:        Ascendion AAVA\nDate:          (Leave it empty)\nDescription:   <one-line description of the purpose>\n====================================================\n\n1. **Overview of Graph/Component**  \n   - Describe the purpose of the Informatica graph (.xml), transformation (transformation), data definition (source definition), plan (workflow), or pset (session).  \n   - Explain the business logic or requirement the graph or component addresses.  \n\n2. **Component Structure and Design**  \n   - Describe the layout and logical grouping of components inside the graph or plan.  \n   - Highlight key components such as `Input File`, `Reformat`, `Join`, `Sort`, `Dedup`, `Rollup`, `Output File`, `Run Program`, and others.  \n   - Mention the connection flow between components and the use of parameters or variables.  \n\n3. **Data Flow and Processing Logic**  \n   - List the key data sources, intermediate files, and final outputs.  \n   - For each logical step:\n     - Describe what it does (e.g., filtering, joining, reformatting, aggregation).  \n     - Mention any `transformation` or `source definition` files used.  \n     - Include any business rules or transformations applied.  \n\n4. **Data Mapping (Lineage)**  \n   - Map fields from input datasets to output datasets.  \n   - In Table Format with the below mentioned columns :\n     ```\n     Target Table Name : <actual target file/table>\n     Target Column Name : <actual column>\n     Source Table Name : <actual source file/table>\n     Source Column Name : <actual column>\n     Remarks : <1:1 Mapping | Transformation | Validation - include logic description>\n     ```\n\n5. **Transformation Logic**  \n   - Document each `transformation` function used or called in the flow.  \n   - Explain what each function does and what fields are involved.  \n   - Note any external function calls or reusable components.  \n\n6. **Complexity Analysis**  \n   - Number of Mappings: <integer>  \n   - Number of Lines of Code (in transformation or workflow): <integer>  \n   - Transform Functions Used: <count>  \n   - Joins Used: <list of types or None>  \n   - Lookup Files or Datasets: <count or 'None'>  \n   - Parameter Sets (session) or Plan Files Used: <count>  \n   - Number of Output Datasets: <integer>  \n   - Conditional Logic or `if-else` flows: <count>  \n   - External Dependencies: <JDBC connections, shell scripts, other tools>  \n   - Overall Complexity Score: <0\u2013100>  \n\n7. **Key Outputs**  \n   - Describe what is written to the final datasets or passed to the next stages.  \n   - Mention the format (Delimited, Fixed Width, etc.) and intended use of the output (e.g., report, downstream system).  \n\n8. **Error Handling and Logging**  \n   - Document any `Reject`, `Error`, or `Log` components used.  \n   - Mention any `transformation` based error tagging, reject thresholds, or control file usage.  \n   - Describe how errors are handled (e.g., auto-abort, routed to reject files, email alerts).\n\n9. **API Cost (Optional for Cloud Informatica Deployments)**  \n   - If relevant, calculate estimated compute or I/O cost based on size of data processed and component usage.  \n   - Formula or logic for cost estimation must be provided if available.\n10. ** Documentation Effort Savings by Agentic AI**\n   -  Estimated Effort Comparison\n   - In Table Format with the below mentioned columns :\n     ```\n     Activity : <actual activities involved>\n     Manual Effort (hrs) : <actual manual in hours>\n     Agentic AI Effort (hrs) : <Agentic AI Effort in hours>\n     ```\n   - Time Saved: ~<saved hours> hours per workflow\n\nThis represents a **<number>% reduction in documentation effort**, enabling faster project delivery, improved consistency, and reduced manual errors.\t \n\n**Input:**  \nAttach or provide the Informatica files (.xml, transformation, source definition, workflow, session). Acceptable formats: plain text, zipped folder, or directory path structure : {{Infa_File}}",
                "expectedOutput": "The generated Markdown documentation should include the following, based on the input Informatica code:\n- **Format:** Markdown  \n- Metadata Requirements: \"<as above>\"  \n1. Overview of Program: \"<3\u20135 line description explaining business purpose>\"  \n2. Code Structure and Design: \"<Detailed explanation of component layout and connection>\"  \n3. Data Flow and Processing Logic:  \n  - Processed Datasets: [\"<list all dataset names>\"]  \n  - Data Flow: \"<Description of end-to-end data journey>\"  \n4. Data Mapping Table:  \n  - Target Table Name: \"<value>\"  \n  - Target Column Name: \"<value>\"  \n  - Source Table Name: \"<value>\"  \n  - Source Column Name: \"<value>\"  \n  - Remarks: \"<Mapping logic>\"  \n5. Transformation Logic: \"<Documentation for each transformation used>\"  \n6. Complexity Analysis:  \n  - Components: <number>  \n  - Joins: <type/count>  \n  - Functions: <count>  \n  - Conditional Paths: <count>  \n  - External Dependencies: \"<list>\"  \n  - Score: <0\u2013100>  \n7. Key Outputs: \"<Summary of outputs>\"  \n8. Error Handling and Logging: \"<How errors are managed>\"  \n9. API Cost: \"<With Proper calculation for call the ai model for this particular task>\"\n10. Documentation Effort Savings by Agentic AI Table:\n  - Activity: \"<value>\"  \n  - Manual Effort (hrs): \"<value>\"  \n  - Agentic AI Effort (hrs): \"<value>\"  "
            }
        },
        {
            "agentName": "DI_INFA_To_PySpark_Analyzer",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "Perform a detailed analysis of the provided Informatica XML files to support their conversion to PySpark. If multiple XML files are included in the ZIP archive, repeat the process for each file individually and generate a separate output for every XML.\n\nFollow these detailed instructions:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output \n\n---\n\n**1. Workflow Overview**\n\n* Provide a high-level summary of the Informatica mapping or workflow.\n* Mention the key business objective it supports, such as data integration, cleansing, enrichment, or loading.\n\n---\n\n**2. Complexity Metrics** (present this in a markdown table format when generating output):\n\n* Number of Source Qualifiers: Count of Source Qualifier transformations.\n* Number of Transformations: Total number of transformations used (Expression, Aggregator, Joiner, Filter, Router, etc.).\n* Lookup Usage: Number of Lookups (connected/unconnected).\n* Expression Logic: Count of complex expressions (IIF, DECODE, nested logic, etc.).\n* Join Conditions: Count and type of joins (normal, outer, heterogeneous).\n* Conditional Logic: Number of Router or Filter conditions.\n* Reusable Components: Number of reusable transformations, mapplets, and sessions.\n* Data Sources: Number of distinct sources (databases, flat files, XML, etc.).\n* Data Targets: Number of unique targets (databases, files, queues).\n* Pre/Post SQL Logic: Number of pre/post SQLs or procedures used in sessions.\n* Session/Workflow Controls: Number of decision tasks, command tasks, and event-based controls.\n* DML Logic: Frequency of INSERT, UPDATE, DELETE, and MERGE operations.\n* Complexity Score (0\u2013100): Based on the depth of logic, control flow usage, nested operations, and transformation types.\n\nAlso highlight high-complexity areas like:\n\n* deeply nested expressions\n* multiple lookups\n* branching logic (Router, Filter)\n* unstructured sources or external scripts\n\n---\n\n**3. Syntax Differences**\n\n* Identify functions used in Informatica that don\u2019t have a direct PySpark equivalent.\n* Mention any necessary data type conversions (e.g., TO\\_DATE, TO\\_CHAR, DECODE).\n* Highlight any workflow/control logic (e.g., Router, Transaction Control) that must be restructured for PySpark.\n\n---\n\n**4. Manual Adjustments**\n\n* List components that require manual implementation in PySpark (e.g., Java transformation, SQL override blocks).\n* Identify external dependencies like pre/post SQLs, stored procedures, or shell scripts.\n* Mention areas where business logic must be reviewed or validated post-conversion.\n\n---\n\n**5. Optimization Techniques**\n\n* Recommend using Spark best practices like partitioning, caching, and broadcast joins.\n* Suggest converting chain filters and joins into a pipeline.\n* Recommend window functions to simplify nested aggregations.\n* Finally, advise whether to **Refactor** (retain most of the original logic) or **Rebuild** (if better optimization is possible in PySpark).\nNote:\nDont give the code in the ouput\n---\n\nInput :\n* For Informatica use the below file/s : {{Infa_File}}",
                "expectedOutput": "**OUTPUT:** If multiple Informatica XML files are included in the ZIP archive, perform this analysis for each file individually. For every XML file, generate a detailed Markdown report consist of \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n-  For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n**1. Workflow Overview - include number of mapping per work flow/session**\n**2. Complexity Metrics - include type in matric values like data source/target types(oracle/sql server/Teradata /mainframe/flat file) etc**\n**3. Syntax Differences**\n**4. Manual Adjustments**\n**5. Optimization Techniques**"
            }
        },
        {
            "agentName": "DI_INFA_to_PySpark_Plan",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "\n\n### Informatica to PySpark Conversion Analyzer \u2013 Prompt Instructions\n\n---\n\n#### Metadata Header (Add once at the top of the output file)\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:          \nDescription:   <One-line summary of the document's purpose>\n=============================================\n```\n\n* Add this header **only once** at the top of the file.\n* Leave the **Date** field empty for dynamic population.\n* Replace `<One-line summary of the document's purpose>` with a concise description of the output.\n\n---\n\n### Conversion Instructions\n\nAs a **Data Migration Specialist**, your task is to analyze the provided Informatica XML files and generate a comprehensive plan for its conversion to PySpark.\n\nFollow these steps:\nThese has to be done for each Informatica XML file in zip separately.\n1. **Analyze the Informatica XML:**\n   * Use the previously generated output from the `DI_INFA_to_PySpark_Analyzer ` agent.\n2. **Compare and Identify Gaps:**\n\n   * Review syntax and functional logic in the Informatica XML.\n   * Identify areas requiring manual intervention (e.g., script components, event handling, third-party tasks).\n   * Do **not** count simple syntax changes; those will be auto-converted.\n\n3. **Effort Estimation:**\n\n   * Estimate the manual effort in hours for each identified gap.\n   * Provide separate effort estimates for:\n\n     * Manual code fixes\n     * Data reconciliation and validation testing\n\n4. **PySpark Environment Considerations:**\n\n   * Use the GCP Dataproc serverless environment details use the file from input\n   * Factor in performance, scalability, and any special configurations (e.g., clusters, job orchestration).\n\n5. **Cost Estimation:**\n\n   * Include the **API cost consumed** for this analysis in the final output.\n   * Format as: `apiCost: <floating-point value> USD` (e.g., `apiCost: 0.0462 USD`)\n\n---\n\nINPUT\nTake the previous  agent \"DDI_INFA_to_PySpark_Analyzer\" output as input.\nFor the input Informatica workflow/mapping, use this file/s: {{Infa_File}}\nFor the input PySpark Environment Details for Azure Databricks, use this file:{{Env_Details}}",
                "expectedOutput": "If more than one Informatica XML available in input zip folder then generate output for each XML\n---\n\n#### Metadata Header (Add only once at the top of the output file)\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:          \nDescription:   Cost and effort estimation for PySpark conversion\n=============================================\n```\n\n* Include this header only once at the beginning.\n* Leave the **Date** field blank for dynamic use.\n---\n\n### 1. Cost Estimation\n\n#### 1.1 PySpark Runtime Cost\n\nProvide a detailed cost breakdown for running the converted PySpark code.\n\nJustify the estimated runtime cost based on:\n\n* Complexity of Informatica transformations involved.\n* Expected data volume and job execution time.\n* Performance optimizations applied in PySpark, if any.\n\n---\n\n### 2. Code Fixing and Testing Effort Estimation\n\n#### 2.1 PySpark Code Manual Fixes and Unit Testing Effort\n\nEstimate the effort (in hours) required to manually fix and validate the PySpark code.\n\nInclude effort for handling:\n\n* Informatica lookups\n* Joins\n* Aggregations\n* Conditional expressions\n\n#### 2.2 Output Validation Effort\n\nEstimate the effort (in hours) required to compare and validate outputs between:\n\n* Original Informatica workflows\n* Converted PySpark scripts\n\n#### 2.3 Total Estimated Effort in Hours\n\nProvide the total estimated effort (in hours).\n\nJustify how the total effort was calculated by summing the individual efforts and accounting for complexity or overlap in tasks.\n\n---\n\n### 3. API Cost Consumption\n\nInclude the cost consumed by the API during this call in the final output.\nEnsure the cost is clearly reported in **floating-point format** with currency, like this:\n\n```\napiCost: 0.0523 USD\n```\n"
            }
        }
    ]
}