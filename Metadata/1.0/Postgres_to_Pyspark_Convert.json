{
    "workflowId": 776,
    "workflowName": "Postgres_to_Pyspark_Convert",
    "nodes": [
        {
            "agentName": "Postgres_to_PySpark_Converter ",
            "model": "o3-mini",
            "tools": [],
            "task": {
                "description": "The AI agent takes an input PostgreSQL query, stored procs and performs the following steps:\n\n1. Carefully analyze the provided PostgreSQL SQL queries and stored procedures.\n2. Identify the main components of each query or procedure, including table references, joins, aggregations, and complex operations.\n3.Parse the SQL query \u2013 Identify key components such as SELECT columns, WHERE conditions, JOINs, GROUP BY, ORDER BY, etc.\n4.Map SQL to PySpark DataFrame transformations \u2013 Convert SQL operations into their corresponding PySpark transformations using filter(), select(), join(), groupBy(), and other functions.\n5.Generate the final PySpark script \u2013 Construct a complete and executable PySpark code snippet, ensuring proper imports, Spark session initialization, and necessary transformations.\n6. Determine the appropriate PySpark DataFrame or SQL functions to replicate the PostgreSQL logic.\n7. Convert each PostgreSQL statement into its PySpark equivalent, ensuring that the logic and functionality remain intact.\n8. Pay special attention to:\n   a. Table creation and data loading\n   b. Join operations\n   c. Window functions\n   d. Aggregations and grouping\n   e. Subqueries and CTEs (Common Table Expressions)\n   f. Date and string manipulations\n   g. User-defined functions (UDFs)\n9. Optimize the PySpark code for performance where possible, considering Spark's distributed computing nature.\n10. Add comments to explain complex transformations or logic that may not be immediately apparent.\n11. Ensure that the resulting PySpark code is well-formatted and follows PEP 8 style guidelines.\n\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For input postgres code use this file : ```%1$s```\n\nNote :\n1. Follow the expected output and return the output in same format. \n2. Just convert the given query \n3. Do not add anything other than what's mentioned in Expected output",
                "expectedOutput": "* Converted PySpark Code\n* Include the cost consumed by the API for this call in the output."
            }
        },
        {
            "agentName": "Postgres_to_PySpark_Unit_Testing",
            "model": "gpt-4o",
            "tools": [],
            "task": {
                "description": "You are tasked with creating a set of unit test cases and a Pytest script for the given PySpark code. Your expertise in PySpark testing methodologies and best practices will be essential in ensuring comprehensive test coverage.\n\nINSTRUCTIONS:\n1. Analyze the provided PySpark code to identify key functionalities, data transformations, and potential edge cases.\n2. Create a list of test cases that cover:\n   a. Happy path scenarios\n   b. Edge cases (e.g., empty DataFrames, null values, boundary conditions)\n   c. Error handling and exception scenarios\n3. Design test cases using PySpark-specific terminology and concepts (e.g., DataFrame operations, UDFs, window functions).\n4. Implement the test cases using Pytest and PySpark testing utilities.\n5. Ensure proper setup and teardown of SparkSession for each test.\n6. Use appropriate assertions to validate expected outcomes.\n7. Include comments explaining the purpose of each test case.\n8. Organize the test cases logically, grouping related tests together.\n9. Implement any necessary helper functions or fixtures to support the tests.\n10. Ensure the Pytest script follows PEP 8 style guidelines.\n\nOUTPUT FORMAT:\n1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* Use the previous postgres to pyspark converter agents converted pyspark script as input",
                "expectedOutput": "1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output."
            }
        },
        {
            "agentName": "Postgres_to_PySpark_Conversion_Tester",
            "model": "gpt-4o",
            "tools": [],
            "task": {
                "description": "You are tasked to process PostgreSQL queries, Stored procs alongside their converted PySpark equivalents and performs the following tasks:\n\n1. Syntax Change Detection: Compares the PostgreSQL and PySpark code to highlight differences, such as:\n-SQL function conversions (e.g., STRING_AGG() \u2192 PySpark equivalent using groupBy().agg())\n-Data type transformations (TEXT \u2192 StringType(), BOOLEAN \u2192 BooleanType(), etc.)\n-Query structure modifications (e.g., JOIN strategies)\n-Aggregation and window function changes\n-Handling of NULL values and case sensitivity adjustments\n2.Recommended Manual Interventions: Identifies potential areas requiring manual fixes like\n-Performance optimizations (broadcast joins, repartitioning)\n-Edge case handling for data inconsistencies\n-Complex expressions requiring PySpark UDFs\n3. Create a comprehensive list of test cases covering:\n   a. Syntax changes\n   b. Manual interventions\n3. Develop a Pytest script for each test case\n* Include the cost consumed by the API for this call in the output.\n\nOutput:\n1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For the input Postgres code Analysis use this file : ```%2$s```\n* And also take the previous Postgres to PySpark converter agents converted output as input.",
                "expectedOutput": "1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output."
            }
        },
        {
            "agentName": "Postgres_to_PySpark_Reconciliation",
            "model": "gpt-4o",
            "tools": [],
            "task": {
                "description": "Your tasked with comparioing  the data output from  PostgreSQL code and its corresponding converted PySpark code implementation. \n\nInstructions\n1. Analyze the Postgre and PySpark code to identify the input data sources and output targets (tables or files) \n2. Create a set of diverse test cases covering various scenarios, including:\n   a. Record insertions\n   b. Record updates\n   c. Record deletions\n3. Develop a pytest script that:\n   a. Executes the Postgre code \n   b. Executes the PySpark code\n   c. Retrieves the output from both ABAP and PySpark targets\n   d. Compares the outputs to identify any discrepancies\n   e. Generates a detailed report of the comparison results in terms records matching, not matching across inserts, updates and deletes\n4. Include appropriate assertions to validate the test results\n5. Implement proper error handling and logging mechanisms\n6. Ensure the pytest script is modular, maintainable, and follows best practices\n*The total cost incurred for the execution of the agent.\n\nINPUT :\n* For the input Postgres code use this file : ```%1$s```\n* Also take the previous Postgres to Pyspark conversion agents converted PySpark script as input",
                "expectedOutput": "1. Test Cases Document:\n   - Test Case ID\n   - Description\n   - Input Data\n   - Expected Output\n\n2. Pytest Script for each of the test cases\n\n3. The total cost incurred for the execution of the agent."
            }
        },
        {
            "agentName": "Postgres_to_PySpark_Reviewer",
            "model": "gpt-4o",
            "tools": [],
            "task": {
                "description": "Your taks is to meticulously analyze and compare the original Postgres code with the newly converted PySpark implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the PySpark environment, code reviewer, compares the Postgres code vs converted PySpark code to determine for any gaps in the conversion\nINSTRUCTIONS:\n1. Carefully read and understand the original Postgres code, noting its structure, logic, and data flow.\n2. Examine the converted PySpark code, paying close attention to:\n   a. Data types and structures\n   b. Control flow and logic\n   c. SQL operations and data transformations\n   d. Error handling and exception management\n3. Compare the Postgres and PySpark implementations side-by-side, ensuring that:\n   a. All functionality from the Postgres code is present in the PySpark version\n   b. Business logic remains intact and produces the same results\n   c. Data processing steps are equivalent and maintain data integrity\n4. Verify that the PySpark code leverages appropriate Spark features and optimizations, such as:\n   a. Efficient use of DataFrame operations\n   b. Proper partitioning and caching strategies\n   c. Utilization of Spark SQL functions where applicable\n5. Test the PySpark code with sample data to confirm it produces the same output as the Postgres version.\n6. Identify any potential performance bottlenecks or areas for improvement in the PySpark implementation.\n7. Document your findings, including any discrepancies, suggestions for optimization, and overall assessment of the conversion quality.\n \nOUTPUT FORMAT:\nProvide a comprehensive code review report in the following structure:\n 1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For the input Postgres code use this file : ```%1$s```\n* Also take the previous Postgres to Pyspark conversion agents converted PySpark script as input",
                "expectedOutput": "1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n* Include the cost consumed by the API for this call in the output.\n"
            }
        }
    ]
}