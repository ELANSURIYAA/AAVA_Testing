{
    "workflowId": 1402,
    "workflowName": "ABAP_To_Pyspark_Convert",
    "nodes": [
        {
            "agentName": "ABAP_to_PySpark_Converter",
            "model": "o3-mini",
            "tools": [],
            "task": {
                "description": "The AI agent takes an input ABAP code and performs the following steps:\n\n1. Carefully analyze the provided ABAP code.\n2. Identify key components of the ABAP code such as data types, internal tables, loops, conditions, and function calls.\n3. Map ABAP logic to equivalent PySpark functions \u2013 Convert ABAP operations into their corresponding PySpark transformations using DataFrame methods like `filter()`, `select()`, `groupBy()`, `join()`, etc.\n4. Convert ABAP loops (such as `LOOP AT`, `LOOP INTO`, etc.) into PySpark DataFrame operations where applicable.\n5. Handle complex ABAP statements involving nested function calls or external operations (e.g., database calls) by mapping them to the corresponding PySpark functionality.\n6. Ensure that the PySpark code is optimized for performance, particularly considering the distributed nature of Spark.\n7. Convert ABAP data types (like `STRING`, `INT`, `CHAR`, etc.) into appropriate PySpark data types.\n8. Pay special attention to:\n   a. Internal tables and arrays\n   b. Loops and iterative structures\n   c. Conditional statements and branching logic\n   d. Function calls and modularization\n   e. String and date manipulation\n   f. ABAP specific constructs such as SELECT SINGLE, INSERT, UPDATE\n9. Ensure that the PySpark code retains the same logic and behavior as the original ABAP code.\n10. Add comments to explain the transformation logic and complex code sections.\n11. Ensure that the resulting PySpark code follows PEP 8 style guidelines and is formatted for readability.\n\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For input ABAP code use this file : ```%1$s```\n\nNote :\n1. Follow the expected output and return the output in the same format. \n2. Just convert the given ABAP code.\n3. Do not add anything other than what's mentioned in Expected output.",
                "expectedOutput": "* Converted PySpark Code.\n* Include the cost consumed by the API for this call in the output."
            }
        },
        {
            "agentName": "ABAP_to_PySpark_Unit_Testing",
            "model": "gpt-4o",
            "tools": [],
            "task": {
                "description": "You are tasked with creating a set of unit test cases and a Pytest script for the given PySpark code, which was originally in ABAP. Your expertise in both ABAP and PySpark testing methodologies will be essential in ensuring comprehensive test coverage.\n\nINSTRUCTIONS:\n1. Analyze the provided PySpark code (converted from ABAP) to identify key functionalities, data transformations, and potential edge cases.\n2. Create a list of test cases that cover:\n   a. Happy path scenarios (e.g., correct data transformation, expected output)\n   b. Edge cases (e.g., empty DataFrames, null values, boundary conditions)\n   c. Error handling and exception scenarios (e.g., incorrect input, invalid data types)\n3. Design test cases using PySpark-specific terminology and concepts (e.g., DataFrame operations, UDFs, window functions).\n4. Implement the test cases using Pytest and PySpark testing utilities.\n5. Ensure proper setup and teardown of SparkSession for each test.\n6. Use appropriate assertions to validate expected outcomes.\n7. Include comments explaining the purpose of each test case.\n8. Organize the test cases logically, grouping related tests together.\n9. Implement any necessary helper functions or fixtures to support the tests.\n10. Ensure the Pytest script follows PEP 8 style guidelines.\n\nOUTPUT FORMAT:\n1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* Use the previous ABAP to PySpark converter's converted PySpark script as input",
                "expectedOutput": "1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output."
            }
        },
        {
            "agentName": "ABAP_to_PySpark_Conversion_Tester",
            "model": "gpt-4o",
            "tools": [],
            "task": {
                "description": "You are tasked with processing ABAP programs, function modules, and reports alongside their converted PySpark equivalents and performing the following tasks:\n\n1. Syntax Change Detection: Compare ABAP and PySpark code to highlight differences, such as:\n-ABAP function/module conversions (e.g., SELECT ... INTO TABLE \u2192 DataFrame transformations)\n-Data type transformations (CHAR \u2192 StringType(), DECIMAL \u2192 DecimalType(), etc.)\n-Looping constructs (LOOP AT ... \u2192 forEach, .map(), etc.)\n-Error handling (MESSAGE handling)\n-Internal table operations (APPEND, COLLECT, READ TABLE, etc.)\n-Date and time function changes\n-Handling of NULL values and case sensitivity adjustments\n2.Recommended Manual Interventions: Identify areas requiring manual fixes, such as:\n-Performance optimizations (broadcast joins, repartitioning)\n-Handling legacy constructs (AT SELECTION-SCREEN, SY-SUBRC)\n-Complex expressions requiring PySpark UDFs\n-Edge cases due to implicit ABAP behaviors\n3.Create a comprehensive list of test cases covering:\n    a.Syntax changes\n    b.Manual interventions\n4.Develop a Pytest script for each test case:\n* Include the cost consumed by the API for this call in the output.\n*Validate data transformations\n*Ensure logical correctness of the conversion\n\nOutput:\n1. Test Case List:\n   - Test case ID\n   - Test case description\n   - Expected outcome\n2. Pytest Script for each test case\n* Include the cost consumed by the API for this call in the output.\n\nInput:\n*ABAP Source Code: Use the provided file:  ```%2$s```\n*Converted PySpark Output: Use the output from the previous ABAP to PySpark Converter agent",
                "expectedOutput": "1.Test Case List:\n    -Test Case ID: Unique identifier for the test case\n    -Test Case Description: Briefly describes what is being tested\n    -Expected Outcome: Defines the expected result if the conversion is correct\n\n2.Pytest Script for Each Test Case:\nA well-structured pytest script that:\nReads ABAP test inputs and their converted PySpark outputs\nValidates syntax transformations\nEnsures correctness of converted logic\nVerifies data consistency between ABAP and PySpark versions\nCost Calculation: Include the cost consumed by the API for the analysis"
            }
        },
        {
            "agentName": "ABAP_To_PySpark_Recon_Tester",
            "model": "gpt-4o",
            "tools": [],
            "task": {
                "description": "You are an expert Data Migration Validation Agent specializing in ABAP to PySpark migrations. Your task is to create a comprehensive Python script that automates the validation process by:\n      -Executing ABAP code\n      -Transferring the results to a distributed storage system\n      -Running equivalent PySpark code\n      -Comparing and validating outputs\n\nFollow these steps to generate the Python script:\n\n1. ANALYZE INPUTS:\n   -Parse the ABAP code input to understand its structure and expected output tables\n   -Parse the previously converted PySpark code to understand its structure and expected output tables\n   -Identify the target tables in both ABAP and PySpark code, focusing on those involved in INSERT, UPDATE, DELETE operations\n\n2. CREATE CONNECTION COMPONENTS:\n  -Establish a connection to the ABAP system (via SAP RFC SDK or pyRFC)\n  -Set up connections for distributed storage (HDFS, S3, or a Data Lake)\n  -Configure a PySpark environment (via Databricks or local Spark setup)\n  -Use secure authentication methods for all connections\n\n3. IMPLEMENT ABAP EXECUTION:\n  -Connect to SAP ABAP system using provided credentials\n  -Execute the provided ABAP SQL-based code\n  -Retrieve the result dataset\n\n4. IMPLEMENT DATA EXPORT & TRANSFORMATION:\n  -Export ABAP output tables to CSV or JSON\n  -Convert files to Parquet format using pandas or pyarrow\n  -Use meaningful naming conventions for files (table_name_timestamp.parquet)\n\n5. IMPLEMENT DISTRIBUTED STORAGE TRANSFER:\n  -Authenticate with HDFS/S3/Data Lake\n  -Transfer Parquet files to the designated storage location\n  -Implement integrity checks to verify successful file transfers\n\n6. IMPLEMENT PYSPARK EXTERNAL TABLES:\n  -Create external tables in PySpark pointing to the uploaded Parquet files\n  -Maintain schema consistency with ABAP tables\n  -Handle data type conversions appropriately\n\n7. IMPLEMENT PYSPARK EXECUTION:\n  -Connect to PySpark using provided configurations\n  -Execute the provided converted PySpark code\n  -Store the resulting output for comparison\n\n8. IMPLEMENT COMPARISON LOGIC:\n  -Compare each pair of corresponding tables (ABAP vs PySpark output)\n  -Implement row count validation\n  -Implement column-wise data validation\n  -Handle data type mismatches and null value scenarios\n  -Compute match percentage for each table\n\n9. IMPLEMENT REPORTING:\n  -Generate a detailed reconciliation report for each table, including:\n  -Match Status (MATCH, NO MATCH, PARTIAL MATCH)\n  -Row count differences (if any)\n  -Column-level discrepancies\n  -Samples of mismatched records for investigation\n  -Summarize results in an overall validation report\n\n10. INCLUDE ERROR HANDLING:\n  -Implement robust error handling for each step\n  -Provide clear, actionable error messages\n  -Ensure the script can recover from failures\n  -Log all operations for auditability and troubleshooting\n\n11. ENSURE SECURITY:\n  -Never hardcode credentials\n  -Follow best practices for secure authentication\n  -Use encrypted connections wherever applicable\n\n12. OPTIMIZE PERFORMANCE:\n  -Use efficient data transfer mechanisms for large datasets\n  -Implement batch processing for scalability\n  -Include progress tracking and reporting for long-running operations\n\nINPUTS:\n*ABAP SQL Code is taken from this file: ```%1$s```\n*Converted PySpark Code from ABAP to PySpark Converter Agent",
                "expectedOutput": "A fully functional, executable Python script that:\n1.Accepts ABAP SQL Code and converted PySpark Code as inputs\n2.Automates all migration and validation steps\n3.Produces a clear reconciliation report for each table\n4.Follows best practices for performance, security, and error handling\n5.Includes detailed comments explaining each section\n6.Can run automatically in scheduled environments\n7.Returns structured results that can be easily parsed by external systems\n\nThe script must handle all edge cases, including:\n  -Different data types\n  -Null values\n  -Large datasets\n  -Schema mismatches\n\nIt should also provide real-time execution logs and a detailed comparison report to ensure transparency and reliability in the ABAP-to-PySpark migration validation process.\n\n* API Cost for this particular api call for the model in USD"
            }
        },
        {
            "agentName": "ABAP_To_PySpark_Reviewer",
            "model": "gpt-4o",
            "tools": [],
            "task": {
                "description": "Your task is to carefully analyze and compare the original ABAP code with the newly converted PySpark implementation. The review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the PySpark environment. You will also examine the structure, logic, and data flow of the ABAP code and ensure that the PySpark code implements the same logic while optimizing for performance.\n\nINSTRUCTIONS:\n\n1. Carefully read and understand the original ABAP code, noting its structure, logic, and data flow.\n2. Examine the converted PySpark code, paying close attention to: a. Data types and structures b. Control flow and logic c. Data transformations and processing steps d. Error handling and exception management\n3. Compare the ABAP and PySpark implementations side-by-side, ensuring that: a. All functionality from the ABAP code is preserved in the PySpark version b. Business logic is maintained, and results are consistent c. Data processing steps are equivalent and maintain data integrity\n4.Verify that the PySpark code efficiently uses Spark features and optimizations, such as: a. DataFrame operations b. Proper partitioning and caching strategies c. Utilization of Spark SQL functions where applicable\n5.Test the PySpark code with sample data to confirm it produces the same output as the ABAP version.\n6.Identify potential performance bottlenecks or areas for optimization in the PySpark implementation.\n7.Document your findings, including any discrepancies, suggestions for optimization, and the overall quality of the conversion.\n\nOUTPUT FORMAT:\nProvide a comprehensive code review report in the following structure:\n1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n*Include the cost consumed by the API for this call in the output.\n\nINPUT:\n\nFor the input ABAP code use this file:  ```%1$s```\n*Also take the previously converted PySpark script from the ABAP to PySpark conversion as input.",
                "expectedOutput": "1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n*Include the cost consumed by the API for this call in the output."
            }
        }
    ]
}