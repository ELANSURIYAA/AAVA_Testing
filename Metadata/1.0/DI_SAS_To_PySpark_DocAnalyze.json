{
    "workflowId": 587,
    "workflowName": "DI_SAS_To_PySpark_Doc&Analyze",
    "nodes": [
        {
            "agentName": "DI_SAS_Documentation",
            "model": "gemini-2.5-pro",
            "tools": [],
            "task": {
                "description": "First you need to extract the uploaded ZIP file and fetch the {{SAS_Code}} SAS file from the extracted contents as a input.\n\nThe documentation should include the following sections:\n- **Format:** Markdown\n\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n2. Overview of Program:  \n   Explain the purpose of the SAS code in detail.  \n   Explain the business problem being addressed .  \n\n3. Code Structure and Design:  \n   Explain the structure and flow of the SAS code in detail.  \n   List the primary SAS components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.  \n\n4. Data Flow and Processing Logic:  \n   - List the processing logic data flow components in the code.  \n   - For each logical processing component\n           -Explain the functionality performed in the code.  \n          - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.  \n\n5. Data Mapping:  \n   - Provide detailed mappings between source and target tables as a lineage in terms how the source table.columns are mapped \n     to the target table.columns\n   - Use the following structured format for each mapping:  \nTarget Table Name : Give actual target table name\n Target Column Name : Give actual target column name\n Source Table Name : Give actual source table name\n Source Column Name : Give actual source column name\n Remarks : Classify as 1 to 1 mapping or Transformation or Validation and provide a brief description\n\n6. Key Outputs:  \n    Describe final outputs created by the code like Inserts, Updates, Deletes.\n - Explain how outputs align with business goals and reporting needs.\n    \n7. API Cost Calculation \nAPI cost with the proper calculation in USD (expample: 0.00 $)\nDont give meta informtion as heading like this\n# 1. Metadata Requirements\n\nThis document provides detailed technical documentation for the SAS code in `000_macros.sas`, outlining its purpose, structure, data flow, mappings, complexity, and outputs as used in healthcare reporting automation.\nFollow the format only",
                "expectedOutput": "- **Format:** Markdown\n1. Metadata Requirements\": \"<Updated metadata requirements> \",\n2. Overview of Program\": \"<5-line paragraph + Explain the purpose of the SAS code in detail and Explain the business problem being addressed >\",\n3. Code Structure and Design\": \"<5-line intro + detailed description of structure, modules, and flow>\",\n  4. Data Flow and Processing Logic\": \n    \"Processed Datasets\": [\"<list all dataset names used>\"],\n    \"Data Flow\": \"<5-line intro + flow description: raw to transformed data>\"\n  5. Data Mapping\": \n| Target Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks             |\n|-------------------|--------------------|--------------------|---------------------|----------------------|\n| <dataset>         | <field>            | <dataset>          | <field>             | <description>        |\n| (Repeat as needed)|                    |                    |                     |                      |\n\n    \"6. Complexity Analysis\": \n| Metric                   | Value                                |\n|--------------------------|--------------------------------------|\n| Number of Lines          | <integer>                            |\n| Datasets Used            | <integer>                            |\n| Joins Used               | <list JOIN types or 'None'>          |\n| TRANSFORM Functions      | <count or 'None'>                    |\n| RECORD Definitions       | <count or 'None'>                    |\n| OUTPUT Statements        | <integer>                            |\n| Conditional Logic        | <count>                              |\n| Indexing and Lookups     | <count or 'None'>                    |\n| Function Calls           | <count or 'None'>                    |\n| Performance Controls     | <description>                        |\n| External Dependencies    | <libraries, systems used>            |\n| Overall Complexity Score | <0\u2013100 integer>                      |\n\n   \"7. Key Outputs\": \n    \"<short description of each key output>\"\n  \"8. API Cost\": \"<API cost with the proper calculation> \"\n"
            }
        },
        {
            "agentName": "DI_SAS_To_PySpark_Analyzer",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "Parse the provided SAS code to generate a detailed analysis and metrics report. Ensure that if multiple files given as input then do analysis for each file is presented as a distinct session.\n \nINSTRUCTIONS:\n\nOutput must include these required fields:\n\n1. Metadata Requirements:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the generated analysis report>\n=============================================\n```\n- For the description, provide a concise summary of what the analysis does.\n-give this only once in top of the output\n2. Script Overview:\nProvide a concise summary of the SAS program\u2019s purpose and the business logic it implements. Highlight the key functional sections such as DATA steps, PROC steps (e.g., PROC SQL, PROC SORT, PROC TRANSPOSE), conditional processing, macro logic, error handling mechanisms, and temporary dataset usage.\n\nBriefly describe the nature of the data workflow or operation (e.g., ETL logic, report generation, data sync).\n\n3. Complexity Metrics:\n| Metric                | Count / Type                                                                 |\n|-----------------------|------------------------------------------------------------------------------|\n| Number of Lines       | Total lines in the SAS code.                                        |\n| Tables Used           | Total number of referenced tables in the SAS code.                                           |\n| Joins                 | Count and types of joins (e.g., INNER, LEFT, CROSS).                         |\n| Temporary Tables      | Count of Volatile and derived tables.                                |\n| Aggregate Functions   | Count of aggregate functions used.                                      |\n| DML Statements        | Count by type: SELECT, INSERT, UPDATE, DELETE,CALL, LOCK , Export, Import operations present in the SAS script.         |\n| Conditional Logic     | Count of conditionals: IF-THEN-ELSE, CASE, DECODE, etc.                      |\n* Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.\n* Highlight high-complexity areas such as SAS clauses.\n\n\n\n- Use of indexing, efficient sorting, and dataset partitioning.\n- Refactoring code using modular macros and efficient PROC steps.\n- Handling semi-structured data using SAS procedures (e.g., PROC JSON, XML Mapper).\n- Recommendations on whether to **Refactor** with minimal changes or **Rebuild** with better design. Justify the choice based on performance, maintainability, and compatibility.\n\n\n4. Syntax Differences:\n* Identify the number of syntax differences between the SAS code and the expected Pyspark equivalent.\n\n5. Manual Adjustments:\n* Recommend specific manual adjustments for functions and clauses incompatible with Pyspark, including:\n    * Function replacements.\n    * Syntax adjustments.\n    * Strategies for rewriting unsupported features.\n\n6. Optimization Techniques:\n* Suggest optimization strategies for Pyspark, such as clustering, partitioning, and query design improvements.\n\n7. Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD\n \n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost in Dollars as float ).\n*Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n(expample: 0.00$)\n\nDont give meta informtion as heading like this\n# 1. Metadata Requirements\n\nThis document provides detailed technical documentation for the SAS code in `000_macros.sas`, outlining its purpose, structure, data flow, mappings, complexity, and outputs as used in healthcare reporting automation.\nFollow the format only\n\nFollow the indexing convention\nInput :\n\n* For SAS code use the below file : {{SAS_File}}\n\nNOTE:\nDo not add anything other than what's given in expected output.",
                "expectedOutput": "All sections from Metadata Requirements to apiCost must be presented for each input file in a **distinct session**.\n1. Metadata Requirements:\n2. Script Overview:\n3. Complexity Metrics:\n4. Syntax & Feature Compatibility Check:\n5. Manual Adjustments for Snowflake Migration:\n6.Optimization Techniques:\n7. apiCost: \n"
            }
        },
        {
            "agentName": "DI_SAS_To_PySpark_Plan",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "you are tasked with providing a comprehensive estimate for running PySpark and the testing effort required when converting SAS scripts to PySpark. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n\n1. Metadata Requirements:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate: (Leave it empty)\nDescription:   <one-line description of the generated plan>\n=============================================\n```\n- For the description, provide a concise summary of what the plan does.You are tasked with providing a comprehensive effort estimate for testing the SAS code\n\nINSTRUCTIONS:\n1. Review the analysis of SAS script file, note syntax differences and areas in the code requiring manual intervention when converting to PySpark\n2. Estimate the effort hours requried for identified manual code fixes and data recon testing effort\n3. Dont consider efforts for syntax differences as they will be converted to equivalent syntax in PySpark\n4. Consider the pricing information for Azure Databricks PySpark environment \n5. Calculate the estimated cost of running the converted PySpark code:\n   a. Use the pricing information and data volume to determine the query cost.\n   b. the number of queries and the data processing done with the base tables and temporary tables\n\n\nOUTPUT FORMAT:\n1. Metadata reequirements\n1.1 Cost Estimation\n   1.1.1 PySpark Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing  and Testing Effort Estimation\n   2.1 PySpark identified manual code fixes and unit testing effort in hours covering the various temp tables, calculations \n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n* replicate the Manual adjustments from the Analyzer output\nDont give meta informtion as heading like this\n# 1. Metadata Requirements\n\nThis document provides detailed technical documentation for the SAS code in `000_macros.sas`, outlining its purpose, structure, data flow, mappings, complexity, and outputs as used in healthcare reporting automation.\nFollow the format only\nInput: \nInput files are the output from the Agent SAS CODE ANALYZER , the  SAS script fiile use this File : {{SAS_File}}\nthe PySpark Environment  Details of the Azure Databricks use this file: {{Env_File}}",
                "expectedOutput": "OUTPUT FORMAT:\n1. Metadata Requirements:\n1. Metadata reequirements\n1.1 Cost Estimation\n   1.1.1 PySpark Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing  and Testing Effort Estimation\n   2.1 PySpark identified manual code fixes and unit testing effort in hours covering the various temp tables, calculations \n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n* replicate the Manual adjustments from the Analyzer output\n\nEnsure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost USD)."
            }
        }
    ]
}