{
    "workflowId": 5336,
    "workflowName": "DI_AgentMapper",
    "nodes": [
        {
            "agentName": "DI_Role_Agent Mapper",
            "model": "gpt-4",
            "tools": [
                "DI_Github_File_Writer_Z"
            ],
            "task": {
                "description": "The task involves analyzing two datasets and creating a logical mapping between agents, roles, and deliverables. The mapping should be based on the agents' goals and functionalities, as well as the requirements of the roles and deliverables. The output should include a confidence score and a reason for the mapping to ensure transparency and traceability. You are provided with:\n\n- **Dataset 1 (Large)**: Contains `AgentName` and `AgentGoal`. Must be split into chunks (max 150 rows per chunk).\n- **Dataset 2 (Small)**: Contains `Phases`, `Deliverables`, and `Roles`. Reuse this as a reference with **every chunk**.\n  \nYour task is to:\n- Map each agent from each chunk to relevant phases, deliverables, and roles.\n- Use the agent's `AgentGoal` to identify and justify mappings.\n- Assign a `Map Confidence Score` between 0 and 100.\n- Save the result of **each chunk** as a **separate CSV file** into a GitHub repository using the **GitHubWriterTool**.\n\n### INSTRUCTIONS:\n\n#### Step 1: Chunk Dataset 1\n- Split Dataset 1 into batches of 150 records per chunk.\n- Chunking example:\n  - Chunk 1: Rows 0\u2013149 \u2192 `logical_mapping_chunk_1.csv`\n  - Chunk 2: Rows 150\u2013299 \u2192 `logical_mapping_chunk_2.csv`\n  - and so on\u2026\n\n#### Step 2: Reuse Dataset 2\n- Use the full Dataset 2 with every chunk of Dataset 1.\n- Dataset 2 defines the valid `Phases`, `Deliverables`, and `Roles`.\n\n#### Step 3: Perform Mapping for Each Chunk\n- For each chunk:\n  - Analyze each `AgentGoal`\n  - Map it to appropriate `Phases`, `Deliverables`, and `Roles`\n  - Score the alignment using `Map Confidence Score` (0\u2013100)\n- Output Format per chunk:\n  ```csv\n  AgentName,Phases,Deliverables,Roles,Map Confidence Score\n\nStep 4: Save Output to GitHub\n* For each chunk, create a separate CSV file using the GitHubWriterTool.\n* File name format: logical_mapping_chunk_<chunk_number>.csv\n* Commit each file to the given GitHub repo using the provided repo URL and token.\n\n5. **Context and Background Information:**  \n   - Agents represent AI or human resources with specific goals and functionalities.  \n   - Roles represent the responsibilities required to produce specific deliverables.  \n   - Deliverables are the outputs or artifacts expected at various project phases.  \n   - The mapping should ensure that agents are aligned with roles and deliverables that match their expertise and goals.  \n\n6. **Scope and Constraints:**  \n   - Ensure that every agent is mapped to at least one role and deliverable, where applicable.  \n   - Avoid redundant mappings unless justified.  \n   - Confidence scores should be between 0 and 100, where 100 represents a perfect match.  \n   - Provide clear reasoning for each mapping to justify the confidence score.  \n\n7. **Process Steps to Follow:**  \n   - Parse the datasets to extract relevant information.  \n   - For each agent, analyze their `AgentGoal` and identify roles and deliverables that align with their functionality.  \n   - Use domain-specific reasoning to assess the strength of the association between the agent\u2019s goal and the role/deliverable.  \n   - Assign a confidence score based on the strength of the alignment.  \n   - Document the mapping in the specified output format.  \n\n8. OUTPUT FORMAT:\nEach file should follow this CSV structure:\nAgentName,Phases,Deliverables,Roles,Map Confidence Score\n\n### Quality Criteria:  \n- **Accuracy:** Ensure mappings are logical and based on the provided datasets.  \n- **Clarity:** The reasoning for each mapping should be concise and clear.  \n- **Consistency:** Follow the specified output format and maintain uniformity in confidence scoring.  \n- **Domain-Specific Reasoning:** Use knowledge of data and insights projects to justify mappings.  \n- **Completeness:** Every chunk must be processed and saved as a separate CSV in GitHub.  \n- **No Overwriting:** Each output file must have a unique name (`chunk_1`, `chunk_2`, etc.)\n- **Mapping Logic:** Use domain-relevant reasoning to associate goals with correct roles \n- **Scoring:** Confidence scores must reflect alignment strength logically        \n- **Reusability:** Dataset 2 must be reused in all chunks     \n\nNOTES:\n\u2705 Chunk Dataset 1\n\u2705 Reuse Dataset 2\n\u2705 Separate file for each chunk\n\u2705 Write using GitHubWriterTool only\n\u2705 No overwrite or loss of processed output                           \n\nInput:\n* Dataset 1 :  Agents created under Data & Insights: {{agent_details}} \n* Dataset 2 :  Phases, deliverables, and roles in a typical Data & Insights Engineering project : {{phase_deliverable_role_details}}\n* GitHub Repository details : {{github_repo}}\n",
                "expectedOutput": "Multiple CSV files in GitHub, such as:\nlogical_mapping_chunk_1.csv\nlogical_mapping_chunk_2.csv\nlogical_mapping_chunk_3.csv\nEach stored in the provided GitHub repo, committed using GitHubWriterTool."
            }
        }
    ]
}