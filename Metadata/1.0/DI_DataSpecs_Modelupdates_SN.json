{
    "workflowId": 1594,
    "workflowName": "DI_DataSpecs_Modelupdates_SN",
    "nodes": [
        {
            "agentName": "DI_Data_Technical_Specification_SN",
            "model": "gpt-4",
            "tools": [
                "DI_Github_File_Writer_Z"
            ],
            "task": {
                "description": "The agent must create a comprehensive technical specification document based on the provided inputs:  \n- JIRA story file  \n- Confluence context file  \n- DDL file for the new source table  \n- Existing source data model  \n- Existing target data model  \n\nThe specification should include:  \n1. **Code Changes Required for the Enhancement:**  \n   - Identify the specific areas in the codebase that need modification.  \n   - Detail the logic and functionality changes required to incorporate the new source table.  \n   - Include pseudocode or code snippets where applicable.  \n\n2. **Updates to the Data Models:**  \n   - Analyze the existing source and target data models.  \n   - Define the updates required to integrate the new source table into the models.  \n   - Ensure consistency and alignment between the source and target models.  \n\n3. **Source-to-Target Mapping:**  \n   - Provide a detailed mapping of fields from the new source table to the target data model.  \n   - Include any transformation rules or business logic required for the mapping.  \n\n**INSTRUCTIONS:**  \n1. **Context and Background Information:**  \n   - Review the JIRA story file to understand the business requirements and objectives.  \n   - Refer to the Confluence context file for additional project details and constraints.  \n   - Analyze the DDL file to understand the structure and schema of the new source table.  \n   - Examine the existing source and target data models to identify dependencies and relationships.  \n\n2. **Scope and Constraints:**  \n   - Ensure the specification aligns with the business requirements outlined in the JIRA story.  \n   - Maintain compatibility with existing systems and processes.  \n   - Adhere to data governance and security standards.  \n\n3. **Process Steps to Follow:**  \n   - Step 1: Extract relevant details from the provided files.  \n   - Step 2: Identify code changes required for the enhancement, including impacted modules and functions.  \n   - Step 3: Define updates to the source and target data models, ensuring logical consistency.  \n   - Step 4: Create a detailed source-to-target mapping, including transformation rules.  \n   - Step 5: Format the technical specification document as per industry standards.  \n4. **OUTPUT FORMAT:**  \n   - **Format:** Markdown  \n   - **Structure Requirements:**  \n- **Metadata Requirements:**\n-=============================================\n-Author: Ascendion AVA+\n-Date: <Leave it blank>\n-Description: <one-line description of the purpose >\n-============================================= \n     - Title: Technical Specification for [Enhancement Name]  \n     - Sections:  \n       - Introduction  \n       - Code Changes  \n       - Data Model Updates  \n       - Source-to-Target Mapping  \n       - Assumptions and Constraints  \n       - References  \n     - Use headings, subheadings, and bullet points for clarity.  \n   - **Quality Criteria:**  \n     - Clear and concise language.  \n     - Logical flow and organization.  \n     - Accurate and complete mapping and transformation rules.  \n   - **Formatting Needs:**  \n     - Use tables for source-to-target mapping.  \n     - Include diagrams for data model updates (if applicable).  \nPoints to Remember:\nRemember Must use the Github File Writer Tool to upload the File in the Github Repo for the Environment Details for github take that from the input\nRemember for the branch input use it as \"main\" and conent is what you give as output save the file as .md format\n\n\nAll the Inputs (Jira Stories, Confluence Documentation, Source Data Model, Target Data Model) are available in the zip folder that is uploaded. *****The input file names are provided in {{Technical_Specifications}}\n* GitHub repo details : {{GitHub_Repo_Details}}\n",
                "expectedOutput": "**OUTPUT FORMAT:**  \n   - **Format:** Markdown  \n   - **Structure Requirements:**\n- **Metadata Requirements:**\n-=============================================\n-Author: Ascendion AVA+\n-Date: <Leave it blank>\n-Description: <one-line description of the purpose >\n-=============================================  \n     - Title: Technical Specification for [Enhancement Name]  \n     - Sections:  \n       - Introduction  \n       - Code Changes  \n       - Data Model Updates  \n       - Source-to-Target Mapping  \n       - Assumptions and Constraints  \n       - References  \n     - Use headings, subheadings, and bullet points for clarity.  \n   - **Quality Criteria:**  \n     - Clear and concise language.  \n     - Logical flow and organization.  \n     - Accurate and complete mapping and transformation rules.  \n   - **Formatting Needs:**  \n     - Use tables for source-to-target mapping.  \n     - Include diagrams for data model updates (if applicable).  \n\n```||||||||Cost Estimation and Justification\n \nCost Section Instructions:\n- Calculate the total **number of input tokens** used (including this prompt + the input SQL).\n- Calculate the total **number of output tokens** used (including the converted SQL + explanation).\n- Automatically detect the model used for processing this prompt.\n- Retrieve the current pricing for that model (for both input and output tokens) from the system or environment, if available.\n-  compute the cost of running this agent:\n- Input Cost = `input_tokens * [input_cost_per_token]`\n- Output Cost = `output_tokens * [output_cost_per_token]`\n- Present the full formula and breakdown clearly:"
            }
        },
        {
            "agentName": "DI_Delta_Model_Changes_SN",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "DMEA accepts two primary inputs:\n**The existing data model (ER diagrams, DDLs, JSON schemas, etc.)\n**The new technical specifications (manually entered or taken as input from upstream agents like TSA - Technical Specification Agent)\nIt performs the following stages:\n1. **Model Ingestion\n**Parse and map existing model: tables, fields, constraints, relationships\n**Build internal representations for current schema using graph/tree formats\n**Supports relational (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB), and modern lakehouse (Delta Lake, Snowflake, BigQuery) models\n2. **Spec Parsing & Mapping\n    Normalize inputs from tech specs to structural requirements\n    Detect:\n         **Additions (new tables/columns/indexes)\n         **Modifications (type changes, nullable, constraints)\n         **Deprecations (dropping columns, soft deletes)\n    Infer indirect changes (e.g., changed business rule implies a column constraint update)\n3. **Delta Computation\n    Compare existing vs desired model\n    Categorize deltas:\n        **New tables/fields\n        **Changed column types/nullability\n        **Added/removed constraints\n        **Modified indexes/PKs\n    Compute version bump impact (patch/minor/major)\n4. **Impact Assessment\n    --Downstream break detection (views, ETL jobs, APIs)\n    --Data loss risk (e.g., narrowing column types, dropping constraints)\n    --Foreign key ripple effects\n    --Platform-specific caveats (e.g., PostgreSQL vs MySQL)\n5. **DDL/Alter Statement Generation\n    --Forward DDLs:\n        CREATE TABLE, ALTER TABLE, ADD CONSTRAINT, DROP COLUMN\n    --Rollback support\n    --Optional zero-downtime deployment (via COPY, rename strategies)\n    --Index rebalancing if necessary\n    --Optional data migration scripts (e.g., populate new tables from old ones)\n6. **Documentation\n    --Side-by-side diff of model (before vs after)\n    --Full DDL logs with change reasons\n    --Visual diagrams (ERD updates)\n    --Change traceability matrix (tech spec section \u2192 DDL line)\n\nThe inputs for this agent which is technical specification and existing DDL's :\n--existing DDL's file names are mentioned in {{Data_Model_Delta}}\n--Take the technical specification requirement from the first agent named ------\"\"\"\"DI_Data_Technical_Specification_SN\"\"\"\" ",
                "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor: Ascendion AVA+\nDate: <Leave it blank>\nDescription: <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what this output entails/captures\n\nTHEN REST AS DESCRIBED BELOW -->\n\nEach run of DMEA returns a \"Data Model Evolution Package\" or \"Data model delta update\", containing:\n1. Delta Summary Report\n    Overview of changes with impact level (low/medium/high)\n    List of:\n        **Additions\n        **Modifications\n        **Deprecations\n    Notes on detected risk (data loss, key impact)\n2. DDL Change Scripts\n    **Forward-only SQL (to evolve model)\n    **Annotated with comments, change reason, tech spec reference\n3. Data Model Documentation\n    **Annotated dictionary (columns with change metadata)\n\n|||||||||Cost Estimation and Justification\n \nCost Section Instructions:\n- Calculate the total **number of input tokens** used (including this prompt + the input SQL).\n- Calculate the total **number of output tokens** used (including the converted SQL + explanation).\n- Automatically detect the model used for processing this prompt.\n- Retrieve the current pricing for that model (for both input and output tokens) from the system or environment, if available.\n- compute cost of running this agent:\n- Input Cost = `input_tokens * [input_cost_per_token]`\n- Output Cost = `output_tokens * [output_cost_per_token]`\n- Present the full formula and breakdown clearly:\n"
            }
        },
        {
            "agentName": "DI_Functional_Test_Cases_SN",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "Your task is to create detailed and specific functional test cases based on the technical specifications provided in Jira. These test cases will serve as the foundation for validating the software's functionality and ensuring it meets the requirements. Follow the instructions below to ensure the test cases are comprehensive, well-structured, and adhere to industry standards.  \n\n### **INSTRUCTIONS:**  \n1. **Understand the Context:**  \n   - Review the Jira tickets provided, including technical specifications, user stories, acceptance criteria, and any attached documentation.  \n   - Identify the key functionalities, requirements, and edge cases described in the tickets.  \n\n2. **Scope and Constraints:**  \n   - Focus only on functional requirements (e.g., input validation, expected outputs, system behavior).  \n   - Exclude non-functional requirements like performance, scalability, or security unless explicitly mentioned.  \n   - Ensure test cases cover both positive and negative scenarios, as well as edge cases.  \n\n3. **Process Steps:**  \n   - **Step 1:** Extract requirements and acceptance criteria from Jira.  \n   - **Step 2:** Break down each requirement into smaller, testable components.  \n   - **Step 3:** Identify edge cases and boundary conditions for each requirement.  \n   - **Step 4:** Write test cases using a structured format (see OUTPUT FORMAT below).  \n   - **Step 5:** Ensure traceability by linking each test case to its corresponding Jira ticket.  \n   - **Step 6:** Review and validate test cases for completeness and accuracy.  \n\n4. **Output Format:**  \n   - Provide test cases in **Markdown** format.  \n   - Use the following structure for each test case:  \n\n     ```markdown\n     ### Test Case ID: TC_<JiraTicketID>_<SequentialNumber>\n     **Title:** [Brief title of the test case]  \n     **Description:** [Detailed description of the test case objective]  \n     **Preconditions:** [Any setup or prerequisites required before executing the test case]  \n     **Steps to Execute:**  \n     1. [Step 1]  \n     2. [Step 2]  \n     3. [Step N]  \n     **Expected Result:** [What the system should do after executing the steps]  \n     **Linked Jira Ticket:** [Jira ticket ID]  \n     ```\n\n\n### **SAMPLE:**  \n```markdown\n### Test Case ID: TC_JIRA1234_01  \n**Title:** Validate user login with valid credentials  \n**Description:** Ensure that a user can successfully log in using valid credentials.  \n**Preconditions:**  \n- The application is running.  \n- A user account with valid credentials exists.  \n\n**Steps to Execute:**  \n1. Navigate to the login page.  \n2. Enter valid username and password.  \n3. Click the \"Login\" button.  \n\n**Expected Result:**  \n- The user is redirected to the dashboard.  \n- A welcome message is displayed.  \n\n**Linked Jira Ticket:** JIRA1234  \n```\n\n\n---\nInput files and its names are available in \"{{Function_test_cases_input}}\"\n",
                "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor: Ascendion AVA+\nDate: <Leave it blank>\nDescription: <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what this output entails/captures\n### **OUTPUT FORMAT:**  \n- Provide test cases in **Markdown** format.  \n- Ensure each test case includes all required fields (Title, Description, Preconditions, Steps to Execute, Expected Result, Linked Jira Ticket).  \n- Maintain traceability by linking test cases to Jira tickets. \n||||||Cost Estimation and Justification\n \nCost Section Instructions:\n- Calculate the total **number of input tokens** used (including this prompt + the input SQL).\n- Calculate the total **number of output tokens** used (including the converted SQL + explanation).\n- Automatically detect the model used for processing this prompt.\n- Retrieve the current pricing for that model (for both input and output tokens) from the system or environment, if available.\n-  compute the cost of running this Agent:\n- Input Cost = `input_tokens * [input_cost_per_token]`\n- Output Cost = `output_tokens * [output_cost_per_token]`\n- Present the full formula and breakdown clearly:"
            }
        }
    ]
}