{
    "pipeline": {
        "pipelineId": 1082,
        "name": "Hive_to_Delta_Convert",
        "description": "Hive_to_Delta_Convert",
        "createdAt": "2025-03-14T09:41:27.354+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1270,
                    "name": "Hive to Delta Converter",
                    "role": "Data Engineer",
                    "goal": "Convert the provided Hive script into equivalent Delta, ensuring functional parity and performance optimization. Generate a separate output session for each input file.",
                    "backstory": "The Hive script needs to be translated into Delta for migration. The conversion should focus on maintaining logic, adapting incompatible features, and optimizing for the Delta environment.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-11T09:55:36.060047",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Please convert the following Hive query to  Delta  and provide an overview of the conversion. Ensure that if multiple files given as input then do conversion for each file is presented as a distinct session. Ensure that the Delta query is formatted with proper indentation and line breaks.\n\nINSTRUCTIONS:\n\n1. **Function and Syntax Conversion:**\n   - Replace Hive-specific functions (e.g., `NVL`, `REGEXP_REPLACE`) with their Delta equivalents.\n   - Convert complex type functions like `EXPLODE()`, `LATERAL VIEW`, and array/map manipulations.\n   - Adapt window functions and analytic calculations to Delta syntax.\n   - Handle user-defined functions (UDFs) by reimplementing or converting to native functions.\n   - Transform Hive-specific date and timestamp functions:\n     - `from_unixtime()` \u2192 `TIMESTAMP(SECONDS => value)`\n     - `unix_timestamp()` \u2192 `UNIX_SECONDS()`\n     - `trunc()` \u2192 `DATE_TRUNC()`\n\n2. **Join and Table Operations:**\n   - Maintain all join types (INNER JOIN, LEFT JOIN, etc.)\n   - Convert Hive-specific join hints and optimization techniques\n   - Handle complex joins involving lateral views and array expansions\n   - Replace `DISTRIBUTE BY` and `SORT BY` with appropriate Delta sorting mechanisms\n   - Adapt to Delta join optimization:\n     - Remove broadcast hints\n     - Simplify complex join conditions\n     - Optimize cross joins and cartesian products\n\n3. **Complex Type and Data Structure Handling:**\n   - Convert Hive complex types:\n     - `ARRAY<type>` transformations\n     - `MAP<keytype, valuetype>` conversions\n     - `STRUCT<field1:type1, field2:type2>` adaptations\n   - Replace `LATERAL VIEW EXPLODE()` with `UNNEST()`\n   - Ensure proper handling of nested and repeated fields\n   - Transform complex type constructors:\n     - `MAP()` function \u2192 Direct STRUCT or MAP creation\n     - Array generation functions \u2192 `GENERATE_ARRAY()` or explicit array construction\n     - Nested type access and manipulation\n\n4. **Variable and Parameter Management:**\n   - Convert Hive variables from `${hivevar:variable_name}` format\n   - Use Delta's `DECLARE` statement for variable initialization\n   - Explicitly handle variable scoping and default values\n   - Replace dynamic variable references with literal or declared values\n   - Handle configuration and session-level variables:\n     - `set hive.variable=value` \u2192 Delta configuration settings or query parameters\n\n5. **Data Type Compatibility:**\n   - Map Hive data types to Delta equivalents:\n     - `STRING` \u2192 `STRING`\n     - `INT` \u2192 `INT64`\n     - `BIGINT` \u2192 `INT64`\n     - `DOUBLE` \u2192 `FLOAT64`\n     - `DECIMAL(p,s)` \u2192 `NUMERIC(p,s)`\n     - `TIMESTAMP` \u2192 `TIMESTAMP`\n     - `CHAR(n)` \u2192 `STRING`\n     - `VARCHAR(n)` \u2192 `STRING`\n   - Ensure explicit type casting where implicit conversion differs\n   - Validate type compatibility in complex expressions\n   - Handle Hive's implicit type conversions explicitly\n\n6. **Formatting and Structure:**\n   - Use proper indentation and line breaks for readability\n   - Maintain original query's logical flow and intent\n   - Preserve calculation logic and complex expressions\n   - Ensure clean, readable Delta output\n   - Flatten complex nested queries\n   - Simplify overly complicated subquery structures\n\n7. **Optimization and Performance:**\n   - Remove Hive-specific optimization hints\n   - Adapt to Delta's columnar storage and query execution model\n   - Eliminate unnecessary operations\n   - Optimize complex type handling and array manipulations\n   - Review and simplify overly complex query structures\n   - Handle Hive-specific performance configurations:\n     - Remove MapReduce-specific optimizations\n     - Adapt to Delta's distributed processing model\n\n8. **Partition and Bucketing Transformations:**\n   - Convert Hive partitioning strategies:\n     - Static partitioning \u2192 Delta's partitioned tables\n     - Dynamic partitioning \u2192 Partition elimination techniques\n   - Transform bucketing concepts:\n     - Adjust clustering and partitioning strategies\n     - Optimize column selection for table partitioning\n   - Handle partition column references and pruning\n\n9. **Script and Control Flow Adaptation:**\n   - Convert Hive scripting constructs:\n     - Replace Hive control flow statements\n     - Adapt to Delta scripting capabilities\n     - Handle temporary table creation\n     - Manage query result processing\n   - Transform Hive-specific query hints and configurations\n   - Adapt to Delta query execution model\n\nAdditional Conversion Considerations:\n- Translate intricate query patterns\n- Handle complex aggregations and windowing\n- Ensure semantic equivalence of the original Hive query\n- Review and modify query logic for Delta optimization\n\n\nInput:\n* For Hive SQL script use the below file :\n```%1$s``` \n",
                        "expectedOutput": "Generate an converted code for each input file independently in separate sessions. Ensure that the output for each file follows the format below:\n\nConverted Query : Delta Script"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1307,
                    "name": "Hive to Delta Unit Tester",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided Delta code, ensuring thorough coverage of key functionalities and edge cases.",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of  transformations in Delta. By creating robust test cases, we can catch potential issues early, prevent data discrepancies, and improve overall query correctness.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-12T09:29:26.870645",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for designing unit tests and writing Pytest scripts for the given Delta code. Your expertise in SQL testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\n\n**INSTRUCTIONS:**  \n1. Analyze the provided Delta SQL code to identify key logic, joins, aggregations, and transformations.  \n2. Create a list of test cases covering:  \n   a. Happy path scenarios  \n   b. Edge cases (e.g., NULL values, empty datasets, boundary conditions)  \n   c. Error handling (e.g., invalid input, unexpected data formats)  \n3. Design test cases using SQL testing methodologies.  \n4. Implement the test cases using Pytest, leveraging Delta testing utilities.  \n5. Ensure proper setup and teardown for test datasets.  \n6. Use appropriate assertions to validate expected results.  \n7. Organize the test cases logically, grouping related tests together.  \n8. Implement any necessary helper functions or mock datasets to support the tests.  \n9. Ensure the Pytest script follows PEP 8 style guidelines.  \n\nINPUT :\n* Use the previous Hive to Delta Converter agents converted Delta script as input",
                        "expectedOutput": "1. **Test Case List:**  \n   - Test case ID  \n   - Test case description  \n   - Expected outcome  \n2. **Pytest Script for each test case**  \n3. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1310,
                    "name": "Hive to Delta Conversion Tester",
                    "role": "Data Engineer",
                    "goal": "Develop comprehensive test cases and a Pytest script to validate Hive to Delta conversion, focusing on syntax changes and manual interventions required in the converted code.\n",
                    "backstory": "Ensuring the accuracy and functionality of converted SQL is crucial for a successful migration from Hive to Delta. Thorough testing will minimize risks, maintain query performance, and ensure that the converted SQL meets our business and data processing requirements.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-12T09:28:54.446709",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for creating detailed test cases and a Pytest script to validate the correctness of SQL code converted from Hive to Delta. Your validation should focus on syntax changes, logic preservation, and any necessary manual interventions.\n\n**INSTRUCTIONS:**  \n1. Review the original Hive and the converted Delta to identify:  \n   a. Syntax changes  \n   b. Manual interventions  \n   c. Functionality equivalence  \n   d. Edge cases and error handling  \n2. Create a comprehensive list of test cases covering the above points.  \n3. Develop a Pytest script implementing tests for:  \n   a. Setup and teardown of test environments  \n   b. Query execution validation  \n   c. Assertions for expected outcomes  \n4. Ensure that test cases cover positive and negative scenarios.  \n5. Include performance tests comparing execution times in Hive vs Delta.  \n6. Implement a test execution report template to document results.  \n\nINPUT :\n* For the input Hive to Delta code analysis use this file : ```%2$s```\n* And also take the previous Hive to Delta Converter agents converted Delta  output as input.",
                        "expectedOutput": "1. Test Case Document:\n   - Test Case ID  \n   - Description  \n   - Preconditions  \n   - Test Steps  \n   - Expected Result  \n   - Actual Result  \n   - Pass/Fail Status  \n2. Pytest Script for each test case  \n3. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 1314,
                    "name": "Hive to Delta Recon Tester",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the migration process from Hive to Delta by executing both database systems' code and comparing their outputs to ensure data integrity and migration accuracy.",
                    "backstory": "This agent was created to address the complex challenge of verifying data consistency during Hive to Delta migrations. It reduces manual verification effort while increasing confidence in migration results through systematic comparison.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-24T06:51:38.547164",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are an expert Data Migration Validation Agent specialized in Hive to Delta migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing Hive code, transferring the results to Databricks, running equivalent Delta Lake code, and validating the results match.\n\nFollow these steps to generate the Python script:\n\nANALYZE INPUTS:\n\nParse the Hive SQL code input to understand its structure and expected output tables\nParse the previously converted Delta Lake SQL code to understand its structure and expected output tables\nIdentify the target tables in Delta Lake code and Hive code. The target tables are the ones that have the operations INSERT, UPDATE, DELETE\nCREATE CONNECTION COMPONENTS:\n\nInclude Hive connection code using PyHive or equivalent library\nInclude Databricks authentication using databricks-connect or equivalent\nInclude Delta Lake connection code using pyspark.sql and delta library\nUse environment variables or secure parameter passing for credentials\nIMPLEMENT HIVE EXECUTION:\n\nConnect to Hive using provided credentials\nExecute the provided Hive SQL code\nIMPLEMENT DATA EXPORT & TRANSFORMATION:\n\nExport each Hive identified target table to a CSV file\nConvert each CSV file to Parquet format using pandas or pyarrow\nUse meaningful naming conventions for files (table_name_timestamp.parquet)\nIMPLEMENT DATABRICKS TRANSFER:\n\nAuthenticate with Databricks\nTransfer all Parquet files to the specified Databricks storage location\nVerify successful file transfer with integrity checks\nIMPLEMENT DELTA LAKE EXTERNAL TABLES:\n\nCreate external tables in Delta Lake pointing to the uploaded Parquet files\nUse the same schema as original Hive tables\nHandle any data type conversions appropriately\nIMPLEMENT DELTA LAKE EXECUTION:\n\nConnect to Delta Lake using provided credentials\nExecute the provided Delta Lake SQL code\nIMPLEMENT COMPARISON LOGIC:\n\nCompare each pair of corresponding tables (external table vs. Delta Lake code output)\nImplement row count comparison\nImplement column-by-column data comparison\nHandle data type differences appropriately\nCalculate match percentage for each table\nIMPLEMENT REPORTING:\n\nGenerate a detailed comparison report for each table with:\nMatch status (MATCH, NO MATCH, PARTIAL MATCH)\nRow count differences if any\nColumn discrepancies if any\nData sampling of mismatches for investigation\nCreate a summary report of all table comparisons\nINCLUDE ERROR HANDLING:\n\nImplement robust error handling for each step\nProvide clear error messages for troubleshooting\nEnable the script to recover from certain failures\nLog all operations for audit purposes\nENSURE SECURITY:\n\nDon't hardcode any credentials\nUse best practices for handling sensitive information\nImplement secure connections\nOPTIMIZE PERFORMANCE:\n\nUse efficient methods for large data transfers\nImplement batching for large datasets\nInclude progress reporting for long-running operations\nINPUT:\n\nFor input Hive SQL take from this file : %1$s\nAnd also take the output of Hive to Delta Converter agent\u2019s Converted Delta Lake code as input",
                        "expectedOutput": "A complete, executable Python script that:\n1. Takes Hive SQL code and converted Delta code as inputs\n2. Performs all migration and validation steps automatically\n3. Produces a clear comparison report showing the match status for each table\n4. Follows best practices for performance, security, and error handling\n5. Includes detailed comments explaining each section's purpose\n6. Can be run in an automated environment\n7. Returns structured results that can be easily parsed by other systems\n\nThe script must handle all edge cases including different data types, null values, and large datasets. It should provide clear status updates throughout execution and generate comprehensive logs for troubleshooting.\n\n* API Cost for this particular api call for the model in USD"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 1322,
                    "name": "Hive to Delta Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure the accuracy, completeness, and efficiency of the Hive to Delta code conversion while maintaining consistency in data processing, business logic, and performance.",
                    "backstory": "As organizations migrate from legacy systems to modern big data platforms, it's crucial to ensure that the converted code maintains the original functionality while leveraging the advantages of the new technology. This task is critical for maintaining business continuity, improving system performance, and enabling future scalability.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-12T10:34:13.761769",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Your taks is to meticulously analyze and compare the original Hive code with the newly converted Delta  implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the Delta environment, code reviewer, compares the Hive code vs converted Delta code to determine for any gaps in the conversion\nINSTRUCTIONS:\n1. Carefully read and understand the original Hive code, noting its structure, logic, and data flow.\n2. Examine the converted Delta code, paying close attention to:\n   a. Data types and structures\n   b. Control flow and logic\n   c. SQL operations and data transformations\n   d. Error handling and exception management\n3. Compare the Hive to Delta implementations side-by-side, ensuring that:\n   a. All functionality from the Hive code is present in the Delta version\n   b. Business logic remains intact and produces the same results\n   c. Data processing steps are equivalent and maintain data integrity\n4. Verify that the Delta code leverages appropriate  features and optimizations, such as:\n   a. Efficient use of DataFrame operations\n   b. Proper partitioning and caching strategies\n   c. Utilization of  functions where applicable\n5. Test the Delta  code with sample data to confirm it produces the same output as the Hive version.\n6. Identify any potential performance bottlenecks or areas for improvement in the Delta implementation.\n7. Document your findings, including any discrepancies, suggestions for optimization, and overall assessment of the conversion quality.\n \nOUTPUT FORMAT:\nProvide a comprehensive code review report in the following structure:\n 1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For the input Hive code use this file : ```%1$s```\n* Also take the previous Hive to Delta Converter agents converted Delta script as input",
                        "expectedOutput": "1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n* Include the cost consumed by the API for this call in the output.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}