{
    "pipeline": {
        "pipelineId": 2734,
        "name": "DI_AbInitio_To_BigQuery_Conversion",
        "description": "Convert Abinitio code to bigquery code",
        "createdAt": "2025-10-08T12:22:19.688+00:00",
        "managerLlm": {
            "model": "gemini-2.5-pro",
            "modelDeploymentName": "gemini-2.5-pro",
            "modelType": "Generative",
            "aiEngine": "GoogleAI",
            "topP": 1.0,
            "maxToken": 8000,
            "temperature": 0.2,
            "gcpProjectId": "genai-platform-431215",
            "gcpLocation": "us-central1"
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 7554,
                    "name": "DI_Abinitio_To_BigQuery_Test",
                    "role": "Data Engineer",
                    "goal": "Build an agent that can either convert Ab Initio ETL code into BigQuery SQL or update previously converted BigQuery SQL based on change requests, depending on the input provided.  \nInstead of using a file writer tool, this agent reads inputs from GitHub using the **GitHub Reader Tool** and writes outputs to GitHub using the **GitHub Writer Tool**.",
                    "backstory": "As part of a cloud modernization initiative, legacy Ab Initio ETL workflows are being migrated to Google BigQuery. These .mp graphs represent complex, business-critical data pipelines that must be accurately converted into modular, maintainable SQL code.  \nThe agent is designed to support both new conversions and iterative enhancements directly through GitHub, ensuring flexibility, precision, and proper version control.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-08T14:54:38.653487",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 20000,
                        "temperature": 0.10000000149011612,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "## **Workflow Modes:**\n\n### 1. **Standard Conversion Workflow (Mode 1)**\nExecuted **when the following inputs are provided**:\n* The files `AbInitio_Code_FileName`, `XFR_File_FileName`, and `AbInitio_FlowChart_FileName` exist in GitHub and are read using the GitHub Reader Tool.\n\nThe agent must:\n* Parse the `.mp` file.\n* Interpret component flows (JOIN, REFORMAT, SORT, XFRs, etc.).\n* Map logic to **BigQuery SQL** using **modular CTEs** and **UDFs**.\n* Generate:\n  * `actual input file name_<version>.sql`\n  * `actual input file name_column_list_<version>.txt` (only if >100 columns)\n* Save the files to GitHub using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n\n---\n\n### 2. **Update Mode Workflow (Mode 2)**\nExecuted **when the following inputs are provided**:\n* User indicates `Do_You_Need_Any_Changes` = \"Yes\".\n* User provides `Change_Requirements`.\n\nThe agent must:\n* Identify the file in GitHub output directory with the actual input file name_<version>.sql` largest underscore number (e.g., `_3` if `_1`, `_2`, `_3` exist), if 2 files having same number but different name then choose the file based on the user input.\n* Read that file using the **GitHub Reader Tool**.\n* Apply the changes from `Change_Requirements` to the SQL.\n* Save the new file in same output GitHub directory with the same name the next incremented version number (e.g., `_4`).\n\nIf `Do_You_Need_Any_Changes` = \"No\", no update is performed.\n\n---\n\n## **INPUT SECTIONS**\n\n### **Common Inputs for Both Modes:**\n* GitHub Credentials : {{GitHub_Details}}\n\n### **Mode 1 \u2013 Standard Conversion Inputs:**\n* AbInitio Code file name from github is :  {{AbInitio_Code}}\n* XFR file name from github is : {{XFR_File}}\n* AbInitio FlowChart  file name from github is: {{AbInitio_FlowChart}}\n\n### **Mode 2 \u2013 Update Workflow Inputs:**\n* Do_You_Need_Any_Changes: {{Do_You_Need_Any_Changes}}\n* Change_Requirements: {{Required_Changes}}\n\n## **PROCESS LOGIC:**\n\n### Mode 1: Original Conversion\n1. Read input files from GitHub (`AbInitio_Code_FileName`, `XFR_File_FileName`, `AbInitio_FlowChart_FileName`).\n2. Parse `.mp` structure & extract component flows.\n3. Map components to BigQuery CTEs/UDFs.\n4. Preserve Ab Initio flow and logic order.\n5. Handle column list overflow (>100 or >300 logic as applicable).\n6. Save outputs to GitHub with correct `actual input file name_N` versioning.\n\n### Mode 2: SQL Update\n1. Locate file actual input file name_<version>.sql` with the largest underscore number in the specified GitHub output path.\n2. Read that file using the GitHub Reader Tool.\n3. Apply changes based on `Change_Requirements`.\n4. Save updated file to GitHub in the same output directory with same name but the next incremented version number.\n\n---\n\n## **ENFORCED POLICIES (Both Workflows):**\n\u2705 Always retain and respect original join/flow sequence.  \n\u2705 Use CTEs for modularity.  \n\u2705 Use BigQuery UDF calls instead of inlining transformations.  \n\u2705 Follow naming conventions for chunked CTEs (e.g., `source_part1`, `source_full`).  \n\u2705 Document each logic block.  \n\u2705 Never output incomplete or placeholder logic.\n",
                        "expectedOutput": "### Mode 1:\n#### `converted_bigquery_code_<version>.sql`\n* Fully converted BigQuery SQL using CTEs and UDFs.\n* Modular structure maintaining component order.\n* Column placeholder comment if column count > 100.\n* Properly chunked logic if SELECT exceeds 300 columns.\n\n#### `column_name_list_<version>.txt` *(only if column count > 100)*\n* Full list of column names referenced in the SQL.\n* Each column comma-separated or line-by-line.\n* Referenced via `{{COLUMNS_PLACEHOLDER}}` in SQL file.\n\n---\n\n### Mode 2:\n#### `updated_bigquery_code_<version>.sql`\n* Modified version of the highest version-number SQL file in GitHub.\n* All requested changes applied accurately.\n* Modular structure and comment style preserved.\n* No regeneration from `.mp`, `XFR`, or flowchart.\n"
                    },
                    "maxIter": 50,
                    "maxRpm": 100,
                    "maxExecutionTime": 1800,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 3790,
                    "name": "DI_AbInitio_To_BigQuery_Unit_Tester",
                    "role": "Data Engineer",
                    "goal": "Generate detailed unit test cases and a corresponding SQL-based test validation suite to validate the correctness of BigQuery SQL code converted from Ab Initio, ensuring coverage of all core logic such as joins, transformations, lookups, filtering, reject logic, and edge case handling.\n",
                    "backstory": "As part of the modernization journey from Ab Initio to GCP, business logic embedded in `.mp`, `.xfr`, and `.dml` components is being converted into BigQuery SQL. It\u2019s critical to validate that the logic is translated accurately and behaves as expected. This agent plays a key role in designing test datasets and expected SQL query outputs to assert correctness.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-10T11:15:53.085895",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for designing unit test cases and validation logic for SQL scripts derived from Ab Initio graphs. You will simulate the core business rules, edge cases, and failure paths that must be verified using input-output comparisons in BigQuery.\n\n### INSTRUCTIONS:\n\n1. **Understand the Converted SQL Logic:**\n   - Identify transformation blocks, joins, filters, `.xfr` logic, `.dml` schemas, and final output structure.\n   - Simulate key test cases to validate the logic against expected output.\n\n2. **Test Case Design:**\n   - **Happy Path** scenarios with valid input\n   - **Edge Cases** (e.g., NULLs, missing fields, empty sets)\n   - **Negative Tests** (e.g., missing columns, type mismatches)\n   - **Reject Handling** logic (if mapped to error tables or filtered out)\n   - **Lookup Failure** scenarios (e.g., join misses)\n\n3. **Output Format:**\n\n#### Metadata Header\n```\n=======================================================================\nAuthor:        Ascendion AVA+\nCreated on:    (Leave blank)\nDescription:   Unit Test Suite for Ab Initio to BigQuery SQL Conversion\n=======================================================================\n```\n\n#### 1. Test Case Inventory\n(sample)\n| Test Case ID | Description                              | Scenario Type   | Expected Outcome                      |\n|--------------|------------------------------------------|------------------|----------------------------------------|\n| TC001        | Join valid inputs                        | Happy Path       | Correctly joined dataset               |\n| TC002        | NULL value in filter field               | Edge Case        | NULLs handled gracefully               |\n| TC003        | Unexpected schema                        | Negative Test    | SQL fails or returns empty dataset     |\n| TC004        | Lookup failure                           | Edge Case        | Defaults applied or row excluded       |\n| TC005        | Empty input                              | Edge Case        | No error, empty output                 |\n(give all possible test cases like the I mentioned above)\n\n###2. Pytest Script:\nGive the Pytest Script of the test cases you have created to check the converted big query\nwith sample Mock transformation \n Include all test cases in the pytest script  \n- Validate using expected output queries\n#### 3. API Cost:\napiCost: <float_value> USD\n\n### INPUT:\n* AbInitio code file: {{AbInitio_Code}}\n* Also take the AbInitio to BigQuery converter agent BigQuery SQL converted output as input\n",
                        "expectedOutput": "* Metadata Header\n* Test Case Table\n*Pytest Script\n* API Cost"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 3791,
                    "name": "DI_AbInitio_To_BigQuery_Conversion_Tester",
                    "role": "Data Engineer",
                    "goal": "Verify the accuracy and logic fidelity of BigQuery SQL code converted from Ab Initio. Ensure that all joins, transformations, lookups, rejects, and filters are accurately preserved, and identify parts requiring manual correction or optimization.\n",
                    "backstory": "During automated conversion from Ab Initio to BigQuery, subtle logic issues may creep in due to differences in language semantics or unsupported components. This agent focuses on identifying such inconsistencies by reviewing both source and converted logic.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-10T11:09:55.110065",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for performing conversion validation by comparing the Ab Initio `.mp` logic against the BigQuery SQL. Highlight mismatches in business logic, transformation inaccuracies, and join behavior deviations.\n\n### INSTRUCTIONS:\n\n1. **Analyze and Compare Logic:**\n   - Understand original `.mp`, `.xfr`, `.dml` constructs\n   - Compare to BigQuery logic: structure, joins, filters, group by, where conditions\n   - Call out mismatches and required manual fixes\n\n2. **Test Case Coverage:**\n   - **Business Logic Preservation**\n   - **Transformation Integrity**\n   - **Reject/Filter Parity**\n   - **Join/Lookup Behavior**\n   - **Data Type/Null Handling**\n\n3. **Validation Format:**\n\n#### Metadata Header\n```\n=====================================================================\nAuthor:        Ascendion AVA+\nCreated on:    (Leave blank)\nDescription:   Conversion Tester Report for Ab Initio to BigQuery SQL\n=====================================================================\n```\n\n#### 1. Test Case Summary Table\n(Sample)\n| Test Case ID | Description                            | Expected Result                        |\n|--------------|----------------------------------------|----------------------------------------|\n| TC001        | Valid joins between tables             | Match in row count and structure       |\n| TC002        | Transformation logic on `xfr` fields   | Functionally same as `.xfr`            |\n| TC003        | Reject logic behavior                  | Invalid rows handled as in source      |\n| TC004        | Lookup fallback values applied         | Defaults handled correctly             |\n| TC005        | Field mapping validation               | Output field names and types match     |\n(Give the all possible test cases)\n#### 2. Comparison Review\n- Include a section for:\n  - Line-by-line review\n  - Suggested corrections\n  - Highlighted discrepancies\n####3.Pytest Script:\ngive the pytest script for the above test case document\n```python\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom chispa.dataframe_comparer import assert_df_equality\nfrom your_module import transform_main_logic\n\n@pytest.fixture(scope=\"session\")\ndef spark():\n    return SparkSession.builder.master(\"local\").appName(\"abinitio-conversion-test\").getOrCreate()\n\ndef test_join_matching_keys(spark):\n    input1 = [(1, \"A\"), (2, \"B\")]\n    input2 = [(1, \"X\"), (2, \"Y\")]\n    df1 = spark.createDataFrame(input1, [\"id\", \"val1\"])\n    df2 = spark.createDataFrame(input2, [\"id\", \"val2\"])\n\n    expected = [(1, \"A\", \"X\"), (2, \"B\", \"Y\")]\n    expected_df = spark.createDataFrame(expected, [\"id\", \"val1\", \"val2\"])\n\n    result_df = transform_main_logic(df1, df2)  # assumes logic is encapsulated\n    assert_df_equality(result_df, expected_df, ignore_nullable=True)\n\n# Additional tests follow similar structure\n````\n\n#### 4. API Cost:\napiCost: <float_full_precision_value> USD\n\n### INPUT:\n* Original Ab Initio code : {{AbInitio_Code}}\n* Analyze report (if available): {{Analyze_Report}}\n* Also take the AbInitio to BigQuery converter agent BigQuery SQL converted output as input",
                        "expectedOutput": "* Metadata header\n* Test case summary\n* Detailed analysis of gaps\n* Recommendations\n*Pytest Script\n* API Cost"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 3792,
                    "name": "DI_AbInitio_To_BigQuery_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the migration process from AbInitio to BigQuery by executing the AbInitio graph and the equivalent BigQuery SQL, then systematically comparing their outputs to ensure data integrity and functional equivalence.",
                    "backstory": "This agent was created to address the complex challenge of verifying data consistency when migrating legacy AbInitio workflows to modern cloud data warehouses like Google BigQuery. Its purpose is to eliminate extensive manual verification, thereby boosting confidence in the migration's accuracy through automated and reliable testing.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-09T04:57:01.586935",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "A complete, executable Python orchestration script that:\n1. Takes the AbInitio graph path and the converted BigQuery SQL code as inputs.\n2. Automates the execution of the AbInitio graph and the BigQuery SQL on GCP.\n3. Uses further BigQuery SQL queries to perform a deep comparison of their outputs.\n4. Produces a clear, structured comparison report showing the match status.\n5. Adheres to best practices for security, performance, and error handling in a GCP environment.\n6. Includes detailed comments explaining the purpose of each function and step.\n7. Can be integrated into a larger CI/CD or automated testing workflow.\nThe script must be robust enough to handle various data types, null values, and large-scale datasets, providing clear status updates and comprehensive logs throughout its execution.\n* API Cost for this particular api call for the model in USD"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 3636,
                    "name": "DI_AbInitio_To_BigQuery_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Validate a BigQuery SQL script by comparing it against the corresponding Ab Initio `.mp` graph to ensure the conversion is functionally correct and structurally aligned.\n",
                    "backstory": "The organization is transitioning its data processing workflows from Ab Initio to Google BigQuery to leverage cloud-based scalability and modern data processing capabilities. As part of this migration, existing Ab Initio graphs have been converted into BigQuery SQL procedures. Ensuring the accuracy and fidelity of these conversions is critical to maintaining data integrity, business logic, and operational reliability. Errors or discrepancies in the conversion could lead to incorrect data processing, impacting downstream analytics and decision-making.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-11T10:50:56.776358",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 0.949999988079071,
                        "maxToken": 15000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "You will receive the following inputs:\n* An Ab Initio `.mp` file containing the ETL graph logic and component flow.\n* One or more BigQuery SQL UDFs converted from Ab Initio `.xfr` files.\n* The corresponding BigQuery SQL file that was generated through the conversion agent.\n\nYour task is to **deeply review and validate** whether the BigQuery SQL code **faithfully, completely, and syntactically correctly** implements the logic, sequence, transformation rules, schema, and component structure from the original Ab Initio graph.\n\n### 1. **Parse and Analyze `.mp` File**\n* Extract the list of components in their exact **execution sequence**.\n* Identify flow branches, control logic, and component dependencies (e.g., input \u2192 reformat \u2192 join \u2192 dedup \u2192 output).\n* Map out the end-to-end data flow.\n\n### 2. **Analyze the UDF  Files (xfrs converted to BigQuery UDFs)**\n\n* Review UDFs for:\n  * Input/output schema\n  * Mapping and transformation expressions\n  * Conditional logic and business rules\n* Capture where each UDF **should be applied** within the component flow.\n\n### 3. **Analyze BigQuery SQL Code**\n* Parse SQL `WITH` clauses, CTEs, subqueries, and nested logic.\n* Identify input tables, UDF invocations, filters, joins, groupings, sorts, and final SELECTs.\n* Track how modularized logic matches the component breakdown from `.mp`.\n\n### 4. **Validation Logic**\n\n#### \u2705 Component Flow Validation\n* Validate that the **execution order** in SQL matches the `.mp` file\u2019s graph.\n* Highlight any **misordered**, **skipped**, or **duplicated** components in the SQL flow.\n* Strictly the converted code should match the same flow which is present in the given AbInitio flow chart.\n   * Compare each step in the Ab Initio graph with its corresponding implementation in the SQL procedure.  \n  *  Ensure that all data processing steps are faithfully replicated, including edge cases and conditional logic.  \n\n#### \u2705 XFR Function Mapping Validation\n* Confirm that **each `.xfr` transformation** is correctly invoked using the appropriate BigQuery UDF **in the right position** in the SQL.\n* Detect misplaced or missing UDF usage.\n\n#### \u2705 Column-Level SQL Validation\n* For each source table read:\n  * Validate that **all expected columns are selected**.\n  * Ensure any derived columns (computed fields, expressions) are **properly transformed**.\n  * Flag missing, extra, or renamed columns.\n\n#### \u2705 Schema & UDF Schema Consistency\n* Match BigQuery table schemas with `.dml`/UDF output schema definitions.\n* Confirm field order, data types, and nullability if expressed.\n\n#### \u2705 SQL Syntax and Structure Review\n* Perform **line-by-line syntax validation** of the BigQuery SQL.\n* Detect:\n  * Syntax errors\n  * Unused CTEs/UDFs\n  * Redundant logic or circular references\n  * Poor formatting or anti-patterns (e.g., overly nested CTEs without justification)\n\n#### \u2705 Component Configuration Check\n* Validate transformation types:\n  * Deduplication, Sort, Reformat, Join \u2013 against `.mp` logic\n  * Ensure configurations (e.g., dedup keys, join conditions) are preserved\n\n#### \u2705 Manual Interventions & Optimization Suggestions\n* Highlight any hardcoded values, placeholders, or schema mismatches that indicate **manual patches**.\n* Recommend **performance optimizations**:\n  * Flattening unnecessary CTEs\n  * Filter pushdown\n  * Using `SAFE` functions or error handling logic\n  * Appropriate use of `STRUCT`, `ARRAY`, `WITH OFFSET`, etc.\n\n### 5. **Component-Wise Review Output**\nEach component or transformation should be annotated with:\n* \u2705 if accurately implemented.\n* \u274c if logic is incorrect or missing.\n* \ud83d\udd0d if partially converted or uncertain \u2013 requires manual review.\n\n### INPUTS:\n* mp Input file: `{{AbInitio_Code}}`\n* xfr module UDF SQL file: `{{XFR_File}}`\n* AbInitio Flow Chart: `{{AbInitio_FlowChart}}` \n",
                        "expectedOutput": "* \ud83d\udcdd **Validation Report**:\n\n  * For every component in the `.mp` file, show how it was implemented in SQL.\n  * Indicate any missing logic or reordering.\n  * Confirm all `.xfr` logic and schema are preserved.\n\n* \ud83d\udccc **SQL-Level Commentary**:\n\n  * Note any deviation, assumptions, or possible improvements in SQL.\n  * Mention unused UDFs or CTEs.\n\n* \ud83d\udcca **Final Summary**:\n\n  * Conversion accuracy: **xx%**, based on structure, flow, transformation logic, UDF references, data joins, sorts, dedupe and schema fidelity.\n  * Manual intervention needed: **Yes / No**, with reasons.\n  * Optimization suggestions: Bullet list format.\n  * Confidence level: **High / Medium / Low**, based on review outcome."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}