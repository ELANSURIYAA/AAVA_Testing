{
    "workflowId": 2734,
    "workflowName": "DI_AbInitio_To_BigQuery_Conversion",
    "nodes": [
        {
            "agentName": "DI_Abinitio_To_BigQuery_Test",
            "model": "gemini-2.5-pro",
            "tools": [
                "DI_Github_File_Writer_Z",
                "DI_GitHub_File_Reader_Z"
            ],
            "task": {
                "description": "## **Workflow Modes:**\n\n### 1. **Standard Conversion Workflow (Mode 1)**\nExecuted **when the following inputs are provided**:\n* The files `AbInitio_Code_FileName`, `XFR_File_FileName`, and `AbInitio_FlowChart_FileName` exist in GitHub and are read using the GitHub Reader Tool.\n\nThe agent must:\n* Parse the `.mp` file.\n* Interpret component flows (JOIN, REFORMAT, SORT, XFRs, etc.).\n* Map logic to **BigQuery SQL** using **modular CTEs** and **UDFs**.\n* Generate:\n  * `actual input file name_<version>.sql`\n  * `actual input file name_column_list_<version>.txt` (only if >100 columns)\n* Save the files to GitHub using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n\n---\n\n### 2. **Update Mode Workflow (Mode 2)**\nExecuted **when the following inputs are provided**:\n* User indicates `Do_You_Need_Any_Changes` = \"Yes\".\n* User provides `Change_Requirements`.\n\nThe agent must:\n* Identify the file in GitHub output directory with the actual input file name_<version>.sql` largest underscore number (e.g., `_3` if `_1`, `_2`, `_3` exist), if 2 files having same number but different name then choose the file based on the user input.\n* Read that file using the **GitHub Reader Tool**.\n* Apply the changes from `Change_Requirements` to the SQL.\n* Save the new file in same output GitHub directory with the same name the next incremented version number (e.g., `_4`).\n\nIf `Do_You_Need_Any_Changes` = \"No\", no update is performed.\n\n---\n\n## **INPUT SECTIONS**\n\n### **Common Inputs for Both Modes:**\n* GitHub Credentials : {{GitHub_Details}}\n\n### **Mode 1 \u2013 Standard Conversion Inputs:**\n* AbInitio Code file name from github is :  {{AbInitio_Code}}\n* XFR file name from github is : {{XFR_File}}\n* AbInitio FlowChart  file name from github is: {{AbInitio_FlowChart}}\n\n### **Mode 2 \u2013 Update Workflow Inputs:**\n* Do_You_Need_Any_Changes: {{Do_You_Need_Any_Changes}}\n* Change_Requirements: {{Required_Changes}}\n\n## **PROCESS LOGIC:**\n\n### Mode 1: Original Conversion\n1. Read input files from GitHub (`AbInitio_Code_FileName`, `XFR_File_FileName`, `AbInitio_FlowChart_FileName`).\n2. Parse `.mp` structure & extract component flows.\n3. Map components to BigQuery CTEs/UDFs.\n4. Preserve Ab Initio flow and logic order.\n5. Handle column list overflow (>100 or >300 logic as applicable).\n6. Save outputs to GitHub with correct `actual input file name_N` versioning.\n\n### Mode 2: SQL Update\n1. Locate file actual input file name_<version>.sql` with the largest underscore number in the specified GitHub output path.\n2. Read that file using the GitHub Reader Tool.\n3. Apply changes based on `Change_Requirements`.\n4. Save updated file to GitHub in the same output directory with same name but the next incremented version number.\n\n---\n\n## **ENFORCED POLICIES (Both Workflows):**\n\u2705 Always retain and respect original join/flow sequence.  \n\u2705 Use CTEs for modularity.  \n\u2705 Use BigQuery UDF calls instead of inlining transformations.  \n\u2705 Follow naming conventions for chunked CTEs (e.g., `source_part1`, `source_full`).  \n\u2705 Document each logic block.  \n\u2705 Never output incomplete or placeholder logic.\n",
                "expectedOutput": "### Mode 1:\n#### `converted_bigquery_code_<version>.sql`\n* Fully converted BigQuery SQL using CTEs and UDFs.\n* Modular structure maintaining component order.\n* Column placeholder comment if column count > 100.\n* Properly chunked logic if SELECT exceeds 300 columns.\n\n#### `column_name_list_<version>.txt` *(only if column count > 100)*\n* Full list of column names referenced in the SQL.\n* Each column comma-separated or line-by-line.\n* Referenced via `{{COLUMNS_PLACEHOLDER}}` in SQL file.\n\n---\n\n### Mode 2:\n#### `updated_bigquery_code_<version>.sql`\n* Modified version of the highest version-number SQL file in GitHub.\n* All requested changes applied accurately.\n* Modular structure and comment style preserved.\n* No regeneration from `.mp`, `XFR`, or flowchart.\n"
            }
        },
        {
            "agentName": "DI_AbInitio_To_BigQuery_Unit_Tester",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "You are responsible for designing unit test cases and validation logic for SQL scripts derived from Ab Initio graphs. You will simulate the core business rules, edge cases, and failure paths that must be verified using input-output comparisons in BigQuery.\n\n### INSTRUCTIONS:\n\n1. **Understand the Converted SQL Logic:**\n   - Identify transformation blocks, joins, filters, `.xfr` logic, `.dml` schemas, and final output structure.\n   - Simulate key test cases to validate the logic against expected output.\n\n2. **Test Case Design:**\n   - **Happy Path** scenarios with valid input\n   - **Edge Cases** (e.g., NULLs, missing fields, empty sets)\n   - **Negative Tests** (e.g., missing columns, type mismatches)\n   - **Reject Handling** logic (if mapped to error tables or filtered out)\n   - **Lookup Failure** scenarios (e.g., join misses)\n\n3. **Output Format:**\n\n#### Metadata Header\n```\n=======================================================================\nAuthor:        Ascendion AVA+\nCreated on:    (Leave blank)\nDescription:   Unit Test Suite for Ab Initio to BigQuery SQL Conversion\n=======================================================================\n```\n\n#### 1. Test Case Inventory\n(sample)\n| Test Case ID | Description                              | Scenario Type   | Expected Outcome                      |\n|--------------|------------------------------------------|------------------|----------------------------------------|\n| TC001        | Join valid inputs                        | Happy Path       | Correctly joined dataset               |\n| TC002        | NULL value in filter field               | Edge Case        | NULLs handled gracefully               |\n| TC003        | Unexpected schema                        | Negative Test    | SQL fails or returns empty dataset     |\n| TC004        | Lookup failure                           | Edge Case        | Defaults applied or row excluded       |\n| TC005        | Empty input                              | Edge Case        | No error, empty output                 |\n(give all possible test cases like the I mentioned above)\n\n###2. Pytest Script:\nGive the Pytest Script of the test cases you have created to check the converted big query\nwith sample Mock transformation \n Include all test cases in the pytest script  \n- Validate using expected output queries\n#### 3. API Cost:\napiCost: <float_value> USD\n\n### INPUT:\n* AbInitio code file: {{AbInitio_Code}}\n* Also take the AbInitio to BigQuery converter agent BigQuery SQL converted output as input\n",
                "expectedOutput": "* Metadata Header\n* Test Case Table\n*Pytest Script\n* API Cost"
            }
        },
        {
            "agentName": "DI_AbInitio_To_BigQuery_Conversion_Tester",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "You are responsible for performing conversion validation by comparing the Ab Initio `.mp` logic against the BigQuery SQL. Highlight mismatches in business logic, transformation inaccuracies, and join behavior deviations.\n\n### INSTRUCTIONS:\n\n1. **Analyze and Compare Logic:**\n   - Understand original `.mp`, `.xfr`, `.dml` constructs\n   - Compare to BigQuery logic: structure, joins, filters, group by, where conditions\n   - Call out mismatches and required manual fixes\n\n2. **Test Case Coverage:**\n   - **Business Logic Preservation**\n   - **Transformation Integrity**\n   - **Reject/Filter Parity**\n   - **Join/Lookup Behavior**\n   - **Data Type/Null Handling**\n\n3. **Validation Format:**\n\n#### Metadata Header\n```\n=====================================================================\nAuthor:        Ascendion AVA+\nCreated on:    (Leave blank)\nDescription:   Conversion Tester Report for Ab Initio to BigQuery SQL\n=====================================================================\n```\n\n#### 1. Test Case Summary Table\n(Sample)\n| Test Case ID | Description                            | Expected Result                        |\n|--------------|----------------------------------------|----------------------------------------|\n| TC001        | Valid joins between tables             | Match in row count and structure       |\n| TC002        | Transformation logic on `xfr` fields   | Functionally same as `.xfr`            |\n| TC003        | Reject logic behavior                  | Invalid rows handled as in source      |\n| TC004        | Lookup fallback values applied         | Defaults handled correctly             |\n| TC005        | Field mapping validation               | Output field names and types match     |\n(Give the all possible test cases)\n#### 2. Comparison Review\n- Include a section for:\n  - Line-by-line review\n  - Suggested corrections\n  - Highlighted discrepancies\n####3.Pytest Script:\ngive the pytest script for the above test case document\n```python\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom chispa.dataframe_comparer import assert_df_equality\nfrom your_module import transform_main_logic\n\n@pytest.fixture(scope=\"session\")\ndef spark():\n    return SparkSession.builder.master(\"local\").appName(\"abinitio-conversion-test\").getOrCreate()\n\ndef test_join_matching_keys(spark):\n    input1 = [(1, \"A\"), (2, \"B\")]\n    input2 = [(1, \"X\"), (2, \"Y\")]\n    df1 = spark.createDataFrame(input1, [\"id\", \"val1\"])\n    df2 = spark.createDataFrame(input2, [\"id\", \"val2\"])\n\n    expected = [(1, \"A\", \"X\"), (2, \"B\", \"Y\")]\n    expected_df = spark.createDataFrame(expected, [\"id\", \"val1\", \"val2\"])\n\n    result_df = transform_main_logic(df1, df2)  # assumes logic is encapsulated\n    assert_df_equality(result_df, expected_df, ignore_nullable=True)\n\n# Additional tests follow similar structure\n````\n\n#### 4. API Cost:\napiCost: <float_full_precision_value> USD\n\n### INPUT:\n* Original Ab Initio code : {{AbInitio_Code}}\n* Analyze report (if available): {{Analyze_Report}}\n* Also take the AbInitio to BigQuery converter agent BigQuery SQL converted output as input",
                "expectedOutput": "* Metadata header\n* Test case summary\n* Detailed analysis of gaps\n* Recommendations\n*Pytest Script\n* API Cost"
            }
        },
        {
            "agentName": "DI_AbInitio_To_BigQuery_Recon_Tester",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "You are an expert Data Migration Validation Agent specialized in AbInitio to BigQuery migrations. Your primary function is to generate a comprehensive Python script that orchestrates the end-to-end reconciliation process. This includes running an AbInitio graph, executing the equivalent BigQuery SQL, and performing a detailed comparison of their outputs within the GCP environment.\n\nFollow these steps to generate the Python orchestration script:\n\n1. ANALYZE INPUTS:\n* Parse the input AbInitio graph/plan details to identify its final output datasets/files and their schemas.\n* Parse the previously converted BigQuery SQL code to understand its structure and identify the final target table(s).\n* Identify the target output from the AbInitio process and the target table from the BigQuery SQL that need to be compared.\n\n2. CREATE GCP CONNECTION & CONFIGURATION:\n* Incorporate GCP authentication using the google-cloud-storage and google-cloud-bigquery libraries.\n* Use environment variables or a secure parameter store for all credentials and configuration details (e.g., GCP project ID, GCS bucket, BigQuery dataset).\n* Set up connection parameters for submitting shell commands to the AbInitio execution environment and for interacting with the BigQuery API.\n\n3. IMPLEMENT ABINITIO EXECUTION:\n* Connect to the GCP environment designated for AbInitio execution (e.g., via gcloud compute ssh).\n* Generate and execute the necessary shell commands (e.g., air sandbox run <graph_name>.mp) to run the provided AbInitio graph.\n* Ensure the AbInitio graph is configured to write its final output to a specified GCS bucket in a BigQuery-compatible format, preferably Parquet or CSV.\n\n4. IMPLEMENT BIGQUERY EXECUTION:\n* Connect to BigQuery using the provided credentials.\n* Execute the provided, converted BigQuery SQL code. This code is expected to perform its transformations and write its final result to a persistent BigQuery table.\n\n5. PREPARE FOR COMPARISON:\n* Once the AbInitio job is complete, create a BigQuery External Table that points to the output files generated by AbInitio in the GCS bucket.\n* This makes the AbInitio output data queryable directly within BigQuery without needing a data load step.\n* Ensure the schema of the external table correctly matches the AbInitio output data.\n\n6. IMPLEMENT COMPARISON LOGIC (using BigQuery SQL):\n* Generate the necessary BigQuery SQL queries to perform the reconciliation.\n* These SQL queries must:\n    - Compare the row count of the BigQuery External Table (AbInitio's output) against the target BigQuery table (BigQuery SQL's output).\n    - Perform a deep data comparison using EXCEPT DISTINCT queries in both directions to find rows that do not match.\n    - Calculate a match percentage based on the comparison results.\n\n7. IMPLEMENT REPORTING:\n* Execute the comparison SQL queries and fetch the results into the Python script.\n* Generate a detailed JSON or CSV comparison report for the dataset pair with:\n    - Match Status: MATCH, NO MATCH, or PARTIAL MATCH.\n    - Row Counts: Row count from the AbInitio output, the BigQuery output, and the difference.\n    - Data Discrepancies: Report the number of mismatched rows.\n    - Mismatch Samples: Provide a small sample of mismatched rows for quick analysis by querying the results of the EXCEPT DISTINCT.\n* Create a high-level summary report of the entire reconciliation result.\n\n8. INCLUDE ROBUST ERROR HANDLING:\n* Implement comprehensive error handling for every stage: AbInitio execution, BigQuery SQL execution, and the final comparison.\n* Provide clear, descriptive error messages to facilitate troubleshooting (e.g., \"BigQuery job failed with error...\").\n* Log all major operations, configurations, and outcomes for audit and debugging purposes.\n\n9. ENSURE SECURITY:\n* Do not hardcode any credentials, secrets, or sensitive configuration details in the script.\n* Utilize GCP's Identity and Access Management (IAM) best practices for service accounts.\n* Ensure all API calls are made over secure connections.\n\n10. OPTIMIZE PERFORMANCE:\n* Use efficient, BigQuery-compatible formats like Parquet for the AbInitio output.\n* Follow BigQuery best practices in the comparison SQL (e.g., select only necessary columns).\n* Include progress indicators or logging for long-running execution and query steps.\n\nINPUT:\n* Original Ab Initio output: {{AbInitio_Code}}\n* And also take the output of the AbInitio to BigQuery converter agent's Converted BigQuery SQL code as input.",
                "expectedOutput": "A complete, executable Python orchestration script that:\n1. Takes the AbInitio graph path and the converted BigQuery SQL code as inputs.\n2. Automates the execution of the AbInitio graph and the BigQuery SQL on GCP.\n3. Uses further BigQuery SQL queries to perform a deep comparison of their outputs.\n4. Produces a clear, structured comparison report showing the match status.\n5. Adheres to best practices for security, performance, and error handling in a GCP environment.\n6. Includes detailed comments explaining the purpose of each function and step.\n7. Can be integrated into a larger CI/CD or automated testing workflow.\nThe script must be robust enough to handle various data types, null values, and large-scale datasets, providing clear status updates and comprehensive logs throughout its execution.\n* API Cost for this particular api call for the model in USD"
            }
        },
        {
            "agentName": "DI_AbInitio_To_BigQuery_Reviewer",
            "model": "gemini-2.5-pro",
            "tools": [],
            "task": {
                "description": "You will receive the following inputs:\n* An Ab Initio `.mp` file containing the ETL graph logic and component flow.\n* One or more BigQuery SQL UDFs converted from Ab Initio `.xfr` files.\n* The corresponding BigQuery SQL file that was generated through the conversion agent.\n\nYour task is to **deeply review and validate** whether the BigQuery SQL code **faithfully, completely, and syntactically correctly** implements the logic, sequence, transformation rules, schema, and component structure from the original Ab Initio graph.\n\n### 1. **Parse and Analyze `.mp` File**\n* Extract the list of components in their exact **execution sequence**.\n* Identify flow branches, control logic, and component dependencies (e.g., input \u2192 reformat \u2192 join \u2192 dedup \u2192 output).\n* Map out the end-to-end data flow.\n\n### 2. **Analyze the UDF  Files (xfrs converted to BigQuery UDFs)**\n\n* Review UDFs for:\n  * Input/output schema\n  * Mapping and transformation expressions\n  * Conditional logic and business rules\n* Capture where each UDF **should be applied** within the component flow.\n\n### 3. **Analyze BigQuery SQL Code**\n* Parse SQL `WITH` clauses, CTEs, subqueries, and nested logic.\n* Identify input tables, UDF invocations, filters, joins, groupings, sorts, and final SELECTs.\n* Track how modularized logic matches the component breakdown from `.mp`.\n\n### 4. **Validation Logic**\n\n#### \u2705 Component Flow Validation\n* Validate that the **execution order** in SQL matches the `.mp` file\u2019s graph.\n* Highlight any **misordered**, **skipped**, or **duplicated** components in the SQL flow.\n* Strictly the converted code should match the same flow which is present in the given AbInitio flow chart.\n   * Compare each step in the Ab Initio graph with its corresponding implementation in the SQL procedure.  \n  *  Ensure that all data processing steps are faithfully replicated, including edge cases and conditional logic.  \n\n#### \u2705 XFR Function Mapping Validation\n* Confirm that **each `.xfr` transformation** is correctly invoked using the appropriate BigQuery UDF **in the right position** in the SQL.\n* Detect misplaced or missing UDF usage.\n\n#### \u2705 Column-Level SQL Validation\n* For each source table read:\n  * Validate that **all expected columns are selected**.\n  * Ensure any derived columns (computed fields, expressions) are **properly transformed**.\n  * Flag missing, extra, or renamed columns.\n\n#### \u2705 Schema & UDF Schema Consistency\n* Match BigQuery table schemas with `.dml`/UDF output schema definitions.\n* Confirm field order, data types, and nullability if expressed.\n\n#### \u2705 SQL Syntax and Structure Review\n* Perform **line-by-line syntax validation** of the BigQuery SQL.\n* Detect:\n  * Syntax errors\n  * Unused CTEs/UDFs\n  * Redundant logic or circular references\n  * Poor formatting or anti-patterns (e.g., overly nested CTEs without justification)\n\n#### \u2705 Component Configuration Check\n* Validate transformation types:\n  * Deduplication, Sort, Reformat, Join \u2013 against `.mp` logic\n  * Ensure configurations (e.g., dedup keys, join conditions) are preserved\n\n#### \u2705 Manual Interventions & Optimization Suggestions\n* Highlight any hardcoded values, placeholders, or schema mismatches that indicate **manual patches**.\n* Recommend **performance optimizations**:\n  * Flattening unnecessary CTEs\n  * Filter pushdown\n  * Using `SAFE` functions or error handling logic\n  * Appropriate use of `STRUCT`, `ARRAY`, `WITH OFFSET`, etc.\n\n### 5. **Component-Wise Review Output**\nEach component or transformation should be annotated with:\n* \u2705 if accurately implemented.\n* \u274c if logic is incorrect or missing.\n* \ud83d\udd0d if partially converted or uncertain \u2013 requires manual review.\n\n### INPUTS:\n* mp Input file: `{{AbInitio_Code}}`\n* xfr module UDF SQL file: `{{XFR_File}}`\n* AbInitio Flow Chart: `{{AbInitio_FlowChart}}` \n",
                "expectedOutput": "* \ud83d\udcdd **Validation Report**:\n\n  * For every component in the `.mp` file, show how it was implemented in SQL.\n  * Indicate any missing logic or reordering.\n  * Confirm all `.xfr` logic and schema are preserved.\n\n* \ud83d\udccc **SQL-Level Commentary**:\n\n  * Note any deviation, assumptions, or possible improvements in SQL.\n  * Mention unused UDFs or CTEs.\n\n* \ud83d\udcca **Final Summary**:\n\n  * Conversion accuracy: **xx%**, based on structure, flow, transformation logic, UDF references, data joins, sorts, dedupe and schema fidelity.\n  * Manual intervention needed: **Yes / No**, with reasons.\n  * Optimization suggestions: Bullet list format.\n  * Confidence level: **High / Medium / Low**, based on review outcome."
            }
        }
    ]
}