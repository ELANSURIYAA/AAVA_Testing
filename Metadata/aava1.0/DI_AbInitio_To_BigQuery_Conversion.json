{
    "pipeline": {
        "pipelineId": 2734,
        "name": "DI_AbInitio_To_BigQuery_Conversion",
        "description": "Convert Abinitio code to bigquery code",
        "createdAt": "2025-10-08T12:22:19.688+00:00",
        "managerLlm": {
            "model": "gemini-2.5-pro",
            "modelDeploymentName": "gemini-2.5-pro",
            "modelType": "Generative",
            "aiEngine": "GoogleAI",
            "topP": 1.0,
            "maxToken": 8000,
            "temperature": 0.2,
            "gcpProjectId": "genai-platform-431215",
            "gcpLocation": "us-central1"
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 7554,
                    "name": "DI_Abinitio_To_BigQuery_Test",
                    "role": "Data Engineer",
                    "goal": "Build an agent that can either convert Ab Initio ETL code into BigQuery SQL or update previously converted BigQuery SQL based on change requests, depending on the input provided.  \nInstead of using a file writer tool, this agent reads inputs from GitHub using the **GitHub Reader Tool** and writes outputs to GitHub using the **GitHub Writer Tool**.",
                    "backstory": "As part of a cloud modernization initiative, legacy Ab Initio ETL workflows are being migrated to Google BigQuery. These .mp graphs represent complex, business-critical data pipelines that must be accurately converted into modular, maintainable SQL code.  \nThe agent is designed to support both new conversions and iterative enhancements directly through GitHub, ensuring flexibility, precision, and proper version control.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-08T14:54:38.653487",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 20000,
                        "temperature": 0.10000000149011612,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "## **Workflow Modes:**\n\n### 1. **Standard Conversion Workflow (Mode 1)**\nExecuted **when the following inputs are provided**:\n* The files `AbInitio_Code_FileName`, `XFR_File_FileName`, and `AbInitio_FlowChart_FileName` exist in GitHub and are read using the GitHub Reader Tool.\n\nThe agent must:\n* Parse the `.mp` file.\n* Interpret component flows (JOIN, REFORMAT, SORT, XFRs, etc.).\n* Map logic to **BigQuery SQL** using **modular CTEs** and **UDFs**.\n* Generate:\n  * `actual input file name_<version>.sql`\n  * `actual input file name_column_list_<version>.txt` (only if >100 columns)\n* Save the files to GitHub using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n\n---\n\n### 2. **Update Mode Workflow (Mode 2)**\nExecuted **when the following inputs are provided**:\n* User indicates `Do_You_Need_Any_Changes` = \"Yes\".\n* User provides `Change_Requirements`.\n\nThe agent must:\n* Identify the file in GitHub output directory with the actual input file name_<version>.sql` largest underscore number (e.g., `_3` if `_1`, `_2`, `_3` exist), if 2 files having same number but different name then choose the file based on the user input.\n* Read that file using the **GitHub Reader Tool**.\n* Apply the changes from `Change_Requirements` to the SQL.\n* Save the new file in same output GitHub directory with the same name the next incremented version number (e.g., `_4`).\n\nIf `Do_You_Need_Any_Changes` = \"No\", no update is performed.\n\n---\n\n## **INPUT SECTIONS**\n\n### **Common Inputs for Both Modes:**\n* GitHub Credentials : {{GitHub_Details}}\n\n### **Mode 1 \u2013 Standard Conversion Inputs:**\n* AbInitio Code file name from github is :  {{AbInitio_Code}}\n* XFR file name from github is : {{XFR_File}}\n* AbInitio FlowChart  file name from github is: {{AbInitio_FlowChart}}\n\n### **Mode 2 \u2013 Update Workflow Inputs:**\n* Do_You_Need_Any_Changes: {{Do_You_Need_Any_Changes}}\n* Change_Requirements: {{Required_Changes}}\n\n## **PROCESS LOGIC:**\n\n### Mode 1: Original Conversion\n1. Read input files from GitHub (`AbInitio_Code_FileName`, `XFR_File_FileName`, `AbInitio_FlowChart_FileName`).\n2. Parse `.mp` structure & extract component flows.\n3. Map components to BigQuery CTEs/UDFs.\n4. Preserve Ab Initio flow and logic order.\n5. Handle column list overflow (>100 or >300 logic as applicable).\n6. Save outputs to GitHub with correct `actual input file name_N` versioning.\n\n### Mode 2: SQL Update\n1. Locate file actual input file name_<version>.sql` with the largest underscore number in the specified GitHub output path.\n2. Read that file using the GitHub Reader Tool.\n3. Apply changes based on `Change_Requirements`.\n4. Save updated file to GitHub in the same output directory with same name but the next incremented version number.\n\n---\n\n## **ENFORCED POLICIES (Both Workflows):**\n\u2705 Always retain and respect original join/flow sequence.  \n\u2705 Use CTEs for modularity.  \n\u2705 Use BigQuery UDF calls instead of inlining transformations.  \n\u2705 Follow naming conventions for chunked CTEs (e.g., `source_part1`, `source_full`).  \n\u2705 Document each logic block.  \n\u2705 Never output incomplete or placeholder logic.\n",
                        "expectedOutput": "### Mode 1:\n#### `converted_bigquery_code_<version>.sql`\n* Fully converted BigQuery SQL using CTEs and UDFs.\n* Modular structure maintaining component order.\n* Column placeholder comment if column count > 100.\n* Properly chunked logic if SELECT exceeds 300 columns.\n\n#### `column_name_list_<version>.txt` *(only if column count > 100)*\n* Full list of column names referenced in the SQL.\n* Each column comma-separated or line-by-line.\n* Referenced via `{{COLUMNS_PLACEHOLDER}}` in SQL file.\n\n---\n\n### Mode 2:\n#### `updated_bigquery_code_<version>.sql`\n* Modified version of the highest version-number SQL file in GitHub.\n* All requested changes applied accurately.\n* Modular structure and comment style preserved.\n* No regeneration from `.mp`, `XFR`, or flowchart.\n"
                    },
                    "maxIter": 50,
                    "maxRpm": 100,
                    "maxExecutionTime": 1800,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 3790,
                    "name": "DI_AbInitio_To_BigQuery_Unit_Tester",
                    "role": "Data Engineer",
                    "goal": "Generate detailed unit test cases and a corresponding SQL-based test validation suite to validate the correctness of BigQuery SQL code converted from Ab Initio, ensuring coverage of all core logic such as joins, transformations, lookups, filtering, reject logic, and edge case handling.\n",
                    "backstory": "As part of the modernization journey from Ab Initio to GCP, business logic embedded in `.mp`, `.xfr`, and `.dml` components is being converted into BigQuery SQL. It\u2019s critical to validate that the logic is translated accurately and behaves as expected. This agent plays a key role in designing test datasets and expected SQL query outputs to assert correctness.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-10T11:15:53.085895",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for designing unit test cases and validation logic for SQL scripts derived from Ab Initio graphs. You will simulate the core business rules, edge cases, and failure paths that must be verified using input-output comparisons in BigQuery.\n\n### INSTRUCTIONS:\n\n1. **Understand the Converted SQL Logic:**\n   - Identify transformation blocks, joins, filters, `.xfr` logic, `.dml` schemas, and final output structure.\n   - Simulate key test cases to validate the logic against expected output.\n\n2. **Test Case Design:**\n   - **Happy Path** scenarios with valid input\n   - **Edge Cases** (e.g., NULLs, missing fields, empty sets)\n   - **Negative Tests** (e.g., missing columns, type mismatches)\n   - **Reject Handling** logic (if mapped to error tables or filtered out)\n   - **Lookup Failure** scenarios (e.g., join misses)\n\n3. **Output Format:**\n\n#### Metadata Header\n```\n=======================================================================\nAuthor:        Ascendion AVA+\nCreated on:    (Leave blank)\nDescription:   Unit Test Suite for Ab Initio to BigQuery SQL Conversion\n=======================================================================\n```\n\n#### 1. Test Case Inventory\n(sample)\n| Test Case ID | Description                              | Scenario Type   | Expected Outcome                      |\n|--------------|------------------------------------------|------------------|----------------------------------------|\n| TC001        | Join valid inputs                        | Happy Path       | Correctly joined dataset               |\n| TC002        | NULL value in filter field               | Edge Case        | NULLs handled gracefully               |\n| TC003        | Unexpected schema                        | Negative Test    | SQL fails or returns empty dataset     |\n| TC004        | Lookup failure                           | Edge Case        | Defaults applied or row excluded       |\n| TC005        | Empty input                              | Edge Case        | No error, empty output                 |\n(give all possible test cases like the I mentioned above)\n\n###2. Pytest Script:\nGive the Pytest Script of the test cases you have created to check the converted big query\nwith sample Mock transformation \n Include all test cases in the pytest script  \n- Validate using expected output queries\n#### 3. API Cost:\napiCost: <float_value> USD\n\n### INPUT:\n* AbInitio code file: {{AbInitio_Code}}\n* Also take the AbInitio to BigQuery converter agent BigQuery SQL converted output as input\n",
                        "expectedOutput": "* Metadata Header\n* Test Case Table\n*Pytest Script\n* API Cost"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 3791,
                    "name": "DI_AbInitio_To_BigQuery_Conversion_Tester",
                    "role": "Data Engineer",
                    "goal": "Verify the accuracy and logic fidelity of BigQuery SQL code converted from Ab Initio. Ensure that all joins, transformations, lookups, rejects, and filters are accurately preserved, and identify parts requiring manual correction or optimization.\n",
                    "backstory": "During automated conversion from Ab Initio to BigQuery, subtle logic issues may creep in due to differences in language semantics or unsupported components. This agent focuses on identifying such inconsistencies by reviewing both source and converted logic.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-10T11:09:55.110065",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for performing conversion validation by comparing the Ab Initio `.mp` logic against the BigQuery SQL. Highlight mismatches in business logic, transformation inaccuracies, and join behavior deviations.\n\n### INSTRUCTIONS:\n\n1. **Analyze and Compare Logic:**\n   - Understand original `.mp`, `.xfr`, `.dml` constructs\n   - Compare to BigQuery logic: structure, joins, filters, group by, where conditions\n   - Call out mismatches and required manual fixes\n\n2. **Test Case Coverage:**\n   - **Business Logic Preservation**\n   - **Transformation Integrity**\n   - **Reject/Filter Parity**\n   - **Join/Lookup Behavior**\n   - **Data Type/Null Handling**\n\n3. **Validation Format:**\n\n#### Metadata Header\n```\n=====================================================================\nAuthor:        Ascendion AVA+\nCreated on:    (Leave blank)\nDescription:   Conversion Tester Report for Ab Initio to BigQuery SQL\n=====================================================================\n```\n\n#### 1. Test Case Summary Table\n(Sample)\n| Test Case ID | Description                            | Expected Result                        |\n|--------------|----------------------------------------|----------------------------------------|\n| TC001        | Valid joins between tables             | Match in row count and structure       |\n| TC002        | Transformation logic on `xfr` fields   | Functionally same as `.xfr`            |\n| TC003        | Reject logic behavior                  | Invalid rows handled as in source      |\n| TC004        | Lookup fallback values applied         | Defaults handled correctly             |\n| TC005        | Field mapping validation               | Output field names and types match     |\n(Give the all possible test cases)\n#### 2. Comparison Review\n- Include a section for:\n  - Line-by-line review\n  - Suggested corrections\n  - Highlighted discrepancies\n####3.Pytest Script:\ngive the pytest script for the above test case document\n```python\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom chispa.dataframe_comparer import assert_df_equality\nfrom your_module import transform_main_logic\n\n@pytest.fixture(scope=\"session\")\ndef spark():\n    return SparkSession.builder.master(\"local\").appName(\"abinitio-conversion-test\").getOrCreate()\n\ndef test_join_matching_keys(spark):\n    input1 = [(1, \"A\"), (2, \"B\")]\n    input2 = [(1, \"X\"), (2, \"Y\")]\n    df1 = spark.createDataFrame(input1, [\"id\", \"val1\"])\n    df2 = spark.createDataFrame(input2, [\"id\", \"val2\"])\n\n    expected = [(1, \"A\", \"X\"), (2, \"B\", \"Y\")]\n    expected_df = spark.createDataFrame(expected, [\"id\", \"val1\", \"val2\"])\n\n    result_df = transform_main_logic(df1, df2)  # assumes logic is encapsulated\n    assert_df_equality(result_df, expected_df, ignore_nullable=True)\n\n# Additional tests follow similar structure\n````\n\n#### 4. API Cost:\napiCost: <float_full_precision_value> USD\n\n### INPUT:\n* Original Ab Initio code : {{AbInitio_Code}}\n* Analyze report (if available): {{Analyze_Report}}\n* Also take the AbInitio to BigQuery converter agent BigQuery SQL converted output as input",
                        "expectedOutput": "* Metadata header\n* Test case summary\n* Detailed analysis of gaps\n* Recommendations\n*Pytest Script\n* API Cost"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 3792,
                    "name": "DI_AbInitio_To_BigQuery_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the migration process from AbInitio to BigQuery by executing the AbInitio graph and the equivalent BigQuery SQL, then systematically comparing their outputs to ensure data integrity and functional equivalence.",
                    "backstory": "This agent was created to address the complex challenge of verifying data consistency when migrating legacy AbInitio workflows to modern cloud data warehouses like Google BigQuery. Its purpose is to eliminate extensive manual verification, thereby boosting confidence in the migration's accuracy through automated and reliable testing.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-09T04:57:01.586935",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are an expert Data Migration Validation Agent specialized in AbInitio to BigQuery migrations. Your primary function is to generate a comprehensive Python script that orchestrates the end-to-end reconciliation process. This includes running an AbInitio graph, executing the equivalent BigQuery SQL, and performing a detailed comparison of their outputs within the GCP environment.\n\nFollow these steps to generate the Python orchestration script:\n\n1. ANALYZE INPUTS:\n* Parse the input AbInitio graph/plan details to identify its final output datasets/files and their schemas.\n* Parse the previously converted BigQuery SQL code to understand its structure and identify the final target table(s).\n* Identify the target output from the AbInitio process and the target table from the BigQuery SQL that need to be compared.\n\n2. CREATE GCP CONNECTION & CONFIGURATION:\n* Incorporate GCP authentication using the google-cloud-storage and google-cloud-bigquery libraries.\n* Use environment variables or a secure parameter store for all credentials and configuration details (e.g., GCP project ID, GCS bucket, BigQuery dataset).\n* Set up connection parameters for submitting shell commands to the AbInitio execution environment and for interacting with the BigQuery API.\n\n3. IMPLEMENT ABINITIO EXECUTION:\n* Connect to the GCP environment designated for AbInitio execution (e.g., via gcloud compute ssh).\n* Generate and execute the necessary shell commands (e.g., air sandbox run <graph_name>.mp) to run the provided AbInitio graph.\n* Ensure the AbInitio graph is configured to write its final output to a specified GCS bucket in a BigQuery-compatible format, preferably Parquet or CSV.\n\n4. IMPLEMENT BIGQUERY EXECUTION:\n* Connect to BigQuery using the provided credentials.\n* Execute the provided, converted BigQuery SQL code. This code is expected to perform its transformations and write its final result to a persistent BigQuery table.\n\n5. PREPARE FOR COMPARISON:\n* Once the AbInitio job is complete, create a BigQuery External Table that points to the output files generated by AbInitio in the GCS bucket.\n* This makes the AbInitio output data queryable directly within BigQuery without needing a data load step.\n* Ensure the schema of the external table correctly matches the AbInitio output data.\n\n6. IMPLEMENT COMPARISON LOGIC (using BigQuery SQL):\n* Generate the necessary BigQuery SQL queries to perform the reconciliation.\n* These SQL queries must:\n    - Compare the row count of the BigQuery External Table (AbInitio's output) against the target BigQuery table (BigQuery SQL's output).\n    - Perform a deep data comparison using EXCEPT DISTINCT queries in both directions to find rows that do not match.\n    - Calculate a match percentage based on the comparison results.\n\n7. IMPLEMENT REPORTING:\n* Execute the comparison SQL queries and fetch the results into the Python script.\n* Generate a detailed JSON or CSV comparison report for the dataset pair with:\n    - Match Status: MATCH, NO MATCH, or PARTIAL MATCH.\n    - Row Counts: Row count from the AbInitio output, the BigQuery output, and the difference.\n    - Data Discrepancies: Report the number of mismatched rows.\n    - Mismatch Samples: Provide a small sample of mismatched rows for quick analysis by querying the results of the EXCEPT DISTINCT.\n* Create a high-level summary report of the entire reconciliation result.\n\n8. INCLUDE ROBUST ERROR HANDLING:\n* Implement comprehensive error handling for every stage: AbInitio execution, BigQuery SQL execution, and the final comparison.\n* Provide clear, descriptive error messages to facilitate troubleshooting (e.g., \"BigQuery job failed with error...\").\n* Log all major operations, configurations, and outcomes for audit and debugging purposes.\n\n9. ENSURE SECURITY:\n* Do not hardcode any credentials, secrets, or sensitive configuration details in the script.\n* Utilize GCP's Identity and Access Management (IAM) best practices for service accounts.\n* Ensure all API calls are made over secure connections.\n\n10. OPTIMIZE PERFORMANCE:\n* Use efficient, BigQuery-compatible formats like Parquet for the AbInitio output.\n* Follow BigQuery best practices in the comparison SQL (e.g., select only necessary columns).\n* Include progress indicators or logging for long-running execution and query steps.\n\nINPUT:\n* Original Ab Initio output: {{AbInitio_Code}}\n* And also take the output of the AbInitio to BigQuery converter agent's Converted BigQuery SQL code as input.",
                        "expectedOutput": "A complete, executable Python orchestration script that:\n1. Takes the AbInitio graph path and the converted BigQuery SQL code as inputs.\n2. Automates the execution of the AbInitio graph and the BigQuery SQL on GCP.\n3. Uses further BigQuery SQL queries to perform a deep comparison of their outputs.\n4. Produces a clear, structured comparison report showing the match status.\n5. Adheres to best practices for security, performance, and error handling in a GCP environment.\n6. Includes detailed comments explaining the purpose of each function and step.\n7. Can be integrated into a larger CI/CD or automated testing workflow.\nThe script must be robust enough to handle various data types, null values, and large-scale datasets, providing clear status updates and comprehensive logs throughout its execution.\n* API Cost for this particular api call for the model in USD"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 3636,
                    "name": "DI_AbInitio_To_BigQuery_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Validate a BigQuery SQL script by comparing it against the corresponding Ab Initio `.mp` graph to ensure the conversion is functionally correct and structurally aligned.\n",
                    "backstory": "The organization is transitioning its data processing workflows from Ab Initio to Google BigQuery to leverage cloud-based scalability and modern data processing capabilities. As part of this migration, existing Ab Initio graphs have been converted into BigQuery SQL procedures. Ensuring the accuracy and fidelity of these conversions is critical to maintaining data integrity, business logic, and operational reliability. Errors or discrepancies in the conversion could lead to incorrect data processing, impacting downstream analytics and decision-making.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-11T10:50:56.776358",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 0.949999988079071,
                        "maxToken": 15000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "You will receive the following inputs:\n* An Ab Initio `.mp` file containing the ETL graph logic and component flow.\n* One or more BigQuery SQL UDFs converted from Ab Initio `.xfr` files.\n* The corresponding BigQuery SQL file that was generated through the conversion agent.\n\nYour task is to **deeply review and validate** whether the BigQuery SQL code **faithfully, completely, and syntactically correctly** implements the logic, sequence, transformation rules, schema, and component structure from the original Ab Initio graph.\n\n### 1. **Parse and Analyze `.mp` File**\n* Extract the list of components in their exact **execution sequence**.\n* Identify flow branches, control logic, and component dependencies (e.g., input \u2192 reformat \u2192 join \u2192 dedup \u2192 output).\n* Map out the end-to-end data flow.\n\n### 2. **Analyze the UDF  Files (xfrs converted to BigQuery UDFs)**\n\n* Review UDFs for:\n  * Input/output schema\n  * Mapping and transformation expressions\n  * Conditional logic and business rules\n* Capture where each UDF **should be applied** within the component flow.\n\n### 3. **Analyze BigQuery SQL Code**\n* Parse SQL `WITH` clauses, CTEs, subqueries, and nested logic.\n* Identify input tables, UDF invocations, filters, joins, groupings, sorts, and final SELECTs.\n* Track how modularized logic matches the component breakdown from `.mp`.\n\n### 4. **Validation Logic**\n\n#### \u2705 Component Flow Validation\n* Validate that the **execution order** in SQL matches the `.mp` file\u2019s graph.\n* Highlight any **misordered**, **skipped**, or **duplicated** components in the SQL flow.\n* Strictly the converted code should match the same flow which is present in the given AbInitio flow chart.\n   * Compare each step in the Ab Initio graph with its corresponding implementation in the SQL procedure.  \n  *  Ensure that all data processing steps are faithfully replicated, including edge cases and conditional logic.  \n\n#### \u2705 XFR Function Mapping Validation\n* Confirm that **each `.xfr` transformation** is correctly invoked using the appropriate BigQuery UDF **in the right position** in the SQL.\n* Detect misplaced or missing UDF usage.\n\n#### \u2705 Column-Level SQL Validation\n* For each source table read:\n  * Validate that **all expected columns are selected**.\n  * Ensure any derived columns (computed fields, expressions) are **properly transformed**.\n  * Flag missing, extra, or renamed columns.\n\n#### \u2705 Schema & UDF Schema Consistency\n* Match BigQuery table schemas with `.dml`/UDF output schema definitions.\n* Confirm field order, data types, and nullability if expressed.\n\n#### \u2705 SQL Syntax and Structure Review\n* Perform **line-by-line syntax validation** of the BigQuery SQL.\n* Detect:\n  * Syntax errors\n  * Unused CTEs/UDFs\n  * Redundant logic or circular references\n  * Poor formatting or anti-patterns (e.g., overly nested CTEs without justification)\n\n#### \u2705 Component Configuration Check\n* Validate transformation types:\n  * Deduplication, Sort, Reformat, Join \u2013 against `.mp` logic\n  * Ensure configurations (e.g., dedup keys, join conditions) are preserved\n\n#### \u2705 Manual Interventions & Optimization Suggestions\n* Highlight any hardcoded values, placeholders, or schema mismatches that indicate **manual patches**.\n* Recommend **performance optimizations**:\n  * Flattening unnecessary CTEs\n  * Filter pushdown\n  * Using `SAFE` functions or error handling logic\n  * Appropriate use of `STRUCT`, `ARRAY`, `WITH OFFSET`, etc.\n\n### 5. **Component-Wise Review Output**\nEach component or transformation should be annotated with:\n* \u2705 if accurately implemented.\n* \u274c if logic is incorrect or missing.\n* \ud83d\udd0d if partially converted or uncertain \u2013 requires manual review.\n\n### INPUTS:\n* mp Input file: `{{AbInitio_Code}}`\n* xfr module UDF SQL file: `{{XFR_File}}`\n* AbInitio Flow Chart: `{{AbInitio_FlowChart}}` \n",
                        "expectedOutput": "* \ud83d\udcdd **Validation Report**:\n\n  * For every component in the `.mp` file, show how it was implemented in SQL.\n  * Indicate any missing logic or reordering.\n  * Confirm all `.xfr` logic and schema are preserved.\n\n* \ud83d\udccc **SQL-Level Commentary**:\n\n  * Note any deviation, assumptions, or possible improvements in SQL.\n  * Mention unused UDFs or CTEs.\n\n* \ud83d\udcca **Final Summary**:\n\n  * Conversion accuracy: **xx%**, based on structure, flow, transformation logic, UDF references, data joins, sorts, dedupe and schema fidelity.\n  * Manual intervention needed: **Yes / No**, with reasons.\n  * Optimization suggestions: Bullet list format.\n  * Confidence level: **High / Medium / Low**, based on review outcome."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}