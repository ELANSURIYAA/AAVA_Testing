{
    "pipeline": {
        "pipelineId": 972,
        "name": "DAG Generator",
        "description": "Generate complete, functional Airflow DAG Python files based on structured parameter specifications.",
        "createdAt": "2025-03-10T15:46:51.393+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1251,
                    "name": "DAG Generator",
                    "role": "Data Engineer",
                    "goal": "Create an Apache Airflow Python DAG file that orchestrates Microsoft Fabric notebooks in the correct sequence with proper dependencies and scheduling configuration. The DAG should handle the entire data pipeline from source to bronze, bronze to silver, and silver to gold layers.",
                    "backstory": "Our data engineering team needs to migrate from manual execution of Microsoft Fabric notebooks to automated orchestration using Airflow to improve reliability, monitoring, and scalability of our ETL processes while maintaining the existing data flow architecture.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-05T07:29:34.102984",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "You are an expert Airflow DAG developer. I will provide you with a Microsoft Fabric Notebook Configuration file containing details about notebooks, their dependencies, scheduling requirements, and environment configurations. Your task is to generate a complete and production-ready Airflow Python DAG file based on this input.\n\nFollow these instructions and guidelines:\n\n1. **DAG Structure:**\n   - Create a properly structured Airflow DAG with an appropriate DAG ID derived from the purpose of the pipeline\n   - Set the correct schedule_interval using the provided scheduling information (4 AM EST)\n   - Configure appropriate DAG default arguments including retries, retry_delay, email notifications, etc.\n   - Add detailed documentation in docstrings\n\n2. **Task Creation:**\n   - Generate a task for each notebook in the configuration file\n   - Use the appropriate Airflow operator (DatabricksRunNowOperator or DatabricksSubmitRunOperator)\n   - Include all necessary parameters for each task (notebook_path, existing_cluster_id, etc.)\n   - Set timeout values based on the expected runtime of each notebook\n\n3. **Dependencies:**\n   - Establish task dependencies based on the dependency information in the input file\n   - Use set_upstream() or >> operator to define the correct execution order\n   - Create a proper directed acyclic graph that follows the data flow (source \u2192 bronze \u2192 silver \u2192 gold)\n\n4. **Environment Configuration:**\n   - Use Airflow variables or connections to store sensitive information\n   - Reference environment details from the configuration file (cluster IDs, database connections, etc.)\n   - Implement proper error handling and logging\n\n5. **Monitoring and Alerting:**\n   - Add appropriate SLAs for critical tasks\n   - Configure failure callbacks for important notification points\n   - Include monitoring tasks if necessary\n\n6. **Code Quality:**\n   - Follow PEP 8 style guidelines\n   - Add comprehensive comments explaining complex logic\n   - Use constants for repeated values\n   - Structure the code in a modular and maintainable way\n\n7. **Additional Features:**\n   - Implement proper error handling for task failures\n   - Add conditional logic if certain prerequisites need to be checked\n   - Include data quality checks if specified in the configuration\n\nINPUT:\n* For input Airflow DAG Python files parameter specifications use this file : ```%1$s```",
                        "expectedOutput": "A complete, production-ready Airflow Python DAG file (.py) with the following components:\n\n1. Proper imports and module dependencies\n2. DAG definition with appropriate scheduling and default arguments\n3. Task definitions for all Microsoft Fabric notebooks\n4. Correctly established task dependencies reflecting the data pipeline flow\n5. Environment configuration integrated with Airflow's variable/connection system\n6. Well-documented code with clear comments\n7. Error handling and logging mechanisms\n8. Monitoring and alerting setup\n\nThe code should be ready to deploy to an Airflow environment with minimal modifications and should follow all best practices for Airflow DAG development.\n\n## API cost for this particular api call to the model, api cost in USD"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}