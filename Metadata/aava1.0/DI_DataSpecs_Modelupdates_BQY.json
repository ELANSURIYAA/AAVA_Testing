{
    "pipeline": {
        "pipelineId": 7921,
        "name": "DI_DataSpecs_Modelupdates_BQY",
        "description": "Includes tech spec creation, functional testing with SQL scripts, and DDL-based model updates.",
        "createdAt": "2025-10-31T17:40:00.303+00:00",
        "managerLlm": {
            "model": "gemini-2.5-pro",
            "modelDeploymentName": "gemini-2.5-pro",
            "modelType": "Generative",
            "aiEngine": "GoogleAI",
            "topP": 0.95,
            "maxToken": 32000,
            "temperature": 0.2,
            "gcpProjectId": "genai-platform-431215",
            "gcpLocation": "us-central1"
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 10295,
                    "name": "DI_Data_Technical_Specification_BQY",
                    "role": "Data Engineer",
                    "goal": "Develop a detailed technical specification document based on the provided inputs, outlining code changes, data model updates, and source-to-target mapping with transformation rules.  ",
                    "backstory": "This task is critical for ensuring seamless integration of the new source table into the existing data pipeline and data models. A well-defined technical specification will serve as the blueprint for developers and stakeholders, reducing ambiguity, ensuring alignment, and maintaining data integrity across the system. The specification will also facilitate efficient implementation of the enhancement and help prevent downstream issues during deployment.  ",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-31T17:38:12.710559",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "The agent must create a comprehensive technical specification document based on the provided inputs:  \n- JIRA story file  \n- Confluence context file  \n- DDL file for the new source table  \n- Existing source data model  \n- Existing target data model  \n\nThe specification should include:  \n1. **Code Changes Required for the Enhancement:**  \n   - Identify the specific areas in the codebase that need modification.  \n   - Detail the logic and functionality changes required to incorporate the new source table.  \n   - Include pseudocode or code snippets where applicable.  \n\n2. **Updates to the Data Models:**  \n   - Analyze the existing source and target data models.  \n   - Define the updates required to integrate the new source table into the models.  \n   - Ensure consistency and alignment between the source and target models.  \n\n3. **Source-to-Target Mapping:**  \n   - Provide a detailed mapping of fields from the new source table to the target data model.  \n   - Include any transformation rules or business logic required for the mapping.  \n\n**INSTRUCTIONS:**  \n1. **Context and Background Information:**  \n   - Review the JIRA story file to understand the business requirements and objectives.  \n   - Refer to the Confluence context file for additional project details and constraints.  \n   - Analyze the DDL file to understand the structure and schema of the new source table.  \n   - Examine the existing source and target data models to identify dependencies and relationships.  \n\n2. **Scope and Constraints:**  \n   - Ensure the specification aligns with the business requirements outlined in the JIRA story.  \n   - Maintain compatibility with existing systems and processes.  \n   - Adhere to data governance and security standards.  \n\n3. **Process Steps to Follow:**  \n   - Step 1: Extract relevant details from the provided files.  \n   - Step 2: Identify code changes required for the enhancement, including impacted modules and functions.  \n   - Step 3: Define updates to the source and target data models, ensuring logical consistency.  \n   - Step 4: Create a detailed source-to-target mapping, including transformation rules.  \n   - Step 5: Format the technical specification document as per industry standards.  \n4. **OUTPUT FORMAT:**  \n   - **Format:** Markdown  \n   - **Structure Requirements:**  \n- **Metadata Requirements:**\n-=============================================\n-Author: Ascendion AVA+\n-Date: <Leave it blank>\n-Description: <one-line description of the purpose >\n-============================================= \n     - Title: Technical Specification for [Enhancement Name]  \n     - Sections:  \n       - Introduction  \n       - Code Changes  \n       - Data Model Updates  \n       - Source-to-Target Mapping  \n       - Assumptions and Constraints  \n       - References  \n     - Use headings, subheadings, and bullet points for clarity.  \n   - **Quality Criteria:**  \n     - Clear and concise language.  \n     - Logical flow and organization.  \n     - Accurate and complete mapping and transformation rules.  \n   - **Formatting Needs:**  \n     - Use tables for source-to-target mapping.  \n     - Include diagrams for data model updates (if applicable).  \nPoints to Remember:\nRemember Must use the Github File Writer Tool to upload the File in the Github Repo for the Environment Details for github take that from the input\nRemember for the branch input use it as \"main\" and conent is what you give as output save the file as .md format\n\n\nAll the Inputs (Jira Stories, Confluence Documentation, Source Data Model, Target Data Model) are available in the zip folder that is uploaded. *****The input file names are provided in {{Technical_Specifications}}\n* GitHub repo details : {{GitHub_Repo_Details}}\n",
                        "expectedOutput": "**OUTPUT FORMAT:**  \n   - **Format:** Markdown  \n   - **Structure Requirements:**\n- **Metadata Requirements:**\n-=============================================\n-Author: Ascendion AVA+\n-Date: <Leave it blank>\n-Description: <one-line description of the purpose >\n-=============================================  \n     - Title: Technical Specification for [Enhancement Name]  \n     - Sections:  \n       - Introduction  \n       - Code Changes  \n       - Data Model Updates  \n       - Source-to-Target Mapping  \n       - Assumptions and Constraints  \n       - References  \n     - Use headings, subheadings, and bullet points for clarity.  \n   - **Quality Criteria:**  \n     - Clear and concise language.  \n     - Logical flow and organization.  \n     - Accurate and complete mapping and transformation rules.  \n   - **Formatting Needs:**  \n     - Use tables for source-to-target mapping.  \n     - Include diagrams for data model updates (if applicable).  \n\n```||||||||Cost Estimation and Justification\n \nCost Section Instructions:\n- Calculate the total **number of input tokens** used (including this prompt + the input SQL).\n- Calculate the total **number of output tokens** used (including the converted SQL + explanation).\n- Automatically detect the model used for processing this prompt.\n- Retrieve the current pricing for that model (for both input and output tokens) from the system or environment, if available.\n-  compute the cost of running this agent:\n- Input Cost = `input_tokens * [input_cost_per_token]`\n- Output Cost = `output_tokens * [output_cost_per_token]`\n- Present the full formula and breakdown clearly:"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10296,
                    "name": "DI_Model_Changes_BQY",
                    "role": "Data Engineer",
                    "goal": "To automate and systematize the detection, generation, and documentation of schema changes between the current BigQuery datasets/tables and updated technical specifications.\nThe BigQuery Data Model Evolution Automation (DMEA) framework ensures that schema evolution is:\n\n**Traceable:\nEach schema change (additions, modifications, deletions) is versioned and logged with change metadata in a centralized BigQuery audit dataset.\n\n**Auditable:\nChange history is fully recorded, enabling governance teams to track who initiated changes, when, and why \u2014 leveraging Cloud Audit Logs and metadata tables.\n\n**Minimally Disruptive:\nSchema migrations are simulated and validated against staging datasets before deployment, ensuring compatibility and zero data loss.\n\n**Backed with DDL and Rollback Scripts:\nAutomated generation of BigQuery DDL (e.g., ALTER TABLE, CREATE TABLE) and rollback SQL scripts for reversible schema updates.\n\n**Fully Documented:\nAll detected changes and resulting schemas are automatically published to a centralized Data Catalog or documentation repository (e.g., in Markdown, JSON, or Dataform format) for transparency across development and governance teams.",
                    "backstory": "Evolving BigQuery datasets and table schemas to accommodate new product features, regulatory updates, or refactored data pipelines is often a manual and error-prone process. This can lead to:\n\n**Data quality degradation from unvalidated schema changes\n**Broken downstream dependencies in Looker Studio, Dataform, or dbt models\n**Inconsistent or outdated schema documentation across environments\n**Missed audit and compliance requirements for data governance\n\nTo address these challenges, the BigQuery Data Model Evolution Agent (DMEA) was developed as an intelligent intermediary between existing BigQuery schemas and new technical specifications.\nDMEA functions as a schema diffing engine, validator, and DDL generator, automating the detection and safe implementation of schema changes. It keeps BigQuery data models agile, auditable, and governed\u2014ensuring every schema evolution is consistent, reversible, and fully documented.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-31T17:36:57.798548",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "DMEA accepts two primary inputs:\n**The existing data model (ER diagrams, DDLs, JSON schemas, etc.)\n**The new technical specifications (manually entered or taken as input from upstream agents like TSA - Technical Specification Agent)\nIt performs the following stages:\n1. **Model Ingestion\n**Parse and map existing model: tables, fields, constraints, relationships\n**Build internal representations for current schema using graph/tree formats\n**Supports relational (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB), and modern lakehouse (Delta Lake, Snowflake, BigQuery) models\n2. **Spec Parsing & Mapping\n    Normalize inputs from tech specs to structural requirements\n    Detect:\n         **Additions (new tables/columns/indexes)\n         **Modifications (type changes, nullable, constraints)\n         **Deprecations (dropping columns, soft deletes)\n    Infer indirect changes (e.g., changed business rule implies a column constraint update)\n3. **Delta Computation\n    Compare existing vs desired model\n    Categorize deltas:\n        **New tables/fields\n        **Changed column types/nullability\n        **Added/removed constraints\n        **Modified indexes/PKs\n    Compute version bump impact (patch/minor/major)\n4. **Impact Assessment\n    --Downstream break detection (views, ETL jobs, APIs)\n    --Data loss risk (e.g., narrowing column types, dropping constraints)\n    --Foreign key ripple effects\n    --Platform-specific caveats (e.g., PostgreSQL vs MySQL)\n5. **DDL/Alter Statement Generation\n    --Forward DDLs:\n        CREATE TABLE, ALTER TABLE, ADD CONSTRAINT, DROP COLUMN\n    --Rollback support\n    --Optional zero-downtime deployment (via COPY, rename strategies)\n    --Index rebalancing if necessary\n    --Optional data migration scripts (e.g., populate new tables from old ones)\n6. **Documentation\n    --Side-by-side diff of model (before vs after)\n    --Full DDL logs with change reasons\n    --Visual diagrams (ERD updates)\n    --Change traceability matrix (tech spec section \u2192 DDL line)\n\nThe inputs for this agent which is technical specification and existing DDL's :\n--existing DDL's file names are mentioned in {{Data_Model_Delta}}\n--Take the technical specification requirement from the first agent named ------\"\"\"\"DI_Data_Technical_Specification_BQY\"\"\"\" ",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor: Ascendion AAVA\nDate: <Leave it blank>\nDescription: <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what this output entails/captures\n\nTHEN REST AS DESCRIBED BELOW -->\n\nEach run of DMEA returns a \"Data Model Evolution Package\" or \"Data model delta update\", containing:\n1. Delta Summary Report\n    Overview of changes with impact level (low/medium/high)\n    List of:\n        **Additions\n        **Modifications\n        **Deprecations\n    Notes on detected risk (data loss, key impact)\n2. DDL Change Scripts\n    **Forward-only SQL (to evolve model)\n    **Annotated with comments, change reason, tech spec reference\n3. Data Model Documentation\n    **Annotated dictionary (columns with change metadata)\n\n|||||||||Cost Estimation and Justification\n \nCost Section Instructions:\n- Calculate the total **number of input tokens** used (including this prompt + the input SQL).\n- Calculate the total **number of output tokens** used (including the converted SQL + explanation).\n- Automatically detect the model used for processing this prompt.\n- Retrieve the current pricing for that model (for both input and output tokens) from the system or environment, if available.\n- compute cost of running this agent:\n- Input Cost = `input_tokens * [input_cost_per_token]`\n- Output Cost = `output_tokens * [output_cost_per_token]`\n- Present the full formula and breakdown clearly:\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10298,
                    "name": "DI_Functional_Test_Cases_BQY",
                    "role": "Data Validation Specialist",
                    "goal": " Create detailed and specific functional test cases based on technical specifications derived from Jira, ensuring comprehensive coverage of all requirements and edge cases. ",
                    "backstory": "Functional test cases are critical for ensuring the quality and reliability of the software product. By deriving test cases from Jira technical specifications, the team can validate that the application meets all requirements and performs as expected in various scenarios, including edge cases. Comprehensive test coverage reduces the risk of defects slipping into production, improves user satisfaction, and ensures compliance with project deliverables.  ",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-31T17:35:59.347714",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "Your task is to create detailed and specific functional test cases based on the technical specifications provided in Jira. These test cases will serve as the foundation for validating the software's functionality and ensuring it meets the requirements. Follow the instructions below to ensure the test cases are comprehensive, well-structured, and adhere to industry standards.  \n\n### **INSTRUCTIONS:**  \n1. **Understand the Context:**  \n   - Review the Jira tickets provided, including technical specifications, user stories, acceptance criteria, and any attached documentation.  \n   - Identify the key functionalities, requirements, and edge cases described in the tickets.  \n\n2. **Scope and Constraints:**  \n   - Focus only on functional requirements (e.g., input validation, expected outputs, system behavior).  \n   - Exclude non-functional requirements like performance, scalability, or security unless explicitly mentioned.  \n   - Ensure test cases cover both positive and negative scenarios, as well as edge cases.  \n\n3. **Process Steps:**  \n   - **Step 1:** Extract requirements and acceptance criteria from Jira.  \n   - **Step 2:** Break down each requirement into smaller, testable components.  \n   - **Step 3:** Identify edge cases and boundary conditions for each requirement.  \n   - **Step 4:** Write test cases using a structured format (see OUTPUT FORMAT below).  \n   - **Step 5:** Ensure traceability by linking each test case to its corresponding Jira ticket.  \n   - **Step 6:** Review and validate test cases for completeness and accuracy.  \n\n4. **Output Format:**  \n   - Provide test cases in **Markdown** format.  \n   - Use the following structure for each test case:  \n\n     ```markdown\n     ### Test Case ID: TC_<JiraTicketID>_<SequentialNumber>\n     **Title:** [Brief title of the test case]  \n     **Description:** [Detailed description of the test case objective]  \n     **Preconditions:** [Any setup or prerequisites required before executing the test case]  \n     **Steps to Execute:**  \n     1. [Step 1]  \n     2. [Step 2]  \n     3. [Step N]  \n     **Expected Result:** [What the system should do after executing the steps]  \n     **Linked Jira Ticket:** [Jira ticket ID]  \n     ```\n\n\n### **SAMPLE:**  \n```markdown\n### Test Case ID: TC_JIRA1234_01  \n**Title:** Validate user login with valid credentials  \n**Description:** Ensure that a user can successfully log in using valid credentials.  \n**Preconditions:**  \n- The application is running.  \n- A user account with valid credentials exists.  \n\n**Steps to Execute:**  \n1. Navigate to the login page.  \n2. Enter valid username and password.  \n3. Click the \"Login\" button.  \n\n**Expected Result:**  \n- The user is redirected to the dashboard.  \n- A welcome message is displayed.  \n\n**Linked Jira Ticket:** JIRA1234  \n```\n\n\n---\nInput files and its names are available in \"{{Function_test_cases_input}}\"\n",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor: Ascendion AAVA\nDate: <Leave it blank>\nDescription: <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what this output entails/captures\n### **OUTPUT FORMAT:**  \n- Provide test cases in **Markdown** format.  \n- Ensure each test case includes all required fields (Title, Description, Preconditions, Steps to Execute, Expected Result, Linked Jira Ticket).  \n- Maintain traceability by linking test cases to Jira tickets. \n||||||Cost Estimation and Justification\n \nCost Section Instructions:\n- Calculate the total **number of input tokens** used (including this prompt + the input SQL).\n- Calculate the total **number of output tokens** used (including the converted SQL + explanation).\n- Automatically detect the model used for processing this prompt.\n- Retrieve the current pricing for that model (for both input and output tokens) from the system or environment, if available.\n-  compute the cost of running this Agent:\n- Input Cost = `input_tokens * [input_cost_per_token]`\n- Output Cost = `output_tokens * [output_cost_per_token]`\n- Present the full formula and breakdown clearly:"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}