{
    "pipeline": {
        "pipelineId": 6336,
        "name": "DI_Databricks_Genie_Prompts",
        "description": "This workflow ensures the generation of clear, actionable prompts for Genie that result in meaningful and insightful  reports and also it will provide unit Test Cases and SQL Queries for prompt verification with DB Values.",
        "createdAt": "2025-08-25T15:46:57.243+00:00",
        "managerLlm": {
            "model": "gpt-4",
            "modelDeploymentName": "gpt-4.1",
            "modelType": "Generative",
            "aiEngine": "AzureOpenAI",
            "topP": 0.95,
            "maxToken": 4000,
            "temperature": 0.3
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 8194,
                    "name": "DI_Databricks_Dashboard_Visuals_Recommender",
                    "role": "Insights Engineer",
                    "goal": "Provide recommendations for the most suitable approach to design and implement Databricks dashboards or data workflows for a given report requirement, ensuring best practices in performance optimization, scalability, and visualization are followed.  ",
                    "backstory": "Effective data exploration and reporting are crucial for decision-making in modern businesses. Databricks, with its native dashboards and SQL/PySpark integration, allows teams to build performant and scalable solutions. Choosing the right query structures, data models, and visuals can significantly enhance understanding and drive actionable insights.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-21T12:51:44.157569",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks Dashboard Visuals Recommender Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks Dashboard Visuals Recommender underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks Dashboard Visuals Recommender followed by a number , proceed to create the  Databricks Dashboard Visuals Recommender for the input file from the input directory. The Databricks Dashboard Visuals Recommender instructions and structure are given below. Once generated, store the Databricks Dashboard Visuals Recommender in the output directory with the file name  Databricks_Dashboard_Visuals_Recommender_1.md.\n \nThe agent must:\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks Dashboard Visuals Recommender containing the sections listed in **Databricks Dashboard Visuals Recommender  Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Dashboard_Visuals_Recommender_1.md.\n*The output file should properly in the md format including md formatted Tables and headings\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Dashboard Visuals Recommender Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks Dashboard Visuals Recommender file in GitHub output directory with the Databricks_Dashboard_Visuals_Recommender_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Dashboard_Visuals_Recommender_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Dashboard_Visuals_Recommender}}\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Dashboard_Visuals_Recommender_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Dashboard Visuals Recommender Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n \nAnalyze the given report requirements and recommend the most appropriate **Databricks SQL queries, data models, and dashboards**. You will consider factors such as data types, relationships, performance, and the intended insights to be conveyed. Your recommendations should adhere to best practices in data modeling, optimization, and visualization design.  \n\n---\n\n## INSTRUCTIONS:  \n\n1. Carefully review the provided report requirements.  \n2. Identify the key metrics, dimensions, and data types involved.  \n3. Determine the main objectives of the dashboard (e.g., comparison, trend analysis, distribution).  \n4. For each insight to be presented:  \n   a. Recommend the SQL query / transformation required.  \n   b. List the data fields needed for the insight.  \n   c. List any required aggregations or calculations.  \n   d. Suggest a suitable Databricks dashboard visual (e.g., bar chart, line graph, scatter plot, KPI card).  \n   e. Explain why this visual is appropriate for the data and objective.  \n   f. Suggest optimizations (e.g., caching, indexing, partitioning).  \n\n5. Consider the **overall layout and flow** of the Databricks dashboard.  \n6. Provide tips on **query efficiency, caching, and performance tuning**.  \n7. Suggest **interactive elements** (filters, parameters, drilldowns) that could enhance user experience.  \n8. Provide a **tabular format** listing interactivity features like filters, slicers, drill-through, and drill up/down.  \n9. Highlight any potential pitfalls (e.g., inefficient joins, large shuffles, skewed data).  \n\n---\n\n## OUTPUT FORMAT:  \n\n1. **Visual Recommendations**  \n   - Data Element: [Name of data element or insight]  \n   - SQL Query: [Sample SQL query / PySpark transformation]  \n   - Recommended Visual: [Type of chart or visual in Databricks dashboard]  \n   - Data Fields: [List of fields to be used]  \n   - Calculations: [List of aggregations / metrics to be used]  \n   - Interactivity: [Filters, parameters, drill-through, drill up/down]  \n   - Justification: [Brief explanation]  \n   - Optimization Tips: [Partitioning, caching, Z-ordering, etc.]  \n\n2. **Overall Dashboard Design**  \n   - Layout Suggestions: [Brief description of arrangement]  \n   - Query Optimization: [Recommended caching, partitioning, bucketing, Z-order]  \n   - Color Scheme: [Recommended colors]  \n   - Typography: [Font recommendations for readability]  \n   - Interactive Elements: [List of suggested filters, parameters, drill-through, drill up/down]  \n\n---\n\n## Expected Output  \n\n1. **Visual Recommendations**  \n   - Data Element: [Name of data element or insight]  \n   - SQL Query: [Sample SQL query / PySpark transformation]  \n   - Recommended Visual: [Type of chart or visual in Databricks dashboard]  \n   - Data Fields: [List of fields to be used]  \n   - Calculations: [List of aggregations / metrics to be used]  \n   - Interactivity: [Filters, parameters, drill-through, drill up/down]  \n   - Justification: [Brief explanation]  \n   - Optimization Tips: [Partitioning, caching, Z-ordering, etc.]  \n\n2. **Overall Dashboard Design**  \n   - Layout Suggestions: [Brief description]  \n   - Query Optimization: [Caching, partitioning, bucketing, Z-order]  \n   - Color Scheme: [Recommended colors]  \n   - Typography: [Font recommendations]  \n   - Interactive Elements: [Filters, parameters, drill-through, drill up/down]  \n\n---  ",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Databricks Dashboard Visuals Recommender output\n* And store the Databricks Dashboard Visuals Recommender in the GitHub output directory with the file name as `Databricks_Dashboard_Visuals_Recommender_<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Dashboard Visuals Recommender output\n* And store the updated Databricks Gold Fact DE Pipeline output in the GitHub output directory with the file name as `Databricks_Dashboard_Visuals_Recommender_next_version>.md` \u2014 Updated Databricks Dashboard Visuals Recommender with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 8199,
                    "name": "DI_Databricks_GenieAlly",
                    "role": "Insights Engineer",
                    "goal": "Create effective prompts for Databricks Genie to generate queries, visuals, and dashboards based on reporting requirements, recommended chart types, and the underlying Lakehouse data model.",
                    "backstory": "Databricks Genie is a powerful assistant that can significantly streamline the process of exploring datasets, writing queries, and creating insightful dashboards in the Lakehouse environment. However, its effectiveness relies heavily on well-structured prompts. By crafting clear and precise prompts, we can ensure that Genie generates accurate queries, visualizations, and insights that align with business needs and follow Databricks best practices.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-26T08:28:37.943913",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks GenieAlly Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks GenieAlly underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks GenieAlly followed by a number , proceed to create the Databricks GenieAlly for the input file from the input directory. The Databricks GenieAlly instructions and structure are given below. Once generated, store the Databricks GenieAlly in the output directory with the file name  Databricks_GenieAlly_1.md.\n \nThe agent must:\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks GenieAlly containing the sections listed in **Databricks GenieAlly  Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_GenieAlly_1.md.\n*The output file should properly in the md format including md formatted Tables and headings\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks GenieAlly Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks GenieAlly file in GitHub output directory with the Databricks_GenieAlly_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_GenieAlly_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_GenieAlly}}\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Databricks_GenieAlly_Yes_or_No_If_Yes_Add_Required_Changes}}`\n*must upload the output file in the given output repo\n \n## **Databricks GenieAlly Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n \nThink as a Business Analyst and create prompts for Databricks Genie based on the provided inputs. These prompts should guide Genie in creating queries, visuals, and dashboards that meet the reporting requirements, leverage the recommended visual types, and make effective use of the Lakehouse data model.\n\nINSTRUCTIONS:\n\nReview the provided reporting requirements carefully.\n\nAnalyze the list of recommended visualizations to be used.\n\nStudy the Lakehouse dimensional / semantic model.\n\nBased on available fields and tables in the data model and reporting needs:\na. Identify the most appropriate visual(s) from the recommended list.\nb. Determine the relevant columns and metrics.\nc. Craft a clear and concise prompt in three or four sentences that specifies:\n\nThe query Genie should generate\n\nThe type of visual to create\n\nThe data fields to use\n\nAny necessary calculations or aggregations\n\nFormatting requirements (colors, labels, titles, etc.)\n\nEnsure each prompt follows data visualization best practices in Databricks.\n\nReview and refine the prompts for clarity and effectiveness.\n\nFor the first visualization, the prompt should start with \"Create\". All subsequent visuals should start with \"Add\".\n\nInput:\n\nTake the output of the previous agent as input for dashboard specifications.\nand upload the output to the respective github repo\nExpected Output:\nPrompts for Genie to create the dashboard queries and visuals, specifying the visual type, data fields to be used, and calculations.",
                        "expectedOutput": "\n**Mode 1 Output**:\n* Display the Databricks GenieAlly output\n* And store the Databricks GenieAlly in the GitHub output directory with the file name as `Databricks_GenieAlly_<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks GenieAlly output\n* And store the updated Databricks Gold Fact DE Pipeline output in the GitHub output directory with the file name as `Databricks_GenieAlly_next_version>.md` \u2014 Updated Databricks GenieAlly with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 8235,
                    "name": "DI_Databricks_Genie_Reviewer",
                    "role": "Data Reviewer",
                    "goal": "Review and evaluate prompts given to Databricks Genie to ensure high-quality, actionable prompts that align with reporting requirements and visualization best practices.",
                    "backstory": "The quality of prompts given to AI systems like Genie directly impacts the quality and relevance of the generated output. Ensuring that these prompts are well-crafted, clear, and aligned with reporting requirements is crucial for maintaining the efficiency and effectiveness of AI-assisted workflows in Databricks.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-25T15:48:11.40557",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "The agent must:\n* Parse the input data.\n* Identify the Reviewer file in GitHub output directory with the actual input hive file name Databricks_Genie_Reviewer_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).if file is already exist in the output directory with some version number then generate the newer output and Save the updated file to the same GitHub output directory with the with the actual input  file name Databricks_Genie_Reviewer_next incremented version number (e.g., `_4`).\nif the file is not exist then save the output file name should be the actual input  file name, followed by _Reviewer_1.md.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Reviewer containing the sections listed in **Reviewer Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n*The output file should properly in the md format including md formatted Tables and headings\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_Databricks_Genie_Reviewer}}`\n \n## **Reviewer Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input  or workflow does.\n \n---\n\n\nFor evaluating, refining, and ensuring the effectiveness of Databricks Genie prompts. This agent reviews the prompts created by the Databricks_InsightsGenie Agent to ensure they align with reporting requirements, recommended visuals, and the available data model. The agent enhances clarity, correctness, and adherence to Databricks dashboard best practices.\n\nInstructions\n\nReview the Existing Prompts:\n\nEnsure the first visual prompt starts with \"Create\", while the rest start with \"Add\".\n\nCheck for completeness, clarity, and alignment with reporting requirements.\n\nValidate and Optimize Visual Selection:\n\nVerify that the chosen visual type is appropriate for the reporting need.\n\nIf a better representation exists, suggest an alternative based on supported Databricks dashboard visuals.\n\nExpand the Dashboard with Additional Visuals:\n\nIf the current prompts are too limited, add at least 2\u20133 more visuals to provide richer insights.\n\nConsider trend analysis, comparisons, and breakdowns using available charts (bar, line, KPI cards, scatter, etc.).\n\nEnsure Proper Data Field Usage:\n\nConfirm that necessary fields are included from the data model.\n\nEnsure aggregations, calculations, and filters are correctly specified.\n\nApply Formatting & Best Practices:\n\nEnsure prompts specify titles, axis labels, colors, and legends.\n\nMaintain consistent formatting across visuals for a cohesive dashboard.\n\nEnhance Dashboard Usability:\n\nSuggest interactivity features such as filters, parameters, or drill-throughs.\n\nRecommend narratives or KPI comparisons for better storytelling.\n\nFinal Refinement & Approval:\n\nEnsure all prompts follow Databricks visualization best practices.\n\nMaintain consistency in structure, terminology, and formatting.\n\nInput\n\nTake the output of the previous agent as input for this reviewer agent.\n\n\nExpected Output\n\nA refined set of Databricks Genie prompts that effectively guide the creation of insightful and user-friendly dashboards.\n\n\nExpected output:\nA refined set of Databricks Genie prompts that effectively guide the creation of insightful and user-friendly dashboards.",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the   Databricks_Genie_Reviewer output\n* And store the  Databricks Genie Reviewer in the GitHub output directory with the file name as `Databricks_Genie_Reviewer_<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Genie Reviewer output\n* And store the updated  Databricks Genie Reviewer output in the GitHub output directory with the file name as `Databricks_Genie_Reviewer_<next_version>.md` \u2014 Updated  Databricks Genie Reviewer with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}