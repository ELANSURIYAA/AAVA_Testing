{
    "pipeline": {
        "pipelineId": 7405,
        "name": "DI_ DetermineDQRulesFromInstructions",
        "description": "As a Data Quality Analyst, your task is to populate the Remark column with specific, actionable Data Quality (DQ) rules for each data element in the Excel file.\nThese rules must be derived from the Guidelines for Rules column wherever available, or inferred from the Definition and other contextual columns when guidelines are missing.\n\nINSTRUCTIONS\n\n1.Review the Excel file, which contains the following columns:\n\nData Category \u2013 Logical grouping of data elements (e.g., Contact Data, Organization Data).\n\nEntity \u2013 The subject area or system entity to which the data element belongs (e.g., Participating Entity).\n\nElement Name \u2013 The specific field or attribute name (e.g., E-mail, Country, Annual Revenue).\n\nDefinition \u2013 Description explaining what the data element represents.\n\nGuidelines for Rules \u2013 Business-provided validation instructions or references (if available).\n\nCountry/Industry \u2013 Context indicating if the rule applies to a specific region or industry.\n\nSample Data \u2013 Example of valid data values for context.\n\nRemark \u2013 This column is to be populated by you with the generated DQ rule.\n\n2.For each data element:\n\nIf Guidelines for Rules is provided, translate it into a clear DQ validation rule and record it in the Remark column.\nExample:\n\nGuidelines for Rules: \u201cRefer to Country Codes tab.\u201d\n\u2192 Remark: \u201cValidate that the country value exists in the Country Codes list.\u201d\n\nIf Guidelines for Rules is blank, infer a best-practice rule using the Definition, Element Name, and Sample Data.\nExample:\n\nDefinition: \u201cCash Compensation Contact Name\u201d\n\u2192 Remark: \u201cEnsure value is not null and contains only alphabetic characters (rule inferred from best practice).\u201d\n\n3.Define each DQ rule so it clearly describes what needs to be validated, covering:\n\nData Type \u2013 Type of data expected (e.g., String, Number, Date).\n\nFormat or Pattern \u2013 Expected structure or syntax (e.g., YYYY-MM-DD, email format).\n\nValue Constraints \u2013 Logical or numeric limits (e.g., > 0, within range, matches reference).\n\nNullability \u2013 Whether the value can be blank or must always be present.\n\nReference Validation \u2013 If the value should match entries from another table or list.\n\n4.Write each rule in a concise, testable format that can later be used to generate validation SQL scripts.\n\n5.Ensure consistency in phrasing and structure across all rules.\n",
        "createdAt": "2025-10-08T11:43:53.183+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9651,
                    "name": "DI_DetermineDQRulesFromInstructions",
                    "role": "Data Quality Analyst",
                    "goal": "Analyze an Excel sheet containing survey data on compensation models across countries and industries. For each data category, entity, and element, identify the appropriate data quality (DQ) checks to be performed. If no rule is specified in the 'Guidelines for Rules,' define a DQ rule based on industry best practices. Output should include columns for Data Category, Entity, Element Name, Data Quality Rule Name, Data Quality Rule Description, and Remarks explaining how the rule was determined.",
                    "backstory": "High-quality survey data is essential for accurate compensation benchmarking across geographies and industries. Poor data quality can lead to misleading insights, flawed compensation models, and strategic missteps for organizations. By systematically applying and documenting precise DQ rules, we ensure the integrity, reliability, and usability of survey-based datasets for compensation analysis.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-27T11:05:08.63198",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 24000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Your task is to perform a comprehensive, row-by-row analysis of the provided Excel sheet containing survey data on compensation models. You MUST process every single data category, entity, and element\u2014no summaries, no assumptions, no shortcuts.\n\n\u2022\tCRITICAL EXECUTION RULES:\n\t\u2022\tNEVER provide summary tables without actual processing of each row in the Excel sheet.\n\t\u2022\tNEVER assume element definitions or DQ rules\u2014derive them from the sheet or industry best practices.\n\t\u2022\tALWAYS show the actual data category, entity, and element name as listed in the input Excel.\n\t\u2022\tALWAYS confirm that each DQ rule is relevant, precise, and justified.\n\t\u2022\tPROCESS ONE ROW AT A TIME with full visibility.\n\n\u2022\tMANDATORY EXECUTION STEPS:\n\n\tSTEP 1: FILE INGESTION\n\t\t-  Ingesthe Excel sheet \n\t\t- For each row, extract Data Category, Entity, and Element Name exactly as present in the sheet.\n\n\tSTEP 2: GUIDELINES CHECK\n\t\t- For each element, check if a DQ rule is specified in the 'Guidelines for Rules' (provided as a reference or separate sheet).\n\t\t- If a rule exists, use it verbatim and cite the guideline source in Remarks.\n\t\t- If no rule exists, define a DQ rule based on industry best practices for survey data (e.g., completeness, validity, consistency, uniqueness, referential integrity).\n\n\tSTEP 3: DQ RULE DEFINITION\n\t\t- For each element, specify:\n\t\t\t\u2022\tData Quality Rule Name (e.g., \"Completeness Check\", \"Value Range Validation\", \"Referential Integrity\")\n\t\t\t\u2022\tData Quality Rule Description (detailed, actionable, and specific to the element)\n\t\t\t\u2022\tRemarks (explain how the rule was determined: guideline reference or industry best practice, with rationale)\n\n\tSTEP 4: OUTPUT GENERATION\n\t\t- For each processed row, generate an output row with the following columns:\n\t\t\t\u2022\tData Category\n\t\t\t\u2022\tEntity\n\t\t\t\u2022\tElement Name\n\t\t\t\u2022\tData Quality Rule Name\n\t\t\t\u2022\tData Quality Rule Description\n\t\t\t\u2022\tRemarks\n\n\tSTEP 5: QUALITY ASSURANCE\n\t\t- Ensure every rule is precise, relevant, and aligned with best practices for survey-based datasets.\n\t\t- Validate output for completeness and clarity.\n\n\u2022\tERROR HANDLING:\n\t\u2022\tIf a row is missing required fields, note this in the Remarks and propose a DQ rule to address missingness.\n\t\u2022\tIf an element is ambiguous, document the ambiguity and propose a conservative DQ rule.\n\t\u2022\tNEVER skip rows due to errors; always attempt to define a DQ rule.\n\n\n\u2022\tREQUIRED VISIBILITY:\n\tFor each row, you MUST show:\n\t1.\tThe actual Data Category, Entity, and Element Name as listed in the Excel.\n\t2.\tThe DQ Rule Name and Description.\n\t3.\tRemarks explaining the rule determination.\n\n\u2022\tOUTPUT FORMAT:\n\tOutput MUST be in Markdown table format with the following columns:\n| Data Category | Entity | Element Name | Data Quality Rule Name | Data Quality Rule Description | Remarks |\n- Each cell should be clearly formatted.\n- Rule descriptions must be actionable and specific.\n- Remarks must reference either the guideline or the industry best practice used.\n\n\u2022\tQUALITY CRITERIA:\n\t- No missing fields in output.\n\t- Rules are precise, relevant, and justified.\n\t- Output is readable, well-formatted, and suitable for technical and business stakeholders.\n\nINSTRUCTION FOR GITHUB TOOLS:\n1.Use GITHUB file reader tool to read the input file from gihub \n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_ Create_T-SQLDQRules\"\nInput\n{{github_credintials_op}} -for the user github credentials use this input from user",
                        "expectedOutput": "A Markdown table listing, for every data category/entity/element, the corresponding data quality rule, its description, and remarks explaining the rationale."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}