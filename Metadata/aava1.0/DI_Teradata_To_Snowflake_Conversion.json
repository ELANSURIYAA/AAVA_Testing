{
    "pipeline": {
        "pipelineId": 3073,
        "name": "DI_Teradata_To_Snowflake_Conversion",
        "description": "Teradata to Snowflake Conversion and testing ",
        "createdAt": "2025-06-13T11:21:04.932+00:00",
        "managerLlm": {
            "model": "gpt-4",
            "modelDeploymentName": "gpt-4.1",
            "modelType": "Generative",
            "aiEngine": "AzureOpenAI",
            "topP": 0.95,
            "maxToken": 4000,
            "temperature": 0.3
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 4314,
                    "name": "DI_Teradata_To_Snowflake_Converter",
                    "role": "Data Engineer",
                    "goal": "Convert Teradata SQL input code into Snowflake SQL format. Generate a separate output session for each input file.  ",
                    "backstory": "Migrating to Snowflake requires accurate and optimized SQL queries that adhere to the platform\u2019s syntax and best practices. This agent automates the conversion process while ensuring readability, functionality, and compatibility with Snowflake's cloud-based architecture.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-12-15T06:30:54.56395",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Convert the following Teradata Query to Snowflake SQL and provide an overview of the conversion. Ensure that if multiple files are given as input, the conversion for each file is presented as a distinct session. Ensure that the Snowflake query is formatted with proper indentation and line breaks so that it is ready to be stored as a `.sql` file.  \n\n### **INSTRUCTIONS:**  \n\n1. **Metadata Requirements:**  \n   - Add the following metadata at the top of each converted/generated file:  \n   ```  \n   =============================================  \n   Author:        Ascendion AVA+  \n   Created on:    (leave it Empty)  \n   Description:   <one-line description of the converted query>  \n   =============================================  \n   ```  \n   - The date MUST be the actual current date when this agent runs, not a static date.  \n   - Explicitly calculate today's date using your system's date functions.  \n   - If the source code already contains metadata headers, update them to match this format.  \n\n2. **Function and Syntax Conversion:**  \n   - Replace Teradata-specific functions (e.g., `NULLIFZERO`, `DATEDIFF`) with their Snowflake equivalents (e.g., `NULLIF`, `DATEDIFF`).  \n   - Ensure correct handling of `EXTRACT` for date components like `YEAR` and `MONTH`.  \n   - Adapt analytic functions like `ROW_NUMBER()` with `PARTITION BY` to Snowflake's syntax. Use the `QUALIFY` clause where necessary.  \n\n3. **Join Adjustments:**  \n   - Replace `ON 1=1` with `CROSS JOIN` in Snowflake, but eliminate unnecessary Cartesian joins if they are redundant or unoptimized.  \n   - Maintain all other join types (e.g., `INNER JOIN`, `LEFT JOIN`, etc.).  \n\n4. **QUALIFY and Filtering:**  \n   - The `QUALIFY` clause is used in Snowflake to filter records *after* window functions are applied.\n   - Ensure `QUALIFY` conditions align with Snowflake's execution context.  \n\n5. **Table References:**  \n   - Preserve table names as they appear in the original SQL query without schema prefixes unless explicitly required.  \n   - Avoid unnecessary changes to table or column references.  \n\n6. **Data Type Compatibility:**  \n   - Ensure that implicit type casting in Teradata is explicitly defined in Snowflake where needed.  \n   - Validate compatibility with Snowflake data types, such as `NUMBER`, `VARCHAR`, etc.  \n\n7. **Formatting and Structure:**  \n   - Use proper indentation and line breaks for readability.  \n   - Ensure that calculations, `CASE` statements, and other complex logic maintain their intended functionality.  \n\n8. **Output Optimization:**  \n   - Review redundant operations like unnecessary `CROSS JOIN` or unused fields and optimize them where possible.  \nPoints to Remember:\nRember the converted Snowflake code should be a Ready to run snowflake code which should work properly in the Snowflake Environment\n\n### **OUTPUT FORMAT:**  \nGenerate a converted code for each input file independently in separate sessions. Ensure that the output for each file follows the format below:  \n\n1. **Converted Snowflake SQL Code:**  \n   - Include the converted SQL code formatted for readability and ready to be stored as a `.sql` file.  \n\n2. **Cost Information:**  \n   - Include the cost consumed by the API for this call in the output.  \n\nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the code only once in top of the output is enough\n\n-dont give it in the top of the test cases or the code\n \n\n### **SAMPLE:**  \n\n**Input:**  \n```sql  \nSELECT \n    CASE WHEN col1 = 0 THEN NULL ELSE col1 END AS col1_transformed, \n    EXTRACT(YEAR FROM date_col) AS year_col\nFROM \n    table1 \nQUALIFY ROW_NUMBER() OVER (PARTITION BY col2 ORDER BY col3 DESC) = 1;  \n```  \n\n**Output:**  \n``` \n\nSELECT \n    CASE WHEN col1 = 0 THEN NULL ELSE col1 END AS col1_transformed, \n    YEAR(date_col) AS year_col,\n    ROW_NUMBER() OVER (PARTITION BY col2 ORDER BY col3 DESC) as rn\nFROM \n    table1 \nQUALIFY rn = 1;  \n\nAPI Cost: $0.05  \n```  \nInput:\n*For Teradata input use this file: {{Teradata}}\n### **OUTPUT:**  \nGenerate a converted Snowflake SQL code for each input file independently in separate sessions, along with metadata and cost information.  ",
                        "expectedOutput": "Generate a converted Snowflake SQL code for each input file independently in separate sessions, along with metadata and cost information.  "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 4316,
                    "name": "DI_Teradata_To_Snowflake_UnitTest",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided Snowflake SQL code, ensuring thorough coverage of key functionalities and edge cases. ",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of SQL transformations in Snowflake. By creating robust test cases, we can catch potential issues early, prevent data discrepancies, and improve overall query correctness. Snowflake's unique features, such as its cloud-native architecture and support for semi-structured data, require careful consideration when designing test cases.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2026-01-21T10:28:57.048826",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "- A detailed list of test cases and a corresponding Pytest script for Snowflake SQL code.  "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 4319,
                    "name": "DI_Teradata_To_Snowflake_ConversionTester",
                    "role": "Data Engineer",
                    "goal": " Develop comprehensive test cases and a Pytest script to validate Teradata-to-Snowflake SQL conversion, focusing on syntax changes, logic preservation, and manual interventions required in the converted code. ",
                    "backstory": "Ensuring the accuracy and functionality of converted SQL is crucial for a successful migration from Teradata to Snowflake. Snowflake offers unique features and syntax that differ from Teradata, making thorough testing essential to minimize risks, maintain query performance, and ensure that the converted SQL meets our business and data processing requirements. This testing will ensure seamless integration with Snowflake's cloud-native architecture and features",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2026-01-21T10:42:42.050196",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "A detailed test case document, Pytest script, and cost analysis.  "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 4320,
                    "name": "DI_Teradata_To_Snowflake_ReconTest",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the migration process from Teradata to Snowflake by executing both database systems' code and comparing their outputs to ensure data integrity and migration accuracy.  ",
                    "backstory": "This agent was created to address the complex challenge of verifying data consistency during Teradata to Snowflake migrations. It reduces manual verification effort while increasing confidence in migration results through systematic comparison.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2026-01-21T10:31:20.300785",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "A Python script that automates Teradata to Snowflake migration validation and generates a detailed comparison report.  "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 4326,
                    "name": "DI_Teradata_To_Snowflake_Reviewer",
                    "role": "Data Engineer",
                    "goal": "As organizations transition from Teradata to Snowflake, it is critical to ensure that the converted queries maintain the original business logic while optimizing for Snowflake\u2019s best practices. A thorough review will ensure correctness, efficiency, and maintainability, enabling seamless migration and leveraging Snowflake's cloud-native features. ",
                    "backstory": "As organizations transition from Teradata to Snowflake, it is critical to ensure that the converted queries maintain the original business logic while optimizing for Snowflake\u2019s best practices. A thorough review will ensure correctness, efficiency, and maintainability, enabling seamless migration and leveraging Snowflake's cloud-native features.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2026-01-21T10:48:02.041413",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Your task is to meticulously analyze and compare the original Teradata code with the newly converted Snowflake implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the Snowflake environment. You will act as a code reviewer, comparing the Teradata code against the converted Snowflake code to identify any gaps in the conversion.  \n\n**INSTRUCTIONS:**  \n\n**Metadata Requirements:**\n- Add the following metadata at the top of file:\n```\n=============================================\nAuthor:        Ascendion AVAA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. **Understand the Original Teradata Code:**  \n   - Carefully read and comprehend the original Teradata SQL code, noting its structure, logic, and data flow.  \n\n2. **Examine the Converted Snowflake Code:**  \n   Pay close attention to:  \n   - Data types and structures  \n   - Control flow and logic  \n   - SQL operations, functions, and data transformations  \n   - Error handling and exception management  \n\n3. **Compare Teradata and Snowflake Implementations:**  \n   Ensure that:  \n   - All functionality from the Teradata code is present in the Snowflake version  \n   - Business logic remains intact and produces the same results  \n   - Data processing steps are equivalent and maintain data integrity  \n\n4. **Verify Snowflake Optimizations:**  \n   - Efficient use of Snowflake's native SQL functions  \n   - Optimization for Snowflake's cloud-native architecture (e.g., virtual warehouses, micro-partitioning)  \n   - Appropriate use of clustering keys, materialized views, and caching mechanisms  \n   - Cost-effective query design to minimize Snowflake processing costs  \n\n5. **Test the Snowflake Code:**  \n   - Validate the correctness of the conversion by running sample data tests  \n   - Ensure the output matches the Teradata version  \n\n6. **Identify Performance Bottlenecks & Improvements:**  \n   - Highlight potential inefficiencies in the Snowflake implementation  \n   - Suggest optimizations for better performance and cost efficiency  \n\n7. **Document Findings:**  \n   - Include any discrepancies, areas for optimization, and overall assessment of the conversion quality  \n\n**OUTPUT FORMAT:**  \nYour output should be structured as follows:  \n\n1. **Summary:**  \n   - Provide a high-level overview of the review findings.  \n\n2. **Conversion Accuracy:**  \n   - Assess how accurately the Teradata code has been converted to Snowflake.  \n\n3. **Discrepancies and Issues:**  \n   - List any gaps, errors, or missing functionality in the Snowflake implementation.  \n\n4. **Optimization Suggestions:**  \n   - Recommend improvements for performance, cost efficiency, and maintainability.  \n\n5. **Overall Assessment:**  \n   - Provide a rating or qualitative assessment of the conversion quality.  \n\n6. **Recommendations:**  \n   - Suggest next steps for addressing issues or further optimizing the Snowflake code.  \n\n7. **API Cost Analysis:**  \n   - Include the cost consumed by the API for this call in the output.  \n\n**Formatting Requirements:**  \n- Ensure clarity and readability with proper headings, bullet points, and tables (if needed).  \n- Provide actionable insights and examples where applicable.  \n\nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the code only once in top of the output is enough\n\n**SAMPLE:**  \n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVAA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n# Teradata-to-Snowflake Conversion Review  \n\n## 1.Summary  \nThe Snowflake implementation largely retains the business logic of the Teradata code, but there are areas for improvement in optimization and error handling.  \n\n## 2.Conversion Accuracy  \n- 95% of the Teradata functionality has been successfully converted.  \n- Minor discrepancies in data type mapping and function usage.  \n\n## 3.Discrepancies and Issues  \n- Missing handling for edge cases in `CASE` statements.  \n- Incorrect mapping of `DATE` functions between Teradata and Snowflake.  \n\n## 4.Optimization Suggestions  \n- Use clustering keys for large tables to improve query performance.  \n- Replace `LEFT JOIN` with `INNER JOIN` where applicable to reduce query cost.  \n\n## 5.Overall Assessment  \nThe conversion is accurate but can benefit from better optimization for Snowflake\u2019s architecture.  \n\n##6. Recommendations  \n- Address discrepancies in function mapping.  \n- Implement clustering keys for large tables.  \n- Test with larger datasets to validate performance.  \n\n## 7. API Cost Analysis  \n- Cost consumed by API: $0.045  \n```\nInput:\n*for the converted Snowflake code use 'CONVERTED_CODE' \n* for the input Terdata code use this file: {{Teradata}}\n*OUTPUT:**  \nA detailed review of the Teradata-to-Snowflake SQL conversion, including accuracy, discrepancies, optimization suggestions, and recommendations.",
                        "expectedOutput": "A detailed review of the Teradata-to-Snowflake SQL conversion, including accuracy, discrepancies, optimization suggestions, and recommendations."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 150,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 149,
        "project": "Conversions",
        "teamId": 150,
        "team": "DataEngineer",
        "callbacks": []
    }
}