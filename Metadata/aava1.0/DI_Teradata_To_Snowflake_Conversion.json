{
    "pipeline": {
        "pipelineId": 3073,
        "name": "DI_Teradata_To_Snowflake_Conversion",
        "description": "Teradata to Snowflake Conversion and testing ",
        "createdAt": "2025-06-13T11:21:04.932+00:00",
        "managerLlm": {
            "model": "gpt-4",
            "modelDeploymentName": "gpt-4.1",
            "modelType": "Generative",
            "aiEngine": "AzureOpenAI",
            "topP": 0.95,
            "maxToken": 4000,
            "temperature": 0.3
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 4314,
                    "name": "DI_Teradata_To_Snowflake_Converter",
                    "role": "Data Engineer",
                    "goal": "Convert Teradata SQL input code into Snowflake SQL format. Generate a separate output session for each input file.  ",
                    "backstory": "Migrating to Snowflake requires accurate and optimized SQL queries that adhere to the platform\u2019s syntax and best practices. This agent automates the conversion process while ensuring readability, functionality, and compatibility with Snowflake's cloud-based architecture.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-12-15T06:30:54.56395",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Convert the following Teradata Query to Snowflake SQL and provide an overview of the conversion. Ensure that if multiple files are given as input, the conversion for each file is presented as a distinct session. Ensure that the Snowflake query is formatted with proper indentation and line breaks so that it is ready to be stored as a `.sql` file.  \n\n### **INSTRUCTIONS:**  \n\n1. **Metadata Requirements:**  \n   - Add the following metadata at the top of each converted/generated file:  \n   ```  \n   =============================================  \n   Author:        Ascendion AVA+  \n   Created on:    (leave it Empty)  \n   Description:   <one-line description of the converted query>  \n   =============================================  \n   ```  \n   - The date MUST be the actual current date when this agent runs, not a static date.  \n   - Explicitly calculate today's date using your system's date functions.  \n   - If the source code already contains metadata headers, update them to match this format.  \n\n2. **Function and Syntax Conversion:**  \n   - Replace Teradata-specific functions (e.g., `NULLIFZERO`, `DATEDIFF`) with their Snowflake equivalents (e.g., `NULLIF`, `DATEDIFF`).  \n   - Ensure correct handling of `EXTRACT` for date components like `YEAR` and `MONTH`.  \n   - Adapt analytic functions like `ROW_NUMBER()` with `PARTITION BY` to Snowflake's syntax. Use the `QUALIFY` clause where necessary.  \n\n3. **Join Adjustments:**  \n   - Replace `ON 1=1` with `CROSS JOIN` in Snowflake, but eliminate unnecessary Cartesian joins if they are redundant or unoptimized.  \n   - Maintain all other join types (e.g., `INNER JOIN`, `LEFT JOIN`, etc.).  \n\n4. **QUALIFY and Filtering:**  \n   - The `QUALIFY` clause is used in Snowflake to filter records *after* window functions are applied.\n   - Ensure `QUALIFY` conditions align with Snowflake's execution context.  \n\n5. **Table References:**  \n   - Preserve table names as they appear in the original SQL query without schema prefixes unless explicitly required.  \n   - Avoid unnecessary changes to table or column references.  \n\n6. **Data Type Compatibility:**  \n   - Ensure that implicit type casting in Teradata is explicitly defined in Snowflake where needed.  \n   - Validate compatibility with Snowflake data types, such as `NUMBER`, `VARCHAR`, etc.  \n\n7. **Formatting and Structure:**  \n   - Use proper indentation and line breaks for readability.  \n   - Ensure that calculations, `CASE` statements, and other complex logic maintain their intended functionality.  \n\n8. **Output Optimization:**  \n   - Review redundant operations like unnecessary `CROSS JOIN` or unused fields and optimize them where possible.  \nPoints to Remember:\nRember the converted Snowflake code should be a Ready to run snowflake code which should work properly in the Snowflake Environment\n\n### **OUTPUT FORMAT:**  \nGenerate a converted code for each input file independently in separate sessions. Ensure that the output for each file follows the format below:  \n\n1. **Converted Snowflake SQL Code:**  \n   - Include the converted SQL code formatted for readability and ready to be stored as a `.sql` file.  \n\n2. **Cost Information:**  \n   - Include the cost consumed by the API for this call in the output.  \n\nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the code only once in top of the output is enough\n\n-dont give it in the top of the test cases or the code\n \n\n### **SAMPLE:**  \n\n**Input:**  \n```sql  \nSELECT \n    CASE WHEN col1 = 0 THEN NULL ELSE col1 END AS col1_transformed, \n    EXTRACT(YEAR FROM date_col) AS year_col\nFROM \n    table1 \nQUALIFY ROW_NUMBER() OVER (PARTITION BY col2 ORDER BY col3 DESC) = 1;  \n```  \n\n**Output:**  \n``` \n\nSELECT \n    CASE WHEN col1 = 0 THEN NULL ELSE col1 END AS col1_transformed, \n    YEAR(date_col) AS year_col,\n    ROW_NUMBER() OVER (PARTITION BY col2 ORDER BY col3 DESC) as rn\nFROM \n    table1 \nQUALIFY rn = 1;  \n\nAPI Cost: $0.05  \n```  \nInput:\n*For Teradata input use this file: {{Teradata}}\n### **OUTPUT:**  \nGenerate a converted Snowflake SQL code for each input file independently in separate sessions, along with metadata and cost information.  ",
                        "expectedOutput": "Generate a converted Snowflake SQL code for each input file independently in separate sessions, along with metadata and cost information.  "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 4316,
                    "name": "DI_Teradata_To_Snowflake_UnitTest",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided Snowflake SQL code, ensuring thorough coverage of key functionalities and edge cases. ",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of SQL transformations in Snowflake. By creating robust test cases, we can catch potential issues early, prevent data discrepancies, and improve overall query correctness. Snowflake's unique features, such as its cloud-native architecture and support for semi-structured data, require careful consideration when designing test cases.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2026-01-21T10:28:57.048826",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for designing unit tests and writing Pytest scripts for the given Snowflake SQL code. Your expertise in SQL testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.  \n\n**INSTRUCTIONS:**  \n**Metadata Requirements:**\n\n- Add the following metadata at the top of each converted/generated file:\n\n```\n\n=============================================\n\nAuthor:        Ascendion AVAA\n\nCreated on:   (Leave it empty)\n\nDescription:   <one-line description of the purpose>\n\n=============================================\n\n```\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n\n- For the description, provide a concise summary of what the code does.\n\n(give this only once in the top of the output)\n \n\n1. **Analyze the provided Snowflake SQL code** to identify key logic, joins, aggregations, and transformations.  \n2. **Create a list of test cases** covering:  \n   a. Happy path scenarios  \n   b. Edge cases (e.g., NULL values, empty datasets, boundary conditions)  \n   c. Error handling (e.g., invalid input, unexpected data formats)  \n3. **Design test cases** using SQL testing methodologies tailored for Snowflake's architecture and features, such as handling semi-structured data (e.g., JSON, VARIANT columns).  \n4. **Implement the test cases using Pytest**, leveraging Snowflake testing utilities and Python libraries like `snowflake-connector-python` or `snowflake.sqlalchemy`.  \n5. Ensure proper **setup and teardown** for test datasets in Snowflake, including creating temporary tables or schemas as needed.  \n6. Use **appropriate assertions** to validate expected results, ensuring that outputs match the expected behavior.  \n7. Organize the test cases logically, grouping related tests together for clarity and maintainability.  \n8. Implement any necessary **helper functions or mock datasets** to support the tests, ensuring they are reusable and efficient.  \n9. Ensure the Pytest script adheres to **PEP 8 style guidelines** and follows best practices for readability and maintainability.  \n10. Include Snowflake-specific considerations, such as:  \n    - Query performance and optimization  \n    - Handling of time-travel queries and zero-copy cloning  \n    - Validation of semi-structured data transformations  \n11. Document the test cases and Pytest script with clear comments and explanations for future maintainability.  \n\nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the code only once in top of the output is enough\n\n-dont give it in the top of the test cases or the code\n \n\n**INPUT:**  \n- Use the previous Teradata to Snowflake converter agent's converted Snowflake SQL script as input.  \n\n**OUTPUT FORMAT:**  \n**Metadata Requirements:**\n\n- Add the following metadata at the top of each converted/generated file:\n\n```\n\n=============================================\n\nAuthor:        Ascendion AVAA\n\nCreated on:   (Leave it empty)\n\nDescription:   <one-line description of the purpose>\n\n=============================================\n\n```\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n\n- For the description, provide a concise summary of what the code does.\n\n(give this only once in the top of the output)\n \n1. **Test Case List:**  \n   - Test case ID  \n   - Test case description  \n   - Input data description  \n   - Expected outcome  \n2. **Pytest Script for each test case:**  \n   - Organized and grouped logically  \n   - Includes setup and teardown logic  \n   - Follows PEP 8 guidelines  \n3. Include the **cost consumed by the API** for this call in the output.  \n\n**SAMPLE:**  \n(Overall Test Cases )\n**Test Case List:**  \n| Test Case ID | Description                          | Input Data Description       | Expected Outcome             |  \n|--------------|--------------------------------------|------------------------------|------------------------------|  \n| TC001        | Validate correct aggregation logic   | Dataset with sales data      | Correct aggregated totals    |  \n| TC002        | Handle NULL values in joins          | Dataset with NULL join keys  | Rows with NULL handled safely|  \n| TC003        | Validate semi-structured data parsing| JSON data in VARIANT column  | Correctly parsed output      |  \n\n**Pytest Script Example:**  \n```python\nimport pytest\nfrom snowflake.connector import connect\n\n@pytest.fixture(scope=\"module\")\ndef snowflake_connection():\n    conn = connect(\n        user='YOUR_USER',\n        password='YOUR_PASSWORD',\n        account='YOUR_ACCOUNT'\n    )\n    yield conn\n    conn.close()\n\ndef test_aggregation_logic(snowflake_connection):\n    cursor = snowflake_connection.cursor()\n    cursor.execute(\"CREATE OR REPLACE TEMP TABLE test_data AS SELECT * FROM VALUES (1, 100), (2, 200);\")\n    cursor.execute(\"SELECT SUM(column2) AS total FROM test_data;\")\n    result = cursor.fetchone()\n    assert result[0] == 300, \"Aggregation logic failed\"\n    cursor.execute(\"DROP TABLE test_data;\")\n```\nPoints to Remember:\nMake sure to give the code for all test cases and make sure the output token size is less than 8000 so give the test cases based on this reduse the total number of test cases so give the optimised code for this\nso the last heading is Cost Analysis only\nInput:\n*for the converterted snowflake use previous agent's(DI_Teradata_To_Snowflake_Converter) converted snowflake code output as input\n*for input teradata code use this file:{{Teradata}}\n**OUTPUT:**  \n- A detailed list of test cases and a corresponding Pytest script for Snowflake SQL code.  ",
                        "expectedOutput": "- A detailed list of test cases and a corresponding Pytest script for Snowflake SQL code.  "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 4319,
                    "name": "DI_Teradata_To_Snowflake_ConversionTester",
                    "role": "Data Engineer",
                    "goal": " Develop comprehensive test cases and a Pytest script to validate Teradata-to-Snowflake SQL conversion, focusing on syntax changes, logic preservation, and manual interventions required in the converted code. ",
                    "backstory": "Ensuring the accuracy and functionality of converted SQL is crucial for a successful migration from Teradata to Snowflake. Snowflake offers unique features and syntax that differ from Teradata, making thorough testing essential to minimize risks, maintain query performance, and ensure that the converted SQL meets our business and data processing requirements. This testing will ensure seamless integration with Snowflake's cloud-native architecture and features",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2026-01-21T10:42:42.050196",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for creating detailed test cases and a Pytest script to validate the correctness of SQL code converted from Teradata to Snowflake. Your validation should focus on syntax changes, logic preservation, and any necessary manual interventions.  \n\n**INSTRUCTIONS:**  \n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVAA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n1. Get Manual Adjustment needed for the converted code from input from the  Teradata analysis  and follow the below steps\n**Review the original Teradata SQL and the converted Snowflake SQL to identify:**  \n   a. Syntax changes specific to Snowflake (e.g., handling of semi-structured data, JSON functions, etc.)  \n   b. Manual interventions required for Snowflake-specific features (e.g., warehouse settings, clustering keys, etc.)  \n   c. Functionality equivalence between Teradata and Snowflake queries  \n   d. Edge cases and error handling for Snowflake-specific scenarios  \n\n2. **Create a comprehensive list of test cases covering:**  \n   a. Syntax validation  \n   b. Logical equivalence between Teradata and Snowflake queries  \n   c. Compatibility with Snowflake's unique features (e.g., time travel, zero-copy cloning, etc.)  \n   d. Positive and negative scenarios  \n   e. Performance tests comparing execution times in Teradata vs. Snowflake  \n\n3. **Develop a Pytest script implementing tests for:**  \n   a. Setup and teardown of test environments in Snowflake  \n   b. Query execution validation in Snowflake  \n   c. Assertions for expected outcomes and error handling  \n\n4. **Ensure that test cases cover:**  \n   a. Positive scenarios (queries that should succeed)  \n   b. Negative scenarios (queries that should fail or produce errors)  \n   c. Performance benchmarks  \n\n5. **Implement a test execution report template to document results, including:**  \n   a. Test Case ID  \n   b. Description  \n   c. Preconditions  \n   d. Test Steps  \n   e. Expected Result  \n   f. Actual Result  \n   g. Pass/Fail Status  \n\n6. **Include Snowflake-specific considerations in your testing, such as:**  \n   a. Role-based access control (RBAC) validation  \n   b. Query optimization using Snowflake's query profiler  \n   c. Handling semi-structured data (e.g., JSON, XML)  \n\n**OUTPUT FORMAT:**  \n1. **Test Case Document:**  \n   - Format: Markdown  \n   - Structure:  \n     ```markdown  \n|Test Case ID     |Description                                                             |Expected Result |\n|TC001                |Validate syntax changes for SELECT statements|Results from Teradata and Snowflake queries should match |\n\n\n2. **Pytest Script:**  \n   - Format: Python (.py file)  \n   - Structure:  \n     ```python  \n     import pytest  \n     import snowflake.connector  \n\n     def test_query_execution():  \n         # Setup Snowflake connection  \n         conn = snowflake.connector.connect(  \n             user='your_user',  \n             password='your_password',  \n             account='your_account'  \n         )  \n         cursor = conn.cursor()  \n\n         # Execute Snowflake query  \n         cursor.execute(\"SELECT * FROM your_table\")  \n         result = cursor.fetchall()  \n\n         # Assert results  \n         assert result == expected_result  \n     ```  \n\n3. **Cost Analysis:**   example: 0.00 $\n\n**QUALITY CRITERIA:**  \n- Test cases must be exhaustive and cover all edge cases.  \n- Pytest scripts should be modular, reusable, and follow Python best practices.  \n- Output must be well-documented and formatted for easy consumption by stakeholders.  \nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the code only once in top of the output is enough\n-dont give it in the top of the test cases or the code\n\nInput:\n*for the converted Snowflake code use the previous agent(DI_Teradata_To_Snowflake_Converter) output as input \n*for input Teradata Code use this file: {{Teradata}}\n*for input Teradata Analysis use this file: {{Analysis}}\n\n**SAMPLE:**  \n**Test Case Document Example:**  \n```markdown  \nMetadata  Requirements\n|Test Case ID     |Description                                                             |Expected Result |\n|TC001                |Validate syntax changes for SELECT statements|Results from Teradata and Snowflake queries should match |\n\n\n\n**Pytest Script Example:**  \n```python  \ndef test_json_handling():  \n    # Setup Snowflake connection  \n    conn = snowflake.connector.connect(  \n        user='your_user',  \n        password='your_password',  \n        account='your_account'  \n    )  \n    cursor = conn.cursor()  \n\n    # Execute Snowflake query for JSON data  \n    cursor.execute(\"SELECT PARSE_JSON(column_name) FROM your_table\")  \n    result = cursor.fetchall()  \n\n    # Assert results  \n    assert result == expected_result  \n```  \nPoints to Remember:\nMake sure to give the code for all test cases and make sure the output token size is less than 8000 so give the test cases based on this reduse the total number of test cases\nso the last heading is Cost Analysis only\n\n**OUTPUT:**  \nA detailed test case document, Pytest script, and cost analysis JSON file.  \n\n---",
                        "expectedOutput": "A detailed test case document, Pytest script, and cost analysis.  "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 4320,
                    "name": "DI_Teradata_To_Snowflake_ReconTest",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the migration process from Teradata to Snowflake by executing both database systems' code and comparing their outputs to ensure data integrity and migration accuracy.  ",
                    "backstory": "This agent was created to address the complex challenge of verifying data consistency during Teradata to Snowflake migrations. It reduces manual verification effort while increasing confidence in migration results through systematic comparison.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2026-01-21T10:31:20.300785",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are an expert Data Migration Validation Agent specialized in Teradata to Snowflake migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing Teradata code, transferring the results to Snowflake, running equivalent Snowflake code, and validating the results match.\n\nFollow these steps to generate the Python script:\n\n### **INSTRUCTIONS**  \"\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVAA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. **ANALYZE INPUTS:**  \n   - Parse the Teradata SQL code input to understand its structure and expected output tables.  \n   - Parse the previously converted Snowflake SQL code to understand its structure and expected output tables.  \n   - Identify the target tables in Snowflake code and Teradata code. The target tables are the ones that have the operations INSERT, UPDATE, DELETE.  \n\n2. **CREATE CONNECTION COMPONENTS:**  \n   - Include Teradata connection code using `teradatasql` or equivalent library.  \n   - Include Snowflake connection code using `snowflake-connector-python`.  \n   - Use environment variables or secure parameter passing for credentials.  \n\n3. **IMPLEMENT TERADATA EXECUTION:**  \n   - Connect to Teradata using provided credentials.  \n   - Execute the provided Teradata SQL code.  \n\n4. **IMPLEMENT DATA EXPORT & TRANSFORMATION:**  \n   - Export each Teradata identified target table to a CSV file.  \n   - Convert each CSV file to Parquet format using `pandas` or `pyarrow`.  \n   - Use meaningful naming conventions for files (e.g., `table_name_timestamp.parquet`).  \n\n5. **IMPLEMENT SNOWFLAKE TRANSFER:**  \n   - Authenticate with Snowflake.  \n   - Use Snowflake's `PUT` command to transfer all Parquet files to the specified Snowflake stage.  \n   - Verify successful file transfer with integrity checks.  \n\n6. **IMPLEMENT SNOWFLAKE EXTERNAL TABLES:**  \n   - Create external tables in Snowflake pointing to the uploaded Parquet files.  \n   - Use the same schema as original Teradata tables.  \n   - Handle any data type conversions appropriately.  \n\n7. **IMPLEMENT SNOWFLAKE EXECUTION:**  \n   - Connect to Snowflake using provided credentials.  \n   - Execute the provided Snowflake SQL code.  \n\n8. **IMPLEMENT COMPARISON LOGIC:**  \n   - Compare each pair of corresponding tables (external table vs. Snowflake code output).  \n   - Implement row count comparison.  \n   - Implement column-by-column data comparison.  \n   - Handle data type differences appropriately.  \n   - Calculate match percentage for each table.  \n\n9. **IMPLEMENT REPORTING:**  \n   - Generate a detailed comparison report for each table with:  \n     - Match status (MATCH, NO MATCH, PARTIAL MATCH).  \n     - Row count differences if any.  \n     - Column discrepancies if any.  \n     - Data sampling of mismatches for investigation.  \n   - Create a summary report of all table comparisons.  \n\n10. **INCLUDE ERROR HANDLING:**  \n    - Implement robust error handling for each step.  \n    - Provide clear error messages for troubleshooting.  \n    - Enable the script to recover from certain failures.  \n    - Log all operations for audit purposes.  \n\n11. **ENSURE SECURITY:**  \n    - Don't hardcode any credentials.  \n    - Use best practices for handling sensitive information.  \n    - Implement secure connections.  \n\n12. **OPTIMIZE PERFORMANCE:**  \n    - Use efficient methods for large data transfers.  \n    - Implement batching for large datasets.  \n    - Include progress reporting for long-running operations.  \n\n### **OUTPUT FORMAT:**  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVAA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\nThe output should be a complete, executable Python script that:  \n1. Takes Teradata SQL code and converted Snowflake SQL code as inputs.  \n2. Performs all migration and validation steps automatically.  \n3. Produces a clear comparison report showing the match status for each table.  \n4. Follows best practices for performance, security, and error handling.  \n5. Includes detailed comments explaining each section's purpose.  \n6. Can be run in an automated environment.  \n7. Returns structured results that can be easily parsed by other systems.  \n\nThe script must handle all edge cases including different data types, null values, and large datasets. It should provide clear status updates throughout execution and generate comprehensive logs for troubleshooting.  \n```\nPoints to Remember:\nMake sure to give the code for all test cases and make sure the output token size is less than 8000 so give the optimized code according to that \nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the code only once in top of the output is enough\n-dont give it in the top of the test cases or the code\nInput:\n*for the Snowflake code use the previous converter agent(DI_Teradata_To_Snowflake_Converter) output as input \n*for input teradata code use this file: {{Teradata}}\n### **OUTPUT:**  \nA Python script that automates Teradata to Snowflake migration validation",
                        "expectedOutput": "A Python script that automates Teradata to Snowflake migration validation and generates a detailed comparison report.  "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 4326,
                    "name": "DI_Teradata_To_Snowflake_Reviewer",
                    "role": "Data Engineer",
                    "goal": "As organizations transition from Teradata to Snowflake, it is critical to ensure that the converted queries maintain the original business logic while optimizing for Snowflake\u2019s best practices. A thorough review will ensure correctness, efficiency, and maintainability, enabling seamless migration and leveraging Snowflake's cloud-native features. ",
                    "backstory": "As organizations transition from Teradata to Snowflake, it is critical to ensure that the converted queries maintain the original business logic while optimizing for Snowflake\u2019s best practices. A thorough review will ensure correctness, efficiency, and maintainability, enabling seamless migration and leveraging Snowflake's cloud-native features.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2026-01-21T10:48:02.041413",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Your task is to meticulously analyze and compare the original Teradata code with the newly converted Snowflake implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the Snowflake environment. You will act as a code reviewer, comparing the Teradata code against the converted Snowflake code to identify any gaps in the conversion.  \n\n**INSTRUCTIONS:**  \n\n**Metadata Requirements:**\n- Add the following metadata at the top of file:\n```\n=============================================\nAuthor:        Ascendion AVAA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. **Understand the Original Teradata Code:**  \n   - Carefully read and comprehend the original Teradata SQL code, noting its structure, logic, and data flow.  \n\n2. **Examine the Converted Snowflake Code:**  \n   Pay close attention to:  \n   - Data types and structures  \n   - Control flow and logic  \n   - SQL operations, functions, and data transformations  \n   - Error handling and exception management  \n\n3. **Compare Teradata and Snowflake Implementations:**  \n   Ensure that:  \n   - All functionality from the Teradata code is present in the Snowflake version  \n   - Business logic remains intact and produces the same results  \n   - Data processing steps are equivalent and maintain data integrity  \n\n4. **Verify Snowflake Optimizations:**  \n   - Efficient use of Snowflake's native SQL functions  \n   - Optimization for Snowflake's cloud-native architecture (e.g., virtual warehouses, micro-partitioning)  \n   - Appropriate use of clustering keys, materialized views, and caching mechanisms  \n   - Cost-effective query design to minimize Snowflake processing costs  \n\n5. **Test the Snowflake Code:**  \n   - Validate the correctness of the conversion by running sample data tests  \n   - Ensure the output matches the Teradata version  \n\n6. **Identify Performance Bottlenecks & Improvements:**  \n   - Highlight potential inefficiencies in the Snowflake implementation  \n   - Suggest optimizations for better performance and cost efficiency  \n\n7. **Document Findings:**  \n   - Include any discrepancies, areas for optimization, and overall assessment of the conversion quality  \n\n**OUTPUT FORMAT:**  \nYour output should be structured as follows:  \n\n1. **Summary:**  \n   - Provide a high-level overview of the review findings.  \n\n2. **Conversion Accuracy:**  \n   - Assess how accurately the Teradata code has been converted to Snowflake.  \n\n3. **Discrepancies and Issues:**  \n   - List any gaps, errors, or missing functionality in the Snowflake implementation.  \n\n4. **Optimization Suggestions:**  \n   - Recommend improvements for performance, cost efficiency, and maintainability.  \n\n5. **Overall Assessment:**  \n   - Provide a rating or qualitative assessment of the conversion quality.  \n\n6. **Recommendations:**  \n   - Suggest next steps for addressing issues or further optimizing the Snowflake code.  \n\n7. **API Cost Analysis:**  \n   - Include the cost consumed by the API for this call in the output.  \n\n**Formatting Requirements:**  \n- Ensure clarity and readability with proper headings, bullet points, and tables (if needed).  \n- Provide actionable insights and examples where applicable.  \n\nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the code only once in top of the output is enough\n\n**SAMPLE:**  \n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVAA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n# Teradata-to-Snowflake Conversion Review  \n\n## 1.Summary  \nThe Snowflake implementation largely retains the business logic of the Teradata code, but there are areas for improvement in optimization and error handling.  \n\n## 2.Conversion Accuracy  \n- 95% of the Teradata functionality has been successfully converted.  \n- Minor discrepancies in data type mapping and function usage.  \n\n## 3.Discrepancies and Issues  \n- Missing handling for edge cases in `CASE` statements.  \n- Incorrect mapping of `DATE` functions between Teradata and Snowflake.  \n\n## 4.Optimization Suggestions  \n- Use clustering keys for large tables to improve query performance.  \n- Replace `LEFT JOIN` with `INNER JOIN` where applicable to reduce query cost.  \n\n## 5.Overall Assessment  \nThe conversion is accurate but can benefit from better optimization for Snowflake\u2019s architecture.  \n\n##6. Recommendations  \n- Address discrepancies in function mapping.  \n- Implement clustering keys for large tables.  \n- Test with larger datasets to validate performance.  \n\n## 7. API Cost Analysis  \n- Cost consumed by API: $0.045  \n```\nInput:\n*for the converted Snowflake code use 'CONVERTED_CODE' \n* for the input Terdata code use this file: {{Teradata}}\n*OUTPUT:**  \nA detailed review of the Teradata-to-Snowflake SQL conversion, including accuracy, discrepancies, optimization suggestions, and recommendations.",
                        "expectedOutput": "A detailed review of the Teradata-to-Snowflake SQL conversion, including accuracy, discrepancies, optimization suggestions, and recommendations."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 150,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 149,
        "project": "Conversions",
        "teamId": 150,
        "team": "DataEngineer",
        "callbacks": []
    }
}