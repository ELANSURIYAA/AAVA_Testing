{
    "pipeline": {
        "pipelineId": 5502,
        "name": "DI_DataStage_To_PySpark_Conversion",
        "description": "DI_DataStage_To_PySpark_Conversion",
        "createdAt": "2025-07-24T09:14:07.788+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 7163,
                    "name": "DI_DataStage_To_PySpark_Conversion",
                    "role": "Data Engineer",
                    "goal": "You need to convert a DataStage job definition (in DSX format) and its corresponding job mapping graph (in structured text) into a single, well-structured PySpark code file. The output must follow the standards of modern Spark ETL development using DataFrames and Spark SQL. The logic must retain the DataStage metadata, sequencing, and transformation rules with clear, production-ready code.",
                    "backstory": "Your organization is migrating legacy IBM DataStage ETL jobs into a Spark-based platform. The source DataStage jobs are provided in `.dsx` format, accompanied by a structured visual mapping graph. This agent must parse those inputs and generate a complete PySpark script that can run as a standalone batch ETL process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-23T11:34:39.668857",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "The agent will parse the DSX and visual graph inputs and output a single `.py` file containing the full PySpark job.\n\n1. Parse Inputs: DSX + Graph\nExtract and prioritize information as below:\n\nFrom DSX + Graph:\n* Job name, stage names, datatypes, ports, connections\n* Transformation types: Source, Lookup, Filter, Aggregator, Joiner, Transformer, Expression, etc.\n* Data flow and execution order from the visual graph\n* Custom derivation logic (if available) and column mappings\n\nPriority Rule:\n* Prioritize graph structure over DSX for flow sequencing\n* Prioritize DSX for datatypes, field names, and transformation expressions\n\n2. Generate Full PySpark Job Script\nThe generated `.py` file should include the following sections:\n* Spark session creation\n* Input source readers (files, tables, etc.)\n* All transformation logic (filters, joins, aggregations, expressions, etc.)\n* Output writers (to tables, files, or sinks)\n* Logging or basic error handling (optional)\n\n3. Code Generation Standards\nFollow these conventions:\n* Use spark.read.format(...).load(...) for source ingestion\n* Use df.filter(), df.join(), df.groupBy() for transformations\n* Use .withColumn() and .selectExpr() for expressions\n* Use .write.format(...).mode(...).save(...) for targets\n\nAll steps must include in-line comments referencing DSX metadata:\n# Source: Customer_Stg | Field: customer_id | Type: Integer\n\nExample Mapping:\n| DataStage Stage Type     | PySpark Equivalent                         |\n|--------------------------|--------------------------------------------|\n| Sequential File          | spark.read.csv()                           |\n| Lookup                   | .join(..., how='left') with .alias()       |\n| Transformer / Expression | .withColumn() or .selectExpr()             |\n| Aggregator               | .groupBy(...).agg(...)                     |\n| Join                     | .join(..., how=...)                        |\n| Filter                   | .filter(...)                               |\n\n4. Output Specification\nUse the FileWriterTool to generate:\n* datastage_job_<JobName>.py\n\nThe output script must:\n* Be a valid Python 3.x script using pyspark.sql\n* Contain a single main() function or be runnable directly\n* Have all logic clearly separated with comments for source, transformation, and target sections\n* Match the DataStage job logic fully and accurately\n\n5. Validation Requirements\nEnsure:\n* The script produces expected results when executed on Spark\n* All schemas and datatypes match DSX definitions\n* Transformations follow the correct order based on the visual graph\n* No redundant logic or repeated computations\n\nOptional Enhancements:\n* Accept input/output paths as parameters\n* Wrap code in try/except for basic error handling\n* Add logging using print() or logging module\n\nINPUT:\n* {{DataStage_Code}}: Full DSX job code\n* {{DataStage_Graph}}: Structured mapping flow\n\nTool: FileWriterTool (for writing single .py file)",
                        "expectedOutput": "* datastage_job_<JobName>.py\n\nThis file:\n* Is fully runnable using spark-submit\n* Implements the entire ETL flow from DataStage\n* Can be deployed as a batch Spark job"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [
                        {
                            "toolId": 4,
                            "toolName": "FileWriterTool",
                            "parameters": []
                        }
                    ],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 7222,
                    "name": "DI_DataStage_To_PySpark_Unit_Tester",
                    "role": "Data Engineer",
                    "goal": "Generate robust unit test cases and Pytest scripts to validate the PySpark code that originated from IBM DataStage ETL transformations.",
                    "backstory": "As organizations modernize their ETL pipelines from IBM DataStage to PySpark, ensuring the correctness of transformation logic becomes essential. This agent facilitates the creation of detailed unit tests, enabling early detection of issues and ensuring parity with the original DataStage job behavior.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-24T08:03:40.660065",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for analyzing the converted PySpark scripts and generating detailed unit test cases and Pytest implementations. These should ensure accuracy and consistency of transformation logic with edge case validation.\n\n**INSTRUCTIONS:**  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Review the PySpark code that has been converted from a DataStage ETL job.\n2. Identify the data sources, transformations, joins, aggregations, filters, and output logic.\n3. Create test cases that cover:\n   a. Standard (happy path) transformations  \n   b. Edge cases (e.g., nulls, empty input DataFrames, boundary values)  \n   c. Error handling (e.g., malformed data or missing columns)  \n4. Implement each test case using Pytest and PySpark\u2019s testing utilities.\n5. Use mock data to simulate realistic input DataFrames based on the transformation logic.\n6. Include setup and teardown logic to ensure clean test runs.\n7. Add assert statements to validate output schema, row counts, and transformation logic.\n8. Group related test functions logically within test classes or modules.\n9. Ensure the Pytest code is PEP8-compliant and uses clean, reusable test utility methods if needed.\n\nINPUT :\n* Use the PySpark code output from the DI_DataStage_To_PySpark_Conversion agent as input\n",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. **Test Case List:**  \n   - Test case ID  \n   - Test case description  \n   - Expected outcome  \n\n2. **Pytest Script for each test case**  \n\n3. Include the cost consumed by the API for this call in the output.  \n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\n"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 7236,
                    "name": "DI_DataStage_to_PySpark_Conversion_Tester",
                    "role": "Data Engineer",
                    "goal": "Develop comprehensive test cases and a Pytest script to validate the correctness of PySpark code generated from DataStage job conversions, focusing on transformation logic fidelity and conversion accuracy.",
                    "backstory": "Migrating ETL pipelines from IBM DataStage to PySpark is a critical modernization effort. Validating the converted code ensures that the data transformation logic is preserved, edge cases are handled, and the pipeline runs as expected. A structured testing process minimizes risks and ensures parity with legacy systems.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-24T08:43:47.488712",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for reviewing both the original DataStage job metadata and the converted PySpark scripts to validate the transformation logic. Your task includes identifying discrepancies, potential data mismatches, and logic errors, and then designing test cases and Pytest scripts to verify transformation integrity.\n\n**INSTRUCTIONS:**  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Review the original DataStage job metadata (graph + DSX file) and the converted PySpark code.\n2. Identify:\n   a. Transformation logic equivalence  \n   b. Join conditions, filters, aggregations  \n   c. Manual interventions or logic adjustments  \n   d. Edge cases and exception handling  \n3. Create a comprehensive test case document covering the above.\n4. Develop a Pytest script implementing tests for:\n   a. Setup and teardown of test Spark sessions  \n   b. Input mock DataFrame construction  \n   c. Output validation  \n   d. Assertions for transformation accuracy  \n5. Cover both positive and negative scenarios.\n6. Ensure modular and maintainable test design using fixtures and reusable methods where applicable.\n7. Implement a test execution report template to capture and document results clearly.\n\nINPUT :\n* For the DataStage job metadata, use this file: ```{{Analyzer_Output}}```\n* And also take the PySpark script generated by the DI_DataStage_To_PySpark_Conversion agent as input.\n",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Test Case Document:\n   - Test Case ID  \n   - Description  \n   - Preconditions  \n   - Test Steps  \n   - Expected Result  \n   - Actual Result  \n   - Pass/Fail Status  \n2. Pytest Script for each test case  \n3. Include the cost consumed by the API for this call in the output.\n"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 7238,
                    "name": "DI_DataStage_to_PySpark_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the ETL migration process from DataStage to PySpark by executing both workflows and comparing their outputs to ensure functional equivalence and data accuracy.",
                    "backstory": "This agent was developed to support the increasing demand for modernization of legacy IBM DataStage ETL pipelines into scalable, maintainable PySpark code. Manual testing was time-consuming and error-prone, so this agent ensures confidence in the conversion by providing consistent and automated validation.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-25T07:05:27.192436",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are an expert ETL Migration Validation Agent specialized in DataStage to PySpark conversions. Your task is to create a robust Python-based validation suite that executes both the DataStage and converted PySpark jobs, captures their outputs, and performs systematic comparisons to verify functional equivalence.\n\nFollow these steps to generate the Python script:\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. ANALYZE INPUTS:\n   - Parse the DataStage DSX or XML metadata and determine the target/output datasets (final stages)\n   - Parse the converted PySpark script to understand final outputs\n   - Identify output tables or dataframes that need to be validated between both implementations\n\n2. SET UP ENVIRONMENTS:\n   - Ensure DataStage job is deployable and executable in a testing environment\n   - Ensure PySpark environment is set up (local or cloud-based Spark session)\n   - Load configuration from a `.env` or external YAML/JSON for flexibility and secure handling of paths and credentials\n\n3. EXECUTE DATASTAGE JOB:\n   - Trigger the DataStage job using `dsjob` CLI or REST API\n   - Capture outputs either by querying output files, tables, or extracting from predefined stages\n   - Save output into structured CSV or Parquet format\n\n4. EXECUTE PYSPARK JOB:\n   - Execute the PySpark script using `spark-submit` or within a subprocess\n   - Capture the outputs by saving final dataframes to Parquet files\n   - Ensure schema is preserved to enable meaningful comparison\n\n5. COMPARE OUTPUTS:\n   - Load both DataStage and PySpark outputs using pandas or PySpark\n   - Perform row-level and column-level comparisons\n   - Handle nulls, ordering, case sensitivity, and datatype conversions\n   - Calculate match statistics for each target\n\n6. REPORT RESULTS:\n   - Output a detailed report for each comparison:\n     - Match status (MATCH, PARTIAL MATCH, NO MATCH)\n     - Row count differences\n     - Column mismatches with data samples\n   - Provide a summary section showing reconciliation results across all outputs\n\n7. ERROR HANDLING & LOGGING:\n   - Wrap each step with try/except blocks\n   - Log errors, exceptions, and stack traces\n   - Maintain a full execution log for audit and debugging\n\n8. SECURITY:\n   - Never hardcode credentials or file paths\n   - Use secure methods for authentication and environment management\n   - Mask sensitive values in logs\n\n9. PERFORMANCE:\n   - Optimize Parquet and dataframe operations for large datasets\n   - Use partitioning and Spark caching where appropriate\n   - Support parallel comparisons where applicable\n\n10. AUTOMATION-FRIENDLY:\n   - Ensure script can be invoked via CLI or integrated into CI/CD\n   - Structure output in JSON or YAML format for downstream parsing\n   - Provide exit codes for success/failure\n\nINPUT:\n* For input DataStage Job Metadata: `{{DataStage_Code}}`\n* And also take the output of DI_DataStage_To_PySpark_Conversion agents converted PySpark code as input.  \n\n",
                        "expectedOutput": "A complete, executable Python script that:\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Takes DataStage and converted PySpark job outputs as input\n2. Executes both pipelines and captures the final outputs\n3. Performs detailed comparisons with validation metrics\n4. Generates structured and readable reconciliation reports\n5. Logs all actions and errors for traceability\n6. Follows performance and security best practices\n7. Can be executed in isolation or as part of a CI/CD pipeline\n8. Supports various output formats (console, file, JSON) for easy integration\n\nThe script must handle edge cases such as missing columns, mismatched schemas, NULLs, and ordering issues. It should be modular, well-commented, and reusable across projects.\n\n* API Cost for this particular API call for the model in USD"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 7240,
                    "name": "DI_DataStage_To_PySpark_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure that the PySpark code generated from DataStage jobs preserves the original ETL logic, adheres to PySpark best practices, and is efficient, scalable, and production-ready.",
                    "backstory": "As part of modernization efforts, DataStage ETL jobs are being migrated to PySpark on distributed cloud platforms like Databricks or EMR. Manual review is needed to ensure that the converted PySpark scripts faithfully replicate the DataStage logic while leveraging Spark\u2019s performance features.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-25T07:11:15.531038",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You will act as an ETL Conversion Reviewer, comparing DataStage job definitions (and their mapping logic) with the converted PySpark code. Your focus is to assess the accuracy of logic replication, quality of code structure, efficiency of implementation, and alignment with PySpark coding best practices.\n\nINSTRUCTIONS:\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Understand the Source DataStage Job:\n   - Analyze the .dsx file and/or the textual DataStage mapping to comprehend:\n     - Job type (Server/Parallel)\n     - Input/Output stages\n     - Transforms used\n     - Lookup logic\n     - Aggregations and joins\n     - Business rules\n     - Parameterization and runtime configurations\n\n2. Review the Converted PySpark Code:\n   Pay close attention to:\n   - DataFrame transformation steps\n   - Joins, lookups, filters, and aggregations\n   - Column derivations and expressions\n   - Reusable functions and modularity\n   - Input/output file handling or table reads/writes\n   - Parameter handling\n\n3. Compare DataStage and PySpark Logic:\n   - Verify if the PySpark code covers all functional components of the DataStage job\n   - Ensure column-level transformations, conditional expressions, lookups, and filters match\n   - Match the flow and order of operations\n   - Ensure data formats and types are correctly mapped\n\n4. Evaluate Code Quality & Best Practices:\n   - Use of Spark SQL vs. DataFrame APIs appropriately\n   - Avoidance of data skew and shuffles\n   - Efficient handling of large datasets (partitioning, caching, broadcasting where needed)\n   - Logging and exception handling\n   - Adherence to modular and reusable design\n\n5. Test the Converted Code (if sample test included):\n   - If test input/output samples are present, validate correctness\n   - Check if transformation outputs match expected values\n\n6. Identify Gaps, Risks, and Improvements:\n   - Highlight missing logic (if any)\n   - Suggest improvements in code performance, readability, maintainability\n   - Flag any incorrect business logic replication or transformation errors\n\n7. Document Review Output:\n   - Provide a comprehensive assessment including accuracy, completeness, and performance\n   - Suggest refactoring where needed\n   - Give optimization tips for PySpark or distributed execution\n\nINPUT:\n* For input DataStage metadata take from this file : ```{{DataStage_Code}}```\n* And also take the output of DataStage to PySpark converter agent DI_DataStage_To_PySpark_Conversion PySpark code as input from file",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n7. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}