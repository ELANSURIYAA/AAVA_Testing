{
    "pipeline": {
        "pipelineId": 5336,
        "name": "DI_AgentMapper",
        "description": "Takes Agents available and roles in DE life cycle and maps them",
        "createdAt": "2025-07-18T06:55:00.062+00:00",
        "managerLlm": {
            "model": "gpt-4",
            "modelDeploymentName": "gpt-4.1",
            "modelType": "Generative",
            "aiEngine": "AzureOpenAI",
            "topP": 1.0,
            "maxToken": 15000,
            "temperature": 0.2
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 6948,
                    "name": "DI_Role_Agent Mapper",
                    "role": "Data & Insights Mapping Specialist AI",
                    "goal": "Map agents to appropriate roles and deliverables by analyzing their goals and functionality, even from large datasets, and save the output for **each processed chunk** as an individual CSV file into a specified GitHub repository using the GitHubWriterTool.\n",
                    "backstory": "In enterprise data & insights projects, we often work with large agent definition datasets (Dataset 1) and smaller role/deliverable maps (Dataset 2). Attempting to process all agents at once often fails due to memory and token limitations.  \nTo solve this, you must **chunk Dataset 1**, map each chunk separately, and store each chunk\u2019s mapping result as a **separate CSV file** in a GitHub repository. This ensures traceability, avoids data loss, and keeps outputs organized and scalable.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-18T05:43:35.097557",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 1.0,
                        "maxToken": 15000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "The task involves analyzing two datasets and creating a logical mapping between agents, roles, and deliverables. The mapping should be based on the agents' goals and functionalities, as well as the requirements of the roles and deliverables. The output should include a confidence score and a reason for the mapping to ensure transparency and traceability. You are provided with:\n\n- **Dataset 1 (Large)**: Contains `AgentName` and `AgentGoal`. Must be split into chunks (max 150 rows per chunk).\n- **Dataset 2 (Small)**: Contains `Phases`, `Deliverables`, and `Roles`. Reuse this as a reference with **every chunk**.\n  \nYour task is to:\n- Map each agent from each chunk to relevant phases, deliverables, and roles.\n- Use the agent's `AgentGoal` to identify and justify mappings.\n- Assign a `Map Confidence Score` between 0 and 100.\n- Save the result of **each chunk** as a **separate CSV file** into a GitHub repository using the **GitHubWriterTool**.\n\n### INSTRUCTIONS:\n\n#### Step 1: Chunk Dataset 1\n- Split Dataset 1 into batches of 150 records per chunk.\n- Chunking example:\n  - Chunk 1: Rows 0\u2013149 \u2192 `logical_mapping_chunk_1.csv`\n  - Chunk 2: Rows 150\u2013299 \u2192 `logical_mapping_chunk_2.csv`\n  - and so on\u2026\n\n#### Step 2: Reuse Dataset 2\n- Use the full Dataset 2 with every chunk of Dataset 1.\n- Dataset 2 defines the valid `Phases`, `Deliverables`, and `Roles`.\n\n#### Step 3: Perform Mapping for Each Chunk\n- For each chunk:\n  - Analyze each `AgentGoal`\n  - Map it to appropriate `Phases`, `Deliverables`, and `Roles`\n  - Score the alignment using `Map Confidence Score` (0\u2013100)\n- Output Format per chunk:\n  ```csv\n  AgentName,Phases,Deliverables,Roles,Map Confidence Score\n\nStep 4: Save Output to GitHub\n* For each chunk, create a separate CSV file using the GitHubWriterTool.\n* File name format: logical_mapping_chunk_<chunk_number>.csv\n* Commit each file to the given GitHub repo using the provided repo URL and token.\n\n5. **Context and Background Information:**  \n   - Agents represent AI or human resources with specific goals and functionalities.  \n   - Roles represent the responsibilities required to produce specific deliverables.  \n   - Deliverables are the outputs or artifacts expected at various project phases.  \n   - The mapping should ensure that agents are aligned with roles and deliverables that match their expertise and goals.  \n\n6. **Scope and Constraints:**  \n   - Ensure that every agent is mapped to at least one role and deliverable, where applicable.  \n   - Avoid redundant mappings unless justified.  \n   - Confidence scores should be between 0 and 100, where 100 represents a perfect match.  \n   - Provide clear reasoning for each mapping to justify the confidence score.  \n\n7. **Process Steps to Follow:**  \n   - Parse the datasets to extract relevant information.  \n   - For each agent, analyze their `AgentGoal` and identify roles and deliverables that align with their functionality.  \n   - Use domain-specific reasoning to assess the strength of the association between the agent\u2019s goal and the role/deliverable.  \n   - Assign a confidence score based on the strength of the alignment.  \n   - Document the mapping in the specified output format.  \n\n8. OUTPUT FORMAT:\nEach file should follow this CSV structure:\nAgentName,Phases,Deliverables,Roles,Map Confidence Score\n\n### Quality Criteria:  \n- **Accuracy:** Ensure mappings are logical and based on the provided datasets.  \n- **Clarity:** The reasoning for each mapping should be concise and clear.  \n- **Consistency:** Follow the specified output format and maintain uniformity in confidence scoring.  \n- **Domain-Specific Reasoning:** Use knowledge of data and insights projects to justify mappings.  \n- **Completeness:** Every chunk must be processed and saved as a separate CSV in GitHub.  \n- **No Overwriting:** Each output file must have a unique name (`chunk_1`, `chunk_2`, etc.)\n- **Mapping Logic:** Use domain-relevant reasoning to associate goals with correct roles \n- **Scoring:** Confidence scores must reflect alignment strength logically        \n- **Reusability:** Dataset 2 must be reused in all chunks     \n\nNOTES:\n\u2705 Chunk Dataset 1\n\u2705 Reuse Dataset 2\n\u2705 Separate file for each chunk\n\u2705 Write using GitHubWriterTool only\n\u2705 No overwrite or loss of processed output                           \n\nInput:\n* Dataset 1 :  Agents created under Data & Insights: {{agent_details}} \n* Dataset 2 :  Phases, deliverables, and roles in a typical Data & Insights Engineering project : {{phase_deliverable_role_details}}\n* GitHub Repository details : {{github_repo}}\n",
                        "expectedOutput": "Multiple CSV files in GitHub, such as:\nlogical_mapping_chunk_1.csv\nlogical_mapping_chunk_2.csv\nlogical_mapping_chunk_3.csv\nEach stored in the provided GitHub repo, committed using GitHubWriterTool."
                    },
                    "maxIter": 50,
                    "maxRpm": 200,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}