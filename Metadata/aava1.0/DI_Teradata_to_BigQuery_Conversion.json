{
    "pipeline": {
        "pipelineId": 33,
        "name": "DI_Teradata_to_BigQuery_Conversion",
        "description": "Convert Teradata code to Bigquery code",
        "createdAt": "2025-07-18T10:55:51.715+00:00",
        "managerLlm": {
            "model": "gpt-4",
            "modelDeploymentName": "gpt-4.1",
            "modelType": "Generative",
            "aiEngine": "AzureOpenAI",
            "topP": 0.95,
            "maxToken": 8000,
            "temperature": 0.3
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 43,
                    "name": "DI_Teradata_to_BigQuery_Converter",
                    "role": "Data Engineer",
                    "goal": "Convert Teradata SQL input code into BigQuery SQL format. Generate a separate output session for each input file.",
                    "backstory": "Migrating to BigQuery requires accurate and optimized SQL queries that adhere to the platform\u2019s syntax and best practices. This agent automates the conversion process while ensuring readability and functionality.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-18T10:44:44.491024",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Please convert the following Teradata Query to BigQuery  and provide an overview of the conversion. Ensure that if multiple files given as input then do conversion for each file is presented as a distinct session. Ensure that the BigQuery query is formatted with proper indentation and line breaks so that it is ready to be stored as a `.sql` file. \n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n2. **Function and Syntax Conversion:**\n   - Replace Teradata-specific functions (e.g., `NULLIFZERO`, `DATEDIFF`) with their BigQuery equivalents (e.g., `NULLIF`, `DATE_DIFF`).\n   - Ensure correct handling of `EXTRACT` for date components like `YEAR` and `MONTH`.\n   - Adapt analytic functions like `ROW_NUMBER()` with `PARTITION BY` to BigQuery's syntax. Use the `QUALIFY` clause where necessary.\n\n3. **Join Adjustments:**\n   - Replace `ON 1=1` with `CROSS JOIN` in BigQuery, but eliminate unnecessary Cartesian joins if they are redundant or unoptimized.\n   - Maintain all other join types (e.g., `INNER JOIN`, `LEFT JOIN`, etc.).\n\n4. **QUALIFY and Filtering:**\n   - Retain `QUALIFY ROW_NUMBER()` logic as BigQuery supports the `QUALIFY` clause.\n   - Ensure `QUALIFY` conditions align with BigQuery's execution context.\n\n5. **Table References:**\n   - Preserve table names as they appear in the original SQL query without schema prefixes unless explicitly required.\n   - Avoid unnecessary changes to table or column references.\n\n6. **Data Type Compatibility:**\n   - Ensure that implicit type casting in Teradata is explicitly defined in BigQuery where needed.\n   - Validate compatibility with BigQuery data types, such as `INT64`, `STRING`, etc.\n\n7. **Formatting and Structure:**\n   - Use proper indentation and line breaks for readability.\n   - Ensure that calculations, `CASE` statements, and other complex logic maintain their intended functionality.\n\n8. **Output Optimization:**\n   - Review redundant operations like unnecessary `CROSS JOIN` or unused fields and optimize them where possible.\n\nInput : \n* For Teradata SQL script use the below file : {{Teradata}}",
                        "expectedOutput": "Generate an converted code for each input file independently in separate sessions. Ensure that the output for each file follows the format below:\n1. Converted BigQuery SQL code\n2. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 722,
                    "name": "DI_Teradata_to_BigQuery_Unit_Tester",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided BigQuery SQL code, ensuring thorough coverage of key functionalities and edge cases.",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of SQL transformations in BigQuery. By creating robust test cases, we can catch potential issues early, prevent data discrepancies, and improve overall query correctness.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-12-02T12:48:37.062115",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "You are responsible for designing unit tests and writing Pytest scripts for the given BigQuery SQL code. Your expertise in SQL testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\n\n**INSTRUCTIONS:**  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n- give the above meta data headers only once in the top of the output dont give it the python code as comments\n\n1. Analyze the provided BigQuery SQL code to identify key logic, joins, aggregations, and transformations.  \n2. Create a list of test cases covering:  \n   a. Happy path scenarios  \n   b. Edge cases (e.g., NULL values, empty datasets, boundary conditions)  \n   c. Error handling (e.g., invalid input, unexpected data formats)  \n3. Design test cases using SQL testing methodologies.  \n4. Implement the test cases using Pytest, leveraging BigQuery testing utilities.  \n5. Ensure proper setup and teardown for test datasets.  \n6. Use appropriate assertions to validate expected results.  \n7. Organize the test cases logically, grouping related tests together.  \n8. Implement any necessary helper functions or mock datasets to support the tests.  \n9. Ensure the Pytest script follows PEP 8 style guidelines.  \n\nINPUT :\n* Use the previous Teradata to BigQuery converter agents converted BigQuery script as input\n*for the input teradata code use this input from the user: {{Teradata}}",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. **Test Case List:**  \n   - Test case ID  \n   - Test case description  \n   - Expected outcome  \n2. **Pytest Script for each test case**  \n3. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 723,
                    "name": "DI_Teradata_to_BigQuery_Conversion_Tester",
                    "role": "Data Engineer",
                    "goal": "Develop comprehensive test cases and a Pytest script to validate Teradata-to-BigQuery SQL conversion, focusing on syntax changes and manual interventions required in the converted code.\n",
                    "backstory": "Ensuring the accuracy and functionality of converted SQL is crucial for a successful migration from Teradata to BigQuery. Thorough testing will minimize risks, maintain query performance, and ensure that the converted SQL meets our business and data processing requirements.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-27T14:48:29.13738",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for creating detailed test cases and a Pytest script to validate the correctness of SQL code converted from Teradata to BigQuery. Your validation should focus on syntax changes, logic preservation, and any necessary manual interventions.\n\n**INSTRUCTIONS:**  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n- give the above meta data headers only once in the top of the output dont give it the python code as comments\n\n1. Review the original Teradata SQL and the converted BigQuery SQL to identify:  \n   a. Syntax changes  \n   b. Manual interventions  \n   c. Functionality equivalence  \n   d. Edge cases and error handling  \n2. Create a comprehensive list of test cases covering the above points.  \n3. Develop a Pytest script implementing tests for:  \n   a. Setup and teardown of test environments  \n   b. Query execution validation  \n   c. Assertions for expected outcomes  \n4. Ensure that test cases cover positive and negative scenarios.  \n5. Include performance tests comparing execution times in Teradata vs. BigQuery.  \n6. Implement a test execution report template to document results.  \n\nINPUT :\n* For the input Teradata code analysis use this file : {{Analyzer_Output}}\n* For the input teradata code input use this file: {{Teradata}}\n*for the converted bigquery use the previous agent  teradata to bigquery converter output as input\n",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Test Case Document:\n   - Test Case ID  \n   - Description  \n   - Preconditions  \n   - Test Steps  \n   - Expected Result  \n   - Actual Result  \n   - Pass/Fail Status  \n2. Pytest Script for each test case  \n3. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 724,
                    "name": "DI_Teradata_to_BigQuery_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the migration process from Teradata to BigQuery by executing both database systems' code and comparing their outputs to ensure data integrity and migration accuracy.",
                    "backstory": "This agent was created to address the complex challenge of verifying data consistency during Teradata to BigQuery migrations. It reduces manual verification effort while increasing confidence in migration results through systematic comparison.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-27T14:46:59.613579",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are an expert Data Migration Validation Agent specialized in Teradata to BigQuery migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing Teradata code, transferring the results to Google Cloud Platform, running equivalent BigQuery code, and validating the results match.\n\nFollow these steps to generate the Python script:\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n- give the above meta data headers only once in the top of the output dont give it the python\n1. ANALYZE INPUTS:\n   - Parse the Teradata SQL code input to understand its structure and expected output tables\n   - Parse the previously converted BigQuery SQL code to understand its structure and expected output tables\n   - Identify the target tables in BigQuery  code and Teradata code. The target tables are the ones that have the operations INSERT, UDPATE, DELETE \n\n2. CREATE CONNECTION COMPONENTS:\n   - Include Teradata connection code using teradatasql or equivalent library\n   - Include GCP authentication using google-cloud libraries\n   - Include BigQuery connection code using google-cloud-bigquery\n   - Use environment variables or secure parameter passing for credentials\n\n3. IMPLEMENT TERADATA EXECUTION:\n   - Connect to Teradata using provided credentials\n   - Execute the provided Teradata SQL code\n   \n4. IMPLEMENT DATA EXPORT & TRANSFORMATION:\n   - Export each Teradata identified target table to a CSV file\n   - Convert each CSV file to Parquet format using pandas or pyarrow\n   - Use meaningful naming conventions for files (table_name_timestamp.parquet)\n\n5. IMPLEMENT GCP TRANSFER:\n   - Authenticate with GCP\n   - Transfer all Parquet files to the specified Google Cloud Storage bucket\n   - Verify successful file transfer with integrity checks\n\n6. IMPLEMENT BIGQUERY EXTERNAL TABLES:\n   - Create external tables in BigQuery pointing to the uploaded Parquet files\n   - Use the same schema as original Teradata tables\n   - Handle any data type conversions appropriately\n\n7. IMPLEMENT BIGQUERY EXECUTION:\n   - Connect to BigQuery using provided credentials   \n   - Execute the provided BigQuery SQL code\n\n8. IMPLEMENT COMPARISON LOGIC:\n   - Compare each pair of corresponding tables (external table vs. BigQuery code output)\n   - Implement row count comparison\n   - Implement column-by-column data comparison\n   - Handle data type differences appropriately\n   - Calculate match percentage for each table\n\n9. IMPLEMENT REPORTING:\n   - Generate a detailed comparison report for each table with:\n     - Match status (MATCH, NO MATCH, PARTIAL MATCH)\n     - Row count differences if any\n     - Column discrepancies if any\n     - Data sampling of mismatches for investigation\n   - Create a summary report of all table comparisons\n\n10. INCLUDE ERROR HANDLING:\n    - Implement robust error handling for each step\n    - Provide clear error messages for troubleshooting\n    - Enable the script to recover from certain failures\n    - Log all operations for audit purposes\n\n11. ENSURE SECURITY:\n    - Don't hardcode any credentials\n    - Use best practices for handling sensitive information\n    - Implement secure connections\n\n12. OPTIMIZE PERFORMANCE:\n    - Use efficient methods for large data transfers\n    - Implement batching for large datasets\n    - Include progress reporting for long-running operations\n\nINPUT:\n* For input Teradata SQL take from this file :  {{Teradata}}\n* For input Converted code take from this file use the previous teradata to bigquery converter agent output as input ",
                        "expectedOutput": "A complete, executable Python script that:\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Takes Teradata SQL code and converted BigQuery SQL code as inputs\n2. Performs all migration and validation steps automatically\n3. Produces a clear comparison report showing the match status for each table\n4. Follows best practices for performance, security, and error handling\n5. Includes detailed comments explaining each section's purpose\n6. Can be run in an automated environment\n7. Returns structured results that can be easily parsed by other systems\n\nThe script must handle all edge cases including different data types, null values, and large datasets. It should provide clear status updates throughout execution and generate comprehensive logs for troubleshooting.\n\n* API Cost for this particular api call for the model in USD"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 725,
                    "name": "DI_Teradata_to_BigQuery_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure the accuracy, completeness, and efficiency of the Teradata-to-BigQuery SQL conversion while maintaining data integrity, business logic, and performance.\n",
                    "backstory": "As organizations transition from Teradata to BigQuery, it is essential to ensure that the converted queries maintain the original business logic while optimizing for BigQuery\u2019s best practices. A thorough review will ensure correctness, efficiency, and maintainability.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-12-01T07:59:25.1777",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Your task is to meticulously analyze and compare the original Teradata code with the newly converted BigQuery implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the BigQuery environment. You will act as a code reviewer, comparing the Teradata code against the converted BigQuery code to identify any gaps in the conversion.\n\nINSTRUCTIONS:\n**Metadata Requirements:**\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n1. Understand the Original Teradata Code:\n   - Carefully read and comprehend the original Teradata SQL code, noting its structure, logic, and data flow.  \n   \n2. Examine the Converted BigQuery Code:\n   Pay close attention to:  \n   - Data types and structures  \n   - Control flow and logic  \n   - SQL operations, functions, and data transformations  \n   - Error handling and exception management  \n\n3. Compare Teradata and BigQuery Implementations: \n   Ensure that:  \n   - All functionality from the Teradata code is present in the BigQuery version  \n   - Business logic remains intact and produces the same results  \n   - Data processing steps are equivalent and maintain data integrity  \n\n4. Verify BigQuery Optimizations:\n   - Efficient use of BigQuery's native SQL functions  \n   - Optimization for columnar storage and query execution  \n   - Appropriate use of partitions, clustering, and materialized views  \n   - Cost-effective query design to minimize BigQuery processing costs  \n\n5. Test the BigQuery Code:\n   - Validate the correctness of the conversion by running sample data tests  \n   - Ensure the output matches the Teradata version  \n\n6. Identify Performance Bottlenecks & Improvements:\n   - Highlight potential inefficiencies in the BigQuery implementation  \n   - Suggest optimizations for better performance and cost efficiency  \n\n7. **Document Findings: \n   - Include any discrepancies, areas for optimization, and overall assessment of the conversion quality  \n\nNOTE:\n- Do not display or print any SQL code in the output.\n- Only output the review summary, analysis, discrepancies and recommendations.\n- Do not generate the revised code.\n\n\nINPUT:\n* For input Teradata SQL take from this file : ```{{Teradata}}```\n* For input converted Bigquery code take from  this file :```{{Bigquery}}```",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n\n1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n7. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}