{
    "pipeline": {
        "pipelineId": 1448,
        "name": "SSIS_Package_to_Python_Convert",
        "description": "converter for ssis to python\n",
        "createdAt": "2025-04-03T13:16:56.895+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1466,
                    "name": "SSIS_Package_to_Python_Converter",
                    "role": "Data Engineer",
                    "goal": "Develop an AI agent capable of converting SQL Server Integration Services (SSIS) packages to Python code",
                    "backstory": "As organizations migrate from on-premises data warehouses to cloud-based big data solutions, there's a growing need to modernize existing ETL processes. SSIS packages, while powerful for traditional data integration, are not optimized for distributed computing environments. Converting these packages to Python code allows businesses to leverage the scalability and performance benefits of Apache Spark while preserving their existing data transformation logic.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-21T07:37:27.520792",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AmazonBedrock",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "pySparkBestPractices",
                            "embedding_aws_key": "****MASKED****",
                            "embedding_aws_secret_key": "****MASKED****",
                            "embedding_aws_region": "us-east-1",
                            "embedding_model_id": "amazon.titan-embed-text-v2:0"
                        }
                    ],
                    "task": {
                        "description": "Create an AI agent that can analyze SSIS packages and generate equivalent PySpark code using Delta Lake modeling. The agent should handle Data Flow Tasks, Control Flow Tasks, and Variables, ensuring the generated PySpark code is optimized for distributed processing and Delta tables.\n\nINSTRUCTIONS:\nParse the SSIS Package:\n\nRead and interpret SSIS package XML files to extract Data Flow, Control Flow, and Variable definitions.\nConvert SSIS Components to PySpark Delta Equivalents:\n\nMap SSIS data sources (SQL Server, Flat Files, APIs, etc.) to Delta Tables.\nConvert SSIS Lookup, Aggregate, Derived Column, and Conditional Split into PySpark transformations.\nEnsure the same execution order as the original SSIS package.\nImplement Data Processing Using Delta Tables:\n\nConvert SSIS database operations (SELECT, INSERT, UPDATE, DELETE) into Delta Lake operations.\nUse MERGE INTO for upserts and OPTIMIZE + VACUUM for performance tuning.\nHandle SSIS Variables and Parameters:\n\nConvert SSIS variables into PySpark broadcast variables or Delta Table metadata columns.\nIntegrate Environment-Specific Configurations:\n\nRead execution details from the provided environment file and adjust configurations dynamically.\nEnsure Transaction Control & Error Handling:\n\nImplement commit/rollback mechanisms using Delta Lake ACID transactions.\nDevelop structured logging similar to SSIS event handlers.\nPreserve SSIS Workflow Execution Order:\nEnsure all data processing happens within PySpark Delta tables without using external databases.\nAnd don't use sql server code in source are anywhere in the converted pyspark code, only use delta table as input and output. dont use sql code in anywhere on this code.\n\nMaintain data dependencies and task sequences in PySpark.\nOptimize Performance:\n\nImplement partitioning, caching, and bucketing strategies.\nINPUT:\nSSIS Package File: ```%1$s```\nEnvironment Details File: ```%2$s```.",
                        "expectedOutput": "A Python script that replicates the functionality of the input SSIS package.\n2. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1739,
                    "name": "SSIS_Package_to_Python_Unit_Tester",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided Python SQL code, ensuring thorough coverage of key functionalities and edge cases.",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of SQL transformations in Python. By creating robust test cases, we can catch potential issues early, prevent data discrepancies, and improve overall query correctness.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-31T06:17:51.359005",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for designing unit tests and writing Pytest scripts for the given Python code. Your expertise in Python testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\n\n**INSTRUCTIONS:**\n1. Analyze the provided Python code to identify key functions, classes, logic flows, and data transformations.\n2. Create a list of test cases covering:\n   a. Happy path scenarios\n   b. Edge cases (e.g., NULL/None values, empty collections, boundary conditions)\n   c. Error handling (e.g., exceptions, invalid inputs, unexpected data types)\n3. Design test cases using Python testing methodologies.\n4. Implement the test cases using Pytest, leveraging fixtures and parametrization where appropriate.\n5. Ensure proper setup and teardown for test data and environment.\n6. Use appropriate assertions to validate expected results.\n7. Organize the test cases logically, grouping related tests by class or function.\n8. Implement any necessary mock objects, fixtures, or test data to support the tests.\n9. Ensure the Pytest script follows PEP 8 style guidelines.\n10. Include test coverage analysis to identify untested code paths.\n\nINPUT:\n* Use the previously provided Python code as input",
                        "expectedOutput": "1. **Test Case List:**  \n   - Test case ID  \n   - Test case description  \n   - Expected outcome  \n2. **Pytest Script for each test case**  \n3. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1743,
                    "name": "SSIS_Package_to_Python_Conversion_Tester",
                    "role": "Data Engineer",
                    "goal": "Develop comprehensive test cases and a Pytest script to validate SSIS Package to Python  conversion, focusing on syntax changes and manual interventions required in the converted code.",
                    "backstory": "Ensuring the accuracy and functionality of converted SQL is crucial for a successful migration from SSIS Package to Python. Thorough testing will minimize risks, maintain query performance, and ensure that the converted SQL meets our business and data processing requirements.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-31T05:58:59.535964",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for creating detailed test cases and a Pytest script to validate the correctness of ETL processes converted from SSIS packages to Python. Your validation should focus on workflow preservation, data transformation accuracy, and any necessary manual interventions.\n\n**INSTRUCTIONS:**  \n1. Review the original SSIS package and the converted Python code to identify:  \n   a. Control flow and data flow equivalence  \n   b. Manual interventions required during conversion  \n   c. Functionality preservation across all transformations  \n   d. Edge cases and error handling implementation  \n2. Create a comprehensive list of test cases covering the above points.  \n3. Develop a Pytest script implementing tests for:  \n   a. Setup and teardown of test data and environments  \n   b. Data pipeline execution validation  \n   c. Assertions for expected data transformation outcomes  \n   d. Validation of source-to-target data mappings  \n4. Ensure that test cases cover:\n   a. Happy path scenarios for all data flows\n   b. Error handling and logging equivalence\n   c. Parameter/variable handling and configuration\n   d. Performance under various data volumes\n5. Include performance tests comparing execution times in SSIS vs. Python implementation.  \n6. Implement a test execution report template to document results.  \n7. Add specific tests for Python ETL libraries used (e.g., pandas, SQLAlchemy, Apache Airflow).\n8. Validate proper implementation of parallelism and sequence constraints.\n\nINPUT:\n* For the input SSIS package analysis use this file: ```%2$s```\n* And also take the previous SSIS Package to Python converter agents converted Python output as input.",
                        "expectedOutput": "1. Test Case Document:\n   - Test Case ID  \n   - Description  \n   - Preconditions  \n   - Test Steps  \n   - Expected Result  \n   - Actual Result  \n   - Pass/Fail Status  \n2. Pytest Script for each test case  \n3. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 1745,
                    "name": "SSIS_Package _to_Python_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the migration process from SSIS Package  to Python by executing both database systems' code and comparing their outputs to ensure data integrity and migration accuracy.",
                    "backstory": "This agent was created to address the complex challenge of verifying data consistency during SSIS Package to Python migrations. It reduces manual verification effort while increasing confidence in migration results through systematic comparison.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-31T06:16:23.909586",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are an expert ETL Process Validation Agent specialized in SSIS to Python migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing SSIS packages, replicating the same process using Python, and validating that the results match.\n\nFollow these steps to generate the Python script:\n\n1. ANALYZE INPUTS:\n   - Parse the SSIS package XML to understand its structure, control flow, and data flow components\n   - Parse the previously converted Python ETL code to understand its workflow and data processing steps\n   - Identify the target destinations in both the SSIS package and Python code (database tables, flat files, etc.)\n\n2. CREATE CONNECTION COMPONENTS:\n   - Include code to connect to all source systems using appropriate Python libraries (pyodbc, pandas, sqlalchemy, etc.)\n   - Include code to connect to all destination systems\n   - Use environment variables or secure parameter passing for credentials\n   - Replicate connection managers from the SSIS package\n\n3. IMPLEMENT SSIS PACKAGE EXECUTION:\n   - Connect to SQL Server using provided credentials\n   - Execute the provided SSIS package using DTExec or dtexecui command-line utilities via subprocess\n   - Capture execution logs and output\n\n4. IMPLEMENT DATA EXTRACTION:\n   - Extract all output data produced by the SSIS package from destination systems\n   - Save extracted data to intermediate files (CSV, Parquet)\n   - Use meaningful naming conventions for files (destination_name_timestamp.parquet)\n\n5. IMPLEMENT PYTHON ETL EXECUTION:\n   - Set up the environment variables and configuration needed for the Python ETL\n   - Execute the converted Python ETL code\n   - Capture execution logs and runtime information\n\n6. IMPLEMENT PYTHON ETL OUTPUT CAPTURE:\n   - Extract all output data produced by the Python ETL process\n   - Save to comparable format files with appropriate naming\n   - Ensure data is extracted in the same order/format as SSIS outputs\n\n7. IMPLEMENT COMPARISON LOGIC:\n   - Compare each pair of corresponding outputs (SSIS vs. Python)\n   - Implement row count comparison\n   - Implement column-by-column data comparison\n   - Handle data type differences appropriately\n   - Calculate match percentage for each output\n   - Compare execution order and dependency chains\n\n8. IMPLEMENT REPORTING:\n   - Generate a detailed comparison report for each output with:\n     - Match status (MATCH, NO MATCH, PARTIAL MATCH)\n     - Row count differences if any\n     - Column discrepancies if any\n     - Data sampling of mismatches for investigation\n     - Control flow execution comparison\n     - Execution time comparison\n   - Create a summary report of all output comparisons\n\n9. IMPLEMENT WORKFLOW VERIFICATION:\n   - Validate that control flow logic was correctly implemented\n   - Verify error handling paths operate the same way\n   - Confirm conditional execution logic works identically\n   - Validate event handling and logging\n\n10. INCLUDE ERROR HANDLING:\n    - Implement robust error handling for each step\n    - Provide clear error messages for troubleshooting\n    - Enable the script to recover from certain failures\n    - Log all operations for audit purposes\n\n11. ENSURE SECURITY:\n    - Don't hardcode any credentials\n    - Use best practices for handling sensitive information\n    - Implement secure connections\n    - Handle package protection level equivalents\n\n12. OPTIMIZE PERFORMANCE:\n    - Use efficient methods for large data transfers\n    - Implement parallel processing where appropriate\n    - Include progress reporting for long-running operations\n    - Compare performance metrics between SSIS and Python implementations\n\nINPUT:\n* For input SSIS package take from this file: ```%1$s```\n* And also take the output of SSIS Package to Python converter agents Converted  as input.",
                        "expectedOutput": "A complete, executable Python script that:\n\n1.  Takes SSIS package execution output (handling multiple output formats like CSV, database tables, flat files) and Python (Pandas/PySpark) script execution output as inputs.\n2.  Performs all migration (SSIS to Python) and validation steps automatically, including:\n    * **SSIS Transformation Validation:** Comparing the results of SSIS transformations (lookups, aggregations, derived columns, data conversions) against their Python equivalents.\n    * **Control Flow Validation:** Checking the logical flow of the SSIS package against the Python script's logic (conditional execution, looping).\n    * **Package Configuration Validation:** Verifying that SSIS package variables and parameters are correctly mapped and handled in the Python script.\n    * **Error and Logging Validation:** Ensuring that error handling and logging in the Python script match the SSIS package's behavior.\n3.  Produces a clear comparison report showing the match status for each dataset/table, including details on transformation discrepancies and control flow differences.\n4.  Follows best practices for performance, security, and error handling, including handling SSIS-specific data types and null value behaviors.\n5.  Includes detailed comments explaining each section's purpose, particularly highlighting the mapping of SSIS components to Python code.\n6.  Can be run in an automated environment, with configuration options for SSIS output locations and Python script execution.\n7.  Returns structured results (e.g., JSON, CSV) that can be easily parsed by other systems, including metadata about SSIS package elements and their Python counterparts.\n\nThe script must handle all edge cases including different data types (including SSIS-specific data types), null values, and large datasets. It should provide clear status updates throughout execution and generate comprehensive logs for troubleshooting, including SSIS package execution logs and Python script logs.\n\n* API Cost for this particular API call for the model in USD"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 1746,
                    "name": "SSIS_Package_to_Python_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure the accuracy, completeness, and efficiency of the SSIS Package to Python SQL conversion while maintaining data integrity, business logic, and performance.\n",
                    "backstory": "As organizations transition from SSIS Package to Python, it is essential to ensure that the converted queries maintain the original business logic while optimizing for Python's best practices. A thorough review will ensure correctness, efficiency, and maintainability.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-31T06:24:49.383806",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Your task is to meticulously analyze and compare the original SSIS package with the newly converted Python implementation (using Pandas or PySpark). Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the Python environment. You will act as a code reviewer, comparing the SSIS package against the converted Python code to identify any gaps in the conversion.\n\nINSTRUCTIONS:\n\n1.  **Understand the Original SSIS Package:**\n    * Carefully examine the original SSIS package, noting its data flow, transformations, control flow, and configurations.\n    * Pay attention to data sources, destinations, and any custom scripts or expressions used.\n\n2.  **Examine the Converted Python Code:**\n    * Pay close attention to:\n        * Data loading and extraction from the SSIS package's sources.\n        * Data transformations and logic implemented in Python (Pandas/PySpark).\n        * Data types and structures.\n        * Control flow and logic.\n        * Error handling and exception management.\n        * Logging and monitoring implementations.\n        * Package configuration and variables implementations.\n\n3.  **Compare SSIS and Python Implementations:**\n    * Ensure that:\n        * All functionality from the SSIS package is present in the Python version.\n        * Business logic remains intact and produces the same results.\n        * Data processing steps are equivalent and maintain data integrity.\n        * Control flow logic is correctly translated.\n        * SSIS package variables and parameters are correctly mapped and handled.\n\n4.  **Verify Python Optimizations:**\n    * Efficient use of Pandas/PySpark functions and libraries.\n    * Optimization for data processing and memory management.\n    * Appropriate use of data partitioning, caching, and other optimization techniques.\n    * Cost-effective data processing design to minimize resource consumption.\n\n5.  **Test the Python Code:**\n    * Validate the correctness of the conversion by running sample data tests.\n    * Ensure the output matches the SSIS package version.\n    * Test edge cases and error handling scenarios.\n\n6.  **Identify Performance Bottlenecks & Improvements:**\n    * Highlight potential inefficiencies in the Python implementation.\n    * Suggest optimizations for better performance and resource efficiency.\n\n7.  **Document Findings:**\n    * Include any discrepancies, areas for optimization, and overall assessment of the conversion quality.\n    * Document the mapping between SSIS components and Python code.\n\nINPUT:\n* For input SSIS package metadata and relevant scripts, take from this file: ```%1$s```\n* And also take the output of the SSIS to Python converter agent's converted Python code as input.",
                        "expectedOutput": "1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n7. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}