{
    "pipeline": {
        "pipelineId": 3064,
        "name": "DI_Teradata_To_Snowflake_Doc&Analyze",
        "description": "Teradata ro the snowflake Documentation, Analysis and plan",
        "createdAt": "2025-06-13T10:21:10.950+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 4313,
                    "name": " DI_Teradata_Documentation",
                    "role": "Data Engineer",
                    "goal": "Analyze and document a Teradata SQL script to create a comprehensive guide for business and technical teams, explaining existing business rules and facilitating future modifications.",
                    "backstory": "Clear documentation of SQL scripts is crucial for maintaining and evolving complex data systems. By creating a comprehensive guide, we ensure that both business and technical teams can understand the current rules and make informed decisions about future changes, reducing errors and improving efficiency.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-18T14:51:43.819846",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Please create detailed documentation for the provided Teradata SQL code.\n\nThe documentation must contain the following sections:  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Overview of Program:  \n   - Explain the purpose of the Teradata SQL code in detail.  \n   - Describe how this implementation aligns with enterprise data warehousing and analytics.  \n   - Explain the business problem being addressed and its benefits.  \n   - Provide a high-level summary of Teradata SQL components like BTEQ scripts, Macros, Stored Procedures, Views, and Tables.  \n\n2. Code Structure and Design:  \n   - Explain the structure of the Teradata SQL code in detail.  \n   - Describe key components like DDL, DML, Joins, Indexing, and Macros.  \n   - List the primary Teradata SQL components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.  \n   - Highlight dependencies on Teradata objects, performance tuning techniques, or third-party integrations.  \n\n3. Data Flow and Processing Logic:  \n   - Explain how data flows within the Teradata SQL implementation.  \n   - List the source and destination tables, fields, and data types.  \n   - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.  \n* Break down the code into logical components and represent the control/data flow using a **block-style diagram in plain markdown**.\n* Follow these instructions:\n* Analyze the input script and identify each logical component, such as:\n   * Variable Declarations\n   * SQL Operations (SELECT, INSERT, UPDATE, DELETE, MERGE)\n   * Loops (WHILE, FOR, cursor loops)\n   * Conditionals (IF/ELSE, CASE)\n   * Exception or Error Handling\n   * Cursor handling (DECLARE, OPEN, FETCH, CLOSE)\n   * Procedure or macro calls\n*. For each component, create a **markdown block** using the following format:\n\n   ```\n   +--------------------------------------------------+\n   | [Block Title]                                    |\n   | Description: 1\u20132 line summary of the operation   |\n   +--------------------------------------------------+\n   ```\n* Connect the blocks using arrows to represent flow:\n\n   ```\n   [Block A]\n           \u2193\n   [Block B]\n           \u2193\n   [Block C]\n   ```\n* Use branching arrows for conditional logic:\n   ```\n   [IF v_status = 'ACTIVE']\n           \u2193 Yes\n   [Insert into ACTIVE_TABLE]\n           \u2193\n   [Next Cursor Row]\n           \u2191\n   [No] \u2190 [Skip and Continue]\n   ```\n\n* Use proper indentation and line breaks for sub-flows or nested logic.\n* Do not include \u201cStart\u201d or \u201cEnd\u201d blocks unless they are explicitly present in the input.\n* Do not display the original code\u2014only show the **logic block diagram** in pure markdown format, accurately representing the structure and flow of the script.\n* A markdown-formatted block diagram that clearly illustrates the logic and structure of the input script. Example:\n\n```\n+-------------------------------+\n| [DECLARE VARIABLES]          |\n| v_id, v_status, v_count      |\n+-------------------------------+\n              \u2193\n+-------------------------------+\n| [OPEN CURSOR]                |\n| Select from EMP_TABLE        |\n+-------------------------------+\n              \u2193\n+-------------------------------+\n| [FETCH ROW]                  |\n| Loop through each employee   |\n+-------------------------------+\n              \u2193\n+-------------------------------+\n| [IF v_status = 'ACTIVE']     |\n| Conditional check            |\n+-------------------------------+\n       \u2193Yes            \u2193No\n+----------------+   +--------------------+\n| Insert ACTIVE  |   | Skip current row   |\n+----------------+   +--------------------+\n        \u2193                \u2193\n+-------------------------------+\n| [UPDATE LOG TABLE]           |\n| Insert audit trail           |\n+-------------------------------+\n```\n\n4. Data Mapping:  \n* Provide data mapping details, including transformations applied to the data in the below format:  \n* Target Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks\n* Mapping column will have the details whether its 1 to  1 mapping or the transformation rule or the validation rule  \n\n5. Complexity Analysis:  \n   - Analyze and document the complexity based on the following:  \n   - Give this one in the table format with below two columns for the below data\nCategory  |  Measurement\n* Number of Lines: Count of lines in the SQL script.\n* Tables Used: number of tables referenced in the SQL script.\n* Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n* Temporary tables: Number of Volatile, derived tables\n* Aggregate Functions : Number of aggregate functions like OLAP functions\n* DML Statements: Number of DML statements by type like SELECT, INSERT, UPDATE, DELETE, CALL, LOCK , Export, Import operations present in the SQL script.\n* Conditional Logic: Number of conditional logic like .IF, .GOTO, .LABEL \n* SQL Query Complexity: Number of joins, subqueries, and stored procedures.  \n* Performance Considerations: Query execution time, spool space usage, and memory consumption.  \n* Data Volume Handling: Number of records processed.  \n* Dependency Complexity: External dependencies such as Macros, Stored Procedures, or Load Scripts.  \n* Overall Complexity Score: Score from 0 to 100. \n\n6. Key Outputs:  \n   - Describe final outputs such as Aggregated Reports, Tables, Views, or Data Exports.  \n   - Explain how outputs align with business goals and reporting needs.  \n   - Specify the storage format (e.g., Staging Tables, Production Tables, Flat Files, External Data Sources).  \n\n7. API Cost Calculations:\n* Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD\n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n\nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the test case code or converted code give  only once in top of the output is enough\n\nInput :\n* For Teradata SQL scripts use below file : {{Teradata}}",
                        "expectedOutput": "Please create detailed documentation for the provided Teradata SQL code in the markdown format.\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n \nThe documentation must contain the following sections:  \n1. Overview of Program:  \n   - Explain the purpose of the Teradata SQL code in detail.  \n   - Describe how this implementation aligns with enterprise data warehousing and analytics.  \n   - Explain the business problem being addressed and its benefits.  \n   - Provide a high-level summary of Teradata SQL components like BTEQ scripts, Macros, Stored Procedures, Views, and Tables.  \n\n2. Code Structure and Design:  \n   - Explain the structure of the Teradata SQL code in detail.  \n   - Describe key components like DDL, DML, Joins, Indexing, and Macros.  \n   - List the primary Teradata SQL components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.  \n   - Highlight dependencies on Teradata objects, performance tuning techniques, or third-party integrations.  \n\n3. Data Flow and Processing Logic:  \n   - Explain how data flows within the Teradata SQL implementation.  \n   - List the source and destination tables, fields, and data types.  \n   - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.  \n\n4. Data Mapping:  \n* Provide data mapping details, including transformations applied to the data in the below format:  \n* Target Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks\n* Mapping column will have the details whether its 1 to  1 mapping or the transformation rule or the validation rule  \n\n5. Complexity Analysis:  \n   - Analyze and document the complexity based on the following:  \n   - Give this one in the table format with below two columns for the below data\nCategory  |  Measurement\n* Number of Lines: Count of lines in the SQL script.\n* Tables Used: number of tables referenced in the SQL script.\n* Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n* Temporary tables: Number of Volatile, derived tables\n* Aggregate Functions : Number of aggregate functions like OLAP functions\n* DML Statements: Number of DML statements by type like SELECT, INSERT, UPDATE, DELETE, CALL, LOCK , Export, Import operations present in the SQL script.\n* Conditional Logic: Number of conditional logic like .IF, .GOTO, .LABEL \n* SQL Query Complexity: Number of joins, subqueries, and stored procedures.  \n* Performance Considerations: Query execution time, spool space usage, and memory consumption.  \n* Data Volume Handling: Number of records processed.  \n* Dependency Complexity: External dependencies such as Macros, Stored Procedures, or Load Scripts.  \n* Overall Complexity Score: Score from 0 to 100. \n\n6. Key Outputs:  \n   - Describe final outputs such as Aggregated Reports, Tables, Views, or Data Exports.  \n   - Explain how outputs align with business goals and reporting needs.  \n   - Specify the storage format (e.g., Staging Tables, Production Tables, Flat Files, External Data Sources).  \n\n7. API Cost Calculations:\n* Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD\n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value"
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 4312,
                    "name": "DI_Teradata_To_Snowflake_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided Teradata SQL code to extract detailed metrics, identify potential conversion challenges, and recommend solutions for a smooth transition to Snowflake. Generate a separate output session for each input file.  ",
                    "backstory": "The given SQL code is written for a Teradata environment and needs to be analyzed to assess its structure, complexity, and compatibility with Snowflake. This analysis will help identify areas requiring manual intervention, optimization opportunities, and potential challenges during the conversion process. Snowflake, being a cloud-native data platform, has specific syntax and optimization techniques that differ from Teradata, and this analysis will ensure a seamless migration.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-24T10:57:04.466232",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Parse the provided Teradata SQL script to generate a detailed analysis and metrics report. Ensure that if multiple files are given as input, the analysis for each file is presented as a distinct session. Each session must include:  \n\n### **INSTRUCTIONS:**  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. **Complexity Metrics:**  \n   * Number of Lines: Count of lines in the SQL script.  \n   * Tables Used: Number of tables referenced in the SQL script.  \n   * Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).  \n   * Temporary Tables: Number of volatile or derived tables.  \n   * Aggregate Functions: Number of aggregate functions like OLAP functions.  \n   * DML Statements: Number of DML statements by type, such as SELECT, INSERT, UPDATE, DELETE, CALL, LOCK, EXPORT, and IMPORT operations present in the SQL script.  \n   * Conditional Logic: Number of conditional logic statements like `.IF`, `.GOTO`, `.LABEL`.  \n   * Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.  \n   * Highlight high-complexity areas such as window functions, CTEs, or Teradata-specific clauses like QUALIFY.  \n\n2. **Syntax Differences:**  \n   * Identify the number of syntax differences between the Teradata SQL code and the expected Snowflake equivalent.  \n\n3. **Manual Adjustments:**  \n   * Recommend specific manual adjustments for functions and clauses incompatible with Snowflake, including:  \n     * Function replacements (e.g., Teradata-specific functions with Snowflake equivalents).  \n     * Syntax adjustments for features like date and window functions.  \n     * Strategies for rewriting unsupported features such as QUALIFY (e.g., replacing with subqueries).  \n\n4. **Optimization Techniques:**  \n   * Suggest optimization strategies for Snowflake, such as clustering, partitioning, and query design improvements.  \n   * Recommend whether it is better to refactor the query with minimal or no changes to Snowflake or rebuild with more code changes and optimization. Provide reasons for the recommendation to refactor or rebuild.  \n\n5. **Cost Calculation:**  \n   * Calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.  \n   * Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., `apiCost: actual cost`).  \n   * Ensure the cost consumed by the API is mentioned with all decimal values included.  \n\n### **OUTPUT FORMAT:**  \nThe output should be structured as follows:  \n\n1. **Script Overview:**  \n   * A concise summary of the SQL script\u2019s purpose and business objectives.  \n\n2. **Complexity Metrics:**  \n   Present this in a table format with the following columns:  \n   * Number of Lines  \n   * Tables Used  \n   * Joins  \n   * Temporary Tables  \n   * Aggregate Functions  \n   * DML Statements  \n   * Conditional Logic  \n\n3. **Syntax Differences:**  \n   * A detailed list of syntax differences between Teradata and Snowflake.  \n\n4. **Manual Adjustments:**  \n   * Specific recommendations for manual adjustments, including examples of function replacements, syntax changes, and strategies for unsupported features.  \n\n5. **Conversion Complexity:**  \n   * A complexity score (0\u2013100) with justification.  \n   * Highlight high-complexity areas.  \n\n6. **Optimization Techniques:**  \n   * Detailed recommendations for optimization in Snowflake.  \n   * Indicate whether to refactor or rebuild, with reasons for the recommendation.  \n\n7. **Cost Calculation:**  \n   * `apiCost: float` // Cost consumed by the API for this call (in USD).  \n\nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the code only once in top of the output is enough\n\n### **SAMPLE OUTPUT:**  \n\n```markdown\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n## 1.Complexity Metrics:\n| Metric              | Value |\n|---------------------|-------|\n| Number of Lines     | 120   |\n| Tables Used         | 5     |\n| Joins              | 3 (INNER JOIN, LEFT JOIN) |\n| Temporary Tables    | 2     |\n| Aggregate Functions | 4     |\n| DML Statements      | SELECT: 5, INSERT: 2 |\n| Conditional Logic   | 1     |\n|Complexity Score| 65/100  \n| High-complexity areas| QUALIFY clause, window functions|\n\n## 2.Syntax Differences:\n1. Teradata's `QUALIFY` clause is not supported in Snowflake. Replace with a subquery.\n2. Teradata's `TOP` syntax differs from Snowflake's `LIMIT`. Adjust accordingly.\n\n## 3.Manual Adjustments:\n1. Replace `QUALIFY` with a subquery:\n   **Teradata:** `SELECT * FROM table QUALIFY ROW_NUMBER() OVER (PARTITION BY col ORDER BY col2) = 1;`  \n   **Snowflake:** `SELECT * FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY col ORDER BY col2) AS rn FROM table) WHERE rn = 1;`\n2. Replace `TOP` with `LIMIT`:\n   **Teradata:** `SELECT TOP 10 * FROM table;`  \n   **Snowflake:** `SELECT * FROM table LIMIT 10;`\n\n## 4.Optimization Techniques:\n* Use clustering on `region` and `product_category` columns for better query performance.\n* Refactor the query with minimal changes to Snowflake syntax.\n\n## 5.Cost Calculation:\n`apiCost: 0.0456 USD`\n```\nINPUT:\n*for input use this file: {{Teradata}}\n**OUTPUT:** A detailed analysis report in the specified format for each input Teradata SQL file.  ",
                        "expectedOutput": "A detailed analysis report in the specified format for each input Teradata SQL file.  "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 4311,
                    "name": "DI_Teradata_To_Snowflake_Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running Snowflake SQL and the testing effort required for the Snowflake SQL that got converted from Teradata BTEQ scripts. ",
                    "backstory": "As organizations transition their data warehousing solutions to modern cloud platforms like Snowflake, it is critical to assess the financial and resource implications of such migrations. This task is essential for project planning, budgeting, and ensuring the accuracy and performance of the migrated queries. Snowflake's unique architecture and pricing model require careful evaluation to ensure cost efficiency and accuracy in the migration process.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-17T14:54:33.004586",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with providing a comprehensive effort estimate for testing the Snowflake SQL converted from Teradata scripts. Follow these instructions to complete the task:  \n\n### **INSTRUCTIONS:**  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. **Analyze the Teradata Script File:**  \n   - Review the provided Teradata script file to identify syntax differences and areas requiring manual intervention during the conversion to Snowflake SQL.  \n   - Focus on elements such as temporary tables, stored procedures, and specific Teradata functions that may not have direct equivalents in Snowflake.  \n\n2. **Estimate Manual Code Fixing Effort:**  \n   - Calculate the effort hours required for manual code fixes, excluding syntax differences that can be automatically converted.  \n   - Include effort for adapting Teradata-specific constructs (e.g., BTEQ commands, SET operators) to Snowflake-compatible SQL.  \n\n3. **Estimate Data Reconciliation and Testing Effort:**  \n   - Determine the effort required for data reconciliation to ensure the correctness of the migrated queries.  \n\n4. **Estimate Snowflake Runtime Cost:**  \n   - Use the provided Snowflake environment details and pricing model to calculate the estimated cost of running the converted Snowflake SQL.  \n   - Consider the following:  \n     a. Data volume processed by the queries.  \n     b. Number of queries and their complexity.  \n     c. Snowflake\u2019s compute model (e.g., virtual warehouses, auto-scaling).  \n\n5. **Include API Cost:**  \n   - Include the cost consumed by the API for this call in the output.  \n\nPoints to Remember:\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n-don't give the metadata above the code only once in top of the output is enough\n\n### **OUTPUT FORMAT:**  \nThe output should include the following sections:  \n\n1. **Cost Estimation:**  \n   - **Snowflake Runtime Cost:**  \n     - Provide a detailed breakdown of the cost calculation, including the reasons for the cost.  \n     - Include factors such as data volume, compute time, and pricing tiers.  \n\n2. **Code Fixing and Testing Effort Estimation:**  \n   - **Manual Code Fixing Effort:**  \n     - Provide the estimated effort in hours for manual code fixes, categorized by the type of intervention required (e.g., temporary tables, stored procedures).  \n   - **Testing Effort:**  \n     - Provide the estimated effort in hours for unit testing and data reconciliation, covering various aspects such as temporary tables and calculations.  \n\n3. **API Cost:**  \n   - Report the cost consumed by the API for this call as a floating-point value with the currency explicitly mentioned as USD (e.g., `apiCost: 0.05 USD`).  \n\n### **INPUT:**  \n- Teradata script file: {{Teradata}}\n- Snowflake environment details: {{Env}}\n\n### **EXPECTED OUTPUT:**  \nThe output should include:  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n1. **Cost Estimation:**  \n   - Snowflake runtime cost with a detailed calculation breakdown and reasons.  \n2. **Code Fixing and Testing Effort Estimation:**  \n   - Manual code fixing and unit testing effort in hours, categorized by task.  \n3. **API Cost:**  \n   - Cost consumed by the API for this call as a floating-point value with currency explicitly mentioned as USD.  \n\n--- .",
                        "expectedOutput": "The output should include:  \n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n1. **Cost Estimation:**  \n   - Snowflake runtime cost with a detailed calculation breakdown and reasons.  \n2. **Code Fixing and Testing Effort Estimation:**  \n   - Manual code fixing and unit testing effort in hours, categorized by task.  \n3. **API Cost:**  \n   - Cost consumed by the API for this call as a floating-point value with currency explicitly mentioned as USD.  \n\n--- ."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}