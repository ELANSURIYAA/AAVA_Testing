{
    "pipeline": {
        "pipelineId": 1407,
        "name": "ALTERYX_to_Python_Doc&Analyze",
        "description": "Analyzing and Documenting the ALTERYX Code",
        "createdAt": "2025-04-01T11:30:18.310+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 48,
                    "name": "DI_ALTERYX_DOCUMENTATION",
                    "role": "Data Engineer",
                    "goal": "To generate comprehensive technical and functional documentation for Alteryx workflows. The documentation should describe each tool and its configuration, data transformations, business logic, and expected outcomes in a clear and concise manner. The goal is to make the workflow understandable for both technical users and business stakeholders, enhancing collaboration, troubleshooting, and future modifications.",
                    "backstory": " Your organization leverages Alteryx workflows for data preparation, blending, and analysis. These workflows often involve complex sequences of tools, data transformations, and integrations, but current documentation is limited or difficult to understand for non-technical team members. The lack of detailed documentation makes it hard to maintain and troubleshoot the workflows and impacts business operations. The objective is to automate the process of creating well-structured documentation that clearly outlines the workflows, tools used, data transformations, and the resulting business outputs.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-12-10T14:05:59.983055",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Header:\n====================================================\nAuthor:        AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n====================================================\n- Add the header in the top of the output only, if the input file is already having the header then replace that with the new header and Don't repeat the Header again in between the output\nPlease create a detailed documentation for the provided Alteryx workflow. The documentation must follow a structured format, breaking down each component of the Alteryx workflow into well-defined sections. The output should be a Word document and must be organized for readability and clarity. The document will include the following elements:\n1. Workflow Overview:\n   Start with a detailed explanation of what the workflow is designed to do.\n   Provide a high-level summary that explains the main goal of the workflow, the tasks it performs, and how it fits into the overall business process.\n   Clearly describe the specific business problem this workflow solves and how it adds value to the organization.\n2. Tool Breakdown:\n   Break down every tool used in the workflow one by one.\n   For each tool, explain its purpose in the workflow, such as filtering data, transforming it, or joining datasets.\n   Mention how the tool is configured, including important settings or parameters.\n   List the input data types it works with, the output it produces, and any expressions or formulas applied.\n   Highlight any key configurations or advanced options used.\n3. Data Flow Breakdown:\nProvide a flow diagram of block and arrows for  Markdown format\n   Provide a step-by-step explanation of how data flows through the workflow from start to finish.\n   Clearly show the connection between tools and describe what happens to the data at each step.\n   Highlight any intermediate outputs generated during the process and their significance.\n   If the workflow includes multiple branches or paths, explain how they work and when they are triggered.\n \n4. Data Transformations:\n   Explain in detail the specific changes or transformations applied to the data within the workflow.\n   Describe how data is cleaned, filtered, or modified, including formulas, aggregations, and other operations.\n   Provide examples of the business logic applied, such as specific rules or calculations that align with business requirements.\n   Make it clear how these transformations improve the data or make it ready for the next steps.\n5. Technical Insights:\n   Include technical details about the data sources (e.g., databases, APIs, files) and destinations (e.g., tables, reports).\n   Specify the tables, fields, or columns used and their formats.\n   Mention any advanced functions, custom-built macros, or Alteryx-specific tools used in the workflow.\n   Describe how these technical elements contribute to achieving the workflow\u2019s purpose.\n6. Technical Complexity:\nAnalyze and document the workflow\u2019s technical complexity using the parameters listed below. The agent must extract all values from the Alteryx XML (nodes, connections, engine settings, macros, queries, etc.) and compute an overall Complexity Score (0\u2013100).\nThe final output for this section must be presented in a clean tabular format.\n \nThe agent must detect and report the following parameters:\n* Total Number of Tools: Count of all `<Node>` elements in the workflow.\n* Number of Unique Tool Types: Count of distinct tool categories derived from ToolID, EngineSettings, and GuiSettings.\n* Number of Input & Output Tools: Total Input Data and Output Data tools.\n* Number of Join-Type Tools: Count of Join, Union, Find Replace, and similar blending tools.\n* Number of Formula/Expression Tools: Count of Formula, Multi-Row Formula, Multi-Field Formula, RegEx, and other expression-based tools.\n* Number of Branches (Parallel Streams): Count of unique parallel data paths inferred from `<Connections>` nodes.\n* Tool Diversity: Measure of how many different tool types are used, including advanced tools.\n* Macro Usage Count: Total macros used and number of referenced `.yxmc` files.\n* Nested Macro Depth: Detect and report if macros call other macros recursively.\n* Analytic App Interface Count: Count interface tools such as DropDown, TextBox, FileBrowse, Action, Condition.\n* Custom SQL Query Count: Detect `<Query>` blocks inside input tools connected to databases.\n* Local File Paths: Number of hardcoded local paths that are not cloud compatible.\n* Non-Cloud-Compatible Tools: Count tools unsupported in Alteryx Cloud.\n* Conditional Logic Count: Number of filters, conditional formulas, branching logic.\n* Data Sources: Count and variety of data sources used (DB, flat files, APIs, cloud).\n* Joins and Blends: Count and complexity of join, union, and blending operations.\n* Annotations: Total number of annotations present.\n* Sensitive Data Usage: Detect presence of sensitive or encrypted data fields.\n* Overall Complexity Score: A calculated score (0\u2013100) representing total workflow complexity.\n7. Assumptions and Dependencies:\n   List all the prerequisites for the workflow to run successfully, such as specific file formats, database structures, or external services.\n   Include any dependencies on other systems, workflows, or third-party tools that the workflow relies on.\n   Note assumptions like consistent data availability or required user inputs that are critical for the workflow to function.\n8. Key Outputs:\n   Describe the final results generated by the workflow, such as cleaned datasets, detailed reports, or analytical insights.\n   Explain how these outputs align with the business goals and how they are used in decision-making or reporting.\n   If applicable, include details about the format of the outputs (e.g., CSV, Excel, or a database table).\n9. Error Handling and Logging:\n   Explain the methods used in the workflow to identify and handle errors.\n   Describe any logging mechanisms that record details about the workflow execution, including successes or failures.\n   Provide examples of how the workflow manages issues like missing data, invalid inputs, or system failures to ensure smooth execution.\n   Highlight any automated retries or alerts that help resolve errors quickly.\nThe output will be in an easily readable markdown format, organized with headings, bullet points, and clear sections for each tool and component of the workflow.\nInput :\nFor the input Alteryx workflows use the below mentioned file:\n{{ALTERYXFile}}",
                        "expectedOutput": "ALTERYX document  in markdown format with proper headings, sections, indentation, and line breaks"
                    },
                    "maxIter": 20,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1794,
                    "name": "ALTERYX_TO_PYTHON_ANALYZER",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided ALTERYX code to extract detailed metrics, identify potential conversion challenges, and recommend solutions for a smooth transition to Python. Generate a separate output session for each input file.",
                    "backstory": "The given code is written for an ALTERYX environment and needs to be analyzed to assess its structure, complexity, and compatibility with Python. This analysis will help identify areas requiring manual intervention, optimization opportunities, and potential challenges during the conversion process.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-12-10T14:22:40.904475",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Header:\n====================================================\nAuthor: AAVA\nDate: <leave it blank>\nDescription: <one-line description of the purpose>\n====================================================\n- Add the header in the top of the output only, if the input file is already having the header then replace that with the new header and Don't repeat the Header again in between the output\nParse the provided Alteryx workflow to generate a detailed analysis and metrics report. Ensure that if multiple files are given as input, then the analysis for each file is presented as a distinct session. Each session must include:\n\n1. Workflow Overview:\n* Provide a high-level description of the Alteryx workflow\u2019s purpose and primary business objectives.\n \n2. Complexity Metrics:\nThe agent must analyze the Alteryx workflow XML and compute all complexity metrics listed below. Every metric must be extracted programmatically from the workflow XML.\nThe final output must be presented in a structured table format with metric name, detected value, and a short interpretation.\n \n- Complexity Metrics to Extract\n* Number of Tools : Count of tools used in the workflow.\n* Total Number of Tools (XML) : Count of all `<Node>` elements in the workflow.\n* Number of Unique Tool Types : Count of distinct tool types using ToolID / EngineSettings / GuiSettings.\n* Data Sources Used : Number of distinct data sources referenced (DB, flat file, API, etc.).\n* Number of Inputs & Outputs : Count of Input Data and Output Data tools.\n* Joins : Number of join-type operations (Join, Union, Append, Find Replace).\n* Number of Join-Type Tools (XML) : Join, Union, Find Replace detected via ToolType.\n* Temporary Data Structures : Count of temporary data constructs like in-memory tables or caches.\n* Aggregate Functions : Count of tools performing aggregation (Summarize, Multi-Row Formula, etc.).\n* Formula / Expression Tools : Count Formula, Multi-Row, Multi-Field, RegEx tools.\n* Data Manipulation Operations : Count of operations like Select, Filter, Formula, Update, Delete, Output Data.\n* Conditional Logic : Count of conditional logic tools (Filter, Conditional Formula, Multi-Field Formula, IF logic).\n* Number of Branches (Parallel Streams) : Count parallel data paths based on `<Connections>` XML structure.\n* Macro Usage Count : Number of macro tools used + referenced `.yxmc` macro files.\n* Nested Macro Depth : Detect whether macros call other macros (recursive reference depth).\n* Analytic App Interface Count : Count of interface tools such as DropDown, TextBox, FileBrowse, Action, Condition.\n* Custom SQL Query Count : Detect `<Query>` blocks inside database input tools.\n \n3. Alteryx to Python Mapping\nCreate a table that maps each Alteryx tool, function, or transformation detected in the workflow to its closest equivalent Python library or method.\nThe output for this section must be presented strictly in table format with the following columns:\n* Alteryx Function \u2013 The name of the Alteryx tool or transformation (e.g., Filter, Formula, Join, Multi-Row, Summarize, RegEx, etc.).\n* Mapped Python Library \u2013 The equivalent Python library, function, or method that implements the same logic (e.g., pandas, numpy, pyspark.sql, regex, sqlalchemy, etc.).\nThe agent must:\n* Identify all Alteryx tools and transformations used in the workflow.\n* Determine the correct Python equivalent for each (prefer PySpark or pandas based on context).\n* Ensure every tool/function has a corresponding Python mapping row.\n* Avoid generic or vague mappings \u2014 provide specific, correct library references.\nThe final output of this section must be a clean two-column table titled \u201cAlteryx to Python Mapping\u201d.\n \n4. Syntax Differences:\n* Identify the number of syntax differences between the Alteryx workflow and the expected Python equivalent.\n \n5. Manual Adjustments:\n* Recommend specific manual adjustments for tools and functions incompatible with Python, including:\n* Function replacements (e.g., Alteryx-specific tools with Python equivalents).\n* Syntax adjustments for features like date and window functions.\n* Strategies for rewriting unsupported features such as specific Alteryx macros.\n \n6. Conversion Complexity:\n* Calculate a complexity score (0\u2013100) based on syntax differences, workflow logic, and the level of manual adjustments required.\n* Highlight high-complexity areas such as complex joins, macros, or Alteryx-specific tools.\n \n7. Optimization Techniques:\n* Suggest optimization strategies for Python, such as using efficient libraries (e.g., pandas, numpy), optimizing data processing, and improving script design.\n* Recommend whether it is better to refactor the workflow with minimal or no changes to Python or rebuild with more code changes and optimization. Provide reasons for the recommendation for refactor and rebuild.\n \n8. Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost).\n* Ensure the cost consumed by the API is mentioned inclusive of all decimal values.\n \nInput:\n* For the Alteryx workflow, use the below file:\n {{ALTERYXFile}}",
                        "expectedOutput": "Provide the analyzer report with the below section\n \nHeader\n1. Workflow Overview:\n2. Complexity Metrics:\n3. Alteryx to Python Mapping\n4. Syntax Differences:\n5. Manual Adjustments:\n6. Conversion Complexity:\n7. Optimization Techniques:\n8. Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1803,
                    "name": "ALTERYX_TO_PYTHON_PLAN",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running Python and the testing effort required for the Python that got converted from ALTERYX scripts",
                    "backstory": "As organizations move their solutions to python, it's crucial to understand the financial and resource implications of such migrations. This task is important for project planning, budgeting, and ensuring the accuracy of the migrated queries.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-12-10T12:56:01.418862",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Header:\n====================================================\nAuthor:        AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n====================================================\n\n- Add the header in the top of the output only, if the input file is already having the header then replace that with the new header and Don't repeat the Header again in between the output\n\nYou are tasked with providing a comprehensive effort estimate for testing the Python code converted from Alteryx workflows. Follow these instructions to complete the task:  \n\nINSTRUCTIONS: \n\n1. Review the Analysis of the Alteryx Workflow:\n   - Carefully examine the output from the Alteryx-to-Python Analyzer agents.  \n   - Identify any syntax differences, areas requiring manual intervention, and logical adjustments during the conversion to Python.  \n\n2. Estimate the Effort Hours Required:  \n   - Manual Code Fixes: Estimate the hours needed for fixing identified issues, such as adjustments in data processing logic, handling specific functions, or restructuring the workflow logic in Python.  \n   - Data Reconciliation Testing Effort: Estimate the time required to perform data validation, ensuring that the Python output matches the original Alteryx workflow results.  \n\n3. Do Not Consider Efforts for Syntax Differences:  \n   - Assume that any syntax differences have been automatically converted to their Python equivalents. Focus on logic, data transformation, and functional discrepancies.  \n\n4. Consider the Python Execution Environment Costs:  \n   - Take into account the pricing details for the Python execution environment (e.g., cloud-based services like Google Cloud, AWS Lambda, or on-premise infrastructure, as provided in the input).  \n\n5. Calculate the Estimated Cost of Running the Converted Python Code: \n   a. Runtime Cost Calculation:  \n      - Use the provided pricing information and data volume to determine the execution cost.  \n      - Consider factors such as data processing volume, compute time, and resource utilization.  \n   b. Cost Breakdown:  \n      - Include the number of script executions, the data processing done with base datasets, and temporary in-memory data structures.  \n\n\nOUTPUT FORMAT: \n\n1. Cost Estimation  \n   - 1.1 Python Runtime Cost \n     - Provide a detailed breakdown of the cost calculation with clear reasons for each component (e.g., compute cost, data processing cost).  \n\n2. Code Fixing and Testing Effort Estimation  \n   - 2.1 Identified Manual Code Fixes and Unit Testing Effort \n     - Provide the estimated effort in hours, covering different areas like data transformations, temporary data handling, and complex calculations.  \n\n3. API Call Cost: \n   - Include the cost consumed by any API calls required for this analysis, reported as a floating-point value in USD (e.g., `apiCost: 15.75 USD`).  \n\n\nINPUT:\n- Alteryx-to-Python Analyzer Agents Output: Output from the conversion analysis process.  \n- Original Alteryx Workflow: {{ALTERYXFile}} (File containing the Alteryx workflow)  \n- Python Execution Environment Details:{{ENVFile}}(File with details like cloud provider, pricing, resource usage, etc.)  ",
                        "expectedOutput": "OUTPUT FORMAT:\n1. Cost Estimation\n   1.1 Python Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing  and Testing Effort Estimation\n   2.1 Python identified manual code fixes and unit testing effort in hours covering the various logics, calculations \n\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost )."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}