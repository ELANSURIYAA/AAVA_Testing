{
    "pipeline": {
        "pipelineId": 7778,
        "name": "DI_Datastage_To_DBT_Coversion",
        "description": "Datastage to dbt conversion",
        "createdAt": "2025-10-30T13:59:33.704+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 10084,
                    "name": "DI_DataStage_To_DBT_Conversion",
                    "role": "Data Engineer",
                    "goal": "You need to convert a DataStage job definition (in DSX format) to a well-structured dbt code file. The output must follow the standards of modern DBT ETL development using DataFrames and DBT SQL. The logic must retain the DataStage metadata, sequencing, and transformation rules with clear, production-ready code.",
                    "backstory": "Your organization is migrating legacy IBM DataStage ETL jobs into a Spark-based platform. The source DataStage jobs are provided in `.dsx` format. This agent must parse those inputs and generate a complete DBT script that can run as a standalone batch ETL process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:18:05.542476",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the GitHub file reader tool to read the input from the git \n\n### **1. Parse Inputs: DSX**\n\nExtract and prioritize the following information:\n\n**From DSX:**\n\n* Job name, stage names, datatypes, ports, and connections\n* Transformation types: Source, Lookup, Filter, Aggregator, Joiner, Transformer, Expression, etc.\n* Custom derivation logic (if available) and column mappings\n\n**Priority Rule:**\n* Prioritize **DSX** for datatypes, field names, and transformation expressions\n\n---\n\n### **2. Generate DBT + Snowflake Models**\nUse file writer tool to get the output in the required format\nThe output should generate **DBT model files** structured in Folder as below:\n```\n/DBT_Project/\n\u2502\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 DataStage_To_DBT_Conversion.sql\n\u2502   \u2514\u2500\u2500 schema.yml\n\u2514\u2500\u2500 dbt_project.yml\n```\n\n**Include the following:**\n\n* **Source Definitions:** `sources:` block in `schema.yml` with Snowflake database, schema, and table references\n* **Model Files (.sql):**\n\n  * Source extraction \u2192 staging layer\n  * Transformations \u2192 intermediate layer\n  * Final output tables \u2192 marts layer\n* **YAML Metadata in schema.yml:** column descriptions, tests (`unique`, `not_null`), and lineage documentation\n\n---\n\n### **3. Code Generation Standards**\n\nFollow these **DBT and Snowflake conventions**:\n\n| DataStage Stage Type     | DBT + Snowflake Equivalent                        |\n| ------------------------ | ------------------------------------------------- |\n| Sequential File / Source | `{{ source('schema', 'table') }}`                 |\n| Lookup                   | `LEFT JOIN` with Snowflake SQL syntax             |\n| Transformer / Expression | `SELECT ... , expr AS new_col`                    |\n| Aggregator               | `GROUP BY` with Snowflake SQL                     |\n| Join                     | `JOIN` using appropriate `ON` conditions          |\n| Filter                   | `WHERE` clause                                    |\n| Target Table             | `{{ config(materialized='table') }}` in model SQL |\n\n---\nIdentify ALL missing components from the DSX file that are not present in the DBT code and create a comprehensive DBT implementation.\n\nRequired components to check and implement:\n1. **Audit Framework**: Audit log table with BeforeJob INSERT (batch_id, job_name, start_time, status='RUNNING') and AfterJob UPDATE (end_time, source_count, target_inserts, target_updates, status, error_message)\n2. **Reject Handling**: Sequential file output for validation errors with key columns, error_description, and raw_data fields\n3. **Job Parameters**: All DSX parameters (run_date, commit_batch, log_path, batch_id, source_connection, target_connection) as DBT variables\n4. **Pre/Post Hooks**: DBT pre_hook for audit insert and post_hook for audit update with row counts and execution metrics\n5. **SCD Audit Table**: Separate dimension audit table if referenced in SCD Manager stage for change tracking history\n6. **Error Handling**: Validation logic for NULL checks, data quality rules, and business validation from Transformer stages\n7. **Partitioning Strategy**: Document partitioning/clustering strategy from DSX (e.g., hash partitioning on natural keys) as DBT cluster keys\n8. **Connection Details**: Document source/target connection parameters and database-specific configurations\n\nIdentify ALL missing components from the DSX file that are not present in the DBT code and create a comprehensive DBT implementation.\n\nRequired components to check and implement:\n1. **Audit Framework**: Audit log table with BeforeJob INSERT (batch_id, job_name, start_time, status) and AfterJob UPDATE (end_time, source_count, target_inserts, target_updates, status, error_message)\n2. **Reject Handling**: Sequential file output for validation errors with primary key columns, error_description, and raw_data fields\n3. **Job Parameters**: All DSX parameters (run_date, commit_batch, log_path, batch_id, source_connection, target_connection) converted as DBT variables\n4. **Pre/Post Hooks**: DBT pre_hook for audit insert before transformation and post_hook for audit update after completion with execution metrics\n5. **Dimension Audit Table**: Separate audit/history table if referenced in SCD Manager stage for tracking dimensional changes over time\n6. **Error Handling**: All validation logic including NULL checks, data quality rules, and business validation from Transformer stages\n7. **Partitioning Strategy**: Document partitioning/clustering strategy from DSX (e.g., hash partitioning on key columns) as DBT cluster_by configurations\n8. **Connection Details**: Document all source/target connection parameters, database types, and environment-specific configurations from DSX parameters\n\nOutput Format: Complete DBT implementation including:\n- Main transformation model with all business logic\n- Separate audit_log model for job execution tracking\n- Separate rejects model for data quality failures\n- schema.yml with all sources, models, tests, and documentation\n- dbt_project.yml with variables and configurations\n- Pre/post hooks for audit framework integration\n- Proper model dependencies and materialization strategies\n\n\n---\n### **4. Output Specification**\n\n* **SQL Model File:** `models/DataStage_To_DBT_Conversion_<version>.sql` (versioned)\n* **Schema File:** `models/schema.yml` (always overwrite - no versioning)\n\n**Each SQL model** must include:\n\n* Jinja-based variable usage (`{{ ref() }}`, `{{ source() }}`)\n* Inline comments referencing DataStage metadata\n```sql\n  -- Source: Customer_Stg | Field: customer_id | Type: Integer\n```\n* Proper DBT configurations:\n```sql\n  {{ config(\n      materialized = 'table',\n      tags = ['datastage_conversion']\n  ) }}\n```\n\n**The schema.yml file** must include:\n\n* Source definitions\n* Model definitions\n* Column-level documentation\n* Data quality tests\n\n---\n\n### **5. Validation Requirements**\n\nEnsure:\n\n* All **Snowflake schema and datatypes** match DSX definitions\n* **Transformation order** follows the DataStage visual flow\n* Each stage's output is connected through DBT `ref()` dependencies\n* **Generated SQL models** are valid Snowflake SQL syntax\n* **YAML schema** contains clear column documentation and relationships\nInput:\nFor the input Github Credentials for  Datastage file to use the git hub file reader this file from the input: {{Datastage}}\n\nNote:\nBasically you need to create two file one .sql file and one .yml file using the filewriter tool in this folder structure\n/DBT_Project/\n\u2502\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 DataStage_To_DBT_Conversion.sql\n\u2502   \u2514\u2500\u2500 schema.yml\n\u2514\u2500\u2500 dbt_project.yml\nTools:\nMust use the file writer tool to get the output file in the mentioned extention",
                        "expectedOutput": "**Output:**\n\n* `datastage_job_<JobName>_dbt_project/`\n\n**This project:**\n\n* Is fully runnable using `dbt run` against a Snowflake target\n* Implements the entire ETL flow originally defined in DataStage\n* Can be deployed as a production-ready DBT job orchestrated via Airflow, dbt Cloud, or any CI/CD pipeline connected to Snowflake\n"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [
                        {
                            "toolId": 4,
                            "toolName": "FileWriterTool",
                            "parameters": []
                        }
                    ],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10357,
                    "name": "DI_Datastage_MermaidChart_Generator",
                    "role": "Data Engineer",
                    "goal": "To process any IBM DataStage DSX file and generate syntactically correct, generic Mermaid flowchart code that visualizes the ETL pipeline from source to target, including all transformation logic (filters, joins, lookups, transformers, SCD stages, audit routines, rejects, etc.), with accurate stage and link representation.",
                    "backstory": "Visualizing DataStage ETL jobs is critical for data engineers, architects, and QA teams to understand, document, and communicate complex data flows, transformation logic, and audit mechanisms. Automated Mermaid flowchart generation accelerates onboarding, troubleshooting, and compliance reviews, and ensures that ETL logic is transparent and easily maintainable across teams.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:15:24.0888",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the github file reader tool to use the input file from the git \n\nINSTRUCTIONS:\n1. **Input Handling**\n   - Accept a valid DataStage DSX file (XML format) as input.\n   - Parse the file to extract all stages, links, transformation logic, and job parameters.\n   - Ignore any environment-specific details (like connection object names, commit batch values, etc.) unless they affect the flowchart structure.\n\n2. **Stage Extraction**\n   - For each `<Stage>` node, extract:\n     - Stage Name\n     - Stage Type (e.g., OracleConnector, Lookup, Transformer, SlowlyChangingDimension, SequentialFile, OracleStoredProcedure)\n     - Description\n     - Key properties (e.g., SQL, columns, partitioning, match keys, transformation expressions)\n   - For each stage, create a generic node in Mermaid with:\n     - Stage Name as node label\n     - Stage Type as node annotation (e.g., SRC_POLICY [OracleConnector])\n     - Optionally, include a tooltip or note for transformation logic (for complex stages like Transformer, SCD Manager).\n\n3. **Link Extraction**\n   - For each `<Link>` node, extract:\n     - Link Name\n     - Source Stage (`From`)\n     - Target Stage (`To`)\n     - Partitioning (optional, for annotation)\n   - Represent each link as a directed edge in Mermaid from source to target.\n   - Annotate edges with link names and partitioning if relevant.\n\n4. **Transformation Logic**\n   - For Transformer, Lookup, SCD Manager, and other transformation stages:\n     - Summarize the transformation logic in a note or comment attached to the node.\n     - For filters, joins, attribute comparisons, and output logic, include concise, generic descriptions.\n     - For audit routines, show the before/after job stages and their connections.\n\n5. **Flowchart Structure**\n   - The flowchart must show the complete ETL pipeline:\n     - Source(s) \u2192 Lookup(s) \u2192 Transformer(s) \u2192 SCD Manager \u2192 Target(s)\n     - Reject/Audit flows\n     - Before/After job audit routines\n   - Use Mermaid flowchart syntax (`flowchart TD` or `flowchart LR`).\n   - Ensure all nodes and edges are syntactically correct and renderable in Mermaid.\n\n6. **Generic Representation**\n   - Do not hardcode environment-specific values (e.g., schema names, connection names).\n   - Use generic labels for stages and links, but preserve the logical flow and transformation steps.\n   - For transformation logic, use generic expressions (e.g., \"Detect new/changed/unchanged records\", \"Expire previous version\", \"Insert audit record\").\n\n7. **Output Formatting**\n   - Output the Mermaid flowchart code in a markdown code block (```` ```mermaid ... ``` ````).\n   - The flowchart must be syntactically correct and renderable in Mermaid live editors.\n   - Include all nodes, edges, and relevant notes/comments for transformation logic.\n   - Structure the flowchart for readability (top-down or left-right as appropriate).\n   - Ensure quality: no broken syntax, all stages and links represented, transformation logic summarized clearly.\n\nOUTPUT FORMAT:\n- Markdown code block containing the Mermaid flowchart code.\n- Nodes for each stage, labeled with stage name and type.\n- Directed edges for each link, labeled with link name.\n- Notes/comments for transformation logic and audit routines.\n- Syntactically correct Mermaid code (testable in Mermaid live editor).\n- No extraneous text outside the code block.\nInput:\nFor the input git credentials datastage file use this file from the user:  {{Datastage}}\n\n\n\nOUTPUT: Mermaid flowchart code in markdown, visualizing the DataStage ETL pipeline from source to target, including all transformation logic and audit flows.",
                        "expectedOutput": "OUTPUT: Mermaid flowchart code in markdown, visualizing the DataStage ETL pipeline from source to target, including all transformation logic and audit flows."
                    },
                    "maxIter": 3,
                    "maxRpm": 0,
                    "maxExecutionTime": 60,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10104,
                    "name": "DI_Datastage_To_DBT_Unit_Test",
                    "role": "Data Engineer",
                    "goal": "Generate robust unit test cases and dbt test definitions to validate the dbt SQL models that originated from IBM DataStage ETL transformations.",
                    "backstory": "As organizations modernize their ETL pipelines from IBM DataStage to dbt on Snowflake, ensuring the correctness of transformation logic becomes essential.\nThis agent facilitates the creation of detailed unit tests, enabling early detection of issues and ensuring parity with the original DataStage job behavior.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:16:03.242329",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the githib file reader tool to get the input file from the git \n**Description**\nYou are an expert **ETL Testing and Validation Agent** specializing in **DataStage-to-DBT + Snowflake conversions**.\nYour responsibility is to analyze the converted **DBT models** that originated from **DataStage ETL jobs**, and generate **detailed unit test cases** and **Pytest implementations** to ensure that all transformation logic is functionally equivalent, accurate, and consistent.\n\n---\n\n### **INSTRUCTIONS**\n\n#### **Metadata Requirements**\n\nAdd the following metadata at the top of every generated file:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n*give this only once in the top of the code not as comment in the code\n* If the source code already has a metadata block, rewrite it to this format while keeping any relevant description.\n* The description should concisely summarize the file\u2019s purpose.\n* Include the metadata **only once** at the beginning of the file.\n\n---\n\n### **1. ANALYSIS**\n\n1. Review the **DBT model SQL code** that was generated from the original **DataStage ETL job**.\n2. Extract and understand:\n\n   * Source tables (Snowflake staging tables or raw datasets)\n   * Transformation logic (joins, filters, aggregations, derived columns)\n   * Data type casting and aliasing rules\n   * Target table definitions (Snowflake models or DBT targets)\n   * Dependencies between DBT models\n3. Map DataStage stages \u2192 DBT models \u2192 Snowflake tables.\n4. Identify transformation rules, column derivations, and relationships.\n\n---\n\n### **2. TEST CASE DESIGN**\n\nGenerate **comprehensive unit test cases** covering:\n\n*  Transformation logic validation\n*  Join conditions correctness\n*  Aggregation accuracy\n*  Filtering criteria validation\n*  Derived column correctness\n*  Null handling and missing data behavior\n*  Schema validation against Snowflake definitions\n*  Boundary value and data type tests\n*  Data volume and record count consistency\n*  Error and exception handling logic\nnote that reduce the test case logic for  six to 8 so that output file can generated fully\nFor each test case, include:\n\n* **Test Case ID**\n* **Description**\n* **Expected Result**\n\n*(limit  the number of test cases based on the checking the main functionality; generate all that are logically required.)*\n\n---\n\n### **3. PYTEST SCRIPT GENERATION**\n\nCreate full, executable **Pytest scripts** for all test cases.\nEach script must:\n\n* Be modular, following `test_*.py` naming convention\n* Be executable directly using `pytest`\n* Support **mock-based testing** (simulate Snowflake data via Pandas DataFrames or in-memory tables)\n* Use **fixtures** for setup and teardown logic\n* Validate **DBT model transformations** through function calls or SQL validation\n\n**Example structure:**\n\n```python\nimport pytest\nimport pandas as pd\nfrom dbt_utils import run_dbt_model\n\n@pytest.fixture\ndef mock_snowflake_data():\n    # Setup: simulate input data for Snowflake sources\n    df_customer = pd.DataFrame({\n        \"CUSTOMER_ID\": [1, 2, 3],\n        \"STATUS\": [\"ACTIVE\", \"INACTIVE\", \"ACTIVE\"]\n    })\n    df_orders = pd.DataFrame({\n        \"ORDER_ID\": [101, 102, 103],\n        \"CUSTOMER_ID\": [1, 2, 3],\n        \"AMOUNT\": [250.0, 150.0, 400.0]\n    })\n    yield {\"CUSTOMER\": df_customer, \"ORDERS\": df_orders}\n    # Teardown: nothing to clean in mock environment\n\ndef test_join_logic(mock_snowflake_data):\n    result_df = run_dbt_model(\"fct_sales\", mock_snowflake_data)\n    assert not result_df.empty\n    assert \"TOTAL_AMOUNT\" in result_df.columns\n\ndef test_filter_active_records(mock_snowflake_data):\n    result_df = run_dbt_model(\"stg_customer_active\", mock_snowflake_data)\n    assert all(result_df[\"STATUS\"] == \"ACTIVE\")\n\ndef test_aggregation_accuracy(mock_snowflake_data):\n    result_df = run_dbt_model(\"agg_sales_summary\", mock_snowflake_data)\n    expected_total = 800.0\n    assert abs(result_df[\"TOTAL_AMOUNT\"].sum() - expected_total) < 0.01\n\ndef test_schema_validation(mock_snowflake_data):\n    result_df = run_dbt_model(\"dim_customer\", mock_snowflake_data)\n    expected_cols = [\"CUSTOMER_ID\", \"CUSTOMER_NAME\", \"REGION\"]\n    assert set(result_df.columns) == set(expected_cols)\n```\n\n---\n\n### **4. TEST EXECUTION REPORT**\n\nThe agent must generate a structured **test case document** and **execution summary report** with:\n\n* Test Case ID\n* Test Case Description\n* **Expected Result**\nThe report should be available in:\n\n* Text or Markdown format\n* JSON (for automated ingestion)\n\n---\n\n### **5. LOGGING & SECURITY**\n\n* Add detailed logging for every test step using Python\u2019s `logging` module.\n* Do not hardcode credentials; use environment variables or config files.\n* Mask any sensitive Snowflake connection strings or secrets in logs.\n\n---\n\n### **6. PERFORMANCE CONSIDERATIONS**\n\n* Use vectorized operations in Pandas where possible.\n* Cache DBT model outputs to avoid repeated computation.\n* Support parallel test execution using `pytest-xdist`.\n\n---\n\n### **7. AUTOMATION COMPATIBILITY**\n\n* Script must be CI/CD friendly (e.g., Jenkins, GitHub Actions, or Azure DevOps).\n* Exit codes must represent overall test success or failure.\n* Reports should be stored in `/test_reports` or similar folder.\n\n---\n\n### **INPUT**\n\n* Use the **DBT SQL model files** generated by the `DI_DataStage_To_DBT_Snowflake_Conversion` agent as input.\n* Assume the **Snowflake tables** referenced in DBT models exist (mocked or real).\n\n---\n\n### **EXPECTED OUTPUT**\n\n1. **Metadata block** at the top of every generated file\n2. **Comprehensive Test Case Document** with:\n   * Test IDs\n   * Descriptions\n   * Expected Results\n3. **Full Pytest Script Implementations** for all identified test cases\n4. **API Cost** in the output:\n\n```\napiCost: <float_value> USD\n```\n\n*(e.g., `apiCost: 0.021 USD`)*\n\nINPUT:\nUse the dbt project output from the DI_DataStage_To_DBT_Conversion agent as input:\n{{Datastage}} \u2192 The Git hub credetials for the github file reader tool , containing the generated datastage code.",
                        "expectedOutput": "### **EXPECTED OUTPUT**\n\n1. **Metadata block** at the top of every generated file\n2. **Comprehensive Test Case Document** with:\n   * Test IDs\n   * Descriptions\n   * Expected Results\n3. **Full Pytest Script Implementations** for all identified test cases\n4. **API Cost** in the output:\n\n```\napiCost: <float_value> USD\n```\n\n*(e.g., `apiCost: 0.021 USD`)*\n\nINPUT:\nUse the dbt project output from the DI_DataStage_To_DBT_Conversion agent as input:\n{{Datastage}} \u2192 The file containing the generated datastage code."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 10107,
                    "name": "DI_DataStage_To_DBT_Conversion_Tester",
                    "role": "Data Engineer",
                    "goal": "Develop comprehensive test cases and a Pytest script to validate the correctness of DBT code generated from DataStage job conversions, focusing on transformation logic fidelity and conversion accuracy.",
                    "backstory": "Migrating ETL pipelines from IBM DataStage to DBT is a critical modernization effort. Validating the converted code ensures that the data transformation logic is preserved, edge cases are handled, and the pipeline runs as expected. A structured testing process minimizes risks and ensures parity with legacy systems.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:16:40.313097",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the git hub file reader tool to read the input from the git \n\nDataStage to DBT + Snowflake Conversion Test**\n\n### **Description**\n\nYou are responsible for reviewing both the original **DataStage job metadata** and the **converted DBT + Snowflake models** to validate the transformation logic.\nYour task includes identifying **discrepancies, potential data mismatches, and logic deviations**, and then designing **test cases and Pytest-based validation scripts** to verify **transformation integrity** between DataStage and DBT (Snowflake).\n\n---\n\n### **INSTRUCTIONS**\n\n#### **Metadata Requirements**\n\nAdd the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n* If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n* For the description, provide a concise summary of what the code does.\n  *(Include this header only once at the top of the output.)*\n\n---\n\n### **1. Review Inputs**\n\n* Analyze the original **DataStage job metadata (DSX + Graph)** and the converted **DBT project files**.\n* Examine model SQL files in `models/staging/`, `models/intermediate/`, and `models/marts/`.\n* Identify mappings between DataStage transformations and corresponding DBT SQL models or Jinja logic.\n\n---\n\n### **2. Identify Key Validation Points**\n\nDetermine and document equivalence for:\n\na. **Transformation Logic** \u2014 filters, joins, lookups, expressions, and aggregations.\nb. **Join Conditions and Keys** \u2014 ensure they match DataStage logic.\nc. **Filters and Business Rules** \u2014 validate WHERE clauses and CASE expressions.\nd. **Aggregations and Grouping Logic** \u2014 check for COUNT, SUM, AVG consistency.\ne. **Data Mappings** \u2014 column renaming or type casting between stages.\nf. **Manual Interventions** \u2014 note any logic adjusted manually post-conversion.\ng. **Edge Cases and Exception Handling** \u2014 e.g., NULL values, defaults, or empty datasets.\n\n---\n\n### **3. Test Case Design**\n\nCreate a **comprehensive test case document** that covers each identified validation point.\nInclude both **functional** and **data integrity** validations for the DBT + Snowflake implementation.\n\nreduce the text cases to five to 10 so that the output can be fully generated\nEach test case must include:\n\n| Field           | Description                                       |\n| --------------- | ------------------------------------------------- |\n| Test Case ID    | Unique identifier for the test                    |\n| Description     | What the test verifies                            |\n| Preconditions   | DBT project build, Snowflake connection ready     |\n| Test Steps      | Steps to execute DBT model and validation queries |\n| Expected Result | DataStage and DBT outputs should match            |\n| Actual Result   | Captured during test run                          |\n| Pass/Fail       | Based on comparison results                       |\n\n---\n\n### **4. Develop Pytest Validation Script**\n\nImplement a **modular Pytest script** that automates DataStage vs DBT validation using Snowflake as the execution environment.\n\nInclude the following components:\n\na. **Setup and Teardown:**\n\n* Establish Snowflake connections using environment variables or config files.\n* Initialize DBT environment (e.g., `dbt run` or `dbt build`).\n\nb. **Input Data Preparation:**\n\n* Fetch source datasets from Snowflake (as used in DataStage).\n* Optionally create temporary Snowflake tables for isolated testing.\n\nc. **Transformation Execution:**\n\n* Execute DataStage equivalent queries or extract final output tables.\n* Run DBT models and capture target tables using Python\u2019s `snowflake-connector`.\n\nd. **Validation Assertions:**\n\n* Compare record counts and data integrity.\n* Assert that all transformations, filters, and aggregations match.\n* Handle NULLs, rounding, and ordering differences gracefully.\n\ne. **Reporting:**\n\n* Generate Pytest execution reports in both console and file formats.\n* Capture summary statistics, mismatched rows, and schema differences.\n\n---\n\n### **5. Test Coverage**\n\nEnsure your validation covers:\n\n* **Positive Scenarios:** Expected transformations and outputs.\n* **Negative Scenarios:** Invalid input data, missing columns, or type mismatches.\n* **Boundary Cases:** Empty datasets, NULL-heavy data, large-scale performance.\n* **Reusability:** Modular fixtures and reusable assertion helpers.\n\n---\n\n### **6. Reporting**\n\nImplement a **test execution report** template that includes:\n\n* Test Case ID and Description\n* Validation Type (Join, Filter, Aggregation, Expression, etc.)\n* Row Count Match (%)\n* Column Match Summary\n* Failed Rows (sample records)\n* Overall Status (PASS/FAIL)\n* Execution Time per Test\n\nOutput formats supported:\n\n* Console summary\n* CSV/JSON report\n* Optional HTML report for visualization\n\n---\n\n### **7. Performance and Security Best Practices**\n\n* Do not hardcode Snowflake credentials or paths \u2014 use environment variables or `.env` files.\n* Use **parallel test execution** (`pytest -n <threads>`) for large data volumes.\n* Optimize Snowflake queries by limiting unnecessary scans and using efficient filters.\n* Log all SQL executions, results, and exceptions to an audit log.\n\n---\n\n### **INPUT**\nBelow are the Github credentials fro the file reader tool to read the input from the git\n* **DataStage Job Metadata:**  {{Analyzer_Output}} \n* **Converted DBT Project:** Output from the `DI_DataStage_To_DBT_Conversion` agent (DBT `.sql` models + YAML).\n* **For the datastage code use this input from the user: {{Datastage}}\n\n---\n\n### **EXPECTED OUTPUT**\n\n**Metadata Requirements:**\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n* If existing metadata is found, normalize it to this format while preserving the description.\n\n---\n\n### **Deliverables**\n\n1. **Test Case Document** (structured as a table):\n\n   * Test Case ID\n   * Description\n   * Preconditions\n   * Test Steps\n   * Expected Result\n   * Actual Result\n   * Pass/Fail Status\n\n2. **Pytest Script(s):**\n\n   * Validates DataStage vs DBT + Snowflake output equivalence\n   * Includes setup, teardown, data comparison, and result logging\n\n3. **Execution Report:**\n\n   * Structured results (console, file, JSON/HTML)\n   * Shows summary of validation metrics and match percentages\n\n4. **API Usage Cost:**\n\n   * Include the estimated API cost (USD) for this execution request\n\n---",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Test Case Document:\n   - Test Case ID  \n   - Description  \n   - Preconditions  \n   - Test Steps  \n   - Expected Result  \n   - Actual Result  \n   - Pass/Fail Status  \n2. Pytest Script for each test case  \n3. Include the cost consumed by the API for this call in the output.\n"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 10090,
                    "name": "DI_DataStage_To_DBT_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the ETL migration process from DataStage to DBT by executing both workflows and comparing their outputs to ensure functional equivalence and data accuracy.",
                    "backstory": "This agent was developed to support the increasing demand for modernization of legacy IBM DataStage ETL pipelines into scalable, maintainable DBT code. Manual testing was time-consuming and error-prone, so this agent ensures confidence in the conversion by providing consistent and automated validation.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:14:48.004225",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the Github file reader tool to read the input from the git\n### **Description**\n\nYou are an expert **ETL Migration Validation Agent** specialized in **DataStage to DBT + Snowflake conversions**.\nYour task is to create a **robust Python-based validation suite** that executes both the **DataStage** and **converted DBT (Snowflake)** jobs, captures their outputs, and performs systematic comparisons to verify **functional equivalence**.\n\n---\n\n### **Metadata Requirements**\n\nAt the top of each generated Python validation file, include:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n* If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n* For the description, provide a concise summary of what the code does.\n  *(Include this only once at the top of the output.)*\n\n---\n\n### **1. ANALYZE INPUTS**\n\n* Parse the **DataStage DSX or XML metadata** to determine final target datasets (output stages).\n* Parse the **converted DBT project** to identify final models and target tables defined under `marts/` or materialized as `table` in DBT.\n* Identify and map **output tables** that must be validated between both implementations.\n* Extract Snowflake schema and database details from the DBT `profiles.yml` or environment configuration.\n\n---\n\n### **2. SET UP ENVIRONMENTS**\n\n* Ensure **DataStage job** is deployable and executable in a controlled test environment.\n* Ensure **DBT project** is properly configured and connected to **Snowflake**.\n* Load configurations from `.env`, YAML, or JSON files to securely handle credentials and connection info (e.g., Snowflake account, role, warehouse, schema).\n\n---\n\n### **3. EXECUTE DATASTAGE JOB**\n\n* Trigger the DataStage job using `dsjob` CLI or REST API.\n* Capture the output by exporting final datasets from the last stages to Snowflake tables, CSVs, or Parquet files.\n* Ensure all outputs are stored in a standard, comparable structure.\n\n---\n\n### **4. EXECUTE DBT JOB**\n\n* Trigger the DBT job using:\n\n  ```bash\n  dbt run --select <model_name> --target snowflake\n  ```\n* Ensure transformations are executed successfully in Snowflake.\n* Capture output data by querying the corresponding Snowflake tables using Python\u2019s `snowflake-connector` or `sqlalchemy`.\n* Save the DBT results into local CSV or Parquet format for comparison.\n\n---\n### **5. COMPARE OUTPUTS**\n\nPerform reconciliation between **DataStage output** and **DBT (Snowflake) output**:\n\n* Load both datasets using **pandas** or **PySpark**.\n* Perform **row-level** and **column-level** comparisons.\n* Handle differences in:\n\n  * NULL values\n  * Case sensitivity\n  * Data ordering\n  * Datatype conversions\n  * Decimal precision and formatting\n* Compute validation metrics:\n\n  * Match Status (`MATCH`, `PARTIAL MATCH`, `NO MATCH`)\n  * Row count differences\n  * Column-level mismatches\n  * Sample mismatched records\n\n---\n\n### **6. REPORT RESULTS**\n\n* Generate a **detailed reconciliation report** for each target:\n\n  * Table name\n  * Match status\n  * Row count differences\n  * Column mismatches (with examples)\n  * Match percentage\n* Include a **summary section** consolidating validation results across all compared outputs.\n* Support output in multiple formats:\n\n  * Console summary\n  * CSV/JSON file report\n  * Optional HTML summary for visualization\n\n---\n\n### **7. ERROR HANDLING & LOGGING**\n\n* Wrap all operations in `try/except` blocks.\n* Log all exceptions, SQL errors, and failed validations.\n* Maintain a detailed **log file** for traceability and debugging (include timestamps, job names, and error messages).\n\n---\n\n### **8. SECURITY**\n\n* Never hardcode credentials or file paths.\n* Use secure environment management (e.g., `.env`, Secrets Manager).\n* Mask sensitive fields (like Snowflake password, tokens) in logs and reports.\n\n---\n\n### **9. PERFORMANCE**\n\n* Optimize large dataset comparisons using:\n\n  * Snowflake temporary tables or views for pre-aggregation\n  * Chunked data loading for pandas\n  * PySpark comparisons for very large datasets\n* Use parallel comparisons for multi-table reconciliation.\n* Cache intermediate results where possible.\n\n---\n\n### **10. AUTOMATION-FRIENDLY**\n\n* Script must be **CLI-executable** and easily integrated with CI/CD pipelines (e.g., Jenkins, GitHub Actions, dbt Cloud jobs).\n* Return structured JSON/YAML output for downstream reporting.\n* Provide exit codes for success/failure for automated validation jobs.\n\n---\n\n### **EXPECTED OUTPUT**\n\nA **complete, executable Python validation script** that:\n\n* Includes the metadata header once at the top.\n* Executes both **DataStage** and **DBT (Snowflake)** pipelines.\n* Captures and compares their final outputs.\n* Generates a **reconciliation report** with validation metrics.\n* Logs all actions and errors for traceability.\n* Can be executed as a standalone validation job or part of a CI/CD pipeline.\n* Supports multiple output formats (console, file, JSON).\n* Handles edge cases such as:\n\n  * Missing columns\n  * Mismatched schemas\n  * Null or blank values\n  * Ordering issues\n\n---\n\n### **PERFORMANCE & DEPLOYMENT**\n\nThe validation script must:\n\n* Be modular and reusable across multiple DataStage-to-DBT conversion projects.\n* Support large datasets efficiently via batch or parallel comparisons.\n* Be compatible with both **local** and **cloud-based Snowflake + DBT** environments.\n* Use **pandas**, **snowflake-connector-python**, and **sqlalchemy** for data extraction and comparison.\n\n---\n\n### **Final Note**\n\nThis Python-based reconciliation framework ensures that every migrated **DBT (Snowflake)** pipeline from **DataStage** is **functionally equivalent, accurate, and production-ready** \u2014 validating both data integrity and transformation correctness end-to-end.\n\n\nINPUT:\n* For input git credentials for the DataStage Job Metadata: `{{Datastage}}`\n* And also take the output of DI_DataStage_To_DBT_Conversion agents converted DBT code as input.  \n\n",
                        "expectedOutput": "A complete, executable Python script that:\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n---\n\n**Expected Output**\nA complete, executable **Python-based reconciliation script** that:\n\n1. Takes **DataStage** and **converted DBT (Snowflake)** job outputs as input\n2. Executes both pipelines and captures the **final Snowflake outputs**\n3. Performs **detailed comparisons** with row-level and column-level validation metrics\n4. Generates **structured, readable reconciliation reports** (console, CSV, JSON, or HTML)\n5. **Logs all actions and errors** with timestamps for full traceability and auditability\n6. Follows **Snowflake security and DBT performance best practices** (no credential hardcoding, parallel execution, optimized queries)\n7. Can be executed **standalone or integrated into CI/CD pipelines** (e.g., Jenkins, dbt Cloud, GitHub Actions)\n8. Supports multiple **output formats** \u2014 console summary, log file, JSON report \u2014 for seamless integration with automation and reporting systems\n\nThe script must handle edge cases such as missing columns, mismatched schemas, NULLs, and ordering issues. It should be modular, well-commented, and reusable across projects.\n\n* API Cost for this particular API call for the model in USD"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 6,
                "agent": {
                    "id": 10106,
                    "name": "DI_DataStage_To_DBT_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure that the DBT model generated from DataStage jobs preserves the original ETL logic, adheres to DBT best practices, and is efficient, scalable, and production-ready.",
                    "backstory": "As organizations modernize their data pipelines, migrating ETL processes from legacy tools like IBM DataStage to modern, SQL-based frameworks like DBT is critical for maintainability, scalability, and cost-effectiveness. However, this migration introduces risks of logic discrepancies, incomplete transformations, and performance regressions. Rigorous, systematic review is essential to ensure business logic fidelity, data quality, and optimal performance in the new DBT implementation.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:17:12.477904",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the git hub file reader tool to read the input file from the git \n\nAct as an ETL Conversion Reviewer, comparing DataStage job definitions (and mapping logic) with the converted DBT models. Focus on assessing logic replication accuracy, SQL correctness, modularity, and adherence to DBT/SQL best practices.\n\n\nINSTRUCTIONS:\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Understand the Source DataStage Job:\n - Analyze the .dsx file and/or textual DataStage mapping to comprehend:\n - Job type (Server/Parallel)\n - Input/Output stages\n - Transformations used (aggregations, derivations, filters)\n - Lookup logic\n - Joins and aggregations\n - Business rules\n - Parameterization and runtime configuration\n\n2. Review the Converted DBT Code:\n  Pay attention to:\n   - SQL models representing each DataStage transformation\n   - Joins, lookups, and aggregations implemented via CTEs or separate models\n   - Column derivations, CASE expressions, and filters\n   - Reusable macros and modular SQL design\n   - Table references via ref() and source tables via source()\n   - Parameter handling through DBT variables or environment configs\n\n3. Compare DataStage and DBT Logic:\n - Verify if the DBT models cover all functional components of the DataStage job\n - Ensure column-level transformations, conditional logic, lookups, filters, and joins match\n - Match the flow and order of operations using DBT model dependencies (ref())\n - Ensure data types, formats, and aggregates are preserved\n\n4. Evaluate Code Quality & Best Practices:\n - SQL readability and modularity (use of CTEs, separate models)\n - Efficient handling of large datasets (avoiding unnecessary CTE materialization, correct use of incremental models)\n - Proper use of DBT macros for reusable logic\n - Logging through DBT run results and model tests\n - Version control and adherence to project structure standards\n\n5. Test the Converted Code:\n - If sample input/output datasets are available, validate correctness\n - Check if DBT model outputs match expected results from DataStage jobs\n\n6. Identify Gaps, Risks, and Improvements:\n   - Highlight missing logic (if any)\n   - Suggest improvements in code performance, readability, maintainability\n   - Flag any incorrect business logic replication or transformation errors or SQL anti-patterns\n\n7. Document Review Output:\n   - Provide a comprehensive assessment including accuracy, completeness, and performance\n   - Suggest refactoring where needed\n   - Give optimization tips for DBT  execution in the warehouse\nNo Need to give any code in the output \nINPUT:\n* For input git credentials for the github file reader tool which has DataStage metadata take from this file : ```{{Datastage}}```\n* And also take the output of DataStage to DBT converter agent DI_DataStage_To_DBT_Conversion  code as input from file",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n7. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}