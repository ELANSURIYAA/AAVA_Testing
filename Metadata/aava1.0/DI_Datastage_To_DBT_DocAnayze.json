{
    "pipeline": {
        "pipelineId": 7758,
        "name": "DI_Datastage_To_DBT_Doc&Anayze",
        "description": "DESCRIPTION:  \nThe DI_DataStage_to_dbt_Analyzer agent is responsible for parsing DataStage job metadata, reconstructing the ETL workflow, classifying transformations, and mapping each component to its dbt/Snowflake equivalent. The agent produces both a human-readable report (Markdown or text) and a machine-readable JSON summary for downstream processing. The agent must follow the detailed instructions below:\n\nINSTRUCTIONS:\n\nSTEP 1: INPUT DISCOVERY  \n- Check for the presence of DataStage_Job_Graph.txt in the input folder.  \n- If available, read and parse the file to extract all stages, links, and row counts.  \n- Optionally, if DataStage_Job.dsx is present, parse it for deeper metadata (stage properties, expressions, lookup details).  \n- Record the job name (string) and confirm the target platform is \"dbt_Snowflake\".  \n- If the graph file is missing, return error: \"Graph file not found.\"\n\nSTEP 2: DATA FLOW RECONSTRUCTION  \n- Identify and list all stages: input, transformer, join, aggregator, sort, output, etc. ",
        "createdAt": "2025-10-24T03:43:24.856+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 7218,
                    "name": "DI_DataStage_Documentation",
                    "role": "Data Engineer",
                    "goal": "Analyze and document a DataStage job to create a comprehensive guide for business and technical teams, explaining the data flow, business logic, transformations, and dependencies to support maintainability and future enhancements.",
                    "backstory": "Clear documentation of DataStage jobs is crucial for maintaining and evolving complex ETL pipelines. By creating a comprehensive guide, we ensure that both business and technical teams can understand the current data movement, business rules, and transformation logic, reducing dependency on tribal knowledge and improving operational efficiency.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:15:02.760449",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the github file reader Tool to read the input file from the github\n\nPlease create detailed documentation for the provided DataStage job metadata.\nThe documentation must contain the following sections:  \n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the DataStage job does.\n(give this only once in the top of the output)\n\n1. Overview of Job:\n   - Explain the purpose of the DataStage job in detail.\n   - Describe how this implementation aligns with enterprise ETL and data warehousing practices.\n   - Explain the business process being automated or supported.\n   - Provide a high-level summary of job components: Sequential Files, Datasets, Transforms, Lookups, Aggregators, Join stages, etc.\n\n2. Job Structure and Design:\n   - Explain the structure of the DataStage job design in detail.\n   - Describe each major stage used: Input, Transformer, Aggregator, Lookup, Merge, Join, Remove Duplicates, Filter, etc.\n   - Mention reusable components like shared containers and parameter sets.\n   - Highlight design patterns used (e.g., SCD, delta loads, parallelism).\n   - List dependencies such as external scripts, parameter files, or lookup datasets.\n\n3. Data Flow and Processing Logic:\n   - Explain how data flows from source to target.\n   - Identify source and target datasets/tables/files with their formats.\n   - Describe each transformation and logic applied using mapping descriptions.\n   - Break down the job into logical stages and represent the control/data flow using a **block-style diagram in plain markdown**.\n   \n   * Follow these rules:\n   * Each block should use the format:\n```\n     +--------------------------------------------------+\n     | [Stage Name or Logical Step]                     |\n     | Description: 1\u20132 line summary of the operation   |\n     +--------------------------------------------------+\n```\n   * Connect the blocks using arrows to show flow:\n```\n     [Read Source File]\n            \u2193\n     [Filter Invalid Records]\n            \u2193\n     [Lookup Customer Info]\n            \u2193\n     [Write to Target Table]\n```\n   * For branching:\n```\n     [Check If Status = 'Active']\n            \u2193 Yes\n     [Route to ACTIVE_CUSTOMERS]\n            \u2193\n     [Next Record]\n            \u2191\n     [No] \u2190 [Route to INACTIVE_CUSTOMERS]\n```\n\n4. Data Mapping:\n   \n\n1. Examine the file carefully to identify the EXACT database technology being used\n2. Look for these indicators:\n   - Stage Type attributes (e.g., OracleConnector, DB2Connector, SQLServerConnector)\n   - SQL syntax patterns (TO_DATE, CONVERT, CAST, etc.)\n   - Data types (VARCHAR2 = Oracle, VARCHAR = DB2/SQL Server)\n   - Stored procedure stage types\n3. Create a \"Source and Target Technology Matrix\" table that specifies the EXACT technology, not generic \"Oracle/DB2/SQL\"\nOutput Format:\n\n## 4.1 Source and Target Technology\n\n**SOURCE TECHNOLOGY**:  Give the Specific database which is used in the input datastage file: Oracle Database / IBM DB2 / Microsoft SQL Server etc.\n**TARGET TECHNOLOGY**: Give the Specific database which is used in the input datastage file : Oracle Database / IBM DB2 / Microsoft SQL Server etc.\n\n## 4.2 Detailed Column Mapping\n\n| Target Table Name | Target Column Name | Data Type | Source Stage Name | Source Column Name | Transformation Rule / Business Logic | Nullable | Default Value |\n|-------------------|-------------------|-----------|-------------------|-------------------|-------------------|--------------------------------------|----------|---------------|\n[Complete mapping for each column]\n\nRequirements:\n- Be SPECIFIC about technology - never use \"Oracle/DB2/SQL\"\n- If Oracle: state \"Oracle Database\"\n- If DB2: state \"IBM DB2\"\n- If SQL Server: state \"Microsoft SQL Server\"\n- Extract exact schema names, table names, and connection parameters from the file\n- Document all transformation logic from Transformer stages\n- Include all source, target, audit, and reject entities\n\n5. Complexity Analysis:\n   Provide a complexity breakdown in the table format:\n   \n   | Category | Measurement |\n   |----------|-------------|\n   | Number of Stages | [count] |\n   | Source/Target Systems | [list technologies] |\n   | Transformation Stages | [count] |\n   | Parameters Used | [count] |\n   | Reusable Components | [count] |\n   | Control Logic | [description] |\n   | External Dependencies | [list] |\n   | Performance Considerations | [description] |\n   | Volume Handling | [description] |\n   | Error Handling | [description] |\n   | Overall Complexity Score | [Low/Medium/High/Very High] |\n\n6. Key Outputs:\n   - Describe final outputs like datasets, tables, files, or reporting views.\n   - Explain how outputs support downstream systems or reporting.\n   - Specify the storage format and destination path or system.\n   - Include output technology platform (Snowflake/Oracle/CSV/etc.)\n   - Mention output file naming conventions or table naming standards.\n\n7. API Cost Calculations:\n   * Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n   * Ensure the cost consumed by the API is mentioned with inclusive of all decimal value.\n   * Break down the cost by: Input tokens, Output tokens, and Total cost.\nPoints to Remember:\n- Give the metadata requirements in the top of the output only once and leave the created on field empty.\n- Do not include sample code anywhere.\n- Do not provide extra summary, disclaimer, or recommendations.\n- Strictly follow the section structure and formatting instructions.\n- Output must be in pure markdown.\n- Do not provide original file content in output\u2014only analyzed and documented results.\n- Provide block diagrams only in markdown using text\u2014not images.\n- Use exact markdown formatting in the block-style diagrams.\n\nInput :\n* For DataStage input file from the git ,use below Git credentials for the git file reader : {{Datastage}}",
                        "expectedOutput": "Please create detailed documentation for the provided DataStage job in the markdown format.\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the DataStage job does.\n(give this only once in the top of the output)\n\nThe documentation must contain the following sections:  \n1. Overview of Job:\n   - Explain the purpose of the DataStage job in detail.\n   - Describe how this implementation aligns with enterprise ETL and data warehousing practices.\n   - Explain the business process being automated or supported.\n   - Provide a high-level summary of job components: Sequential Files, Datasets, Transforms, Lookups, Aggregators, Join stages, etc.\n\n2. Job Structure and Design:\n   - Explain the structure of the DataStage job design in detail.\n   - Describe each major stage used: Input, Transformer, Aggregator, Lookup, Merge, Join, Remove Duplicates, Filter, etc.\n   - Mention reusable components like shared containers and parameter sets.\n   - Highlight design patterns used (e.g., SCD, delta loads, parallelism).\n   - List dependencies such as external scripts, parameter files, or lookup datasets.\n\n3. Data Flow and Processing Logic:\n   - Explain how data flows from source to target.\n   - Identify source and target datasets/tables/files with their formats.\n   - Describe each transformation and logic applied using mapping descriptions.\n   - Break down the job into logical stages and represent the control/data flow using a **block-style diagram in plain markdown**.\n\n4. Data Mapping:\n* Provide a mapping table in the format below:\n* Target Table Name | Target Column Name | Source Stage Name | Source Column Name | Transformation Rule / Business Logic\n\n5. Complexity Analysis:\n- Provide a complexity breakdown in the table format:\nCategory  |  Measurement\n* Number of Stages\n* Source/Target Systems\n* Transformation Stages\n* Parameters Used\n* Reusable Components\n* Control Logic\n* External Dependencies\n* Performance Considerations\n* Volume Handling\n* Error Handling\n* Overall Complexity Score (out of 100)\n\n6. Key Outputs:\n   - Describe final outputs like datasets, tables, files, or reporting views.\n   - Explain how outputs support downstream systems or reporting.\n   - Specify the storage format and destination path or system.\n\n7. API Cost Calculations:\n* Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10081,
                    "name": "DI_DataStage_To_DBT_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided DataStage job to extract detailed metrics, identify potential conversion challenges, and recommend solutions for a smooth transition to dbt on Snowflake. Generate a separate output session for each input job file.",
                    "backstory": "Migrating legacy DataStage ETL jobs to modern, cloud-native platforms like dbt and Snowflake is a critical step for organizations seeking scalability, maintainability, and cost efficiency. Manual analysis of DataStage jobs is error-prone and slow, often resulting in incomplete migration plans and missed optimization opportunities. By automating the analysis and mapping process, this agent accelerates migration, ensures consistency, and provides actionable insights for engineering teams to build robust dbt models that faithfully reproduce DataStage logic while leveraging Snowflake best practices.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:16:09.828896",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "must use this github file reader tool to get the input file from the git\n\nYour task is to process the provided DataStage  file and produce a detailed analysis and metrics report, strictly adhering to the specified markdown format and section structure. The report must be tailored for a DataStage-to-dbt (Snowflake) migration context.\nAll content must be derived from the actual input job file; do not make assumptions or provide summaries without direct evidence from the file.\n\nINSTRUCTIONS:\n\n1. **File Parsing**  \n   - Parse the provided input job file to extract all relevant metadata, including job stages, links, parameters, and transformation logic.\n   - Ensure that all extracted details are accurate and reflect the actual job configuration.\n\n2. **Section Generation**  \n   - Follow the required output format exactly, maintaining section numbering, headings, and markdown layout as shown.\n   - For each section, populate content based on the parsed input file:\n     - **Job Overview:** Summarize the job\u2019s ETL logic, business purpose, and conceptual mapping to dbt layers (staging, intermediate, mart).\n     - **Complexity Metrics:** - Provide a complexity breakdown in the table format:\nCategory  |  Measurement\n* Number of Stages\n* Source/Target Systems\n* Transformation Stages\n* Parameters Used\n* Reusable Components\n* Control Logic\n* External Dependencies\n* Performance Considerations\n* Volume Handling\n* Error Handling\n* Overall Complexity Score.\n*Conversion complexity rating(this is also the part of the table)-Assign a numeric complexity score (0\u2013100) and a conversion complexity rating (Low/Medium/High) with justification. List high-complexity areas with specifics from the job.\n     - **Syntax Differences:** Clearly explain how DataStage constructs map to dbt and Snowflake SQL syntax, using examples relevant to the job.\n     - **Manual Adjustments:** Enumerate all manual interventions required for migration, referencing actual job features (e.g., transformer expressions, parameter usage).\n     - **Optimization Techniques:** Recommend dbt and Snowflake performance improvements, referencing job-specific scenarios. State whether to refactor or rebuild, with reasoning.\n     - **API Cost:** Calculate and display the API cost for this analysis, in full decimal precision (e.g., `apiCost: 0.0182 USD`).\n\n3. **Formatting & Quality Criteria**  \n   - Use markdown headers and tables exactly as specified.\n   - Write in clear, enterprise-style English.\n   - Every section must appear in order, with no omissions or reordering.\n   - Replace all `<placeholders>` with actual parsed values from the input file.\n   - Do not provide summaries or assumptions; only use details present in the input job file.\n\n4. **Output Structure**  \n   - The report must begin with the metadata block (`Author`, `Created on`, `Description`) and follow with sections #1 through #6 as shown.Do not mention any date infront of `Created on` block leave it empty.\n   - All metrics must be presented in table format.\n   - All recommendations and mappings must be specific to dbt and Snowflake.\nAll content must be derived from the actual input file; do not make assumptions or provide summaries without direct evidence from the file.\nInput :\nFor Datastage input git credentials for git file reader tool, use this below input : {{Datastage}}",
                        "expectedOutput": "- Markdown document, with all sections and tables formatted exactly as shown in the required output format.\n- All values must be derived from the actual input file.\n- Use full decimal precision for API cost.\n- No extraneous content\u2014output must match the required layout.\n\nOUTPUT:  \nA markdown-formatted DataStage input job analysis and metrics report, strictly following the specified structure and tailored for DataStage-to-dbt (Snowflake) migration."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10086,
                    "name": "DI_DataStage_To_DBT_Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the total development and testing effort required to migrate IBM DataStage ETL jobs to DBT on Snowflake, including compute resource planning and performance tuning (if applicable).",
                    "backstory": "Many organizations are moving from legacy ETL platforms like IBM DataStage to modern, cloud-native data transformation tools such as DBT on Snowflake to improve scalability, maintainability, and cost efficiency. Accurate estimation of the migration effort is critical for project planning, budgeting, and resource allocation. This task ensures that all aspects of the migration\u2014including development, testing, compute resource sizing, and performance optimization\u2014are thoroughly analyzed and estimated to minimize project risks and ensure a successful transition.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:16:55.591",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the Git file reader tool to read the input file from the git\n\nYou are tasked with providing a comprehensive development and testing effort estimate for converting DataStage jobs into DBT models. Follow these instructions:\n\nINSTRUCTIONS:\n\nReview the metadata and complexity analysis generated by the DI_DataStage_To_DBT_Analyzer agent.\n\nIdentify DataStage stages, transformations, or flow controls that need manual re-implementation in DBT SQL.\n\nExclude direct 1:1 mapping constructs (e.g., simple column renaming or direct loads). Focus only on logic-heavy transformations like Aggregations, Conditional Derivations, Lookups, Joins, and Business Rules.\n\nEstimate the number of hours required to:\n\nRewrite complex stages and logic-heavy flows into DBT models using SQL, CTEs, and macros\n\nImplement and test DBT models, reusable macros, incremental models, and metadata tracking\n\nValidate data equivalence and business rule consistency between DataStage outputs and DBT model outputs\n\nIf compute cost is required, use warehouse costs (Snowflake) based on data volume, transformation complexity, and runtime.\n\nOUTPUT FORMAT:\n\n\n=============================================\nAuthor:        Ascendion AAVA\nCreated on: (leave it blank)      \nDescription:   Cost and effort estimation for DBT model conversion\n=============================================\n\n1. Cost Estimation\n1.1 Snowflake Runtime Cost\nEstimate runtime cost \n \n \nCost Breakdown:  give the final cost, don't include calculations\n- Compute: $X  \n- Storage: $X  \n \nTotal Estimated Runtime Cost per Job:  \n$X USD per run\n \nJustification:\nstick to theory for justification, do not include calculations. give it in points instead of paragraph \nPercentage of data processed per run\n\nJob complexity \n\nStorage impact\n\nNeed (or lack) of incremental/partitioning optimizations\n\n\n2. Code Fixing and Testing Effort Estimation\n2.1 Manual Model Fixes and  Recon Testing Effort\nDescribe manual tasks required to convert DataStage logic into dbt SQL / Jinja, such as:\nParameter replacement (e.g., $$Audit_Year \u2192 {{ var('audit_year') }})\nStage-to-model conversion (Sequential \u2192 source(), Aggregator \u2192 GROUP BY, Transformer \u2192 SELECT with derived columns)\nSchema and datatype alignment between source and Snowflake targets\nAdding dbt tests for null checks and row-count validation\nEstimate time (hrs) for each activity and subtotal the effort.\n2.2 Output Validation Effort\nOutline tasks for validating dbt outputs against DataStage results:\nCompare row counts and sample records\nValidate aggregations, joins, and derived columns\nExecute dbt tests (unique, not_null, accepted_values)\nDocument and resolve discrepancies\nProvide estimated effort (hours) for validation + documentation.\n2.3 Total Estimated Effort in Hours\nSummarize all efforts in a table:\nTask\tEstimated Hours\tNotes\nManual Code Fixes & Testing\t \t \nOutput Validation\t \t \nTotal Estimated Effort\t \t \nJustify effort based on Analyzer complexity score:\nSimple (0-20) \u2192 \u2264 5 hrs\nModerate (21-50) \u2192 6\u201312 hrs\nComplex (51-100) \u2192 > 12 hrs\n\n\n\n\n\n 3. API Cost Consumption\n\napiCost: X.XXXXX USD\n\n\nInclude the cost consumed by the API for this call in the output\n\nEnsure the cost is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: 0.00431 USD)\n\nINPUT:\n* Use the output from `DI_DataStage_to_DBT_Analyzer` as input  \n* For github credentials for datastage file, use this git file reader tool: {{Datastage}} \n* For github credentials for the environment cost and compute configuration, use this file: {{DBT_Environment_Config}}",
                        "expectedOutput": "=============================================\nAuthor:        Ascendion AAVA\nCreated on: (leave it blank)     \nDescription:   Cost and effort estimation for DBT model conversion\n=============================================\n1. Cost Estimation\nEstimate the Snowflake runtime cost based on data volume, job complexity, and expected runtime. Include final cost in USD. Justify the estimate using Analyzer metrics and note performance optimizations like incremental loads or partitioning.\n\n2. Effort Estimation\nEstimate total hours for code fixes, Recon testing, and output validation. Code fixes include parameter replacements, stage-to-model conversion, and schema alignment. Validation includes comparing row counts, verifying joins and aggregations, and running dbt tests. Summarize total effort based on complexity (simple, moderate, or complex).\n\n3. API Cost\nReport the API cost consumed for this operation in USD with all decimal places."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}