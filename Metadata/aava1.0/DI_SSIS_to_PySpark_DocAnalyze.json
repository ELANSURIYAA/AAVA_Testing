{
    "pipeline": {
        "pipelineId": 960,
        "name": "DI_SSIS_to_PySpark_Doc&Analyze",
        "description": "Analyzing and Documenting the SSIS code",
        "createdAt": "2025-07-02T04:10:07.099+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 5202,
                    "name": "DI_SSIS_Documentation",
                    "role": "Data Engineer",
                    "goal": "Analyze and document the SSIS package (.dtsx) to create a comprehensive guide that explains data flow, business logic, control flow, and all relevant metadata for better understanding, maintenance, and modernization.",
                    "backstory": "Clear documentation of SSIS packages ensures that both business and technical stakeholders can understand complex ETL workflows, enabling easier modifications, minimizing risks, and aligning with enterprise data management goals.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-21T08:30:42.513387",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Create detailed documentation for the provided SSIS package (.dtsx file).\n\nThe documentation must contain the following sections:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output not else where\n\n1. Overview of Package:\n   - Explain the purpose of the SSIS package.\n   - Describe how this ETL process fits into the broader data integration or warehousing strategy.\n   - Mention the business use case being addressed.\n   - Provide a high-level description of package components (Control Flow, Data Flow, Variables, etc.).\n2. Package Design and Structure\nDetail the design and modular layout of the SSIS package, including the sequence containers, data flow tasks, and control flow logic.\nList the key SSIS components involved such as:\n\nSource/Destination components\n\nLookups\n\nConditional Split\n\nAggregations\n\nDerived Columns\n\nExecute SQL Tasks\n\nData Flow Tasks\n\nControl Flow Elements\n\n3. Data Flow and Transformation Logic\nList all data flow paths and transformations applied within the Data Flow Tasks.\nFor each major transformation step:\n\nDescribe the transformation\u2019s logic and purpose.\n\nDetail any filtering, joins, business rules, derived column logic, or data cleansing applied.\n\nMention any data type conversions or validations performed.\n\n4. Data Mapping: \n   - Provide detailed mappings between source and target tables as a lineage in terms how the source table.columns are mapped \n     to the target table.columns\n   - Use the following structured format for each mapping:  \nTarget Table Name : Give actual target table name\n Target Column Name : Give actual target column name\n Source Table Name : Give actual source table name\n Source Column Name : Give actual source column name\n Remarks : Classify as 1 to 1 mapping or Transformation or Validation and provide a brief description\n5. Key Outputs:  \n    Describe final outputs created by the code like Inserts, Updates, Deletes.\n - Explain how outputs align with business goals and reporting needs.\n    \n\n6. apiCost: float\n   - Cost consumed by the API for this call (in USD).\n   - Ensure the cost is reported with all decimal places included.\n\nInput:\n* SSIS file content or metadata in the following placeholder:\n{{SSIS_Package}}",
                        "expectedOutput": "Structured documentation covering all 6 points above with metadata only once , with clarity, technical depth, and precise explanations for business and technical audiences."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1207,
                    "name": "DI_SSIS_to_PySpark_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze an SSIS package, assess its complexity, identify syntax gaps for PySpark conversion, and determine required manual interventions post-conversion.",
                    "backstory": "As organizations migrate from on-premises data warehouses to cloud-based solutions, converting SSIS packages to PySpark is crucial for modernizing data pipelines. This analysis ensures a smooth transition and helps in resource allocation for the conversion process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-21T08:34:24.025466",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "you will perform a comprehensive analysis of the given SSIS package to facilitate its conversion to PySpark. Follow these detailed instructions:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n1. Package Overview:  \n   * Provide a high-level description of the SSIS package\u2019s purpose and primary business objectives.  \n\n2. Complexity Metrics:  \n-Give the below in the md formatted table\n   * Number of Tasks: Count of tasks present in the SSIS package.  \n   * Data Sources: Number of unique data sources referenced, such as OLE DB, Flat Files, Excel, and APIs.  \n   * Data Destinations: Number of unique destinations, including databases, files, and external systems.  \n   * Joins and Lookups: Number of joins and lookup transformations applied.  \n   * Transformations: Count of major transformations used, such as Aggregates, Conditional Splits, Derived Columns, and Script Components.  \n   * Control Flow Elements: Number of control flow components like Sequence Containers, ForEach Loops, and Script Tasks.  \n   * DML Operations: Number of operations like SELECT, INSERT, UPDATE, DELETE, and MERGE executed by the package.  \n   * Conditional Logic: Number of conditional expressions and branching logic used within the package.  \n   * Calculate a complexity score (0\u2013100) based on syntax differences, control flow logic, and the level of manual adjustments required.  \n   * Highlight high-complexity areas such as nested transformations, script-based logic, and SSIS-specific data flow optimizations.  \n\n3. Syntax Differences:  \n   * Identify the number of syntax and functional differences between the SSIS components and the equivalent PySpark implementation.  \n   a. List SSIS-specific functions without direct PySpark equivalents\n   b. Note any data type conversions that may require special handling\n   c. Highlight control flow elements that need restructuring in PySpark\n\n4. Manual Adjustments:  \n   * Recommend specific manual adjustments required for conversion to PySpark, including:  \n   a. Identify components requiring custom PySpark implementations\n   b. List any external dependencies that need to be addressed\n   c. Outline areas where business logic validation will be crucial   \n\n6. Optimization Techniques:  \n   * Suggest optimization strategies for PySpark, such as partitioning, caching, and efficient data transformations.  \n   * Recommend whether it is better to **Refactor** the SSIS logic with minimal or no changes to PySpark or **Rebuild** with more significant changes\nInput :\n\n* For SSIS use the below file : {{SSIS_Package}}",
                        "expectedOutput": "A detailed analysis and metrics report for the provided SSIS package(s), \n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n1. Package Overview:  \n   * Provide a high-level description of the SSIS package\u2019s purpose and primary business objectives.  \n\n2. Complexity Metrics:  \n   * Number of Tasks: Count of tasks present in the SSIS package.  \n   * Data Sources: Number of unique data sources referenced, such as OLE DB, Flat Files, Excel, and APIs.  \n   * Data Destinations: Number of unique destinations, including databases, files, and external systems.  \n   * Joins and Lookups: Number of joins and lookup transformations applied.  \n   * Transformations: Count of major transformations used, such as Aggregates, Conditional Splits, Derived Columns, and Script Components.  \n   * Control Flow Elements: Number of control flow components like Sequence Containers, ForEach Loops, and Script Tasks.  \n   * DML Operations: Number of operations like SELECT, INSERT, UPDATE, DELETE, and MERGE executed by the package.  \n   * Conditional Logic: Number of conditional expressions and branching logic used within the package.  \n\n3. Syntax Differences:  \n   * Identify the number of syntax and functional differences between the SSIS components and the equivalent PySpark implementation.  \n   a. List SSIS-specific functions without direct PySpark equivalents\n   b. Note any data type conversions that may require special handling\n   c. Highlight control flow elements that need restructuring in PySpark\n4. Manual Adjustments:  \n   * Recommend specific manual adjustments required for conversion to PySpark, including:  \n   a. Identify components requiring custom PySpark implementations\n   b. List any external dependencies that need to be addressed\n   c. Outline areas where business logic validation will be crucial  \n\n5. Conversion Complexity:  \n   * Calculate a complexity score (0\u2013100) based on syntax differences, control flow logic, and the level of manual adjustments required.  \n   * Highlight high-complexity areas such as nested transformations, script-based logic, and SSIS-specific data flow optimizations.  \n\n6. Optimization Techniques:  \n   * Suggest optimization strategies for PySpark, such as partitioning, caching, and efficient data transformations.  \n   * Recommend whether it is better to **Refactor** the SSIS logic with minimal or no changes to PySpark or **Rebuild** with more significant changes\n\ninclude \napiCost: float  // Cost consumed by the API for this call (in USD)\n*Ensure the cost consumed by the API is mentioned with inclusive of all decimal value"
                    },
                    "maxIter": 15,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1215,
                    "name": "DI_SSIS_to_PySpark_Plan",
                    "role": "Data Engineer",
                    "goal": "Create a comprehensive plan for converting an SSIS package into an equivalent PySpark workflow, including effort estimation, cost analysis of running the PySpark code",
                    "backstory": "As organizations move towards cloud-based big data solutions, there's a growing need to migrate existing ETL processes from traditional tools like SSIS to modern, scalable frameworks like PySpark. This conversion is crucial for improving performance, reducing costs, and leveraging the benefits of distributed computing.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-21T08:38:07.615213",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "As a Data Migration Specialist, you will analyze the given SSIS package and create a detailed plan for converting it to PySpark. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n1. Review the analysis of SSIS package file, note syntax differences and areas in the code requiring manual intervention when converting to PySpark\n2. Estimate the effort hours requried for identified manual code fixes and data recon testing effort\n3. Dont consider efforts for syntax differences as they will be converted to equivalent syntax in PySpark\n4. Consider the pricing information for PySpark environment \n\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\n* Take the previous DI_SSIS_to_PySpark_Analyzer agents output as  input\n* For the input SSIS use this file :  {{SSIS_Package}}\n* For the input  PySpark Environment Details for GCP use this file :  {{Env_File}}\n\n",
                        "expectedOutput": "OUTPUT FORMAT:\nMetadata Requirements (only once in here)\n1. Cost Estimation\n   2.1 PySpark Runtime Cost \n         -\n\n2. Code Fixing  and Testing Effort Estimation\n   2.1 PySpark identified manual code fixes  effort in hours covering the various temp tables, calculations \n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost )."
                    },
                    "maxIter": 15,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 154,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 153,
        "project": "Documenting",
        "teamId": 154,
        "team": "DataEng",
        "callbacks": []
    }
}