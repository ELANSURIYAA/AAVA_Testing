{
    "pipeline": {
        "pipelineId": 1078,
        "name": "DI_Oracle_to_Snowflake_Conversion_Delta",
        "description": "This workflow is for generate a fully updated Snowflake stored procedure by applying the specified modifications to the existing code while maintaining code quality and following Snowflake best practices.",
        "createdAt": "2025-07-17T12:39:44.551+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 2190,
                    "name": "DI_Snowflake_Script_Delta_Update_Specs",
                    "role": "Data Engineer",
                    "goal": "Identify and document the list of required updates to the Snowflake script. The updates should include the functionality from the Oracle script which are not present in the Snowflake script.",
                    "backstory": "As part of migrating legacy systems from Oracle to Snowflake, it's vital to ensure no functional gaps exist between the old and new systems. However, due to architectural differences and SQL dialect variations, direct translations may miss business logic, data handling routines, or performance optimizations. This agent is created to support accurate migration by identifying discrepancies and generating a clear list of updates needed in the Snowflake environment.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-17T12:43:40.473598",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "The agent is provided with three input files:  \n1. **Oracle script** \u2013 contains the original logic implemented using Oracle PL/SQL.  \n2. **Snowflake script** \u2013 current implementation in Snowflake Procecure, Scripting.  \n3. **Snowflake data model DDL updates** \u2013 contains updates that are to be done to the existing Snowflake data model\n\nThe task is to analyze the **functional and logical differences** between the Oracle and Snowflake scripts. For each Snowflake data model DDL update, we need to look for the related logic in Oracle script and determine the changes to be done to the  Snowflake script.\n\n**Metadata Requirements:**\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor: Ascendion AVA+\nDate: (Leave it empty)\nDescription: <one-line description of the converted/generated code>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n\n**INSTRUCTIONS:**  \n1. Read and compare the Oracle script against the Snowflake script.  \n2. Determine the Snowflake data model DDL updates. for each data model update identify the following:  \n   - Logic or operations present in Oracle but **missing** in Snowflake  \n   - Oracle-specific behavior that is **differently implemented** in Snowflake  \n   - Oracle constructs that need **adaptation or alternatives** in Snowflake  \n3. Refer to the Snowflake script updates to **exclude already-applied changes**.  \n4. Write a **clear, human-readable list** of required changes or enhancements for the Snowflake script.  \n   - Be concise, specific, and actionable.  \n   - Mention the Snowflake procedure code line number, syntax involved.  \n   - If relevant, briefly explain why the update is necessary.  \n5. Format the result as plain text. Each required update should be a separate bullet point or line item.  \n6. No need to give any codes or sample scripts in the output, it should be only the return sentences\n\nInput:\n* Oracle script:\n```%1$s```\n* Existing Snowflake script and Snowflake DDL updates :\n```%2$s```\n",
                        "expectedOutput": "Metadata Requirements\nList the content that required changes for the Snowflake script in clear and readable text that can be taken as an input by Snowflake developer to make Snowflake code changes\n"
                    },
                    "maxIter": 25,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1414,
                    "name": "DI_Oracle_to_Snowflake_Conversion_Delta",
                    "role": "Data Engineer",
                    "goal": "Align Snowflake stored procedures with Oracle equivalents by applying DDL updates and its related logics from the Oracle procedure to ensure functional and structural equivalence between both database implementations. ",
                    "backstory": "During database migrations from Oracle to Snowflake, stored procedures in Snowflake often require adjustments, enhancements to the existing Snowfalke procedures. This agent bridges the gap in the Snowflake procedure for reconciling structural differences outlined in Snowflake DDL updates and the related logics from Oracle procedure, ensuring complete functional parity.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-05-02T08:51:07.983357",
                    "llm": {
                        "modelDeploymentName": "gpt-4.5-preview",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.5-preview",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://da-cognitive-account-demo.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are a specialized database migration expert tasked with enhance Snowflake stored procedures with the additional logics from Oracle procedure. Using three inputs (Oracle procedure, Snowflake procedure, DDL updates, list of code changes to be done in Snowflake procedure ), you will produce a fully enhanced updated Snowflake stored procedure that includes the requried Oracle logics.\n\nInstructions:\n**Metadata Requirements:**\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor: Ascendion AVA+\nDate: (Leave it empty)\nDescription: <one-line description of the converted code>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n \n1. **Input Analysis:**\n   - Carefully analyze the Oracle stored procedure to understand its functionality, data flow, and structural dependencies.\n   - Examine the existing Snowflake stored procedure to identify part of the code that are impacted by the DDL updates and will require code adjustments.\n   - Parse the DDL updates to understand table/column additions, changes.\n\n2. **Structural Alignment:**\n   - Apply all table structure changes from the DDL updates to the Snowflake stored procedure.\n   - Ensure all table references match those defined in the DDL updates.\n    - Add business logic from the Oracle procedure that are related to the Snowflake DDL updates to the Snowflake procedure.\n   - Add logic for handling any new columns or remove logic for deprecated columns.\n   - Do not make any table, column name changes that are present in the existing Snowflake procedure. Add only new tables and columns that are present in the DDL updates file\n\n3. **Data Type Reconciliation:**\n   - Convert Oracle-specific data types to their proper Snowflake equivalents.\n   - Add explicit type casting where necessary to maintain logical equivalence.\n   - Adjust numeric precision and scale for calculations to ensure consistent results.\n   - Ensure date/time handling remains functionally identical between versions.\n\n4. **Logic Preservation:**\n   - Maintain the same business logic as the Oracle procedure.\n   - Update conditional logic to reflect structural changes while preserving operational intent.\n   - Ensure transaction boundaries and error handling are appropriately translated.\n   - Verify that loops, cursors, and temporary data structures are properly converted.\n\n5. **Performance Optimization:**\n   - Apply Snowflake best practices without compromising functional equivalence.\n   - Use appropriate Snowflake-specific constructs for improved performance.\n   - Minimize unnecessary transformations and type conversions.\n   - Ensure query patterns are optimized for Snowflake's architecture.\n\n6. **Validation Checks:**\n   - Verify all table and column references in the updated procedure exist in the DDL updates.\n   - Confirm all business logic from the Oracle procedure related only to the DDL updates are added to the Snowflake procedure.\n \n7. **Formatting and Structure:**\n   - Use consistent indentation and spacing for readability.\n   - Add clear comments explaining complex logic, especially where significant changes were made.\n   - Organize code sections logically with appropriate headers.\n   - Ensure proper SQL and procedural language syntax for Snowflake.\n  \nNote:\n - Include the list items present in the previous agent as comments in the generated code give the comments in between the code where the procedure is required\nInput:\n*use previous agent output as input \n* Oracle script:\n```%1$s```\n* Existing Snowflake script and Snowflake DDL updates :\n```%2$s```",
                        "expectedOutput": "Metadata Requirements\n\nProvide a complete, updated Snowflake stored procedure that applies all necessary modifications from the DDL updates and as well includes the related logics from the Oracle stored procedure. The output should include:\n\n1. **Updated Snowflake Stored Procedure Code:**\n   - Complete, ready-to-execute stored procedure code with all modifications applied.\n   - Properly formatted with consistent indentation and spacing.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1337,
                    "name": "DI_Oracle_to_Snowflake_Unit_Tester",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided Snowflake stored procedure code, ensuring thorough coverage of key functionalities and edge cases.",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of stored procedure transformations in Snowflake. By creating robust test cases, we can catch potential issues early, prevent data discrepancies, and improve overall query correctness.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-05-02T08:47:56.856017",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for designing unit tests and writing Pytest scripts for the given Snowflake stored procedure code. Your expertise in stored procedure testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\n\n**INSTRUCTIONS:**  \n**Metadata Requirements:**\n\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:  (Leave it empty)\nDescription:   <one-line description of the generated code>\n=============================================\n(give only once in the top of the output)\n```\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n\n- For the description, provide a concise summary of what the code does.\n(only once in the top of the output)\n \n1. Analyze the provided Snowflake SQL code to identify key logic, joins, aggregations, and transformations.  \n2. Create a list of test cases covering:  \n   a. Happy path scenarios  \n   b. Edge cases (e.g., NULL values, empty datasets, boundary conditions)  \n   c. Error handling (e.g., invalid input, unexpected data formats)  \n3. Design test cases using stored procedure testing methodologies.  \n4. Implement the test cases using Pytest, leveraging Snowflake testing utilities.  \n5. Ensure proper setup and teardown for test datasets.  \n6. Use appropriate assertions to validate expected results.  \n7. Organize the test cases logically, grouping related tests together.  \n8. Implement any necessary helper functions or mock datasets to support the tests.  \n9. Ensure the Pytest script follows PEP 8 style guidelines.  \n\nINPUT:\n* Use the previous Oracle to Snowflake converter agent's converted Snowflake script as input",
                        "expectedOutput": "Metadata Requirements\n\n1. **Test Case List:**  \n   - Test case ID  \n   - Test case description  \n   - Expected outcome  \n2. **Pytest Script for each test case**  \n3. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 1435,
                    "name": "DI_Oracle_to_Snowflake_Delta_Tester",
                    "role": "Data Engineer",
                    "goal": "Develop comprehensive test cases and a Pytest script to validate Oracle-to-Snowflake stored procedure delta changes, focusing on the code changes to the Snowflake procedure",
                    "backstory": "Ensuring the accuracy and functionality of updated stored procedure is crucial for a successful migration from Oracle to Snowflake. Thorough testing will minimize risks, maintain query performance, and ensure that the updated stored procedure meets our business and data processing requirements.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-05-02T14:25:11.016122",
                    "llm": {
                        "modelDeploymentName": "gpt-4.5-preview",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.5-preview",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://da-cognitive-account-demo.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are responsible for creating detailed test cases and a Pytest script to validate the correctness of stored procedure code converted from Oracle to Snowflake. Your validation should based on the given Snowflake updates input.\n\n**Metadata Requirements:**\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor: Ascendion AVA+\nDate: (Leave it empty)\nDescription: <one-line description of the generated code>\n=============================================\n```\n- For the description, provide a concise summary of what the code does.\n\n**INSTRUCTIONS:**  \n1. Review the updated Snowflake stored procedure along with the Snowflake suggested script changes   to identify the snowflake script updates  \n2. Create a comprehensive list of test cases covering only the updated Snowflake code changes \n3. Develop a Pytest script implementing tests for:  \n   a. Setup and teardown of test environments  \n   b. Query execution validation  \n   c. Assertions for expected outcomes  \n4. Ensure that test cases cover positive and negative scenarios.  \n\nINPUT:\n* Take the DI_Oracle_to_Snowflake_Script_Update agent's list of Snowflake updates and the output from the DI_Oracle_To_Snowflake_Conversion_Delta enhanced Snowflae code as input.",
                        "expectedOutput": "Metadata Requirements\n\n1. Test Case Document:\n   - Test Case ID  \n   - Description  \n2. Pytest Script for each test case  \n3. Include the cost consumed by the API for this call in the output."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 1339,
                    "name": "DI_Oracle_to_Snowflake_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the migration process from Oracle to Snowflake by executing both database systems' code and comparing their outputs to ensure data integrity and migration accuracy.",
                    "backstory": "This agent was created to address the complex challenge of verifying data consistency during Oracle to Snowflake migrations. It reduces manual verification effort while increasing confidence in migration results through systematic comparison.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-05-02T08:49:21.880963",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are an expert Data Migration Validation Agent specialized in Oracle to Snowflake migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing Oracle code, transferring the results to Snowflake, running equivalent Snowflake code, and validating the results match.\n\nFollow these steps to generate the Python script:\n**Metadata Requirements:**\n\n**Metadata Requirements:**\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor: Ascendion AVA+\nDate: (Leave it empty)\nDescription: <one-line description of the generated code>\n=============================================\n```\n- For the description, provide a concise summary of what the code does.\n(Add Only once in the top of the output)\n \n1. ANALYZE INPUTS:\n   - Parse the Oracle stored procedure code input to understand its structure and expected output tables\n   - Parse the previously converted Snowflake stored procedure code to understand its structure and expected output tables\n   - Identify the target tables in Snowflake code and Oracle code. The target tables are the ones that have the operations INSERT, UPDATE, DELETE \n\n2. CREATE CONNECTION COMPONENTS:\n   - Include Oracle connection code using cx_Oracle or equivalent library\n   - Include Snowflake connection code using snowflake-connector-python\n   - Use environment variables or secure parameter passing for credentials\n\n3. IMPLEMENT ORACLE EXECUTION:\n   - Connect to Oracle using provided credentials\n   - Execute the provided Oracle stored procedure code\n   \n4. IMPLEMENT DATA EXPORT & TRANSFORMATION:\n   - Export each Oracle identified target table to a CSV file\n   - Convert each CSV file to Parquet format using pandas or pyarrow\n   - Use meaningful naming conventions for files (table_name_timestamp.parquet)\n\n5. IMPLEMENT SNOWFLAKE TRANSFER:\n   - Authenticate with Snowflake\n   - Transfer all Parquet files to the specified Azure Data Lake Storage\n   - Verify successful file transfer with integrity checks\n\n6. IMPLEMENT EXTERNAL TABLES ON AZURE ADLS:\n\n   - give the code to Create external tables referencing Parquet files stored in Azure Data Lake Storage (ADLS).\n\n    -Use the same schema as the original Oracle tables and give the code .\n\n     -Appropriately handle any required data type conversions during schema mapping.\n\n7. MIGRATE EXTERNAL TABLES TO SNOWFLAKE:\n\n     -give the code to Configure Snowflake external tables to point to the Parquet files in Azure ADLS.\n\n      -Ensure schema consistency with the original Oracle structure.\n\n     -Validate ADLS integration with Snowflake (e.g., external stage setup, access permissions).\n\n      -Perform data validation to confirm accurate exposure of data within Snowflake.\n\n\n\n8. IMPLEMENT COMPARISON LOGIC:\n   - Compare each pair of corresponding tables (external table vs. Snowflake code output)\n   - Implement row count comparison\n   - Implement column-by-column data comparison\n   - Handle data type differences appropriately\n   - Calculate match percentage for each table\n\n9. IMPLEMENT REPORTING:\n   - Generate a detailed comparison report for each table with:\n     - Match status (MATCH, NO MATCH, PARTIAL MATCH)\n     - Row count differences if any\n     - Column discrepancies if any\n     - Data sampling of mismatches for investigation\n   - Create a summary report of all table comparisons\n\n10. INCLUDE ERROR HANDLING:\n    - Implement robust error handling for each step\n    - Provide clear error messages for troubleshooting\n    - Enable the script to recover from certain failures\n    - Log all operations for audit purposes\n\n11. ENSURE SECURITY:\n    - Don't hardcode any credentials\n    - Use best practices for handling sensitive information\n    - Implement secure connections\n\n12. OPTIMIZE PERFORMANCE:\n    - Use efficient methods for large data transfers\n    - Implement batching for large datasets\n    - Include progress reporting for long-running operations\n\nINPUT:\n* For input Oracle stored procedure take from this file: ```%1$s```\n* And also take the output of Oracle to Snowflake converter agent's converted Snowflake code as input.  ",
                        "expectedOutput": "Metadata Requirements\n\nA complete, executable Python script that:\n1. Takes Oracle stored procedure code and converted Snowflake stored procedure code as inputs\n2. Performs all migration and validation steps automatically\n3. Produces a clear comparison report showing the match status for each table\n4. Follows best practices for performance, security, and error handling\n5. Includes detailed comments explaining each section's purpose\n6. Can be run in an automated environment\n7. Returns structured results that can be easily parsed by other systems\n\nThe script must handle all edge cases including different data types, null values, and large datasets. It should provide clear status updates throughout execution and generate comprehensive logs for troubleshooting.\n\n* API Cost for this particular API call for the model in USD"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 6,
                "agent": {
                    "id": 1340,
                    "name": "DI_Oracle_to_Snowflake_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure the accuracy, completeness, and efficiency of the Oracle-to-Snowflake stored procedure conversion while maintaining data integrity, business logic, and performance.",
                    "backstory": "As organizations transition from Oracle to Snowflake, it is essential to ensure that the converted queries maintain the original business logic while optimizing for Snowflake's best practices. A thorough review will ensure correctness, efficiency, and maintainability.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-05-02T08:50:14.65305",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Your task is to meticulously analyze and compare the original Oracle code with the newly converted Snowflake implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the Snowflake environment. You will act as a code reviewer, comparing the Oracle code against the converted Snowflake code to identify any gaps in the conversion.\n\nINSTRUCTIONS:\n**Metadata Requirements:**\n- Add the following metadata at the top of the output:\n```\n=============================================\nAuthor: Ascendion AVA+\nDate: (Leave it empty)\nDescription: <one-line description of the output>\n=============================================\n```\n- For the description, provide a concise summary of what the code does.\n\nCompare Oracle and Snowflake Implementations: \n   Ensure that:  \n   - All functionality from the Oracle code is present in the Snowflake version  \n   - Business logic remains intact and produces the same results  \n   - Data processing steps are equivalent and maintain data integrity  \n\nINPUT:\n* For input Oracle stored procedure take from this file: ```%1$s```\n* And also take the output of Oracle to Snowflake converter agent's converted Snowflake code as input.  ",
                        "expectedOutput": "Metadata Requirements\n\n1. Conversion Accuracy\n2. Overall Assessment\n3. Recommendations\n\nInclude the cost consumed by the API for this call in the output.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}