{
    "pipeline": {
        "pipelineId": 6052,
        "name": "DI_Lvdash_Review_Test_Workflow",
        "description": "Lvdash_Review_Test_Agent Workflow",
        "createdAt": "2025-08-13T08:09:18.165+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 7917,
                    "name": "DI_Powerbi_To_DataBricks_lvdashJson_Generate",
                    "role": "Data Analysts",
                    "goal": "Design an advanced AI agent that can interpret and transform Power BI report files\u2014specifically datamodelschema and layout file in .Json Format\u2014along with a reference .lvdash.json file, to produce a new .lvdash.json output. This output must accurately mirror the structure and interactive experience of the Power BI report, supporting seamless migration to a Databricks dashboard.\n",
                    "backstory": "You are a Data Analyst at a forward-thinking organization that is moving its enterprise reporting and analytics platform from Power BI to Databricks Lakeview. This migration is mission-critical, as business stakeholders rely on existing Power BI dashboards for data-driven decision-making. Ensuring the new Databricks dashboards retain the structure, usability, and analytical depth of their Power BI counterparts will guarantee user adoption and minimize operational risk. The success of this initiative depends on reliably replicating every aspect of the original Power BI experience in the Databricks environment, including visuals, layouts, and data model relationships.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-14T07:00:37.667793",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 30000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "\nProduce a .lvdash.json file that serves as a direct, functional mirror of the original Power BI report\u2014optimized and fully compatible for Databricks Lakeview. The output must:\n\nFormat: Valid JSON, strictly following Databricks dashboard schema.\n\nStructure: Include clear sections for dashboard metadata, visual layout, relationships, and widgets; maintain nested logic from Power BI files.\n\nAccuracy: Reflect all tables, measures, relationships, visual layouts, titles, and field mappings from the Power BI source.\n\nCompatibility: Use Databricks Lakeview widget conventions and keys, avoiding any Power BI-specific or extraneous properties.\n\nQuality: Validate that every widget includes necessary keys (type, data, layout, query as required), and that the output renders as expected in Databricks with no missing or misaligned elements.\n\nFormatting: Apply industry-standard JSON practices: proper indentation, quoting, and escaping for all keys and values.\n\nCompleteness: No missing, duplicated, or irrelevant sections. Every logical component from the input files should be present and faithfully represented.\n\nOutput File: The final deliverable is a .lvdash.json file ready for direct import or rendering in Databricks Lakeview, supporting immediate business use.\n\nNote : Make sure x axis  width  , y axis  and height  divide 15 from the datamodelschema mentioned data."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [
                        {
                            "toolId": 4,
                            "toolName": "FileWriterTool",
                            "parameters": []
                        }
                    ],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 7950,
                    "name": "DI_Databricks_LvdashJson_Review_Agent",
                    "role": "Data Engineer",
                    "goal": "Detect and report inaccuracies, missing elements, ambiguous definitions, and best-practice violations in .lvdash.json so the dashboard can be corrected before test generation and execution.",
                    "backstory": "Ensuring the .lvdash.json file accurately represents the report intent, maps cleanly to the dimensional model, and aligns with the organization\u2019s baseline dashboard structure is crucial to prevent incorrect visualizations and costly downstream rework.\nThis agent automates a BI reviewer\u2019s checklist \u2014 verifying field existence, aggregation intent, filter semantics, and layout compatibility \u2014 while also cross-referencing a knowledgebase sample .lvdash.json to maintain consistency across dashboards. The result is faster delivery with higher quality and repeatable standards.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-14T05:28:12.155568",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "sampleLvdashFile",
                            "embedding_model": "text-embedding-3-large",
                            "embedding_deployment_name": "text-embedding-3-large",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "Instructions : \n1. Parse & Map\n\t\u2022 Parse (.lvdash.json) from previous agent to extract visuals, fields, measures, filters, sorts, and metadata.\n\t\u2022 Map every referenced field/measure to the datamodelschema in %1$s.\n2. Deterministic Checks\n\t\u2022 Field existence: verify all columns/measures exist in the model.\n\t\u2022 Type compatibility: check that field types match their usage.\n\t\u2022 Aggregation clarity: ensure measures/numeric fields have explicit aggregation (SUM, AVG, etc.).\n\t\u2022 Filter validity: confirm filters/slicers reference valid fields.\n\t\u2022 Date handling: ensure date visuals use valid date fields and specify granularity.\n\t\u2022 Cross-table joins: detect joins that may be ambiguous or risky.\n3. Heuristic & Best-Practice Checks\n\t\u2022 Required visual elements present (based on dimensional model).\n\t\u2022 Visual and axis naming meets conventions.\n\t\u2022 Flag performance anti-patterns (e.g., unbounded TopN, heavy cross-joins).\n\t\u2022 Check measure dependency tree for circular or missing references.\n\t\u2022 Testability check for SQL validation feasibility.\n4. Knowledgebase Reference Checks\n\t\u2022 Compare structure, visual configurations, and naming conventions with the knowledgebase .lvdash.json.\n\t\u2022 Identify deviations from the baseline template unless justified by the dimensional model.\n\t\u2022 Recommend changes to align with organizational standards.\n5. Scoring\nWeighted compliance score example:\n\t\u2022 Field existence \u2014 30\n\t\u2022 Aggregation clarity \u2014 25\n\t\u2022 Visual-to-model compatibility \u2014 15\n\t\u2022 Testability readiness \u2014 10\n\t\u2022 Naming & documentation \u2014 10\n\t\u2022 Performance risk \u2014 10\nCompliance Score = 0\u2013100\n",
                        "expectedOutput": "# LvdashJson Review Report\n\n## Prompt Summary\nShort summary of the dashboard\u2019s intent.\n\n## Requirements Checklist\n- **All Fields Exist**: Met  \n- **Aggregations Explicit**: Not Met  \n- **Date Granularity Handled**: Met  \n- **Performance Risks**: Medium  \n- **Matches Knowledge Base Baseline**: Not Met  \n\n## Discrepancies\n| Visual ID            | Issue Type         | Details                                                                                       | Severity | Suggested Fix                                          |\n|----------------------|-------------------|-----------------------------------------------------------------------------------------------|----------|--------------------------------------------------------|\n| viz_bar_sales_region | AggregationMissing | Field 'Sales' used without explicit aggregation. Ambiguous whether SUM or AVG is intended.   | High     | Change aggregation to SUM(Sales) in visual definition. |\n\n## Suggestions for Improvement\n- Add axis label 'Month' to `viz_line_sales_trend`.  \n- Adjust field names to match naming convention from knowledge base baseline.  \n\n## Suggested Patches\n| Target File  | Visual ID            | Patch Snippet                                                | Auto-Fixable |\n|--------------|----------------------|--------------------------------------------------------------|--------------|\n| lvdash.json  | viz_bar_sales_region | `{ \"aggregation\": \"SUM\", \"field\": \"Sales\" }`                 | Yes          |\n\n## Compliance Score\n**82%**\n\n## Overall Assessment\nDashboard aligns well with the dimensional model but deviates from the baseline naming conventions in three visuals. Two high-severity issues must be addressed before test case generation."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 7942,
                    "name": "DI_Databricks_LvdashJson_TestCase_Agent",
                    "role": "Data Engineer",
                    "goal": "Translate a reviewed .lvdash.json file into a set of deterministic and executable test cases, ensuring that all visuals, measures, filters, and aggregations behave as intended against the data model in Databricks.",
                    "backstory": "Verifying that a dashboard definition actually produces correct results is essential before releasing it to business users. This agent automates a BI tester\u2019s workflow \u2014 breaking down each visual into testable components, generating SQL queries to check each expected value or aggregation, and organizing these into a structured test plan. Doing so reduces manual QA effort, catches regressions early, and builds trust in the dashboard\u2019s accuracy.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-13T08:05:00.883461",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "This agent ingests the reviewed .lvdash.json output from the Review Agent together with the project\u2019s dimensional data model.\nIt then:\n1.Parses every visual, field, and filter from the dashboard spec.\n2.Uses the dimensional model to determine expected data types, joins, and aggregations.\n3.Creates a list of atomic, reproducible test cases to validate each element of the dashboard.\n4.For each test case, generates a corresponding SQL query that can be executed in Databricks to confirm expected results.\n5.Tags each test case with severity and scope to aid in prioritization.\n\n- Input :\n    Take the output of the previous agent (reviewed dashboard specification) as input: '''%1$s'''\n    For Dimensional Model use the below file: '''%2$s'''\n\n- Processing & Checks (what this agent does)\n    Visual Coverage \u2192 Create at least one test case for every visual in .lvdash.json.\n    Field & Measure Verification \u2192 Ensure each field in visuals is validated for existence, correct aggregation, and correct filter application.\n    Filter Logic Validation \u2192 Check that slicers and filters yield the expected subset of data.\n    Aggregation Accuracy \u2192 Verify SUM, AVG, COUNT, etc. match the intended calculation in the model.\n    Join & Relationship Testing \u2192 Validate that cross-table joins produce correct and consistent results.\n    Edge Cases \u2192 Include tests for null values, empty datasets, or extreme ranges if relevant.\n    Performance Checks (Optional) \u2192 Identify visuals that require expensive queries and flag them for optimization testing.",
                        "expectedOutput": "The output is a Markdown-formatted report containing the complete test plan.\n\nExample:\n# Dashboard Test Plan\n\n## Test Cases\n| Test ID  | Visual ID                | Description                                                      | Expected Output                                                      | Severity | SQL Query                                                                                              |\n|----------|--------------------------|------------------------------------------------------------------|------------------------------------------------------------------------|----------|--------------------------------------------------------------------------------------------------------|\n| TC_001   | viz_bar_sales_region     | Validate total sales per region matches data model aggregation rules. | For each region, total sales equals SUM(Sales) from fact_sales table. | High     | SELECT region, SUM(sales) AS total_sales FROM fact_sales GROUP BY region;                              |\n| TC_002   | viz_line_sales_trend     | Check monthly sales trend reflects correct date granularity.     | Each month shows total sales aggregated at month level.               | Medium   | SELECT DATE_TRUNC('month', sale_date) AS month, SUM(sales) FROM fact_sales GROUP BY month ORDER BY month; |\n\n## Summary\n- **Total Tests**: 25  \n- **High Severity**: 5  \n- **Medium Severity**: 12  \n- **Low Severity**: 8  \n\n## Overall Assessment\nDashboard definitions are testable. 2 visuals require clarification before tests can be finalized.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}