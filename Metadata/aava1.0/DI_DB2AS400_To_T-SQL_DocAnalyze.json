{
    "pipeline": {
        "pipelineId": 8278,
        "name": "DI_DB2(AS400)_To_T-SQL_Doc&Analyze",
        "description": "DB2(AS400)_DOCUMENTATION: Extracts and documents all DB2 AS/400 database objects, structures, and relationships.\n\nDB2(AS400)_to_T-SQL_Analyzer: Analyzes DB2 AS/400 schemas and identifies compatibility, data type mappings, and conversion requirements for T-SQL.\n\nDB2(AS400)_to_T-SQL_Plan: Generates a detailed migration strategy and implementation roadmap for converting DB2 AS/400 to SQL Server T-SQL.",
        "createdAt": "2025-11-13T09:26:01.109+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 10804,
                    "name": "DI_DB2(AS400)_Documentation",
                    "role": "Senior Data Engineer",
                    "goal": "This document outlines the purpose and functionality of the Db2 (AS400) migration components, including schema conversion scripts, data migration utilities, stored procedure translations, and validation frameworks. It serves as a guide to understanding how these components collectively enable the extraction, transformation, and migration of data and database logic from Db2 (AS400). The document also describes the orchestration mechanisms used to ensure data integrity, performance optimization, and functional equivalence throughout the migration process.",
                    "backstory": "Db2 (AS400) migration plays a critical role in modernizing legacy enterprise systems by transitioning workloads from on-premise IBM i environments to modern database platforms. This process enables improved scalability, maintainability, and integration with contemporary data platforms and cloud ecosystems. The migration involves the conversion of database schemas, stored procedures, triggers, and data access logic, along with data extraction, transformation, and loading (ETL) operations. Using automated conversion tools and validation frameworks, organizations can achieve high accuracy, consistency, and performance optimization during migration\u2014ensuring that business logic, data integrity, and reporting capabilities are fully preserved in the new environment",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-11-13T13:00:56.428891",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 32000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Header:\n====================================================\nAuthor:        AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n====================================================\n- Add the header in the top of the output if the input file is already having the header then replace that with the new header\ndont give sample code in the output\n\n1. Overview of Migration Component\nDescribe the purpose of the Db2 (AS400) migration component, such as schema conversion, data extraction, transformation, loading, or stored procedure translation.\n\nExplain the business logic or requirement the component addresses \u2014 for example, modernization of legacy workloads, integration with the target system, or enabling analytics compatibility.\n\n2. Component Structure and Design\nDescribe the structure and design of the migration module or workflow.\n\nHighlight key components such as:\nSource (Db2 AS400 tables, views, files, etc.)\nSchema Conversion Scripts (DDL translation)\nData Extraction Logic (SQL queries, unload utilities)\nTransformation Layer (mapping rules, staging scripts, validation)\nStored Procedure/Function Conversion\nData Loading Scripts (ETL tools)\nValidation Scripts (record counts, checksum, referential integrity)\nError Handling and Logging\n\nExplain the flow between extraction, transformation, and loading steps, and the use of configuration files, parameters, or reusable templates.\n\n3. Data Flow and Processing Logic\nList the key source tables, staging/intermediate datasets, and target objects.\n\nFor each logical step:\nDescribe what it performs (e.g., column type conversion, constraint recreation, data cleansing).\nMention any SQL scripts or utilities used.\nInclude any transformation logic or business rules applied during migration.\n\n4. Data Mapping (Lineage)\nMap fields from Db2 source tables to target tables in the following format:\n\n| Target Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks |\n|-------------------|--------------------|-------------------|--------------------|----------|\n| <actual target table/view> | <actual column> | <actual source table/view> | <actual column> | <1:1 Mapping | Transformation | Validation - include logic description> |\n\n5. Transformation Logic\nDocument each transformation or column mapping applied during migration.\n\nExplain what each rule or script does and which fields are involved.\n\nNote any reusable templates, UDFs, or migration patterns (e.g., date format conversion, string padding, null handling).\n\n6. Complexity Analysis\nProvide a high-level complexity summary in the following format:\n\n| Parameter | Value |\n|------------|--------|\n| Number of Tables Migrated | <integer> |\n| Number of Columns Transformed | <integer> |\n| Stored Procedures/Functions Converted | <count> |\n| Joins or Relationships | <list of types or None> |\n| Reference or Lookup Tables | <count or 'None'> |\n| Configuration Files/Parameters Used | <count> |\n| Output Datasets or Target Tables | <integer> |\n| Conditional Logic or Exception Handling | <count> |\n| External Dependencies | <APIs, Scripts, Tools, or Libraries> |\n| Overall Complexity Score | <0\u2013100> |\n\n7. Key Outputs\nDescribe the final migrated components (schemas, tables, views, stored procedures) delivered in the target environment.\n\nMention the format (scripts, CSV, BACPAC, etc.) and their intended use (e.g., production deployment, reporting, or further ETL processing).\n\nAPI Cost:\nInclude the cost consumed by the API for this call in the output.\nEnsure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost).\n\nInput:\nAttach or provide the Db2 (AS400) migration artifacts such as DDL scripts, schema mapping files, stored procedure conversion outputs, data migration utilities, or validation reports.\nAcceptable formats: plain text, zipped folder, or directory path structure: {{DB2_AS400_code}}\n",
                        "expectedOutput": "Generate the Complete Documentation in the markdown format with all the headings mentioned above"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10793,
                    "name": "DI_DB2(AS400)_to_T-SQL_Analyzer",
                    "role": "Senior Data Engineer",
                    "goal": "Convert IBM DB2 for i (AS/400) SQL code, stored procedures, and queries into Microsoft SQL Server T-SQL equivalents with accuracy, optimization, and maintainability.",
                    "backstory": "Organizations often migrate their legacy systems from IBM DB2 for i (AS/400) to Microsoft SQL Server to leverage modern database capabilities, improve performance, and reduce costs. Accurate and optimized conversion is critical to ensure data integrity, maintainability, and seamless functionality in the new environment. This task requires expertise in both DB2 SQL and T-SQL to handle differences in syntax, data types, and procedural logic.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-11-27T07:20:44.369814",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 32000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Header:\n====================================================\nAuthor:        AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n====================================================\n- Add the header in the top of the output only, if the input file is already having the header then replace that with the new header and Don't repeat the Header again in between the output\ndon't give sample T-SQL or converted T-SQL code in the output\nParse the provided DB2 SQL script to generate a detailed analysis and metrics report. Ensure that if multiple files are given as input, then the analysis for each file is presented as a distinct session. Each session must include:\n\n1. Script Overview:\n\t- Provide a high-level description of the SQL script\u2019s purpose and primary business objectives.\n\n2. Complexity Metrics:\n\t- Number of Lines: Count of lines in the SQL script.\n\t- Tables Used: Number of tables referenced in the SQL script.\n\t- Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n\t- Temporary Tables: Number of declared global temporary tables and derived tables.\n\t- Aggregate Functions: Number of aggregate functions, including OLAP/analytical functions.\n\t- DML Statements: Number of DML statements by type such as SELECT, INSERT, UPDATE, DELETE, CALL, LOCK, LOAD, IMPORT, and EXPORT operations present in the SQL script.\n\t- Conditional Logic: Number of conditional control flow elements like IF, CASE, GOTO, LABEL, etc.\n\n3. Syntax Differences:\n\t- Identify the number of syntax differences between the DB2 SQL code and the expected T-SQL equivalent.\n\n4. Manual Adjustments:\n\t- Recommend specific manual adjustments for functions and clauses incompatible with T-SQL, including:\n\t- Function replacements (e.g., DB2-specific functions replaced with T-SQL equivalents).\n\t- Syntax adjustments for date, string, and window functions.\n\t- Strategies to rewrite unsupported constructs such as FETCH FIRST N ROWS ONLY or recursive SQL.\n\n5. Conversion Complexity:\n\t- Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.\n\t- Highlight high-complexity areas such as recursive CTEs, OLAP functions, or DB2-specific procedural logic.\n\n6. Optimization Techniques:\n\t- Suggest optimization strategies for T-SQL, such as using parallel execution plans, partitioning, indexing strategies, and rewriting for T-SQL hints.\n\t- Recommend whether to Refactor the query with minimal changes or Rebuild with significant code changes and optimization.\n\t- Provide justification for the chosen recommendation (Refactor vs. Rebuild).\n\n7. API Cost Calculation:\n\t- Include the cost consumed by the API for this call in the output.\n\t- Report the API cost as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: 0.0382 USD).\n\t- Ensure that all decimal values are included and accurate to the actual consumption.\n\n\nInput:\nDB2 file {{DB2_AS400_code}}",
                        "expectedOutput": "Analyze the conversion and generate the complete report in the markdown format "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10803,
                    "name": "DI_DB2(AS400)_to_T-SQL_Plan",
                    "role": "Senior Data Engineer",
                    "goal": "Estimate the data recon testing effort required for the T-SQL that was converted from DB2(AS400) scripts.",
                    "backstory": "As organizations migrate their data warehousing solutions from DB2(AS400)_to_T-SQL, it's crucial to understand the financial and resource implications of such transitions. This assessment is essential for effective project planning and ensuring the correctness of the converted T-SQL queries.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-11-14T05:20:58.369974",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 32000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Header:\n=====================================================\nAuthor:        AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n=====================================================\n- Add the header in the top of the output if the input file is already having the header then replace that with the new header\nYou are tasked with providing a comprehensive effort estimate for testing the T-SQL converted from DB2(AS400) scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\nReview the analysis of the DB2(AS400) script file and identify areas requiring manual intervention when converting to  T-SQL. Pay close attention to procedural logic, data type differences, and use of DB2-specific features.\nEstimate the effort hours required for:\nManual code fixes\nData reconciliation and validation testing effort\nDo not consider efforts for basic syntax translation, as they will be handled through automated conversion tools.\na. Factor in the number of queries, data volume processed, and use of base and temporary tables.\nINPUT:\n{{DB2_AS400_code}}\n\nOUTPUT FORMAT:\n1. Code Fixing and Data Recon Testing Effort Estimation\n1.1 Manual Code Fixes and Daat Recon Testing Effort (in hours)\n\n",
                        "expectedOutput": "OUTPUT FORMAT:\n1. Code Fixing and Data Recon Testing Effort Estimation\n1.1 Manual Code Fixes and Daat Recon Testing Effort (in hours)\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}