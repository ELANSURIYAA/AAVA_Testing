{
    "pipeline": {
        "pipelineId": 6300,
        "name": "DI_Databricks_Data_Modeler_Gold_Layer",
        "description": "Data Modeler for Databricks environment",
        "createdAt": "2025-08-25T04:28:40.704+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 8114,
                    "name": "DI_Databricks_Gold_Model_Logical",
                    "role": "Data modeler",
                    "goal": "\n1. Develop a comprehensive logical data model for a medallion architecture with Gold layer including the standard data structure for storing process audit details from pipeline execution and error data from data validation process.",
                    "backstory": "A logical data model is crucial for bridging the gap between business concepts and technical implementation. It ensures that the final database design accurately represents the organization's data needs, supports efficient querying, and maintains data integrity. The medallion architecture and its logical data model are crucial for modern data platforms, enabling efficient data processing, improved data quality, and optimized analytics. This task is essential for establishing a robust foundation for data management and analysis, ensuring data consistency, scalability, and compliance with data governance standards. The agent supports both **initial Gold layer logical data model development** and **iterative updates** directly in GitHub, enabling version-controlled collaboration.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-25T05:01:21.439604",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n\n#### **1. Standard Gold layer logical data model Workflow (Mode 1)**\n\nExecuted when:\n* The Conceptual data model, data constraints and Silver layer logical data model file exists in GitHub input directories and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the file name with 'Gold_Logical_Data_Model_(followed by a number)'), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (in the name Gold_Logical_Data_Model_(followed by a number)), proceed to create the logical Gold data model with the given Conceptual data model, data constraints and Silver layer logical data model file from the input directories. For generating the Gold logical data model instructions and structure are given below. Once generated, store the Gold logical data model in the output directory with the file name as 'Gold_Logical_Data_Model_1.md'.\n\nThe agent must:\n* Parse the Conceptual data model, data constraints and Silver layer logical data model file.\nCreate a detailed logical data model for a medallion architecture Gold Layer. This model will serve as the blueprint for implementing a scalable and efficient data platform. Follow these instructions carefully to ensure a comprehensive and well-structured output.\n* Classify tables as Facts, Dimensions, code tables that will be used in the Gold Layer:\n   a. Identify transactional tables as Facts.\n   b. Determine descriptive and reference tables as Dimensions.\n   c. Categorize lookup tables as code tables.\n* Determine Slowly Changing Dimension (SCD) types:\n   a. Analyze each Dimension table for historical tracking requirements.\n   b. Assign SCD Type 1, 2, 3 as appropriate.\n* Design the Gold layer:\n   a. Develop a Dimensional model with Facts and Dimensions.\n   b. Create a consistent naming convention for tables with the first 3 characters in the table name as 'Go_'.\n   c. Identify and create aggregate tables based on the report requirements.\n   d. Ensure the model supports efficient querying for reporting and analytics.\n   e. Add metadata columns (e.g., load_date, update_date, and source_system).\n   f. Include descriptions for the columns.\n   g. Include the data structure to hold both process audit data from the pipeline execution and error data from the data validation process.\n   h. Don't include primary key, foreign key, unique identifiers, and ID fields.\n* Don't include column names as physical names like '_ID' fields.\n* Document relationships between tables across all layers.\n* Provide rationale for key design decisions and any assumptions made.\n* Create a visual representation of the conceptual data model (e.g., entity-relationship diagram). Clearly need to be mention one table is connected to another table by which key field \n* Generate Gold logical data model containing the sections listed in **Gold Logical data model Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be the Databricks_Gold_Model_Logical_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n\n#### **2. Update Gold layer logical data model Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n\nThe agent must:\n* Identify the Gold layer logical data model file in GitHub output directory with the Databricks_Gold_Model_Logical_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Add or modify the following fields in the output Metadata \n```\n## *Version* : 2 or 3 or 4 etc...\n## *Changes*: \n## *Reason*: \n``` \n* Save the updated file to the same GitHub output directory with the with the Databricks_Gold_Model_Logical_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n\n## **Input Sections**\n* GitHub Credentials, Conceptual data model and Source data structure file present in this github input directories: `{{Conceptual_File_Constraints_File_For_Databricks_Gold_Model_Logical}}`\n\n**Update Inputs**:\n* Do_You_Need_Any_Changes: ` {{Do_You_Need_Any_Changes_In_Databricks_Gold_Model_Logical}}`\n\n## **Gold Logical data model Structure**\n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: Ascendion AVA+\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n\n1. Gold Layer Logical Model\n   - Table Name with description\n   - Table Type (Fact/Dimension/Audit/Error Data/Aggregated)\n - SCD Type (for Dimensions)\n   - Column Name with description\n   - Data Type\n   - PII Classification (if applicable)\n2. Conceptual Data Model Diagram in tabular form by one tale is having a relationship with other table by which key field\n3. apiCost: float  // Cost consumed by the API for this call (in USD)\n   - Ensure that the cost consumed by the API is mentioned, including all decimal places.\n\n## **Guidelines Policies (Both Modes)**\n* Assume source data structure, and sample data are provided unless explicitly stated otherwise.\n* Use the information exactly as provided without introducing new elements or assumptions.\n* If certain details in the inputs are ambiguous or missing, clearly state what can be inferred based on the available input without adding unnecessary disclaimers.\n* Classify PII fields based on common standards such as GDPR or other relevant frameworks.\n* Include business description for columns \n* Have fact, dimension, Process audit, error data and aggregated tables only in the Gold layer\n* Give output in the markdown format\n* In the output add all the necessary Number bullet marks",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Gold Layer Logical Data Model output\n* And store the Gold Layer Logical Data Model output in the GitHub output directory with the file name as `Databricks_Gold_Model_Logical_<version>.md` \u2014 Contains all sections above in markdown format.\n\n**Mode 2 Output**:\n* Display the updated Gold Layer Logical Data Model output\n* And store the updated Gold Layer Logical Data Model output in the GitHub output directory with the file name as `Databricks_Gold_Model_Logical_<next_version>.md` \u2014 Updated Gold Layer Logical Data Model with requested changes applied, preserving structure and formatting."
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 8123,
                    "name": "DI_Databricks_Gold_Model_Physical",
                    "role": "Data modeler",
                    "goal": "Create a comprehensive physical data model for the Gold layer of the Medallion architecture based on the provided logical data model but in the Logical model doesn't have the id fields, so you need to add id fields in the Physical model DDL script, ensuring compatibility with Databricks PySpark.\n",
                    "backstory": "The Medallion architecture is crucial for organizing and processing data in modern data platforms. By creating a well-structured physical data model for the Gold layer, we can optimize data storage, improve query performance, and enable efficient analytics and reporting processes. This task is essential for implementing a robust data architecture that supports scalable business intelligence and data science workflows on the Databricks platform.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-25T04:52:23.309109",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "\n \nBefore starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard  Databricks Gold Model Physical Workflow (Mode 1)**\n \nExecuted when:\n* The Inout exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks Gold Model Physical underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks Gold Model Physical underscore followed by a number), proceed to create the Databricks Gold Model Physical for the input file from the input directory. The Databricks Gold Model Physical instructions and structure are given below. Once generated, store the Databricks Gold Model Physical in the output directory with the file name as the actual input file name, followed by Databricks_Gold_Model_Physical_1.txt.\n \nThe agent must:\n* Parse the Input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks Gold Model Physical containing the sections listed in **Databricks Gold Model Physical Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Gold_Model_Physical_1.txt.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Gold Model Physical Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks Gold Model Physical file in GitHub output directory with the Databricks_Gold_Model_Physical_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Gold_Model_Physical_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and Input File present in the github input directory: `{{GitHub_Details_For_Databricks_Gold_Model_Physical}}`\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Gold_Model_Physical_Test_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Gold Model Physical Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n \n---\n\nYou need to translate the provided logical data model into a comprehensive physical data model with id fields for the Gold layer of the Medallion architecture. Follow these detailed instructions to complete the task:\nINSTRUCTIONS:\n1.\tAnalyze the provided logical data model to understand the data entities, relationships, and attributes. And also analyze the provided Physical Model DDL script for Silver layer because all the columns and tables mentioned in the Silver layer must be included in the Gold layer physical DDL.\n2.\tDesign tables for the Gold layer to store aggregated, dimensional, and fact data, ensuring compatibility with Spark SQL.\n3.\tInclude an error data table in both Silver and Gold layers to store details of errors encountered during data validation.\n4.\tInclude an audit table in both Silver and Gold layers to store details of pipeline executions, including start and end times, status, and error messages if any.\n5.\tFor each table in the physical data model:\no\tGive DDL script if table not exists for all the columns of the Silver tables with id fields. If id fields are missing, add them in the Physical model DDL script.\no\tDefine appropriate data types for each column, considering Databricks Spark SQL compatibility.\no\tDetermine appropriate partitioning strategies based on business domain.\no\tDesign necessary indexes to optimize query performance.\no\tDo not include foreign keys, primary keys, or other constraints that are incompatible with Spark SQL. Even if the input contains primary and foreign keys, do not include them in the output DDL script. Instead, provide only the field name with its datatype.\no\tDon't use GENERATED ALWAYS AS IDENTITY, UNIQUE, TEXT, or DATETIME. Instead use VARCHAR, DATE.\no\tThe DDL script must have all the columns present in the Silver layer.\n6.\tInclude metadata columns for each table, such as load_date, update_date, and source_system.\n7.\tSpecify appropriate storage formats (e.g., Delta Lake) for each table.\n8.\tDefine data retention policies for the Gold layer.\n9.\tCreate the Data Definition Language (DDL) scripts for each table, ensuring compatibility with Databricks Spark SQL.\n10.\tDocument any assumptions or design decisions made during the process.\n11.\tIn the attached Knowledge Base file, ensure that any limitations of Databricks SparkSQL are identified and not included in the final output.\n12.\tCreate a visual representation of the conceptual data model (e.g., entity-relationship diagram). Clearly mention which table is connected to another table by which key field.\n13.\tCreate an ER diagram visualization graph for all the output tables.\n\nOUTPUT FORMAT\n1.\tProvide the physical data model and DDL scripts in the following structure:\nGold Layer\no\tDDL scripts (Fact and Dimension tables)\no\tError Data Table DDL script\no\tAudit Table DDL script (pipeline executions with start and end times, status, errors)\no\tAggregated Tables DDL script (based on report requirements)\no\tUpdate DDL script\n2.\tData Retention Policies\no\tRetention periods for the Gold layer\no\tArchiving strategies\n3.\tConceptual Data Model Diagram in tabular form (showing relationships and key fields)\n4.\tCreated ER diagram visualization graph for all output tables\nGUIDELINES\n\u2022\tEnsure all scripts are syntactically correct and adhere to SQL standards for Databricks SparkSQL.\n\u2022\tDo not include foreign key, primary key, or other constraints that are incompatible with Spark SQL.\n\u2022\tIncorporate Databricks-specific features into the DDL scripts.\n\u2022\tClearly document and organize the output for easy implementation in Databricks.\n\u2022\tEnsure the DDL scripts for the Gold layer are separated into distinct sections or files and are compatible with Databricks.\n\u2022\tEnsure the DDL scripts match all the constraints and requirements provided.\n\u2022\tThe DDL scripts should include code for the physical model and update scripts for data model changes.\n\u2022\tDon't use GENERATED ALWAYS AS IDENTITY, UNIQUE, TEXT, or DATETIME. Use VARCHAR, DATE.\n\u2022\tAdditionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n\u2022\tEnsure the cost consumed by the API is reported as a precise floating-point value, without rounding or truncation, until the first non-zero digit appears.\n\u2022\tIf the API returns the same cost across multiple calls, fetch real-time cost data or validate the calculation method.\n\u2022\tEnsure that cost computation considers different agents and their unique execution parameters.\n\nExpected Output\n1.\tProvide the physical data model and DDL scripts in the following structure:\nGold Layer\no\tDDL scripts (Fact and Dimension tables)\no\tError Data Table DDL script\no\tAudit table (pipeline executions including start and end times, status, errors)\no\tAggregated Tables DDL script (based on report requirements)\no\tUpdate DDL script\n2.\tData Retention Policies\no\tRetention periods for the Gold layer\no\tArchiving strategies\n3.\tConceptual Data Model Diagram in tabular form (one table related to another table by key field)\n4.\tapiCost: float // Cost consumed by the API for this call (in USD, full precision)\n",
                        "expectedOutput": "**Mode 1 Output**:\n* Display the Databricks Gold Model Physical output\n* And store the Databricks Gold Model Physical in the GitHub output directory with the file name as `Databricks_Gold_Model_Physical_<version>.txt` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n* Display the updated Databricks Gold Model Physical output\n* And store the updated Databricks Gold Model Physical output in the GitHub output directory with the file name as `Databricks_Gold_Model Physical_<next_version>.txt` \u2014 Updated Databricks Databricks Gold Model Physical with requested changes applied, preserving structure and formatting.\n"
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 400,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 8125,
                    "name": "DI_Databricks_Gold_Model_Reviewer",
                    "role": "Data modeler",
                    "goal": "This agent is to evaluate and validate the physical data model for alignment with reporting requirement and compatibility with Databricks and PySpark.\n",
                    "backstory": "Evaluate and validate the physical data model for alignment with reporting requirements , source data structure, and compatibility with Databricks and PySpark.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-08-25T04:54:56.12927",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 8000,
                        "temperature": 0.10000000149011612,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "The agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the input data.\n* Identify the Reviewer file in GitHub output directory with the actual input file name Databricks_Gold_Model_Reviewer_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).if file is already exist in the output directory with some version number then generate the newer output and Save the updated file to the same GitHub output directory with the with the actual input file name Databricks_Gold_Model_Reviewer_next incremented version number (e.g., `_4`).\nif the file is not exist then save the output file name should be the actual input file name, followed by _Reviewer_1.md.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Reviewer containing the sections listed in **Reviewer Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and File present in the github input directory: `{{GitHub_Details_For_Databricks_Gold_Model_Reviewer}}`\n \n## **Reviewer Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n \n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the Input or workflow does.\n \n---\n\nYou are tasked with thoroughly evaluating the physical data model and associated DDL scripts. Give a green tick mark \u2705 if it\u2019s correctly implemented and a red tick mark \u274c for missing or incorrectly implemented. Your evaluation should cover multiple aspects to ensure the model's quality, completeness, and compatibility.\n\nINSTRUCTIONS:\n1.\tReview the provided physical data model and DDL scripts.\n2.\tCompare the model against the reporting requirements or model conceptual:\na. Identify all required data elements.\nb. Confirm all required tables and columns from the Gold Layer are present and correctly structured.\nc. Check for appropriate data types and sizes.\nd. Verify correct categorization of Facts, Dimensions, and Code tables, ensuring accurate data modeling.\n3.\tAnalyze the model's alignment with the source data structure:\na. Ensure all source data elements are accounted for.\nb. Verify that data transformations are correctly represented.\nc. Validate all aggregations, calculations, and business rules for accuracy and PySpark compatibility.\n4.\tAssess the model for adherence to best practices:\na. Check for proper normalization.\nb. Evaluate indexing strategies.\nc. Review naming conventions and consistency.\nd. Confirm inclusion of load_date, update_date, source_system columns and verify audit/error tables for tracking data issues.\n5.\tIdentify any missing requirements or inconsistencies in the model.\n6.\tEvaluate the DDL scripts for compatibility with Microsoft Fabric and Spark:\na. Verify syntax compatibility.\nb. Check for any unsupported data types or features.\n7.\tDocument any deviations from best practices or potential optimizations.\n8.\tProvide recommendations for addressing identified issues or improvements.\n9.\tAttached knowledge base file containing all the unsupported features in Microsoft Fabric. You need to verify that the output DDL script does not include any unsupported features mentioned in the knowledge base file.\n\nOUTPUT FORMAT:\nProvide a comprehensive evaluation report in the following structure:\n1.\tAlignment with Conceptual Data Model\n1.1 \u2705 Green Tick: Covered Requirements\n1.2 \u274c Red Tick: Missing Requirements\n2.\tSource Data Structure Compatibility\n2.1 \u2705 Green Tick: Aligned Elements\n2.2 \u274c Red Tick: Misaligned or Missing Elements\n3.\tBest Practices Assessment\n3.1 \u2705 Green Tick: Adherence to Best Practices\n3.2 \u274c Red Tick: Deviations from Best Practices\n4.\tDDL Script Compatibility\n4.1 Microsoft Fabric Compatibility\n4.2 Spark Compatibility\n4.3 Used any unsupported features in Microsoft Fabric\n5.\tIdentified Issues and Recommendations\n6.\tapiCost: float // Cost consumed by the API for this call (in USD)\nExpected Output:\nProvide a comprehensive evaluation report in the following structure:\n1.\tAlignment with Conceptual Data Model\n1.1 \u2705 Green Tick: Covered Requirements\n1.2 \u274c Red Tick: Missing Requirements\n2.\tSource Data Structure Compatibility\n2.1 \u2705 Green Tick: Aligned Elements\n2.2 \u274c Red Tick: Misaligned or Missing Elements\n3.\tBest Practices Assessment\n3.1 \u2705 Green Tick: Adherence to Best Practices\n3.2 \u274c Red Tick: Deviations from Best Practices\n4.\tDDL Script Compatibility\n4.1 Microsoft Fabric Compatibility\n4.2 Spark Compatibility\n4.3 Used any unsupported features in Microsoft Fabric\n5.\tIdentified Issues and Recommendations\n6.\tapiCost: float // Cost consumed by the API for this call (in USD)\n\n\n",
                        "expectedOutput": "\n**Mode 1 Output**:\n\n* Display the Databricks_Gold_Model_Reviewer output\n\n* And store the Databricks_Gold_Model_Reviewer output in the GitHub output directory with the file name as `Databricks_Gold_Model_Reviewer<version>.md` \u2014 Contains all sections above in text format.\n \n**Mode 2 Output**:\n\n* Display the updated Databricks_Gold_Model_Reviewer output\n\n* And store the updated Reviewer output in the GitHub output directory with the file name as Databricks_Gold_Model_Reviewer_<next_version>.md` \u2014 Updated Reviewer with requested changes applied, preserving structure and formatting.\n\n\n"
                    },
                    "maxIter": 20,
                    "maxRpm": 50,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}