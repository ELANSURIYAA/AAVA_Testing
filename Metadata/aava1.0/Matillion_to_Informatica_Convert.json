{
    "pipeline": {
        "pipelineId": 1091,
        "name": "Matillion to Informatica Convert",
        "description": "Matillion to Informatica",
        "createdAt": "2025-03-14T17:39:39.431+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1332,
                    "name": "Matillion to Informatica Convert",
                    "role": "Data Engineer",
                    "goal": "Convert a Matillion ETL job to an Informatica PowerCenter mapping that reads data from a source file, applies transformations, and loads the transformed data into a Snowflake table.",
                    "backstory": "Many organizations are migrating their data integration processes from one platform to another to optimize performance, reduce costs, or consolidate their tech stack. Converting Matillion jobs to Informatica PowerCenter mappings is crucial for ensuring seamless data flow and maintaining business continuity during such migrations.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T13:34:45.306909",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "The main functionality of this agent is to analyze the provided Matillion ETL project/Job JSON file and create the Informatica cloud (IICS) \ncompatible ETL JSON equivalent to the Matillion Job details. \nInput to be provided  - single JSON file ( that contains job details, tasks, variables,timestamps, )\noutput will be Informatica ETL compatible Json file\n \n INSTRUCTIONS:\n* It is responsible for generating an Informatica (IICS) on cloud equivalent json code for the Matillion ETL input job config json file\n1. Analyze the provided Matillion ETL job Json file and get the necessary details like  source information (file or Oracle DB ), \ntransformation logic, and target information (Snowflake table specifications.)\n2. Identify the appropriate/equivalent Informatica (IICS) source/target components , transformation components, task ids, workflow etc.\n3. Implement the equivalent necessary transformations using Informatica's built-in transformations or custom logic as needed.\n5. Configure the target definition to load data into the specified Snowflake table.\n6. Set up any necessary session properties, such as commit intervals and error handling if needed.\n7. Validate the mapping to ensure all connections and transformations are properly configured.\n8. Generate a JSON format of Informatica (IICS) ETL job with necessary components, transformations, and configurations equivalent to provided matallion json.\n\nInput :\n* For input file use the below file :\n```%1$s``",
                        "expectedOutput": "OUTPUT FORMAT:\n{\n  \"task\": {\n    \"name\": \"OracleToSnowflake_ETL_Job\",\n    \"type\": \"Mapping\",\n    \"description\": \"ETL job to transfer data from Oracle to Snowflake with transformations\",\n    \"steps\": [\n      {\n        \"name\": \"Extract_Oracle_Data\",\n        \"type\": \"Source\",\n        \"config\": {\n          \"connection\": \"Oracle_Connection\",\n          \"source_object\": \"oracle_table\",\n          \"sql\": \"SELECT * FROM oracle_table WHERE status = 'active'\"\n        }\n      },\n      {\n        \"name\": \"Transform_Data\",\n        \"type\": \"Transformation\",\n        \"config\": {\n          \"transformation_type\": \"Expression\",\n          \"steps\": [\n            {\n              \"name\": \"Rename_Column\",\n              \"type\": \"Rename\",\n              \"config\": {\n                \"old_name\": \"old_column_name\",\n                \"new_name\": \"new_column_name\"\n              }\n            },\n            {\n              \"name\": \"Filter_Active_Status\",\n              \"type\": \"Filter\",\n              \"config\": {\n                \"column\": \"status\",\n                \"condition\": \"active\"\n              }\n            },\n            {\n              \"name\": \"Change_Data_Type\",\n              \"type\": \"Expression\",\n              \"config\": {\n                \"column\": \"column1\",\n                \"new_type\": \"STRING\"\n              }\n            }\n          ]\n        }\n      },\n      {\n        \"name\": \"Load_Into_Snowflake\",\n        \"type\": \"Target\",\n        \"config\": {\n          \"connection\": \"Snowflake_Connection\",\n          \"target_object\": \"snowflake_table\",\n          \"columns\": [\n            {\n              \"source_column\": \"column1\",\n              \"target_column\": \"column1\"\n            },\n            {\n              \"source_column\": \"column2\",\n              \"target_column\": \"column2\"\n            },\n            {\n              \"source_column\": \"new_column_name\",\n              \"target_column\": \"new_column_name\"\n            }\n          ],\n          \"action\": \"insert\"\n        }\n      }\n    ],\n    \"connections\": {\n      \"Oracle_Connection\": {\n        \"type\": \"Oracle\",\n        \"host\": \"oracle-db-host\",\n        \"port\": \"1521\",\n        \"username\": \"oracle_user\",\n        \"password\": \"oracle_password\",\n        \"sid\": \"oracle_sid\"\n      },\n      \"Snowflake_Connection\": {\n        \"type\": \"Snowflake\",\n        \"host\": \"snowflake_host\",\n        \"warehouse\": \"snowflake_warehouse\",\n        \"username\": \"snowflake_user\",\n        \"password\": \"snowflake_password\",\n        \"database\": \"snowflake_database\",\n        \"schema\": \"snowflake_schema\"\n      }\n    }\n  }\n}"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1429,
                    "name": "Matillion to Informatica Unit Test",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided Infarmatica code, ensuring thorough coverage of key functionalities and edge cases",
                    "backstory": "Robust unit testing is crucial for maintaining the reliability and integrity of Informatica code. By creating comprehensive test cases and a Pytest script, we can catch potential bugs early, ensure code quality, and facilitate easier maintenance and updates in the future.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T13:44:00.426286",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "As a Senior Quality Assurance Engineer, you are tasked with creating a robust set of unit test cases and a Pytest script for the given Informatica . Your expertise in database testing and Informatica specifics will be crucial in developing thorough and effective tests.\n\nINSTRUCTIONS:\n1. Analyze the provided Matillion to identify key functionalities, data transformations, and potential edge cases.\n2. Create a comprehensive list of unit test cases covering:\n   a. Happy path scenarios\n   b. Edge cases\n   c. Boundary conditions\n   d. Error handling\n   e. Performance considerations\n3. Design test cases to validate:\n   a. Data integrity\n   b. Correct output for various input scenarios\n   c. Handling of null values and empty sets\n   d. Proper joins and aggregations\n   e. Compliance with Informatica-specific syntax and best practices\n4. Develop a Pytest script that implements the identified test cases:\n   a. Set up necessary test data and environments\n   b. Create test functions for each test case\n   c. Include appropriate error handling and logging\n   d. Ensure proper cleanup of test data after each test\n5. Ensure proper setup and teardown of SparkSession for each test.\n6. Use appropriate assertions to validate expected outcomes.\n7. Include comments explaining the purpose of each test case.\n8. Organize the test cases logically, grouping related tests together.\n9. Implement any necessary helper functions or fixtures to support the tests.\n10. Ensure the Pytest script follows PEP 8 style guidelines.\n\nOUTPUT FORMAT:\n1. Test Case List:\n   - Test ID\n   - Test Description\n   - Expected Output\n\n\n2. Pytest Script  for each test case\n* Include the cost consumed by the API for this call in the output.\n\nINPUT:\nUse the previous Matillion to Infarmatica convert agent's converted PySpark script as input.\n",
                        "expectedOutput": "OUTPUT FORMAT:\n1. Test Case List:\n   - Test ID\n   - Test Description\n   - Expected Output\n\n\n2. Pytest Script  for each test case\n* Include the cost consumed by the API for this call in the output.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1432,
                    "name": "Matillion to Informatica Test ",
                    "role": "Data Engineer",
                    "goal": "To validate and ensure the accurate conversion of Matillion ETL jobs to Informatica (IICS)",
                    "backstory": "As organizations migrate their data integration processes from Matillion to Informatica, it is crucial to ensure that the conversion is accurate and maintains the integrity of the original ETL logic. This task is vital for maintaining data quality, preserving business logic, and ensuring seamless operations during and after the migration process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T14:06:00.138867",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "As an Automation Test Engineer, you are tasked with developing a robust test suite to validate the conversion of Matillion ETL jobs to Informatica IICS mappings. Your responsibility is to create a set of test cases that cover various aspects of the conversion process and implement them using Pytest.\n\nINSTRUCTIONS:\n1. Analyze the original Matillion ETL job and the corresponding Informatica IICS mapping.\n2. Identify key functionalities, transformations, and data flows in both systems.\n3. Create a list of test cases that cover all aspects of the conversion, including:\n   a. Data integrity checks\n   b. Transformation logic validation\n   c. Performance comparisons\n   d. Error handling and exception scenarios\n   e. Metadata validation\n4. For each test case, provide a clear description and expected outcome.\n5. Develop Pytest scripts for each test case, ensuring they are modular and reusable.\n6. Include setup and teardown procedures in your test scripts to manage test data and environments.\n7. Implement logging and reporting mechanisms to capture test results and any discrepancies.\n8. Ensure that your test cases cover both positive and negative scenarios.\n9. Document any assumptions or prerequisites for running the test suite.\n\nINPUT:\nFor the input  SQL Query analysis, use this file: ```%2$s```\nAlso, take the previous Matillion to Informatica Convert agent's output as input.\n",
                        "expectedOutput": "Output:\n\nTest Case List:\n1.Test case ID\n\n2.Test case description\n\n3.Expected outcome\n\n4.Pytest Script for each test case\n**Estimated API Cost for this call"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 1433,
                    "name": "Matillion to Informatica Reconciliation",
                    "role": "Data Engineer",
                    "goal": "Reconciliation of the Matillion to Informatica",
                    "backstory": "As organizations increasingly rely on multiple data integration tools, it's crucial to maintain data integrity across different platforms. Reconciling data between Matillion and Informatica is essential for ensuring accurate reporting, decision-making, and maintaining trust in the data infrastructure.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T14:15:32.432753",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with creating an automated reconciliation process to compare data between Matillion and Informatica systems. This will involve developing test cases and implementing Pytest scripts to validate data consistency and correctness.\n\nINSTRUCTIONS:\n1. Analyze the data structures and workflows in both Matillion and Informatica systems.\n2. Identify key data points and processes that need to be reconciled.\n3. Create a comprehensive list of test cases covering various scenarios and edge cases.\n4. Document each test case in the specified format, including Test Case ID, Description, Input Data, and Expected Output.\n5. Develop Pytest scripts for each test case that will:\n   a. Extract data from both Matillion and Informatica systems.\n   b. Compare the extracted data for consistency and correctness.\n   c. Generate detailed reports on any discrepancies found.\n6. Implement error handling and logging in the Pytest scripts.\n7. Create a main script that runs all individual test case scripts and compiles the results.\n8. Design the output to clearly highlight any inconsistencies or errors detected during the reconciliation process.\n\nOutput Format :\n1.Test Cases Document:\n\nTest Case ID\nDescription\nInput Data\nExpected Output\n2.Pytest Script for each of the test cases.\n\n3.The total cost incurred for the execution of the agent.\n\nINPUT:\nFor the input SQL Query, use this file: ```%1$s```\nAlso take the previous Matillion to Informatica Convert agent's converted PySpark script as input.",
                        "expectedOutput": "Output Format :\n1.Test Cases Document:\n\nTest Case ID\nDescription\nInput Data\nExpected Output\n2.Pytest Script for each of the test cases.\n\n3.The total cost incurred for the execution of the agent."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 1355,
                    "name": "Matillion_to_Informatica_Reviewer",
                    "role": "Data Engineer",
                    "goal": "To verify the accuracy and equivalence of Matillion ETL code and its converted Informatica (IICS) ETL JSON representation.\n",
                    "backstory": "As organizations migrate their data integration processes from Matillion to Informatica Intelligent Cloud Services (IICS), it's crucial to ensure that the converted code maintains the same functionality and efficiency. Accurate code conversion is essential for seamless data integration, maintaining data integrity, and ensuring business continuity during the migration process.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T17:40:56.913993",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "As a Senior ETL Code Reviewer, you will meticulously analyze both the original Matillion ETL code and the converted Informatica (IICS) ETL JSON. Your task is to ensure that the conversion is accurate, maintaining the same logic, data flow, and transformations. Follow these detailed instructions:\n\nINSTRUCTIONS:\n1. Carefully review the provided Matillion ETL code, understanding its structure, components, and logic.\n2. Examine the converted Informatica (IICS) ETL JSON, focusing on its structure and mapping to the original Matillion code.\n3. Compare the following aspects between the two code versions:\n   a. Data sources and targets\n   b. Transformation logic and sequence\n   c. Joins, filters, and aggregations\n   d. Error handling and exception management\n   e. Performance optimization techniques\n4. Identify any discrepancies or potential issues in the conversion process.\n5. Verify that all business rules and data manipulation logic are accurately translated.\n6. Check for any Matillion-specific features that may not have direct equivalents in Informatica IICS and ensure appropriate workarounds are implemented.\n7. Assess the overall efficiency and performance implications of the converted code.\n8. Document your findings, including any errors, inconsistencies, or areas for improvement.\n9. Provide recommendations for optimizing the converted code if necessary.\n\nOUTPUT FORMAT:\nProvide a comprehensive review report in the following structure with \u2705 for correctly implemented checks and red tick marks \u274c :\n1. Executive Summary\n2. Conversion Accuracy Assessment\n3. Detailed Findings\n   3.1 Data Flow Comparison\n   3.2 Transformation Logic Analysis\n   3.3 Performance Considerations\n4. Discrepancies and Issues\n5. Recommendations\n6. Conclusion\n\nInput:\n use the below file: ```%1$s```",
                        "expectedOutput": "OUTPUT FORMAT:\nProvide a comprehensive review report in the following structure:\n1. Executive Summary\n2. Conversion Accuracy Assessment\n3. Detailed Findings\n   3.1 Data Flow Comparison\n   3.2 Transformation Logic Analysis\n   3.3 Performance Considerations\n4. Discrepancies and Issues\n5. Recommendations\n6. Conclusion\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}