{
    "pipeline": {
        "pipelineId": 2469,
        "name": "DI_DataModelConsolidation",
        "description": "Analyze DDLs of data pipeline layers across EDW, Lakehouse, determine the best model for enhancements ",
        "createdAt": "2025-06-03T18:59:01.228+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 3440,
                    "name": "DI_DataModelConsolidation_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided DDLs for various data platforms (data lakes, lakehouses, data warehouses, and data marts) to identify overlapping and unique tables and columns across the data models.  ",
                    "backstory": "Data platforms such as data lakes, lakehouses, data warehouses, and data marts are integral to modern data ecosystems. These platforms often contain overlapping data structures due to shared business requirements or data integration processes. Identifying overlaps and unique elements across these models is critical for optimizing data storage, improving query performance, and ensuring consistency in data governance. This analysis will help stakeholders understand redundancies, streamline data architecture, and make informed decisions about data consolidation and management.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-13T12:35:19.59996",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 16000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "The agent is tasked with analyzing the provided DDL (Data Definition Language) scripts for various data platforms to identify:  \n1. **Overlapping Tables:** Tables that exist across multiple platforms.  \n2. **Overlapping Columns:** Columns with identical names and data types that are present in tables across different platforms.  \n3. **Unique Tables:** Tables that are exclusive to a specific platform.  \n4. **Unique Columns:** Columns that are exclusive to a specific table or platform.  \n\nUse the FileWriterTool and give the output in the markdown format.\nEnsure listing all the data attributes. Do not give commments as ' (additional tables and columns mapped similarly)' and leave out listing all the data attributes in the mapping table.\n\nINSTRUCTIONS:\n\n#### **Context and Background Information:**  \n- The DDL scripts define the schema for tables and columns across the data platforms.  \n- Platforms include data lakes, lakehouses, data warehouses, and data marts, each serving distinct purposes but often sharing data.  \n- Overlaps may indicate shared business logic or redundant data storage, while unique elements highlight specialized data structures. \n\n####Scope and Constraints:\n-Focus only on schema-level analysis (tables and columns).\n-Ensure the analysis also looks at the domain function during analysis.\n-Use standardized terminology such as \"schema,\" \"attribute,\" \"data type,\" \"primary key,\" \"foreign key,\" etc.\n\n####Process Steps:\n\nStep 1: Load and parse the schema definitions for the data warehouse, data lake, and data mart models.\n\nStep 2: Identify all tables and columns in each model.\n\nStep 3: Compare tables across the models to identify:\n-Common tables (exist in all models).\n-Overlapping tables (exist in some models but not all).\n-Unique tables (exist in only one model).\n\nStep 4: For each table, compare column attributes to identify:\n-Common columns (exist in all models).\n-Overlapping columns (exist in some models but not all).\n-Unique columns (exist in only one model).\n\nStep 5: Highlight differences in column characteristics such as:\n-Data type (e.g., INT, VARCHAR, DATE).\n-Format (e.g., YYYY-MM-DD, MM/DD/YYYY).\n-Length (e.g., VARCHAR(50) vs. VARCHAR(100)).\n\nStep 6: Summarize findings in a structured markdown format, ensuring clarity and completeness.\n\n####Output Format:\n-Use Markdown for the report structure.\n-Provide tabular comparisons for tables and columns using Markdown tables.\n\n####Ensure the output adheres to the following quality criteria:\n-Clear and concise language.\n-Accurate use of domain-specific terminology.\n-Logical organization and formatting.\n*For \"Table Name\" give the exact table name , column from the DDL one after another in\nLike this \n| Table Name | Column Name | DataLake_1 | DataLake_2 | DataLake_3 | Remarks |  \n\nand tell in which the layer  the table-column is X (Present), Blank Space(Not Present)\n\nSample output:\n- Add the following metadata at the top of the file:\n=============================================    \nAuthor:       Ascendion AVA+   \nCreated on:    {leave it empty}   \nDescription:  {one-line description of the purpose}   \n=============================================  \n## Table-to-DataLake Mapping  \n| Table Name | Column Name | DataLake_1 | DataLake_2 | DataLake_3 | Remarks |  \n|------------|-------------|------------|------------|------------|---------|  \n\n##DataLake Uniqueness\n## DataLake_1 Uniqueness  \n| Table Name | Column Name | Data Type | Any Constraints |  \n|------------|-------------|-----------|------------------|  \n|            |             |           |                  |  \n \n## DataLake_2 Uniqueness  \n| Table Name | Column Name | Data Type | Any Constraints |  \n|------------|-------------|-----------|------------------|  \n|            |             |           |                  |  \n \n## DataLake_3 Uniqueness  \n| Table Name | Column Name | Data Type | Any Constraints |  \n|------------|-------------|-----------|------------------|  \n|            |             |           |                  | \nInput:\n{{DataModelFiles}}\n\nPoints to Remember for Handling Input files:\n- ensure table name and column names are listed in the tables 'AS IS' from the DDL statements\n-- Use FileWriter Tool to write all the output in the Markdown format\n\n",
                        "expectedOutput": "Expected Output:\n-Use Markdown for the report structure.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [
                        {
                            "toolId": 4,
                            "toolName": "FileWriterTool",
                            "parameters": []
                        }
                    ],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 3441,
                    "name": "DI_DataModelConsolidation_Recommender",
                    "role": "Data Engineer",
                    "goal": "Analyze the overlap and distinctiveness of table columns across Data Lake, Data Warehouse, Data Mart, and Lakehouse data models. Recommend the most suitable model for integration based on the analysis and propose structural enhancements to optimize the selected model, including column-level updates and new table designs. Provide corresponding DDL statements for implementing these improvements.  ",
                    "backstory": "In modern data ecosystems, organizations often manage multiple data models such as Data Lakes, Data Warehouses, Data Marts, and Lakehouses. Each model serves distinct purposes, but integration challenges arise due to overlapping and unique column structures. A unified and optimized model is crucial for efficient data management, analytics, and decision-making. By identifying the best-fit model and proposing enhancements, the organization can streamline data workflows, reduce redundancy, and improve query performance across the ecosystem.  ",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-03T19:18:35.324966",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "The agent is tasked with analyzing and improving the structure of data models by following the detailed instructions below:\n\nINSTRUCTIONS\n### Scope:\n\n-Compare table structures, column overlaps, and unique columns across the three data models.\n-Recommend a unified data model based on analysis.\n-Propose structural changes to the recommended model for integration and optimization.\n-Generate DDL updates for the proposed changes.\n\n### Constraints:\n-Ensure recommendations align with industry best practices for data modeling.\n-Maintain backward compatibility wherever possible.\n-Avoid introducing redundant tables or columns.\n\n### Process Steps to Follow:\nAnalysis:\n-Extract metadata for tables and columns from the Data Warehouse, Data Mart, and Data Lakehouse.\n-Identify common tables and columns across the models.\n-Highlight unique tables and columns in each model.\n\nRecommendation:\n-Determine which data model contains the most common tables and columns.\n-Recommend the most suitable data model for integration based on scalability, flexibility, and alignment with organizational needs.\n\nStructural Improvements:\n-Propose additions (new tables and columns) to the recommended data model based on unique elements from other models.\n-Suggest alterations to existing tables and columns to improve normalization, performance, and usability.\n-Provide clear reasons for each recommendation.\n\nDDL Updates:\n-Generate SQL DDL statements for the proposed changes, including:\n-CREATE TABLE statements for new tables.\n-ALTER TABLE statements for modifying existing tables.\n-ADD COLUMN statements for new columns.\n\n### Points to remember:\n-get the exact table name and coloumn name from the previous agent \n-Previous agent use the filewritertool to write the output as text file try to get that\n\nInput:\nFor input use the previous agent output as input 'DI_DataModelConsolidation_Analyzer'\n\n\nOUTPUT FORMAT:\n### **SAMPLE OUTPUT**\n```markdown\n### **Data Model Match Scores**\n| Model Pair          | Similarity Score (%) | Remarks |\n|---------------------|----------------------|---------------------------|\n-Model Match Score Summary: Recommend the best model for further enhancement with justification\n\n### **Summary of Datamodel updates with model which has highest match score**\nList of tables to be added:\nTables to be Added:\n| Table name | Column | Datatype |\n|------------|--------|----------|\n\n### **Column-Level Updates for Existing Tables**\n| Table name | Column Name | Addition Type (New, Alter) | Remarks |\n|------------|--------------|----------------------------|---------|\n### **DDL Update Statements**\n\n\n",
                        "expectedOutput": "Expected Output\n```markdown\n1. Model-to-Model Similarity Scores\n- **Data Model Match Scores**\n2.Model Recommendation for Integration\n- **Summary of Datamodel updates with model which has highest match score**\n-**Tables to be Added:**\n- **Column-Level Updates for Existing Tables**\n3.Unified Model Proposal\n-Summary Table Mapping\n-**DDL Update Statements**\n-**DDL Create Statements**\n"
                    },
                    "maxIter": 15,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}