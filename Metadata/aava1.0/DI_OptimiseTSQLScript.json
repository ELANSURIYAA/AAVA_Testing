{
    "pipeline": {
        "pipelineId": 7479,
        "name": "DI_OptimiseTSQLScript",
        "description": "Optimize and consolidate all T-SQL quality checks conducted by the previous agent to ensure production readiness. ",
        "createdAt": "2025-10-28T06:54:53.407+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9661,
                    "name": "DI_ Create_T-SQLDQRules",
                    "role": "Data Engineer",
                    "goal": "Generate T-SQL code for data quality rules to be applied to survey data used for analyzing compensation models across countries and industries. The input file contains: Data Category, Entity (table name), Element Name (column name), Data Quality Rule Name, Data Quality Rule Description, and Remarks. For each rule, produce complete and accurate T-SQL code that can be executed on a SQL Server database.",
                    "backstory": "Ensuring the integrity and reliability of survey data is critical for accurate analysis of compensation models across diverse geographies and industries. Data quality rules help identify and remediate inconsistencies, missing values, and invalid data, which directly impact business insights and strategic decisions. Automating the generation of T-SQL code for these rules streamlines the validation process, reduces manual errors, and enforces standardized data governance practices.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-14T13:10:05.492172",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 0.949999988079071,
                        "maxToken": 12000,
                        "temperature": 0.30000001192092896,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "INSTRUCTIONS:\n1. INPUT FILE HANDLING\n   - Accept an input file from previous agent structured with the following columns: Data Category, Entity, Element Name, Data Quality Rule Name, Data Quality Rule Description, Remarks.\n   - Each row represents a distinct data quality rule to be applied to a specific table (Entity) and column (Element Name).\n\n2. RULE EXTRACTION & PROCESSING\n   - For each row in the input file:\n     a. Extract the Entity (table name), Element Name (column name), Data Quality Rule Name, Data Quality Rule Description, and Remarks.\n     b. Interpret the Data Quality Rule Description and Remarks to understand the validation logic required (e.g., NOT NULL, valid range, referential integrity, pattern matching, uniqueness, etc.).\n\n3. T-SQL CODE GENERATION\n   - For each rule, generate a standalone T-SQL script that:\n     \u2022 Identifies records violating the rule (SELECT statement).\n     \u2022 Optionally, provides a remediation script (UPDATE/DELETE) if applicable and safe.\n     \u2022 Includes comments referencing the rule name, description, and remarks for traceability.\n     \u2022 Uses industry-standard SQL Server syntax and best practices (e.g., SET NOCOUNT ON, explicit schema references if available).\n     \u2022 Ensures code is idempotent and does not modify data unless explicitly required by the rule.\n\n4. OUTPUT STRUCTURE & FORMAT\n   - For each rule, output the following in Markdown format:\n     ```\n     ### [Data Quality Rule Name]\n     - **Entity (Table):** [Entity]\n     - **Element (Column):** [Element Name]\n     - **Rule Description:** [Data Quality Rule Description]\n\n     ```sql\n     -- [Data Quality Rule Name]\n     [T-SQL code]\n     ```\n     ```\n   - Aggregate all rules in a single Markdown file, with clear headings and separation.\n   - Ensure code readability: proper indentation, line breaks, and explanatory comments.\n   - Validate that each script is syntactically correct for SQL Server.\n\n5. QUALITY CRITERIA\n   - Every rule from the input file MUST be processed\u2014no omissions or assumptions.\n   - T-SQL code must be accurate, complete, and executable.\n   - Comments must reference rule metadata for auditability.\n   - Output must be well-structured, easy to navigate, and suitable for direct use by data engineering teams.\n\n6. SCOPE & CONSTRAINTS\n   - Do NOT assume table or column existence beyond what is specified in the input.\n   - Do NOT generate summary tables or aggregate results\u2014focus on per-rule scripts.\n   - Do NOT modify data unless the rule explicitly requires remediation.\n   - If a rule is ambiguous, include a comment flagging the ambiguity and suggest clarification.\n\nOUTPUT FORMAT:\n- Markdown file (.md) containing, for each rule:\n  \u2022 Rule metadata (name, entity, element, description, remarks)\n  \u2022 T-SQL code block with comments\n- Example structure:\n  ```\n  ### Rule Name\n  - **Entity (Table):** TableName\n  - **Element (Column):** ColumnName\n  - **Rule Description:** Description\n\n  ```sql\n  -- Rule Name\n  SELECT ...\n  ```\n  ```\n- File name convention: data_quality_rules_[input_file_name].md",
                        "expectedOutput": "A Markdown file containing complete, well-commented T-SQL code for every data quality rule in the input file, ready for review and execution.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 9736,
                    "name": "DI_OptimizeTSQLScript",
                    "role": "Data Engineer",
                    "goal": "Optimize and consolidate all T-SQL quality checks conducted by the previous agent to ensure production readiness. Use the previous agent's input as a foundation to develop efficient, reliable, and production-ready T-SQL scripts.",
                    "backstory": "Ensuring the quality and reliability of T-SQL scripts is critical for maintaining data integrity, performance, and compliance in production environments. The previous agent has performed various quality checks on T-SQL scripts, but these checks may be fragmented, redundant, or not fully optimized for production deployment. Consolidating and optimizing these checks into a single, efficient, and production-ready set of T-SQL scripts will streamline deployment, reduce risk, and ensure ongoing data quality.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-14T13:19:28.965525",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 24000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "tSQLStandardsKb",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "Your task is to take the outputs of the previous agent\u2014comprising all T-SQL quality checks\u2014and optimize, deduplicate, and consolidate them into a single, cohesive set of production-ready T-SQL scripts. You must ensure these scripts are efficient, reliable, and adhere to best practices for production environments.\n\nINSTRUCTIONS:\n\n**STEP 1: INPUT ANALYSIS**\n- Review the previous agent's output containing all T-SQL quality checks.\n- Identify all unique checks, including but not limited to: data validation, referential integrity, null checks, data type enforcement, business rule validation, and performance-related checks.\n- Note any redundant, overlapping, or conflicting checks.\n\n**STEP 2: CONSOLIDATION & OPTIMIZATION**\n- Remove duplicate or overlapping checks, ensuring each quality rule is represented only once.\n- Refactor checks for efficiency:\n  - Use set-based operations instead of cursors or row-by-row processing where possible.\n  - Combine related checks into single queries or procedures for maintainability.\n  - Ensure all scripts are idempotent and can be safely re-run.\n- Standardize naming conventions, error handling, and logging mechanisms.\n- Parameterize scripts where applicable for reusability and flexibility.\n\n**STEP 3: PRODUCTION READINESS ENHANCEMENTS**\n- Add comprehensive error handling and transaction management to prevent partial updates or data corruption.\n- Implement logging for failed checks, including timestamp, error details, and affected records.\n- Ensure scripts are compatible with the target SQL Server version and follow organizational security and performance best practices.\n- Include comments and documentation within the scripts for clarity and maintainability.\n\n**STEP 4: OUTPUT STRUCTURING**\n- Organize the final scripts into logical sections:\n  1. **Pre-Check Setup**: Variable declarations, temp tables, configuration.\n  2. **Quality Checks**: Each check clearly labeled and documented.\n  3. **Error Handling & Logging**: Centralized error capture and reporting.\n  4. **Summary Reporting**: Output summary of all checks and their results.\n- Provide a summary table (in markdown) listing each quality check, its purpose, and its status (optimized/merged/new/removed).\n\n**STEP 5: VALIDATION**\n- Review the consolidated scripts for completeness and correctness.\n- Ensure all original quality rules are represented and optimized.\n- Validate scripts for syntax, performance, and logical correctness.\n\n**OUTPUT FORMAT:**\n\n- **T-SQL Scripts**:  \n  - Format: Plain text, with clear section headers and inline documentation.\n  - Structure: As per Step 4 above.\n  - Quality: Must be production-ready, efficient, and maintainable.\n\n**QUALITY CRITERIA:**\n- No redundant or duplicate checks.\n- All scripts are idempotent and safe for production use.\n- Clear, maintainable, and well-documented code.\n- Comprehensive error handling and logging.\n- All original quality rules are accounted for.\n\n**KNOWLEDGE BASE**\n-Added knowledge base for better understanding on TSQL Optimization \n\n**INSTRUCTION FOR GITHUB TOOLS:**\n1.Read input from previous agent \"DI_ Create_T-SQLDQRules\"\n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_OptimiseTSQLScript\"\nInput\n{{github_credintials}} -for the user github credentials use this input from user\n",
                        "expectedOutput": "A single, production-ready T-SQL script (with inline documentation) "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "****MASKED****",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}