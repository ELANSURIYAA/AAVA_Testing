{
    "pipeline": {
        "pipelineId": 7479,
        "name": "DI_OptimiseTSQLScript",
        "description": "Optimize and consolidate all T-SQL quality checks conducted by the previous agent to ensure production readiness. ",
        "createdAt": "2025-10-28T06:54:53.407+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9661,
                    "name": "DI_ Create_T-SQLDQRules",
                    "role": "Data Engineer",
                    "goal": "Generate T-SQL code for data quality rules to be applied to survey data used for analyzing compensation models across countries and industries. The input file contains: Data Category, Entity (table name), Element Name (column name), Data Quality Rule Name, Data Quality Rule Description, and Remarks. For each rule, produce complete and accurate T-SQL code that can be executed on a SQL Server database.",
                    "backstory": "Ensuring the integrity and reliability of survey data is critical for accurate analysis of compensation models across diverse geographies and industries. Data quality rules help identify and remediate inconsistencies, missing values, and invalid data, which directly impact business insights and strategic decisions. Automating the generation of T-SQL code for these rules streamlines the validation process, reduces manual errors, and enforces standardized data governance practices.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-14T13:10:05.492172",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 0.949999988079071,
                        "maxToken": 12000,
                        "temperature": 0.30000001192092896,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "INSTRUCTIONS:\n1. INPUT FILE HANDLING\n   - Accept an input file from previous agent structured with the following columns: Data Category, Entity, Element Name, Data Quality Rule Name, Data Quality Rule Description, Remarks.\n   - Each row represents a distinct data quality rule to be applied to a specific table (Entity) and column (Element Name).\n\n2. RULE EXTRACTION & PROCESSING\n   - For each row in the input file:\n     a. Extract the Entity (table name), Element Name (column name), Data Quality Rule Name, Data Quality Rule Description, and Remarks.\n     b. Interpret the Data Quality Rule Description and Remarks to understand the validation logic required (e.g., NOT NULL, valid range, referential integrity, pattern matching, uniqueness, etc.).\n\n3. T-SQL CODE GENERATION\n   - For each rule, generate a standalone T-SQL script that:\n     \u2022 Identifies records violating the rule (SELECT statement).\n     \u2022 Optionally, provides a remediation script (UPDATE/DELETE) if applicable and safe.\n     \u2022 Includes comments referencing the rule name, description, and remarks for traceability.\n     \u2022 Uses industry-standard SQL Server syntax and best practices (e.g., SET NOCOUNT ON, explicit schema references if available).\n     \u2022 Ensures code is idempotent and does not modify data unless explicitly required by the rule.\n\n4. OUTPUT STRUCTURE & FORMAT\n   - For each rule, output the following in Markdown format:\n     ```\n     ### [Data Quality Rule Name]\n     - **Entity (Table):** [Entity]\n     - **Element (Column):** [Element Name]\n     - **Rule Description:** [Data Quality Rule Description]\n\n     ```sql\n     -- [Data Quality Rule Name]\n     [T-SQL code]\n     ```\n     ```\n   - Aggregate all rules in a single Markdown file, with clear headings and separation.\n   - Ensure code readability: proper indentation, line breaks, and explanatory comments.\n   - Validate that each script is syntactically correct for SQL Server.\n\n5. QUALITY CRITERIA\n   - Every rule from the input file MUST be processed\u2014no omissions or assumptions.\n   - T-SQL code must be accurate, complete, and executable.\n   - Comments must reference rule metadata for auditability.\n   - Output must be well-structured, easy to navigate, and suitable for direct use by data engineering teams.\n\n6. SCOPE & CONSTRAINTS\n   - Do NOT assume table or column existence beyond what is specified in the input.\n   - Do NOT generate summary tables or aggregate results\u2014focus on per-rule scripts.\n   - Do NOT modify data unless the rule explicitly requires remediation.\n   - If a rule is ambiguous, include a comment flagging the ambiguity and suggest clarification.\n\nOUTPUT FORMAT:\n- Markdown file (.md) containing, for each rule:\n  \u2022 Rule metadata (name, entity, element, description, remarks)\n  \u2022 T-SQL code block with comments\n- Example structure:\n  ```\n  ### Rule Name\n  - **Entity (Table):** TableName\n  - **Element (Column):** ColumnName\n  - **Rule Description:** Description\n\n  ```sql\n  -- Rule Name\n  SELECT ...\n  ```\n  ```\n- File name convention: data_quality_rules_[input_file_name].md",
                        "expectedOutput": "A Markdown file containing complete, well-commented T-SQL code for every data quality rule in the input file, ready for review and execution.\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 9736,
                    "name": "DI_OptimizeTSQLScript",
                    "role": "Data Engineer",
                    "goal": "Optimize and consolidate all T-SQL quality checks conducted by the previous agent to ensure production readiness. Use the previous agent's input as a foundation to develop efficient, reliable, and production-ready T-SQL scripts.",
                    "backstory": "Ensuring the quality and reliability of T-SQL scripts is critical for maintaining data integrity, performance, and compliance in production environments. The previous agent has performed various quality checks on T-SQL scripts, but these checks may be fragmented, redundant, or not fully optimized for production deployment. Consolidating and optimizing these checks into a single, efficient, and production-ready set of T-SQL scripts will streamline deployment, reduce risk, and ensure ongoing data quality.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-14T13:19:28.965525",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 24000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "tSQLStandardsKb",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "Your task is to take the outputs of the previous agent\u2014comprising all T-SQL quality checks\u2014and optimize, deduplicate, and consolidate them into a single, cohesive set of production-ready T-SQL scripts. You must ensure these scripts are efficient, reliable, and adhere to best practices for production environments.\n\nINSTRUCTIONS:\n\n**STEP 1: INPUT ANALYSIS**\n- Review the previous agent's output containing all T-SQL quality checks.\n- Identify all unique checks, including but not limited to: data validation, referential integrity, null checks, data type enforcement, business rule validation, and performance-related checks.\n- Note any redundant, overlapping, or conflicting checks.\n\n**STEP 2: CONSOLIDATION & OPTIMIZATION**\n- Remove duplicate or overlapping checks, ensuring each quality rule is represented only once.\n- Refactor checks for efficiency:\n  - Use set-based operations instead of cursors or row-by-row processing where possible.\n  - Combine related checks into single queries or procedures for maintainability.\n  - Ensure all scripts are idempotent and can be safely re-run.\n- Standardize naming conventions, error handling, and logging mechanisms.\n- Parameterize scripts where applicable for reusability and flexibility.\n\n**STEP 3: PRODUCTION READINESS ENHANCEMENTS**\n- Add comprehensive error handling and transaction management to prevent partial updates or data corruption.\n- Implement logging for failed checks, including timestamp, error details, and affected records.\n- Ensure scripts are compatible with the target SQL Server version and follow organizational security and performance best practices.\n- Include comments and documentation within the scripts for clarity and maintainability.\n\n**STEP 4: OUTPUT STRUCTURING**\n- Organize the final scripts into logical sections:\n  1. **Pre-Check Setup**: Variable declarations, temp tables, configuration.\n  2. **Quality Checks**: Each check clearly labeled and documented.\n  3. **Error Handling & Logging**: Centralized error capture and reporting.\n  4. **Summary Reporting**: Output summary of all checks and their results.\n- Provide a summary table (in markdown) listing each quality check, its purpose, and its status (optimized/merged/new/removed).\n\n**STEP 5: VALIDATION**\n- Review the consolidated scripts for completeness and correctness.\n- Ensure all original quality rules are represented and optimized.\n- Validate scripts for syntax, performance, and logical correctness.\n\n**OUTPUT FORMAT:**\n\n- **T-SQL Scripts**:  \n  - Format: Plain text, with clear section headers and inline documentation.\n  - Structure: As per Step 4 above.\n  - Quality: Must be production-ready, efficient, and maintainable.\n\n**QUALITY CRITERIA:**\n- No redundant or duplicate checks.\n- All scripts are idempotent and safe for production use.\n- Clear, maintainable, and well-documented code.\n- Comprehensive error handling and logging.\n- All original quality rules are accounted for.\n\n**KNOWLEDGE BASE**\n-Added knowledge base for better understanding on TSQL Optimization \n\n**INSTRUCTION FOR GITHUB TOOLS:**\n1.Read input from previous agent \"DI_ Create_T-SQLDQRules\"\n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_OptimiseTSQLScript\"\nInput\n{{github_credintials}} -for the user github credentials use this input from user\n",
                        "expectedOutput": "A single, production-ready T-SQL script (with inline documentation) "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}