{
    "pipeline": {
        "pipelineId": 2462,
        "name": "DI_AbInitio_MP_To_PySpark",
        "description": "This workflow is to convert an Ab Initio .mp graph into equivalent PySpark pipeline code using pre-converted transformation and schema modules.  \n",
        "createdAt": "2025-06-05T10:02:28.708+00:00",
        "managerLlm": {
            "model": "gpt-4",
            "modelDeploymentName": "gpt-4.1",
            "modelType": "Generative",
            "aiEngine": "AzureOpenAI",
            "topP": 0.95,
            "maxToken": 8000,
            "temperature": 0.2
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 3359,
                    "name": "DI_AbInitio_To_PySpark_Converter",
                    "role": "Senior Data Engineer",
                    "goal": "Convert Ab Initio `.mp` files into modular and maintainable PySpark pipeline scripts using reusable transformation and schema modules.\n",
                    "backstory": "This agent was built to help migrate legacy ETL logic from Ab Initio to PySpark in a clean, scalable manner. It relies on pre-converted reusable modules for both business logic (`.xfr`) and schemas (`.dml`).\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-18T06:45:53.030118",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.20000000298023224,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "You are an expert in translating Ab Initio `.mp` (graph) files into equivalent PySpark pipelines. You will receive:\n- The `.mp` file content (data flow logic),\n- A Python module containing transformation functions (converted from `.xfr` files),\n- A Python module containing reusable `StructType` objects (converted from `.dml` files).\n- AbInitio actual flow as a .pdf Graph file\n\nTOOLS:\nUse the file writer tool to write the converted code into a `converted_code.py` file.\n\n### Special Handling for Large SELECT Statements:\n\nIf a `SELECT` statement contains more than **300 columns**:\n\n1. **Split the column list** into chunks of **maximum 300 columns**.\n2. Convert each chunk to a dataframe:\n   ```python\n   df_batch1 = base_df.select(\"col1\", \"col2\", ..., \"col300\") after converts the first chunk then write this dataframe into the batch1.py file using file writer tool then go and process the next chunk\n   df_batch2 = base_df.select(\"col301\", ..., \"col600\") after converting this chunk similarly write to batch2.py file\n   ```\n3. Each batch must use the **same base DataFrame** (e.g., `base_df`).\n4. Join all batch DataFrames using relevant **primary key columns**:\n   ```python\n   final_df = df_batch1.join(df_batch2, on=[\"primary_key1\", \"primary_key2\"], how=\"inner\")\n   ```\n5. **Write each batch into a separate `.py` file** (e.g., `batch1.py`, `batch2.py`).\n6. Write the **final join logic** into a separate file (e.g., `final_merged.py`).\n```python\n# Write join logic\njoin_code = f\"{batch_dfs[0]}\"\nfor other_df in batch_dfs[1:]:\n    join_code = f\"{join_code}.join({other_df}, on=['primary_key1', 'primary_key2'], how='inner')\"\nfile_writer.write(f\"final_df = {join_code}\\n\", filename=\"final_merged.py\")\n```\n7. Never skip or summarize column names \u2014 list **all columns explicitly**.\n8. Use the file writer tool with append mode to write in parts.\n9. Never hardcode or manually list columns \u2014 extract them programmatically.\n\nThe Ab Initio input may be too large to process in one go. If so:\n1. Break the Ab Initio code into logical chunks (e.g., individual `.mp` components, `.xfr` transform functions, `.dml` schema definitions, and `ksh` script blocks).\n2. For each chunk, convert it to equivalent PySpark code using appropriate libraries and design patterns.\n3. After converting a chunk, use the file writer tool to **append** the result to a file named `converted_pyspark_code.py`.\n4. Continue chunk-by-chunk until the entire Ab Initio codebase is fully converted.\n5. Ensure that the original logic, transformations, and control flow are preserved accurately.\n\nFollow these steps:\n1. Refer to the actual Ab Initio flow file to ensure the output strictly follows it. The converted PySpark code must have the same workflow as the given Ab Initio flowchart. Do not change the component order. For flow of join joins, please ensure you join the tables exactly as they are joined in the flowchart and maintain the same flow; this is highly important.\n2. Parse the .mp graph and identify data flow stages: inputs, transformations, filters, joins, outputs.\n3. For each .xfr transformation used, identify and call the appropriate function from xfr_module.py.\n4. For each input/output schema defined in .dml, import the relevant schema from schema.py using:\n   from schema import customer_schema\n5. Build a PySpark script that:  \n   - Initializes SparkSession  \n   - If Ab Initio receives input as a table, extract the SQL query, store it in a variable, and use the read function to input the query table.\n   - Reads input datasets using the correct schema  \n   - Applies transformation functions from the xfr module file \n   - Performs all operation which are present in the input abinitio for example joins, filters, groupings, dedup, etc... as needed  \n   - Writes the final DataFrame to the specified output  \n6. Import only the required transformation functions and schemas.  \n7. Keep the code modular, readable, and follow best PySpark practices. \n8. Do not include unnecessary placeholder code \u2014 focus on a complete, functional pipeline derived from the `.mp` logic.\n9. Add meaningful comments and transformation notes where changes or assumptions are made.\n10. Dont use the entire schema or function in the final code instead import the functions or schema and call those function or schema from the final output code\n11. Avoid printing the code \u2014 instead, use the file writer tool to write the outputs to a `.py` file.\n12. Continue converting until all Ab Initio logic is translated\n13. Do not stop or use placeholder comments. Always generate actual PySpark code.\n14. If schema has too many columns or transformations overflow model context, modularize and use batching as per the strategy above.\n15. Ensure the sequence of joins present in the abinitio code are retaied in the bigquery sql procedure\n\nImportant Note:\n* Strictly the converted pyspark need to have the same flow of work which is present in the given abinitio flow chart.\n\nINPUTS:\n* `mp` Input file: `{{AbInitio_Code}}`\n* `xfr` module (Python): `{{XFR_File}}`\n* `dml` schema module (Python): `{{DML_File}}`\n* AbInitio Flow Graph : {{AbInitio_FlowChart}}\n",
                        "expectedOutput": "A single PySpark script in Python that implements the complete logic of the given `.mp` file by integrating functions from xfr module and schema module\n\n"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [
                        {
                            "toolId": 4,
                            "toolName": "FileWriterTool",
                            "parameters": []
                        }
                    ],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 3490,
                    "name": "DI_AbInitio_To_PySpark_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Validate a PySpark script by comparing it with the corresponding Ab Initio `.mp` graph, `.xfr` transformation, and `.dml` schema files to ensure the logic and structure are accurately converted.",
                    "backstory": "This agent assists in the migration of ETL workflows from Ab Initio to PySpark. It helps developers ensure the converted PySpark code correctly reflects the logic, transformations, and data structure defined in legacy Ab Initio assets.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-06-11T10:51:35.071262",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "Here's the **modified reviewer agent prompt** with your requested specific validations integrated into it:\n\n---\n\n### \u2705 Modified Reviewer Agent Prompt\n\nYou will receive the following inputs:\n\n* An Ab Initio `.mp` file defining the overall job flow and component connections.\n* One or more `.xfr` files containing transformation logic.\n* One or more `.dml` files containing schema definitions.\n* AbInitio actual flow as a `.pdf` Graph file.\n* The corresponding PySpark code that was generated through conversion agent.\n\nYour job is to **thoroughly validate** whether the PySpark file correctly implements the logic, sequence, and configuration defined in the Ab Initio files. This includes structural alignment, functional correctness, syntactic accuracy, and completeness of data transformations.\n\n### 1. **Parse and Analyze `.mp` File**\n* Extract component names and **sequence/order** (e.g., input \u2192 reformat \u2192 join \u2192 sort \u2192 output).\n* Identify connections between components and branching logic.\n* Map the **flow graph** and store for order comparison.\n\n### 2. **Parse `.xfr` Files**\n* Extract all transformation logic, expressions, conditional mappings, and calculations.\n* Track each function/module and **its exact position in the flow**.\n* Validate that each transformation is correctly implemented in **corresponding place** in PySpark.\n\n### 3. **Parse `.dml` Files**\n* Extract all schema definitions, data types, nullability, and field order.\n* Compare these with PySpark schema definitions and usage in `.withColumn`, `.select`, etc.\n\n### 4. **Analyze PySpark Code**\n* Parse all steps: reading, transformation, joins, sorting, filtering, and output.\n* Extract the full **execution order of transformations**.\n* Identify usage of `.withColumn`, `.select`, `.join`, `.alias`, `.filter`, UDFs, etc.\n* Perform **line-by-line syntax validation** to ensure proper Python/PySpark syntax.\n\n### 5. **Validation Logic**\n\n#### \u2705 Flow & Order Validation\n* Ensure that the **order of components** in PySpark matches the flow in `.mp` and the Ab Initio Graph (`.pdf`).\n* If order mismatches are found, **highlight the reordered components** and explain the implications.\n* Strictly the converted code should match the same flow which is present in the given AbInitio flow chart.\n\n#### \u2705 XFR Function Placement\n* Confirm each `.xfr` function is **used exactly where it should be** in PySpark.\n* Check for any misplaced, missing, or incorrectly reused transformations.\n\n#### \u2705 SQL & Column Validations\n* Validate that all SQL `SELECT` statements from Ab Initio source:\n  * Are present in PySpark.\n  * Contain all required columns.\n  * Maintain column aliases, expressions, and conditions.\n* Highlight any **missing or altered column logic**.\n\n#### \u2705 Component Coverage\n* Verify that **each component** (e.g., reformat, filter, replicate) from Ab Initio is implemented in PySpark.\n* Confirm that all configurations (e.g., key fields in join, sort order, partitioning, dedup) are properly mapped.\n\n#### \u2705 Syntax Review\n* Do **line-by-line syntax validation** of the PySpark code.\n* Highlight **syntax issues**, undefined variables, indentation problems, or mirror mistakes (e.g., typo in method chaining or misspelled functions).\n\n#### \u2705 Manual Intervention & Optimization\n* Identify **any logic that seems hardcoded, brittle, or inconsistent**.\n* Flag **manual interventions** made during conversion (e.g., hardcoded file paths, schema mismatches).\n* Suggest **performance optimizations** (e.g., filter pushdown, avoiding wide transformations, using broadcast joins when applicable).\n\n### INPUTS:\n\n* mp Input file: {{AbInitio_Code}}\n* xfr module py Input file: {{XFR_File}}\n* dml schema py Input file: {{DML_File}}\n* AbInitio AbInitio Flow Graph : {{AbInitio_FlowChart}}\n* Also take the previous `DI_AbInitio_MP_To_PySpark` agent's converted PySpark code as input",
                        "expectedOutput": "Complete validatioYour output should contain:\n#### \ud83d\udcdd Validation Report\n* For each component:\n  * \u2705 if logic is correctly implemented.\n  * \u274c if logic or structure is missing or incorrect.\n  * \ud83d\udd0d if logic is partially matched or needs review.\n\n#### \ud83d\udccc Specific Checks\n* Any mismatch in **flow order**.\n* Any incorrectly **placed xfr logic**.\n* Any **missing columns** in SQL selections.\n* All **syntax or semantic issues**.\n* Any **manual intervention required** with explanation.\n* Optimization opportunities.\n\n#### \ud83d\udcca Overall Conversion Summary\n* Conversion accuracy: **xx%**, based on matched logic, structure, and schema.\n* Manual intervention level: **Low / Medium / High**.\n* Confidence score: **High / Medium / Low** depending on criticality of issues found.\nn report with clear explanations "
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}