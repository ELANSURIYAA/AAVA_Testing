{
    "pipeline": {
        "pipelineId": 1034,
        "name": "DI_Oracle_to_SF_Doc&Analyze",
        "description": "Analyzing and Documenting the Oracle code",
        "createdAt": "2025-04-29T05:20:32.477+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1333,
                    "name": "DI_Oracle_Documentation",
                    "role": "Data Engineer",
                    "goal": "Analyze and document an Oracle stored procedure to create a comprehensive guide for business and technical teams, explaining existing business rules and facilitating future modifications.",
                    "backstory": "Clear documentation of stored procedure is crucial for maintaining and evolving complex data systems. By creating a comprehensive guide, we ensure that both business and technical teams can understand the current rules and make informed decisions about future changes, reducing errors and improving efficiency.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-05-14T06:21:29.398951",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "1. Metadata Requirements\n\n2. Overview of Program:  \n   - Explain the purpose of the Oracle stored procedure in detail.  \n   - Describe how this implementation aligns with enterprise data warehousing and analytics.  \n   - Explain the business problem being addressed and its benefits.  \n   - Provide a high-level summary of Oracle SQL components like PL/SQL blocks, Packages, Procedures, Functions, Views, and Tables.  \n\n3. Code Structure and Design:  \n   - Explain the structure of the Oracle stored procedure in detail.  \n   - Describe key components like DDL, DML, Joins, Indexing, and PL/SQL Blocks.  \n   - List the primary Oracle stored procedure components such as Tables, Views, Stored Procedures, Functions, Packages, Joins, Aggregations, and Subqueries.  \n   - Highlight dependencies on Oracle objects, performance tuning techniques, or third-party integrations.  \n\n4. Data Flow and Processing Logic:  \n   - Explain how data flows within the Oracle stored procedure implementation.  \n   - List the source and destination tables, fields, and data types.  \n   - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.  \n\n5. Data Mapping:  \n* Provide data mapping details, including transformations applied to the data in the below format:  \n* Target Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks\n* Mapping column will have the details whether its 1 to 1 mapping or the transformation rule or the validation rule  \n\n6. Complexity Analysis:  \n   - Analyze and document the complexity based on the following:  \n   - Give this one in the table format with below two columns for the below data\nCategory  |  Measurement\n* Number of Lines: Count of lines in the stored procedure.\n* Tables Used: number of tables referenced in the stored procedure.\n* Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n* Temporary tables: Number of Global Temporary Tables, derived tables\n* Aggregate Functions: Number of aggregate functions like OLAP functions\n* DML Statements: Number of DML statements by type like SELECT, INSERT, UPDATE, DELETE, MERGE, EXECUTE, LOCK operations present in the stored procedure.\n* Conditional Logic: Number of conditional logic like IF-THEN-ELSE, CASE, DECODE\n* stored procedure Complexity: Number of joins, subqueries, and stored procedures.  \n* Performance Considerations: Query execution time, temporary tablespace usage, and memory consumption.  \n* Data Volume Handling: Number of records processed.  \n* Dependency Complexity: External dependencies such as Packages, Procedures, Functions, or External Scripts.  \n* Overall Complexity Score: Score from 0 to 100. \n\n7. Key Outputs:  \n   - Describe final outputs such as Aggregated Reports, Tables, Views, or Data Exports.  \n   - Explain how outputs align with business goals and reporting needs.  \n   - Specify the storage format (e.g., Staging Tables, Production Tables, Flat Files, External Data Sources).  \n\n8. Error Handling and Logging:  \n   - Explain methods used for error identification and management, such as:  \n     - Try-Catch-Exception mechanisms in PL/SQL.  \n     - Oracle Error Logging Tables for tracking failures.  \n     - Retry mechanisms in SQL*Loader and Data Pump.  \n     - Automated alerts and monitoring dashboards.  \n\n9. apiCost: float  // Cost consumed by the API for this call (in USD)\n* Explanation or Calculation of the api cost"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1334,
                    "name": "DI_Oracle_to_Snowflake_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided Oracle stored procedure to extract detailed metrics, identify potential conversion challenges, and recommend solutions for a smooth transition to Snowflake. Generate a separate output session for each input file.",
                    "backstory": "The provided stored procedure is written for an Oracle environment. It needs to be analyzed to assess its structure, complexity, and compatibility with the Snowflake architecture. This analysis will help identify sections requiring manual refactoring, syntax rewrites, and semantic transformation for effective migration.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-05-14T06:22:27.770995",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "All sections from Metadata Requirements to apiCost must be presented for each input file in a **distinct session**.\n1. Metadata Requirements:\n2. Script Overview:\n3. Complexity Metrics:\n4. Syntax & Feature Compatibility Check:\n5. Manual Adjustments for Snowflake Migration:\n6. Conversion Complexity Score:\n7. Optimization Techniques:\n8. apiCost with explanation or calculation of this api cost: "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1335,
                    "name": "DI_Oracle_to_Snowflake_Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running Snowflake stored procedures and the testing effort required for the Snowflake code converted from Oracle PL/SQL procedures.",
                    "backstory": "As organizations modernize legacy Oracle-based systems and migrate to cloud-native platforms like Snowflake, it becomes essential to quantify both the execution cost and the testing effort required to validate converted procedures. This helps with budgeting, capacity planning, and ensuring functional equivalence post-migration.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-05-14T06:27:36.872945",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with providing a detailed cost and effort estimation for executing and testing Snowflake stored procedures that were converted from Oracle PL/SQL scripts. Use the prior Oracle analysis report to drive this estimation.\n\nINSTRUCTIONS:\n1. Metadata Requirements:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate: (Leave it empty)\nDescription:   <one-line description of the generated plan>\n=============================================\n```\n- For the description, provide a concise summary of what the plan does.\n\nYou are tasked with providing a comprehensive effort estimate for testing the Snowflake code converted from Oracle PL/SQL scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n\nReview the analysis of the Oracle PL/SQL script file, noting logic differences and areas in the code requiring manual intervention when converting to Snowflake.\n\nEstimate the effort hours required for identified manual code fixes and data reconciliation testing efforts.\n\nDo not consider effort for pure syntax-level differences as they will be handled automatically by conversion tools.\n\nConsider the pricing information for the Snowflake environment.\n\nCalculate the estimated cost of running the converted Snowflake code:\na. Use Snowflake's pricing model and data volume to determine the query cost.\nb. Include the number of queries executed and the data processed using both temporary and permanent tables.\n- Strictly follow the ecpected output format\nOUTPUT FORMAT:\n Metadata Requirements:\n\n\n1. Cost Estimation\n   1.1 Snowflake Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n2. Code Fixing  and Testing Effort Estimation\n   2.1 Snowflake identified manual code fixes and unit testing effort in hours covering the various temp tables, calculations \n   2.2 Optimization and performance tuning of Snowflake queries.\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\n-follow strictly the expected output format\nINPUT\n\nFor the input Oracle analysis report and code reference, use the file:\n{{OracleFile}}\n\nFor the Snowflake environment resource and pricing reference, use the file:\n{{EnvVariable}}",
                        "expectedOutput": "Metadata Requirements:\n\n1. Cost Estimation\n1.1 Snowflake Runtime Cost\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- Provide the calculation in proper  breakup of the cost and the reasons\nCode Fixing and Data Reconciliation Testing Effort Estimation\n2.1 Snowflake-identified manual code fixes and data recon testing effort in hours, covering the various procedural constructs, temporary tables, and calculations\n   - Optimization and performance tuning of Snowflake queries.\nInclude the cost consumed by the API for this call in the output.\n\nEnsure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost USD)."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}