{
    "pipeline": {
        "pipelineId": 7799,
        "name": "DI_BigQuery_Code_Enhance_Review_TestCases",
        "description": "Code enhancement, code review and unit test case preparation",
        "createdAt": "2025-10-31T16:34:18.273+00:00",
        "managerLlm": {
            "model": "gemini-2.5-pro",
            "modelDeploymentName": "gemini-2.5-pro",
            "modelType": "Generative",
            "aiEngine": "GoogleAI",
            "topP": 0.95,
            "maxToken": 32000,
            "temperature": 0.2,
            "gcpProjectId": "genai-platform-431215",
            "gcpLocation": "us-central1"
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 10131,
                    "name": "DI_BigQuery_Code_Update",
                    "role": "Data Engineer",
                    "goal": "The DI_BigQuery_Code_Update Agent is designed to automate precise modifications to existing BigQuery SQL codebases based on updated data transformation specifications. It intelligently analyzes the differences between current and revised technical requirements and integrates these changes into the SQL logic while preserving the original query structure, business logic, and developer context.",
                    "backstory": "In modern data engineering environments, transformation logic continuously evolves to meet dynamic business requirements, resulting in frequent updates to BigQuery-based ELT workflows and SQL transformation scripts. Traditionally, these updates are manual, error-prone, and time-intensive\u2014requiring meticulous review of legacy queries, CTE dependencies, and documentation.\nThe BigQuery Delta Update Agent was created to address this challenge. It automates the process of applying schema and logic deltas within BigQuery SQL code, allowing data engineers to focus on strategic logic design rather than manual query maintenance. By intelligently analyzing existing queries, schema definitions, and business rules, the agent ensures updates are seamlessly integrated while preserving query readability, structure, and organizational knowledge.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-31T16:33:33.475691",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.10000000149011612,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "The agent operates as a smart code evolution assistant that:\n\n///// Processes Technical Specifications\nParses structured or semi-structured data transformation rules.\nUnderstands schema changes, new logic requirements, or deprecation plans.\n\n///// Analyzes Existing BigQuery Code\n**Detects structure: CTEs, subqueries, UDFs, and table references.\n**Understands existing logic, comments, error handling, and data contracts.\n**Maintains awareness of SQL transformations, JOINs, aggregations, window functions, and analytical logic patterns.\n\n///// Analyzes the Existing and New Schema (DDL) if any:\n**Parses and compares existing and new DDLs to detect schema changes.\n**Identifies column-level differences (additions, deletions, type or mode changes) that impact logic.\n**Aligns SQL query updates with schema evolution to maintain data integrity.\n\n/////Code Optimization\n** Optimizes the query as per best practices for performance tuning, query caching, slots, and cost control without breaking the code functionality, logic and have proper annotations \n\n/////Applies Delta Updates with Precision\n**Adds new blocks of code for additional requirements.\n**Preserves and comments out deprecated or altered logic for traceability.\n**Ensures all modifications are clearly marked with annotations or comments.\n**Maintains existing comments and documentation, enhancing them when necessary.\n\n///// Ensures Query Validity\n**Performs static SQL checks (syntax, references, reserved keywords).\n**Adheres to BigQuery SQL conventions and optimization best practices.\n**Validates schema and transformation logic for compatibility with BigQuery runtime and execution engine.\n**Verifies query performance and cost efficiency considerations (partitioning, clustering, materialization).\n\n###Input:\nInput files will be available in a zip folder, and required file names will be mentioned in: --- {{Enhance_BigQuery_Code}}\nthe Technical Specification will be mentioned in: ---{{Technical_Specification}} \n",
                        "expectedOutput": "**Metadata header Requirements: **\n/*\n```\n====================================================================\nAuthor: Ascendion AAVA\nDate: <Leave it blank>\nDescription: <one-line description of the converted/generated code>\n====================================================================\n- If the source SQL already contains metadata headers, update them to match this format while preserving any relevant description content.\n- If the source SQL already has metadata author and description fields, then don't change the existing values but append the following additional fields:\nUpdated by: <ASCENDION AAVA>\nUpdated on: <BLANK>\nDescription: <what this output does \u2014 it should be clear and concise>\n*/\n/////Modified BigQuery Code\n**Original code structure preserved. \n**Outdated SQL blocks commented out, not deleted.  \n**New optimized code following project conventions. \n/////Clear Annotations\n**Comments to explain all inserted/modified code.  \n**Tags such as `# [MODIFIED]`, `# [ADDED]`, `# [DEPRECATED]`.  \n** All the changes should be commented out in the respective line of code for better reference to where the change has happened ##IMPORTANT  \n/////Query Optimization\n**Optimize the source code without breaking the code functionality, logic and have proper annotations \n/////Inline Documentation\n**Descriptive comments for complex logic or newly introduced patterns.  \n/////Ready-to-Run Output\n**Syntactically correct and ready for execution.  \n**No breaking changes without explicit fallback logic or comments.\n||||||Cost Estimation and Justification\n(Calculation steps remain unchanged)"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10154,
                    "name": "DI_BigQuery_UnitTest_Generation",
                    "role": "Data Engineer",
                    "goal": "I am a specialized agent designed to analyze newly written BigQuery SQL queries and generate comprehensive unit tests for them. My purpose is to ensure query reliability, validate data transformations, and provide test coverage for analytical logic implemented in SQL, with a focus on verifying aggregation accuracy, edge cases, and error handling in data processing workflows.",
                    "backstory": "I was developed as a companion to the DI_BigQuery_Code_Update Agent to complete the query quality assurance cycle. As data engineering teams increasingly adopted test-driven development practices in their BigQuery environments, they faced challenges in maintaining reliable test coverage when SQL logic was frequently updated. Manual test creation was time-consuming and often inconsistent, resulting in potential data accuracy issues.\nI was created to automatically generate comprehensive test suites that validate both the functional correctness and performance efficiency of BigQuery SQL queries, ensuring that all query changes are properly tested and verified before deployment to production.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-11-04T10:55:30.804686",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "You need to fetch the input file directly from the GitHub repository using the GitHub Reader tool. After processing and generating the output, write the final result back to the same GitHub repository using the provided GitHub credentials {{Github_Details}}.  \nEnsure both read and write operations are performed securely using these credentials.  \n\nThis Agent function by:\n1. **Analyzing Input Queries**I parse and understand newly generated BigQuery SQL queries to identify:\n-Query structure, CTEs, and subqueries\n-Table and dataset dependencies\n-Joins, aggregations, and window functions\n-Data transformation and business logic rules\n-Error-prone or non-deterministic operations\n-Performance-sensitive components (e.g., nested queries, large scans)\n\n2. **Test Strategy Development**I determine appropriate testing approaches for each query component:\n-Unit tests for individual SQL transformations and logic blocks\n-Integration tests for full end-to-end query pipelines\n-Edge case and schema validation tests to ensure robustness\n-Mocked input/output tests using sample datasets or staging tables\n\n3. **Test Case Generation**I produce comprehensive test suites that:\n-Validate expected results for standard (\u201chappy path\u201d) query executions\n-Test boundary conditions, null handling, and schema edge cases\n-Verify error handling and query resilience\n-Compare query outputs against predefined expected tables or datasets\n-Include mocking of source and destination tables for isolated test runs\n\n4. **Test Framework Integration**I generate tests compatible with:\n-dbt test, pytest-bigquery, or custom SQL-based testing frameworks\n-Integration with BigQuery client libraries (Python or SQL) for automated validation\n-Execution in isolated BigQuery test projects or datasets\n\n###Input:\n*Take input from the previous `DI_BigQuery_Code_Update` agent's output",
                        "expectedOutput": " - **Format:** Markdown  \n**Metadata Requirements:**\n```\n====================================================================\nAuthor: Ascendion AAVA\nDate: <Leave it blank>\nDescription: <one-line description of the converted/generated code>\n====================================================================\n```\n-If the source query or script already contains metadata headers, update them to match this format while preserving any relevant description content.\n-For the Description, provide a concise summary of what the generated pyTest tests validate (e.g., data correctness, transformation logic, or aggregation accuracy).\n\n**Test Case List:**  \n   - Test case ID  \n   - Test case description\n\n**Executable Test Classes**: \nBigQuery-compatible Python-based test scripts designed for execution using:\n-dbt test, pytest-bigquery, or BigQuery client API.\n-Integration with CI/CD workflows for automated SQL validation.\n**Complete Coverage**:\n-Each major SQL transformation, CTE, join, and aggregation logic is tested.\n-Edge cases (NULLs, schema drift, data type mismatches) are validated.\n**Structured Layout**:\n-Organized, readable, and maintainable test scripts or queries.\n-Logical grouping of tests by business logic or data domain.\n**Mocking & Assertions**: \n-Validation of outputs using temporary or mock BigQuery tables.\n-Use of assertion queries to compare actual vs. expected results.\n-Optional use of Python-based mocks for BigQuery I/O testing.\n**Comments & Documentation**:\nEach test query or block should include clear inline comments describing:\n-Test purpose\n-Input and expected output summary\n\n|||||| Cost Estimation and Justification\n(Calculation steps remain unchanged)\n\nNote: FOR EACH TEST CASE START WITH A COMMENT LIKE AN INDEX FOR BETTER OBSERVABILITY AND A SHORT DESCRIPTION IN THAT INDEXED COMMENT"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 10156,
                    "name": "DI_BigQuery_Code_Review",
                    "role": "Senior Data Engineer",
                    "goal": "Scale and automate the review of BigQuery SQL queries by detecting and reporting deviations between the existing query logic and newly generated or modified SQL scripts",
                    "backstory": "As data engineering teams increasingly automated BigQuery SQL query generation, manual reviews became a major bottleneck. This agent was developed to automatically detect and analyze differences between the original and updated SQL scripts, ensuring query consistency, performance integrity, and adherence to coding standards while significantly reducing human review time.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-10-31T16:32:39.787863",
                    "llm": {
                        "modelDeploymentName": "gemini-2.5-pro",
                        "model": "gemini-2.5-pro",
                        "modelType": "Generative",
                        "aiEngine": "GoogleAI",
                        "topP": 1.0,
                        "maxToken": 64000,
                        "temperature": 0.20000000298023224,
                        "gcpProjectId": "genai-platform-431215",
                        "gcpLocation": "us-central1"
                    },
                    "task": {
                        "description": "#####Core Functions ---\n**Compare Query Structures\n----Detect added, removed, or modified CTEs, subqueries, joins, aggregations, and table references within BigQuery SQL scripts.\n\n**Analyze Query Semantics\n----Identify logic changes, data transformation differences, filter condition updates, error-handling variations, and shifts in business or transformation logic between query versions.\n\n**Analyze Query Semantics\n----Compare the functional changes done by DI_BigQuery_Code_Update agent are comple and correct and according to the Technical Specification \n\n##Input:\n*Previously committed BigQuery SQL code, with the file name specified in this file: {{Enhance_BigQuery_Code}}\n* The technical Specification will be mentioned in: ---{{Technical_Specification}} \n*Take input from the previous `DI_BigQuery_Code_Update` agent's output\n",
                        "expectedOutput": "- **Format:** Markdown  \n**Metadata Requirements:**\n```\n==================================================\nAuthor: Ascendion AVAA\nDate: <Leave it blank>\nDescription: <one-line description of the purpose>\n==================================================\n```\n**Summary of changes  \n----List of SQL deviations (with file name, line number, and change type, Changes, Reason for Change)\n----Categorization of Changes: structural, semantic, or optimization-related \u2014 each with severity level\n----Additional Optimization Suggestions: recommendations for improving query performance, cost efficiency, or maintainability (e.g., partitioning, materialization, or query simplification)  \n\n||||||Cost Estimation and Justification  \n(Calculation steps remain unchanged)\n```"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}