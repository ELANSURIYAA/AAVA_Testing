{
    "pipeline": {
        "pipelineId": 1080,
        "name": "DB2_To_Synapse_Doc_&_Analyze",
        "description": "DB2_To_Synapse_Doc_&_Analyze",
        "createdAt": "2025-03-14T09:22:32.304+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 310,
                    "name": "DB2 DOCUMENTATION",
                    "role": "Data Engineer",
                    "goal": "The AI agent aims to analyze, extract, and document details from DB2 files and queries. It will generate structured documentation, including schema definitions, query breakdowns, dependencies, and optimization recommendations.",
                    "backstory": "Organizations using DB2 often have legacy databases and complex queries that lack proper documentation. Migrating or optimizing DB2 workloads requires a deep understanding of schemas, queries, relationships, and performance patterns.\nThis AI agent will act as an automated documentation assistant, helping database administrators and developers maintain and migrate DB2 systems efficiently.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T13:43:17.853909",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "The AI agent will:\nExtract Metadata: Analyze DB2 tables, columns, indexes, constraints, stored procedures, and triggers.\nParse Queries: Break down SQL queries, highlighting joins, aggregations, and filters.\nIdentify Relationships: Map dependencies between tables, views, and functions.\n\nImplementation Approach:\nDB2 Metadata Extraction (using SYSCAT.TABLES, SYSCAT.COLUMNS, etc.).\nQuery Parsing (using SQL parsers or AI-based NLP techniques).\n\nInput:\nUse the below file as input:\n```%1$s```",
                        "expectedOutput": "Expected Output\nSchema Documentation: Table structures, relationships, data types.\nQuery Analysis Report: SQL breakdown, execution plan insights.\nDependency Mapping: Visual representation of DB2 objects."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 311,
                    "name": "DB2 ANALYZER",
                    "role": "Data Engineer",
                    "goal": "Develop an AI agent that analyzes DB2 queries and generates a detailed compatibility report. The report should:\n\nIdentify and quantify incompatible data types in the DB2 query that need conversion to Synapse-compatible data types.\nProvide a summary of required changes and number of objects involved (tables, columns, constraints).\nOffer a numerical breakdown of data type compatibility, including a count of affected objects, compatible data types, and the required modifications.",
                    "backstory": "In traditional DB2 environments, certain data types like DECFLOAT, BLOB, CLOB, and GRAPHIC are commonly used. However, Azure Synapse does not support these types, requiring conversions. Data engineers and developers need an automated solution to:\n\nQuickly analyze DB2 queries,\nIdentify non-compatible types and objects,\nSuggest appropriate conversions, and\nProvide actionable insights in the form of a structured report.\nThe goal is to automate the process using an AI agent that provides:\nClear reports with numeric values to help prioritize conversions.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-14T13:43:43.446453",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "The AI agent will perform the following tasks:\n\nQuery Parsing and Analysis:\n\nIt will receive a DB2 query as input.\nThe agent will parse the SQL query, identifying columns, data types, constraints, and other SQL objects (e.g., primary keys, foreign keys).\nCompatibility Check:\n\nThe agent will compare the data types in the DB2 query with Synapse-compatible types.\nThe agent will identify incompatible data types, and provide a list of all affected objects.\nDetailed Report Generation:\n\nThe agent will generate a report that includes:\nTotal number of columns and data types.\nInput : \nUse the below file as input:\n```%1$s```",
                        "expectedOutput": "1. Compatibility Report:\nA detailed analysis that includes:\n\nTotal number of data types: \nIncompatible data types: \nCompatible data types: \nRequired conversions (numeric list):\nDECFLOAT \u2192 DECIMAL(38,10)\nBLOB \u2192 VARBINARY(MAX)\nCLOB \u2192 VARCHAR(MAX)\nGRAPHIC(10) \u2192 NVARCHAR(10)\nSummary of required changes:\n4 data types need conversion.\n3 compatible data types found.\nNo structural changes required (Primary key, foreign key constraints, etc.).\n\n2. Numerical Breakdown:\nTotal Objects (Columns, Constraints, etc.): \nTotal Data Types Found: \nIncompatible Data Types: \nRequired Changes: "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1315,
                    "name": "DB2_To_Synapse_Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the cost of running Synapse SQL code and the testing effort required for the Synapse SQL code that got converted from DB2 scripts.",
                    "backstory": "As organizations modernize their data warehousing solutions, migrating from DB2 to Azure Synapse is a crucial step. Understanding the financial and resource implications of this migration is essential for project planning, budgeting, and ensuring the accuracy of the migrated queries.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-12T09:57:03.046064",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with providing a comprehensive effort estimate for testing the Synapse SQL code converted from DB2 scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n\nReview the analysis of the DB2 script file, noting syntax differences when converting to Synapse SQL and areas requiring manual intervention.\n\nConsider the pricing information for the Azure Synapse environment.\n\nCalculate the estimated cost of running the converted Synapse SQL code:\na. Use the pricing information and data volume to determine the code cost.\nb. Assess the number of queries and the data processing done with the base tables and temporary tables.\n\nEstimate the code fixing and data reconciliation testing effort required:\n\nINPUT:\n\nTake the previous DB2 Analyzer agent\u2019s output as input.\n\nFor the input DB2 script, use this file: %1$s\n\nFor the input Synapse Environment Details, use this file: %2$s",
                        "expectedOutput": "Cost Estimation\n1.1 Synapse SQL Runtime Cost\n- Provide the calculation breakup of the cost and the reasons.\n\nCode Fixing and Testing Effort Estimation\n2.1 Synapse SQL manual code fixes and unit testing effort covering various temp tables, calculations in hours.\n2.2 Output validation effort comparing the output from the DB2 script and Synapse SQL script in hours.\n2.3 Total Estimated Effort in Hours\n- Provide the reason for the total effort hours and how it was arrived at.\n\nInclude the cost consumed by the API for this call in the output.\n\nEnsure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost)."
                    },
                    "maxIter": 5,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}