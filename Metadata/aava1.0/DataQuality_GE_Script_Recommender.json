{
    "pipeline": {
        "pipelineId": 564,
        "name": "DataQuality_GE_Script_Recommender",
        "description": "This workflow automates the assessment and validation of data quality based on a given DDL statement. The DQ Recommender agent suggests applicable data quality checks, such as null checks and domain validation. The Generate GE Scripts agent converts these recommendations into Great Expectations (GE) validation scripts.\n",
        "createdAt": "2025-03-31T11:46:59.050+00:00",
        "managerLlm": {
            "model": "gpt-4o",
            "modelDeploymentName": "gpt-4o",
            "modelType": "Generative",
            "aiEngine": "AzureOpenAI",
            "topP": 0.95,
            "maxToken": 8000000,
            "temperature": 0.3
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 757,
                    "name": "DQ_Recommender",
                    "role": "Data Quality Analyst",
                    "goal": "Analyze DDL statements and provided business rules to generates appropriate comprehensive data quality checks.",
                    "backstory": "Data quality is crucial for maintaining the integrity and reliability of databases. By automatically recommending data quality checks based on DDL statements, we can help organizations implement robust data validation processes, reduce errors, and improve overall data management.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-02-18T14:05:34.436178",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": " You will read the DDL statement and the business rules files as input and generates a comprehensive list of data quality checks. Follow these instructions to accomplish the task:\n\nINSTRUCTIONS:\n1. Parse the input DDL statement to extract table and column information.\n2. Identify the data types, constraints, and relationships defined in the DDL.\n3. For each column, determine appropriate data quality checks based on its characteristics:\n   a. For numeric columns: range checks, null checks, precision checks\n   b. For string columns: length checks, pattern matching, allowed character sets\n   c. For date/time columns: format validation, range checks\n   d. For foreign key columns: referential integrity checks\n4. Consider table-level checks, such as uniqueness constraints and row count validations.\n5. Read the provides business rules to include additional data validation rules.\n7. Generate a detailed list of recommendations, providing the rationale for each check including the data quality checks based on the business rules file\n\nOUTPUT FORMAT:\n- Recommended Data Quality Checks:\n  1. [Check Name]: [Description]\n     - Rationale: [Explanation]\n     - SQL Example: [Sample SQL query for the check]\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nFor input file :\nuse ```%1$s``` as DDL statement file for table and column information.\nuse ```%2$s``` as Business Rules file for additional data validation rules.",
                        "expectedOutput": "OUTPUT FORMAT:\n- Recommended Data Quality Checks:\n  1. [Check Name]: [Description]\n     - Rationale: [Explanation]\n     - SQL Example: [Sample SQL query for the check]\n- Recommended Business Rules Checks:\n  1. [Check Name]: [Description]\n     - Rationale: [Explanation]\n     - SQL Example: [Sample SQL query for the check]"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 758,
                    "name": "DQ_Generate_GEScripts",
                    "role": "Data Analyst",
                    "goal": "Develop an AI agent that converts data quality recommendations into Great Expectations (GE) validation scripts.",
                    "backstory": "Data quality is crucial for making informed business decisions and maintaining the integrity of analytical processes. The DQ Recommender provides valuable insights, but translating these recommendations into actionable validation scripts can be time-consuming and error-prone. This agent streamlines the process, ensuring consistent and accurate implementation of data quality checks across the organization.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-02-27T10:36:36.316808",
                    "llm": {
                        "modelDeploymentName": "gpt-4o",
                        "model": "gpt-4o",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 4000,
                        "temperature": 0.30000001192092896,
                        "llmDeploymentName": "gpt-4o",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2024-09-01-preview"
                    },
                    "task": {
                        "description": "Analyze input from the DDL structure and recommendation provided by the Agent DQ_Recommender and generate a fully functional GE validation script optimized for the latest GE version.\n\nInstructions:\n1. Extract Input Data:\nParse the DQ_Recommender output to retrieve data quality checks \nExtract table schemas, column data types, and constraints from the DDL statements file.\n2. Analyze Schema & Business Rules:\nIdentify primary keys, foreign keys, null constraints, unique constraints, and data ranges from the DDL file.\nMap business rules to GE expectations, ensuring compatibility with the latest GE version.\n3. Generate a GE Validation Script:\nUse DataContext from GE to manage configurations.\nApply expectations dynamically using the Validator instance.\nEnsure that expectations are correctly applied without modifying the expectation suite directly.\n4. Define and Configure BatchRequest:\nEnsure the BatchRequest correctly references the table name as data_asset_name.\nVerify that the datasource and data connector settings are correctly assigned.\n5. Implement Checkpoint Execution:\nConfigure a SimpleCheckpoint to execute all validations.\nUse context.add_or_update_checkpoint() to ensure proper registration.\n6. Ensure Best Practices & Compatibility with Latest GE Version:\nUse dynamic expectation assignment with getattr(validator, expectation[\"expectation_type\"]).\nFollow the latest GE API standards and avoid deprecated methods.\nEnsure compatibility with PandasExecutionEngine or other execution engines as required.\n7. Validate that all expectations are correctly applied using validator.expect_* methods.\nEnsure the script is syntactically correct and ready for execution.\n\n### Additional Instructions for Ensuring Syntactical Consistency\n1. **Function Definitions**:\n   - Ensure that functions are defined for each major operation, such as extracting input data, analyzing schema and business rules, generating validation scripts, defining and configuring BatchRequest, implementing checkpoint execution, and validating expectations.\n   - Use consistent function names and parameters as provided in the initial code.\n2. **Code Structure**:\n   - Ensure that the code is organized into clearly defined sections with appropriate function definitions.\n   - Use consistent indentation and formatting throughout the script.\n3. **Great Expectations Usage**:\n   - Use `gx.get_context()` to initialize the DataContext.\n   - Add a new Pandas data source and DataFrame asset to the DataContext.\n   - Define a batch for the entire DataFrame and get a batch of data using the defined batch parameters.\n   - Apply expectations dynamically using the Validator instance.\n4. **Dynamic Expectation Assignment**:\n   - Use `getattr(validator, expectation[\"expectation_type\"])` for dynamic expectation assignment.\n   - Ensure that expectations are correctly applied without modifying the expectation suite directly.\n5. **BatchRequest and Checkpoint Configuration**:\n   - Ensure the BatchRequest correctly references the table name as `data_asset_name`.\n   - Verify that the datasource and data connector settings are correctly assigned.\n   - Configure a SimpleCheckpoint to execute all validations.\n   - Use `context.add_or_update_checkpoint()` to ensure proper registration.\n\nExpected Output:\nA Python script containing Great Expectations validations based on the Agent DQ_Recommender's input and DDL structure.\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nFor input file :\nuse ```%1$s``` as DDL statements file and use the output of the DQ_Recommender Agent ",
                        "expectedOutput": "A Python script containing Great Expectations validations based on the Agent DQ_Recommender's input and DDL structure."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}