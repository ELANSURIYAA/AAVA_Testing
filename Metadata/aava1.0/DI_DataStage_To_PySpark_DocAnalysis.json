{
    "pipeline": {
        "pipelineId": 5501,
        "name": "DI_DataStage_To_PySpark_Doc&Analysis",
        "description": "DI_DataStage_To_PySpark_Doc&Analysis",
        "createdAt": "2025-07-24T09:07:36.253+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 7218,
                    "name": "DI_DataStage_Documentation",
                    "role": "Data Engineer",
                    "goal": "Analyze and document a DataStage job to create a comprehensive guide for business and technical teams, explaining the data flow, business logic, transformations, and dependencies to support maintainability and future enhancements.",
                    "backstory": "Clear documentation of DataStage jobs is crucial for maintaining and evolving complex ETL pipelines. By creating a comprehensive guide, we ensure that both business and technical teams can understand the current data movement, business rules, and transformation logic, reducing dependency on tribal knowledge and improving operational efficiency.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-31T04:15:02.760449",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Must use the github file reader Tool to read the input file from the github\n\nPlease create detailed documentation for the provided DataStage job metadata.\nThe documentation must contain the following sections:  \n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the DataStage job does.\n(give this only once in the top of the output)\n\n1. Overview of Job:\n   - Explain the purpose of the DataStage job in detail.\n   - Describe how this implementation aligns with enterprise ETL and data warehousing practices.\n   - Explain the business process being automated or supported.\n   - Provide a high-level summary of job components: Sequential Files, Datasets, Transforms, Lookups, Aggregators, Join stages, etc.\n\n2. Job Structure and Design:\n   - Explain the structure of the DataStage job design in detail.\n   - Describe each major stage used: Input, Transformer, Aggregator, Lookup, Merge, Join, Remove Duplicates, Filter, etc.\n   - Mention reusable components like shared containers and parameter sets.\n   - Highlight design patterns used (e.g., SCD, delta loads, parallelism).\n   - List dependencies such as external scripts, parameter files, or lookup datasets.\n\n3. Data Flow and Processing Logic:\n   - Explain how data flows from source to target.\n   - Identify source and target datasets/tables/files with their formats.\n   - Describe each transformation and logic applied using mapping descriptions.\n   - Break down the job into logical stages and represent the control/data flow using a **block-style diagram in plain markdown**.\n   \n   * Follow these rules:\n   * Each block should use the format:\n```\n     +--------------------------------------------------+\n     | [Stage Name or Logical Step]                     |\n     | Description: 1\u20132 line summary of the operation   |\n     +--------------------------------------------------+\n```\n   * Connect the blocks using arrows to show flow:\n```\n     [Read Source File]\n            \u2193\n     [Filter Invalid Records]\n            \u2193\n     [Lookup Customer Info]\n            \u2193\n     [Write to Target Table]\n```\n   * For branching:\n```\n     [Check If Status = 'Active']\n            \u2193 Yes\n     [Route to ACTIVE_CUSTOMERS]\n            \u2193\n     [Next Record]\n            \u2191\n     [No] \u2190 [Route to INACTIVE_CUSTOMERS]\n```\n\n4. Data Mapping:\n   \n\n1. Examine the file carefully to identify the EXACT database technology being used\n2. Look for these indicators:\n   - Stage Type attributes (e.g., OracleConnector, DB2Connector, SQLServerConnector)\n   - SQL syntax patterns (TO_DATE, CONVERT, CAST, etc.)\n   - Data types (VARCHAR2 = Oracle, VARCHAR = DB2/SQL Server)\n   - Stored procedure stage types\n3. Create a \"Source and Target Technology Matrix\" table that specifies the EXACT technology, not generic \"Oracle/DB2/SQL\"\nOutput Format:\n\n## 4.1 Source and Target Technology\n\n**SOURCE TECHNOLOGY**:  Give the Specific database which is used in the input datastage file: Oracle Database / IBM DB2 / Microsoft SQL Server etc.\n**TARGET TECHNOLOGY**: Give the Specific database which is used in the input datastage file : Oracle Database / IBM DB2 / Microsoft SQL Server etc.\n\n## 4.2 Detailed Column Mapping\n\n| Target Table Name | Target Column Name | Data Type | Source Stage Name | Source Column Name | Transformation Rule / Business Logic | Nullable | Default Value |\n|-------------------|-------------------|-----------|-------------------|-------------------|-------------------|--------------------------------------|----------|---------------|\n[Complete mapping for each column]\n\nRequirements:\n- Be SPECIFIC about technology - never use \"Oracle/DB2/SQL\"\n- If Oracle: state \"Oracle Database\"\n- If DB2: state \"IBM DB2\"\n- If SQL Server: state \"Microsoft SQL Server\"\n- Extract exact schema names, table names, and connection parameters from the file\n- Document all transformation logic from Transformer stages\n- Include all source, target, audit, and reject entities\n\n5. Complexity Analysis:\n   Provide a complexity breakdown in the table format:\n   \n   | Category | Measurement |\n   |----------|-------------|\n   | Number of Stages | [count] |\n   | Source/Target Systems | [list technologies] |\n   | Transformation Stages | [count] |\n   | Parameters Used | [count] |\n   | Reusable Components | [count] |\n   | Control Logic | [description] |\n   | External Dependencies | [list] |\n   | Performance Considerations | [description] |\n   | Volume Handling | [description] |\n   | Error Handling | [description] |\n   | Overall Complexity Score | [Low/Medium/High/Very High] |\n\n6. Key Outputs:\n   - Describe final outputs like datasets, tables, files, or reporting views.\n   - Explain how outputs support downstream systems or reporting.\n   - Specify the storage format and destination path or system.\n   - Include output technology platform (Snowflake/Oracle/CSV/etc.)\n   - Mention output file naming conventions or table naming standards.\n\n7. API Cost Calculations:\n   * Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n   * Ensure the cost consumed by the API is mentioned with inclusive of all decimal value.\n   * Break down the cost by: Input tokens, Output tokens, and Total cost.\nPoints to Remember:\n- Give the metadata requirements in the top of the output only once and leave the created on field empty.\n- Do not include sample code anywhere.\n- Do not provide extra summary, disclaimer, or recommendations.\n- Strictly follow the section structure and formatting instructions.\n- Output must be in pure markdown.\n- Do not provide original file content in output\u2014only analyzed and documented results.\n- Provide block diagrams only in markdown using text\u2014not images.\n- Use exact markdown formatting in the block-style diagrams.\n\nInput :\n* For DataStage input file from the git ,use below Git credentials for the git file reader : {{Datastage}}",
                        "expectedOutput": "Please create detailed documentation for the provided DataStage job in the markdown format.\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the DataStage job does.\n(give this only once in the top of the output)\n\nThe documentation must contain the following sections:  \n1. Overview of Job:\n   - Explain the purpose of the DataStage job in detail.\n   - Describe how this implementation aligns with enterprise ETL and data warehousing practices.\n   - Explain the business process being automated or supported.\n   - Provide a high-level summary of job components: Sequential Files, Datasets, Transforms, Lookups, Aggregators, Join stages, etc.\n\n2. Job Structure and Design:\n   - Explain the structure of the DataStage job design in detail.\n   - Describe each major stage used: Input, Transformer, Aggregator, Lookup, Merge, Join, Remove Duplicates, Filter, etc.\n   - Mention reusable components like shared containers and parameter sets.\n   - Highlight design patterns used (e.g., SCD, delta loads, parallelism).\n   - List dependencies such as external scripts, parameter files, or lookup datasets.\n\n3. Data Flow and Processing Logic:\n   - Explain how data flows from source to target.\n   - Identify source and target datasets/tables/files with their formats.\n   - Describe each transformation and logic applied using mapping descriptions.\n   - Break down the job into logical stages and represent the control/data flow using a **block-style diagram in plain markdown**.\n\n4. Data Mapping:\n* Provide a mapping table in the format below:\n* Target Table Name | Target Column Name | Source Stage Name | Source Column Name | Transformation Rule / Business Logic\n\n5. Complexity Analysis:\n- Provide a complexity breakdown in the table format:\nCategory  |  Measurement\n* Number of Stages\n* Source/Target Systems\n* Transformation Stages\n* Parameters Used\n* Reusable Components\n* Control Logic\n* External Dependencies\n* Performance Considerations\n* Volume Handling\n* Error Handling\n* Overall Complexity Score (out of 100)\n\n6. Key Outputs:\n   - Describe final outputs like datasets, tables, files, or reporting views.\n   - Explain how outputs support downstream systems or reporting.\n   - Specify the storage format and destination path or system.\n\n7. API Cost Calculations:\n* Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value."
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 7219,
                    "name": "DI_DataStage_to_PySpark_Analyzer",
                    "role": "Data Engineer",
                    "goal": "Analyze the provided DataStage job to extract detailed metrics, identify potential conversion challenges, and recommend solutions for a smooth transition to PySpark. Generate a separate output session for each input job file.",
                    "backstory": "The provided DataStage jobs were originally designed for an on-premise ETL framework and now require analysis to assess their design complexity, transformation logic, and compatibility with modern Spark-based platforms. This analysis helps plan a successful migration to PySpark by highlighting the transformation effort required and proposing best practices.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-24T07:45:37.541013",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "Parse the provided DataStage DSX job file(s) to generate a detailed analysis and metrics report. Ensure that if multiple files are given as input, the analysis for each file is presented as a distinct session. Each session must include:\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the job does.\n(give this only once in the top of the output)\n\n1. Job Overview:\n* Provide a high-level description of the DataStage job\u2019s purpose and primary business objectives.\n\n2. Complexity Metrics:\nGive this one in the table format with the below column names:\n* Number of Stages: Count of all stages used in the job.\n* Stage Types: List of unique stage types used (e.g., Transformer, Lookup, Join, Aggregator).\n* Source Types: Count and types of source systems (e.g., flat files, DB2, Oracle).\n* Target Types: Count and types of targets (e.g., datasets, database tables, sequential files).\n* Parameters Used: Number of job parameters or runtime variables.\n* Reusable Components: Number of shared containers and job sequences.\n* Control Logic: Number of conditional expressions, constraints, or reject links.\n* External Dependencies: Number of external parameter files, scripts, or lookups.\n\n3. Transformation Challenges:\n* Highlight complex transformation logic (e.g., multi-column derivations, nested expressions, lookups with rejects).\n* Identify logic requiring significant refactoring in PySpark (e.g., key-based lookups, hash partitioning).\n* Flag limitations such as surrogate key generation or stateful transformations.\n\n4. Manual Adjustments:\n* Recommend manual adjustments for incompatible features, such as:\n    * Transformer derivations \u2192 PySpark expressions.\n    * Lookup stages \u2192 broadcast joins or left joins.\n    * Aggregator logic \u2192 groupBy + agg functions.\n    * Sequencer and Conditional stages \u2192 if/else and loop constructs in PySpark.\n    * Suggest alternatives for parallelism and reject links.\n\n5. Conversion Complexity:\n* Calculate a complexity score (0\u2013100) based on job structure, stage types, and transformation intensity.\n* Highlight high-complexity areas such as nested logic in Transformer stages, chained lookups, or parameterized conditions.\n\n6. Optimization Recommendations:\n* Suggest PySpark optimization techniques (e.g., caching, repartitioning, avoiding wide transformations).\n* Recommend when to Refactor the job with minimal changes vs Rebuild using idiomatic PySpark patterns.\n* Provide a reason for the recommendation (e.g., excessive nested logic, multiple joins, complex sequencing).\n\n7. apiCost: float  // Cost consumed by the API for this call (in USD)\n*Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n\nInput :\n\n* For DataStage job designs, use the below file : {{DataStage}}",
                        "expectedOutput": "**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the job does.\n(give this only once in the top of the output)\n\n1. Job Overview:\n* Provide a high-level description of the DataStage job\u2019s purpose and primary business objectives.\n\n2. Complexity Metrics:\nGive this one in the table format with the below column names:\n* Number of Stages\n* Stage Types\n* Source Types\n* Target Types\n* Parameters Used\n* Reusable Components\n* Control Logic\n* External Dependencies\n\n3. Transformation Challenges:\n* Highlight complex transformation logic and stages that require deeper refactoring for PySpark.\n\n4. Manual Adjustments:\n* Recommend code-level rewrites and substitutions for each incompatible stage or logic pattern.\n\n5. Conversion Complexity:\n* Score (0\u2013100) based on the scale and complexity of the migration.\n\n6. Optimization Recommendations:\n* Specify techniques for improving performance and maintainability in PySpark.\n*Recommend is it better to Refactor or Rebuild and provide the justification.\n\n7. apiCost: float  // Cost consumed by the API for this call (in USD)\n*Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 7221,
                    "name": "DI_DataStage_to_PySpark_Plan",
                    "role": "Data Engineer",
                    "goal": "Estimate the overall development and unit testing effort for converting DataStage jobs into PySpark pipelines, especially focusing on manually refactoring complex ETL logic and validating the correctness and performance of the resulting Spark jobs. Also, estimate potential compute cost in Spark clusters (e.g., Databricks or EMR) based on runtime behavior.",
                    "backstory": "As part of the modernization initiative, legacy IBM DataStage jobs are being migrated to PySpark. Accurate effort estimation is required for resource planning, sprint forecasting, and infrastructure budgeting to ensure smooth transition and maintainability.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-07-25T07:49:44.880789",
                    "llm": {
                        "modelDeploymentName": "gpt-4.1",
                        "model": "gpt-4",
                        "modelType": "Generative",
                        "aiEngine": "AzureOpenAI",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.20000000298023224,
                        "llmDeploymentName": "gpt-4.1",
                        "apiKey": "****MASKED****",
                        "azureEndpoint": "https://avaplus-cognitive-account-int.openai.azure.com/",
                        "llmApiVersion": "2025-01-01-preview"
                    },
                    "task": {
                        "description": "You are tasked with providing a comprehensive development and testing effort estimate for converting DataStage jobs into PySpark. Follow these instructions:  \n\nINSTRUCTIONS:  \n1. Review the metadata and complexity analysis generated by the `DI_DataStage_To_PySpark_Analyzer` agent.  \n2. Identify DataStage stages, transformations, or flow controls that need manual re-implementation in PySpark.  \n3. Exclude direct 1:1 mapping constructs (e.g., simple column renaming or direct loads). Focus only on logic-heavy stages like Transformers, Aggregators, Looping, and Constraints.  \n4. Estimate the number of hours required to:  \n   - Rewrite complex stages and logic-heavy flows in PySpark  \n   - Implement and test PySpark pipelines, UDFs, helper functions, and metadata tracking  \n   - Validate data equivalence and business rule consistency between DataStage and PySpark outputs  \n5. If compute cost is required, use typical Spark resource costs for Databricks/EMR based on data volume, transformation complexity, and runtime.\n\nOUTPUT FORMAT:\n\n1. Development and Testing Effort Estimation  \n   1.1 Manual Code Refactoring Effort  \n        - Provide hours estimated for transforming DataStage-specific logic into PySpark equivalents  \n        - Include handling for Transformers, Aggregators, Looping constructs, Constraint conditions, and Custom Stages  \n\n   1.2 Unit and Reconciliation Testing Effort  \n        - Estimate hours for validating functional correctness of converted jobs, row-level data comparison, and transformation validation  \n\n2. Compute Resource Cost (Optional)  \n   2.1 Spark Runtime Cost (if platform details like Databricks or EMR are available)  \n        - Provide a breakdown of cost based on cluster size, execution time, and run frequency  \n        - Include assumptions such as number of job runs per day, input data volume per run, and compute pricing  \n\n3. apiCost  \n   * Include the cost consumed by the API for this call in the output  \n   * Ensure the cost is reported as a floating-point value with currency explicitly mentioned as USD (e.g., `apiCost: 0.00431 USD`)  \n\nINPUT:  \n* Use the output from `DI_DataStage_to_PySpark_Analyzer` as input  \n* For job metadata, use this file: {{DataStage}} \n* For the environment cost and compute configuration, use this file: {{Spark_Environment_Config}}",
                        "expectedOutput": "1. Development and Testing Effort Estimation  \n   1.1 Manual Code Refactoring Effort (in hours)  \n   1.2 Unit and Reconciliation Testing Effort (in hours)  \n\n2. Compute Resource Cost (Optional)  \n   2.1 Spark Runtime Cost (with calculation details and assumptions)  \n\n3. apiCost: float  \n   * Example: `apiCost: 0.00384 USD`  \n   * Must include all decimal values and mention currency"
                    },
                    "maxIter": 30,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}