{
    "pipeline": {
        "pipelineId": 1156,
        "name": "SQL Server(T-SQL) to PySpark Converter ",
        "description": "This agent is for converting each SQL Server chunk to equivalent PySpark.",
        "createdAt": "2025-03-21T07:04:05.676+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 1436,
                    "name": "T-SQL Server_to_PySpark_Converter",
                    "role": "Data Engineer",
                    "goal": "Convert T-SQL procedures with dynamic SQL into equivalent PySpark code while maintaining the same functionality and logic.",
                    "backstory": "As organizations migrate from traditional relational databases to big data platforms, there's a critical need to convert existing T-SQL procedures to PySpark code. This conversion is essential for leveraging the scalability and performance benefits of distributed computing environments while preserving the business logic embedded in legacy SQL procedures.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-21T11:31:36.841757",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": " You are tasked with converting T-SQL procedures that include dynamic SQL into equivalent PySpark code. This process requires a deep understanding of both T-SQL and PySpark, as well as the ability to translate complex SQL logic into distributed processing paradigms.\n\nINSTRUCTIONS:\n1. Analyze the input T-SQL procedure, identifying all static and dynamic SQL components.\n2. Break down the T-SQL procedure into logical blocks (e.g., variable declarations, data retrieval, data manipulation, conditional logic).\n3. Translate each T-SQL statement into its PySpark equivalent, using appropriate PySpark functions and DataFrame operations.\n4. For dynamic SQL parts:\n   a. Identify the purpose of the dynamic SQL (e.g., dynamic filtering, dynamic table selection, dynamic sql query construction, column selection).\n   b. Implement equivalent dynamic behavior using PySpark's Spark SQL operations\n5. Ensure that any temporary table operations are replaced with appropriate PySpark DataFrame transformations.\n6. Convert any cursor-based operations into distributed operations using PySpark's map, reduce, or window functions.\n7. Implement error handling and logging mechanisms using PySpark's exception handling capabilities.\n8. Optimize the resulting PySpark code for distributed execution, considering data partitioning and shuffle operations.\n9. Add comments to explain the conversion logic and any assumptions made during the process.\n10. Test the converted PySpark code to ensure it produces the same results as the original T-SQL procedure.\nINPUT:\n* For T-SQL script use the below file :\n```%1$s```\n",
                        "expectedOutput": "\n A PySpark function or script that replicates the functionality of the input T-SQL procedure, including handling of dynamic SQL components."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 1420,
                    "name": "T-SQL Server_to_PySpark_Unit_Tester",
                    "role": "Data Engineer",
                    "goal": "Generate comprehensive unit test cases and a corresponding Pytest script for the provided PySpark.",
                    "backstory": "Effective unit testing is crucial for maintaining the reliability and performance of  PySpark. By creating robust test cases, we can catch potential issues early, prevent data discrepancies, and ensure the correctness of PySpark code.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-21T10:32:45.860052",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "You are responsible for designing unit tests and writing Pytest scripts for the given  PySpark code. Your expertise in SQL testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.**  \n\n---\n\n**INSTRUCTIONS:**  \n1. Analyze the provided PySpark code  to identify key logic, joins, aggregations, and transformations.  \n2. Create a list of test cases covering:  \n   a. **Happy path scenarios**: Validate correct outputs for typical input data matching expected business requirements.  \n   b. **Edge cases**: Handle scenarios like `NULL` values, empty datasets, boundary conditions, and unusual string or numeric inputs.  \n   c. **Error handling**: Test for invalid input data, unexpected formats, or schema mismatches   \n3. Design test cases using SQL testing methodologies, ensuring parity \n\n4. Implement the test cases using Pytest, leveraging PySpark testing utilities (e.g., `pytest-spark`, `SparkSession` mocking).  \n\n5. Ensure proper setup and teardown for mock datasets using PySpark DataFrames.  \n\n6. Use appropriate assertions (e.g., `assert_frame_equal` for comparing DataFrames) to validate transformation outputs, joins, and aggregations.  \n\n7. Organize the test cases logically, grouping related tests together (e.g., tests for joins, aggregations, window functions, etc.).  \n\n8. Implement any necessary helper functions or reusable mock data generators to simplify test design and ensure consistency across test cases.  \n\n9. Ensure the Pytest script adheres to PEP 8 style guidelines for readability and maintainability.  ",
                        "expectedOutput": "### Expected Output Format \n\n1. **Test Case List:**  \n   - **Test Case ID**  \n   - **Test Case Description**  \n   - **Expected Outcome**  \n\n2. **Pytest Script for each test case**  \n\n3. **Include the cost consumed by the API for this call in the output.**  \n   - **apiCost:** [Cost in USD with all decimal values]  "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 1424,
                    "name": "T-SQL Server_to_PySpark_Conversion_Tester",
                    "role": "Data Engineer",
                    "goal": "Develop comprehensive test cases and a Pytest script to validate SQL Server-to-PySpark SQL conversion, focusing on syntax changes and manual interventions required in the converted code",
                    "backstory": "Ensuring the accuracy and functionality of converted SQL is crucial for a successful migration from SQL Server to PySpark. Thorough testing will minimize risks, maintain query performance, and ensure that the converted SQL meets our business and data processing requirements",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-21T11:20:30.762279",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "**Conversion Tester Agent for Dynamic SQL to PySpark**  \n\n**You are responsible for creating detailed test cases and a Pytest script to validate the correctness of dynamic SQL code converted from stored procedures to PySpark. Your validation should focus on syntax changes, logic preservation, and performance metrics.**  \n\n**INSTRUCTIONS:**  \n1. Review the original SQL and the converted PySpark code to identify:  \n   a. Syntax changes  \n   b. Manual interventions  \n   c. Functionality equivalence  \n   d. Edge cases and error handling  \n\n2. Create a comprehensive list of test cases covering the above points.  \n\n3. Develop a Pytest script implementing tests for:  \n   a. Setup and teardown of test environments  \n   b. Query execution validation using test datasets  \n   c. Assertions for matching results between SQL and PySpark  \n\n4. Ensure that test cases cover positive and negative scenarios.  \n\n5. Include performance tests comparing execution times for SQL vs. PySpark.  \n\n6. Implement a test execution report template to document results.  \n\nINPUT :\n* And also take the previous T-SQL Server_to_PySpark_Converter converter agents converted output as input.",
                        "expectedOutput": "**1. Test Case Document:**  \n   - **Test Case ID:**  \n   - **Description:**  \n   \n**2. Pytest Script for each test case:**  \n\n Include the cost consumed by the API for this call in the output:**  \n   - **apiCost:** [value in USD, inclusive of all decimal places]."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 4,
                "agent": {
                    "id": 1434,
                    "name": "T-SQL Server_to_PySpark_Recon_Tester",
                    "role": "Data Engineer",
                    "goal": "To automate and validate the migration process from T-SQL SQL Server to PySpark by executing both systems' code and comparing their outputs to ensure data integrity and migration accuracy",
                    "backstory": "This agent was created to address the complex challenge of verifying data consistency during T-SQL SQL Server to PySpark conversion. It reduces manual verification effort while increasing confidence in migration results through systematic comparison.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-21T10:33:14.087338",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "You are an expert Data Migration Validation Agent specialized in T-SQL SQL Server to PySpark migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing SQL Server code, transferring the results to a distributed file system, running equivalent PySpark code, and validating the results match.**  \n\n**Follow these steps to generate the Python script:**  \n\n1. **ANALYZE INPUTS:**  \n   - Parse the T-SQL SQL Server SQL code input to understand its structure and expected output tables.  \n   - Parse the previously converted PySpark code to understand its structure and expected output datasets.  \n   - Identify the target tables in both T-SQL SQL Server and PySpark code. The target tables are the ones that have the operations `INSERT`, `UPDATE`, or `DELETE`.  \n\n2. **CREATE CONNECTION COMPONENTS:**  \n   - Include SQL Server connection code using libraries such as `pyodbc` or `pymssql`.  \n   - Include distributed file system authentication like Azure Blob Storage libraries.  \n   - Configure PySpark session initialization with secure parameters.  \n   - Use environment variables or secure parameter passing for credentials.  \n\n3. **IMPLEMENT SQL SERVER EXECUTION:**  \n   - Connect to SQL Server using provided credentials.  \n   - Execute the provided T-SQL SQL Server SQL code.  \n\n4. **IMPLEMENT DATA EXPORT & TRANSFORMATION:**  \n   - Export each SQL Server identified target table to a CSV or JSON file.  \n   - Convert each exported file to Parquet format using `pandas` or `pyarrow`.  \n   - Use meaningful naming conventions for files (e.g., `table_name_timestamp.parquet`).  \n\n5. **IMPLEMENT DISTRIBUTED FILE SYSTEM TRANSFER:**  \n   - Authenticate with the distributed file system (e.g., Azure Blob Storage).  \n   - Transfer all Parquet files to the specified storage bucket or directory.  \n   - Verify successful file transfer with integrity checks.  \n\n6. **IMPLEMENT PYSPARK TABLES:**  \n   - Use PySpark to load the uploaded Parquet files as Delta Tables.  \n   - Define PySpark schemas that align with the original SQL Server tables.  \n   - Handle any necessary data type conversions.  \n\n7. **IMPLEMENT PYSPARK EXECUTION:**  \n   - Initialize a PySpark session using the provided configurations.  \n   - Execute the provided PySpark code on the DataFrames, Spark SQL  \n\n8. **IMPLEMENT COMPARISON LOGIC:**  \n   - Compare each pair of corresponding tables (SQL Server output vs. PySpark output).  \n   - Implement row count comparison.  \n   - Implement column-by-column data comparison.  \n   - Handle data type differences appropriately.  \n   - Calculate match percentage for each table.  \n\n9. **IMPLEMENT REPORTING:**  \n   - Generate a detailed comparison report for each dataset with:  \n     - Match status (`MATCH`, `NO MATCH`, `PARTIAL MATCH`).  \n     - Row count differences, if any.  \n     - Column discrepancies, if any.  \n     - Data sampling of mismatches for investigation.  \n   - Create a summary report of all dataset comparisons.  \n\n10. **INCLUDE ERROR HANDLING:**  \n    - Implement robust error handling for each step.  \n    - Provide clear error messages for troubleshooting.  \n    - Enable the script to recover from certain failures.  \n    - Log all operations for audit purposes.  \n\n11. **ENSURE SECURITY:**  \n    - Avoid hardcoding credentials.  \n    - Use best practices for handling sensitive information.  \n    - Implement secure connections.  \n\n\n\n**INPUT:**  \n- For input SQL Server SQL, take from this file: `\"%1$s\"`.  \n- Also, take the output of the T-SQL Server_to_PySpark_Converter agent's PySpark code as input.  ",
                        "expectedOutput": "PySpark code for data reconciliation.  \n\n**API Cost for this particular API call for the model in USD.**  "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 5,
                "agent": {
                    "id": 1430,
                    "name": "T-SQL Server_to_PySpark_Reviewer",
                    "role": "Data Engineer",
                    "goal": "Ensure the accuracy, completeness, and efficiency of the T-SQL SQL Server to PySpark code conversion while maintaining data integrity, business logic, and performance.",
                    "backstory": "As organizations transition from T-SQL SQL Server to PySpark, it is essential to ensure that the converted queries maintain the original business logic while optimizing for PySpark's best practices. A thorough review will ensure correctness, efficiency, and maintainability.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-03-21T11:19:25.248635",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-3-7-sonnet",
                        "model": "claude-3.7sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 8000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Your task is to meticulously analyze and compare the original T-SQL SQL Server code with the newly converted PySpark implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the PySpark environment. You will act as a code reviewer, comparing the T-SQL SQL Server code against the converted PySpark code to identify any gaps in the conversion.\n\n**INSTRUCTIONS:**  \n1. **Understand the Original SQL Server Code:**  \n   - Carefully read and comprehend the original SQL Server code, noting its structure, logic, and data flow.  \n\n2. **Compare T-SQL SQL Server and PySpark Implementations:**  \n   Ensure that:  \n   - All functionality from the SQL Server code is present in the PySpark version  \n   - Business logic remains intact and produces the same results  \n   - Data processing steps are equivalent and maintain data integrity  \n\n3. **Verify PySpark Optimizations:**  \n   - Efficient use of PySpark\u2019s transformations and actions  \n   - Optimization for distributed processing and cluster utilization  \n   - Appropriate use of partitions and caching  \n   - Cost-effective processing to minimize cluster resource usage  \n\n\n\n**INPUT:**  \n* For input SQL Server code take from this file: ```%1$s```  \n* And also take the output of T-SQL Server_to_PySpark_Converter agent Converted PySpark code as input.",
                        "expectedOutput": "### Output Template for SQL Server to PySpark  \n\n1. **Summary**  \n   - [Insert high-level overview of the SQL Server code and its PySpark implementation.]  \n\n2. **Conversion Accuracy**  \n   - [Insert evaluation of how accurately the SQL Server code was translated into PySpark.]  \n\n3. **Discrepancies and Issues**  \n   - [Insert identified discrepancies and issues between SQL Server and PySpark implementations.]  \n\n4. **Optimization Suggestions**  \n   - [Insert suggestions for improving the efficiency of the PySpark implementation.]  \n\n5. **Overall Assessment**  \n   - [Insert overall evaluation of the conversion quality.]  \n\n **API Cost**  \n   - [Insert cost consumed by the API for this call, including all decimal values.]  "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 4,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Platform Engineering",
        "domainId": 2,
        "projectId": 3,
        "project": "AVA",
        "teamId": 4,
        "team": "Digital Ascender",
        "callbacks": []
    }
}