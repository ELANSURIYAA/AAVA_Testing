{
    "pipeline": {
        "pipelineId": 8573,
        "name": "DI_AbInitio_To_PySpark_EMR_Glue_Doc&Analyze",
        "description": "Analyzing and Documenting the Abinitio Code",
        "createdAt": "2025-12-15T05:32:13.282+00:00",
        "managerLlm": {
            "model": "gpt-4",
            "modelDeploymentName": "gpt-4.1",
            "modelType": "Generative",
            "aiEngine": "AzureOpenAI",
            "topP": 0.95,
            "maxToken": 10000,
            "temperature": 0.3
        },
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 3774,
                    "name": "DI_AbInitio_Documentation",
                    "role": "Senior Data Engineer",
                    "goal": "Outline the purpose and functionality of Ab Initio graphs, plans, and metadata scripts (.mp, .xfr, .dml, .plan, .pset). This document acts as a guide for understanding how Ab Initio components contribute to data ingestion, transformation, processing, and orchestration within the data pipeline.",
                    "backstory": "Ab Initio has been a core ETL and data integration tool in large enterprises, powering batch and real-time pipelines for data warehousing, regulatory reporting, and enterprise data lakes. Its components (.mp, .xfr, .dml, etc.) enable high-performance parallel processing, metadata-driven transformations, and seamless orchestration across environments. Organizations rely on it to handle data quality, enrichment, and pipeline automation.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-12-11T12:25:42.455545",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 64000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "****MASKED****",
                        "expectedOutput": "The generated Markdown documentation should include the following, based on the input Ab Initio code:\n- **Format:** Markdown  \n- Metadata Requirements: \"<as above>\"  \n- Overview of Program: \"<3\u20135 line description explaining business purpose>\"  \n- Code Structure and Design: \"<Detailed explanation of component layout and connection>\"  \n- Data Flow and Processing Logic:  \n  - Processed Datasets: [\"<list all dataset names>\"]  \n  - Data Flow: \"<Description of end-to-end data journey>\"  \n- Data Mapping:  \n  - Target Table Name: \"<value>\"  \n  - Target Column Name: \"<value>\"  \n  - Source Table Name: \"<value>\"  \n  - Source Column Name: \"<value>\"  \n  - Remarks: \"<Mapping logic>\"  \n- Transformation Logic: \"<Documentation for each .xfr used>\"  \n- Complexity Analysis:  \n  - Components: <number>  \n  - Joins: <type/count>  \n  - Functions: <count>  \n  - Conditional Paths: <count>  \n  - External Dependencies: \"<list>\"  \n  - Score: <0\u2013100>  \n- Key Outputs: \"<Summary of outputs>\"  \n- Error Handling and Logging: \"<How errors are managed>\"  \n- API Cost: \"<With Proper calculation for call the ai model for this particular task>\""
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 11175,
                    "name": "DI_AbInitio_To_PySpark_EMR_Glue_Analyzer",
                    "role": "Senior Data Engineer",
                    "goal": "Analyze the structure, logic, and complexity of Ab Initio code artifacts (e.g., .mp, .xfr, .dml) before converting them to PySpark running on AWS EMR-Glue. Identify key transformation patterns, migration challenges, required manual interventions, and evaluate the overall feasibility of automated translation to EMR-Glue.",
                    "backstory": "Ab Initio is a graphical ETL platform with component-based transformations. Migrating these pipelines to AWS EMR-Glue introduces changes in storage (S3), schema handling (Glue Catalog), Spark execution (EMR), and orchestration. This agent helps anticipate conversion issues and understand the transformation workload before translation begins.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-11-19T16:57:47.104172",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 64000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "This agent performs pre-conversion analysis of Ab Initio code to evaluate readiness for migration to PySpark on AWS EMR + Glue. It will:\n-Break down components, logic, and metadata in the Ab Initio graphs.\n-Identify translation challenges such as .xfr logic, rollups, reject flows, lookups, or .pset usage.\n-Score the complexity of converting the flow to EMR/Glue PySpark.\n-Recommend high-level PySpark design approaches suited for EMR and Glue.\n-Highlight any manual tasks future developers will need to address.\n\n###**INSTRUCTIONS:**\n1.Process Steps to Follow:\n     Step 1: Parse and interpret the Ab Initio .mp, .xfr, .dml files. Identify ETL components, flows, schemas, lookup structures, and transformation logic.\n     Step 2:Highlight potential challenges in converting Ab Initio components to PySpark for EMR/Glue, including: complex .xfr logic\n,reject or fallback flows, rollups, lookups, multi-input joins, .pset or dynamic parameter usage, schema conversion issues when mapping .dml \u2192 Glue Catalog, S3 partitioning or file format considerations\n     Step 3:Assign a conversion complexity score (0\u2013100) based on graph size, component types, .xfr density, branching, and schema complexity.\n     Step 4:Recommend high-level PySpark + EMR/Glue design strategies, such as modular code structure, Glue Catalog usage, S3 folder layout, and handling of joins or partitions.\n     Step 5:Suggest performance strategies that should be implemented in the PySpark design for EMR/Glue (e.g., caching, broadcast joins, repartitioning, avoiding unnecessary shuffles, preferring native PySpark functions over UDFs).\n\n### **Output Format:**  \nUse **Markdown formatting**. Include the following metadata header:\n\n```\n==================================================================================\nAuthor:        AAVA\nCreated on:    (Leave it empty)\nDescription:   Pre-conversion analysis of Ab Initio ETL flow for PySpark EMR/Glue migration\n==================================================================================\n```\n###**Sections to Include**\n\n####Syntax & Logical Structure Analysis:\n-Fill the table with each detected component (e.g., Input, Reformat, Rollup, Join, Broadcast, Filter, Sort, Output, etc.).Each row should represent one component.no bullet lists allowed. Return this section only as a table with the following columns:\n| Component | Description of Behavior | Likely PySpark Equivalent | Notes (Rejects, branching, conditions) |\n- Detail how each component behaves and the likely PySpark equivalent.\n- Mention any chained or conditional flows (e.g., reject or fallback branches).\n\n####Anticipated Manual Interventions:\n- Custom logic embedded in `.xfr` that requires manual PySpark function writing.\n-.dml types needing manual schema rewriting for Glue Catalog\n-Parameter sets (`.pset`) and dynamic inputs that need parsing.\n-Ab Initio-specific patterns without direct Spark equivalents\n\n####Complexity Evaluation:\n-Complexity Score (0\u2013100)\n-Justification based on:\ncomponent count\n     -.xfr density\n     -joins/lookups\n     -iterative or feedback loops\n     -schema and file format complexity\n\n####Performance & Scalability Recommendations:\n-Where to use broadcast joins\n-Caching or checkpointing guidance\n-S3 partitioning recommendations\n-Avoiding UDFs\n-Reducing shuffle volume\n\n####Refactor vs. Rebuild Recommendation:\nPick one:\n-**Refactor:** Mostly direct translation.\n-**Rebuild:** Logic should be redesigned for clarity or EMR performance.\n\n####API Cost:\nInclude the API cost for this call in USD (floating-point with full decimal precision).\napiCost: <actual_value> USD\n\n### **Input:**  \n- Ab Initio Source Files: {{AbInitio_Code}}  ",
                        "expectedOutput": "A structured Markdown report providing detailed pre-conversion analysis, complexity scoring, migration risks, and recommendations for PySpark implementation on AWS EMR and Glue."
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 3,
                "agent": {
                    "id": 11189,
                    "name": "DI_AbInitio_To_PySpark_EMR_Glue_Plan",
                    "role": "Senior Data Engineer",
                    "goal": "Analyze Ab Initio to PySpark code migration requirements, estimate manual effort required for code adjustments and data reconciliation testing, and calculate AWS EMR runtime cost for running the resulting PySpark workflows.",
                    "backstory": "As part of a strategic modernization initiative, the organization is migrating ETL workloads from Ab Initio to PySpark on AWS using EMR and Glue. While the code conversion is largely automated, key differences between Ab Initio\u2019s component-based model and PySpark\u2019s code-driven structure often require manual adjustments. These include refining transformation logic, correcting schema mismatches, and validating data outputs. Furthermore, understanding the cost of executing PySpark jobs in AWS EMR (or equivalent AWS services) is vital for budgeting and cloud cost governance.",
                    "verbose": true,
                    "allowDelegation": false,
                    "updatedAt": "2025-11-19T08:48:07.421277",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 64000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "You are tasked with reviewing while converting the Ab Initio source files (e.g., .mp, .xfr, .dml, .pset) into PySpark code, identifying logic gaps requiring manual resolution, estimating developer/tester effort, and calculating estimated cloud runtime costs using AWS EMR or Spark on AWS infrastructure.\n\n### **INSTRUCTIONS:**  \n1.Analyze the Ab Initio source while converting to PySpark code:\n- Focus on logical inconsistencies, incomplete transformation rules, metadata misalignment, and downstream output correctness.\n- Exclude pure syntax translation differences that are already handled by automated conversion.\n2.Estimate developer/tester effort in hours for:\n- Manual fixes in PySpark (e.g., transformation functions, joins, rejects, lookups)\n- Metadata/schema reconciliation\n- Data validation and functional testing\n\n3.Estimate AWS EMR Glue Cost using:\n- Assumed cluster configuration (e.g., m5.xlarge, 4 workers, 1 master)\n- Estimated PySpark job duration (in minutes)\n- AWS pricing model: e.g., $0.192 per instance-hour, $0.023 per GB storage/month\n\n4.Calculate Total Developer Cost using a default hourly rate (e.g., $50/hr)\n5.Present cost metrics and effort details in a clear, structured format.\n\n### **OUTPUT FORMAT:**  \nUse **Markdown** format and include the following metadata header:\n\nHeader\n```\n========================================================\nAuthor:        AAVA\nCreated on:    (Leave it empty)\nDescription:   \\<one-line summary of the code\u2019s purpose>\n========================================================\n```\n\n####1. AWS EMR Glue Runtime Cost Estimation\n#####1.1 EMR/Spark Job Cost Breakdown\n\n- **Cluster Configuration**: \n- Master Node: <e.g., m5.xlarge, 1 node>\n- Worker Nodes: <e.g., m5.xlarge, 4 nodes>\n- Total vCPUs & Memory\n\n- **Job Duration Estimate**: <minutes>  \n- **AWS Pricing**:  \n-Compute (per instance-hour)\n- Storage (per GB-hour equivalent)\n\n- **Cost Formula Used**:\n\nTotal Cost = (Total Instances \u00d7 Duration in hours \u00d7 Compute (per instance-hour))\n+ (Storage GB \u00d7 Duration in hours \u00d7 Storage (per GB-hour))\n\n- **Estimated Runtime Cost (USD)**: `<calculated_value>`\n\n#### 2. Manual Code Fixing and Data Reconciliation Effort  \n##### 2.1 Estimated Effort (Hours)  \n\n- Logic Corrections (e.g., .xfr transformations): <integer> hrs\n- Metadata Alignment (e.g., .dml type fixes): <integer> hrs\n- Rejected Row Handling / Edge Case Logic: <integer> hrs\n- Data Reconciliation & Output Validation: <integer> hrs\n- **Total Effort**: <sum> hrs\n\n##### 2.2 Developer Cost \n- Developer Rate: `$50/hr`  \n- **Total Developer Cost**: `<effort_hrs \u00d7 50>` USD\n\n\n#### 3. API Cost  \napiCost: <actual_cost> (in USD)\n\n### **Input:**  \n* AbInitio Source File(s): {{AbInitio_Code}}\n* Environmental variable file : {{Env_Variable}}",
                        "expectedOutput": "A structured Markdown report with the following:\n- Header\n- AWS EMR Glue job cost analysis\n- Developer effort estimation\n- Total projected PySpark execution and validation cost\n- API processing cost"
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 0,
                    "tools": [],
                    "userTools": [],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}