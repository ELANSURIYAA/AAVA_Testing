{
    "pipeline": {
        "pipelineId": 7737,
        "name": "DI_UnitTesting_Tsql",
        "description": "Develop a robust, automated framework for unit testing T-SQL scripts. The framework must validate query logic, ensure data integrity, and verify that outputs match predefined test cases. The solution should be designed to enhance code quality and reliability, enabling streamlined, error-free deployment of database code.\n",
        "createdAt": "2025-10-27T05:54:39.203+00:00",
        "pipeLineAgents": [
            {
                "serial": 1,
                "agent": {
                    "id": 9736,
                    "name": "DI_OptimizeTSQLScript",
                    "role": "Data Engineer",
                    "goal": "Optimize and consolidate all T-SQL quality checks conducted by the previous agent to ensure production readiness. Use the previous agent's input as a foundation to develop efficient, reliable, and production-ready T-SQL scripts.",
                    "backstory": "Ensuring the quality and reliability of T-SQL scripts is critical for maintaining data integrity, performance, and compliance in production environments. The previous agent has performed various quality checks on T-SQL scripts, but these checks may be fragmented, redundant, or not fully optimized for production deployment. Consolidating and optimizing these checks into a single, efficient, and production-ready set of T-SQL scripts will streamline deployment, reduce risk, and ensure ongoing data quality.",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-14T13:19:28.965525",
                    "llm": {
                        "modelDeploymentName": "Anthropic.claude-4-sonnet",
                        "model": "anthropic.claude-4-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 24000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-20250514-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "embedding": [
                        {
                            "aiEngine": "AzureOpenAI",
                            "chroma_end_point": "http://chromadb.da.svc.cluster.local",
                            "chroma_port": "80",
                            "index_collection": "tSQLStandardsKb",
                            "embedding_model": "text-embedding-ada-002",
                            "embedding_deployment_name": "ava-text-embedding-ada-002",
                            "embedding_api_version": "2024-09-01-preview",
                            "embedding_api_key": "****MASKED****",
                            "embedding_azure_endpoint": "https://da-cognitive-account-demo.openai.azure.com/"
                        }
                    ],
                    "task": {
                        "description": "Your task is to take the outputs of the previous agent\u2014comprising all T-SQL quality checks\u2014and optimize, deduplicate, and consolidate them into a single, cohesive set of production-ready T-SQL scripts. You must ensure these scripts are efficient, reliable, and adhere to best practices for production environments.\n\nINSTRUCTIONS:\n\n**STEP 1: INPUT ANALYSIS**\n- Review the previous agent's output containing all T-SQL quality checks.\n- Identify all unique checks, including but not limited to: data validation, referential integrity, null checks, data type enforcement, business rule validation, and performance-related checks.\n- Note any redundant, overlapping, or conflicting checks.\n\n**STEP 2: CONSOLIDATION & OPTIMIZATION**\n- Remove duplicate or overlapping checks, ensuring each quality rule is represented only once.\n- Refactor checks for efficiency:\n  - Use set-based operations instead of cursors or row-by-row processing where possible.\n  - Combine related checks into single queries or procedures for maintainability.\n  - Ensure all scripts are idempotent and can be safely re-run.\n- Standardize naming conventions, error handling, and logging mechanisms.\n- Parameterize scripts where applicable for reusability and flexibility.\n\n**STEP 3: PRODUCTION READINESS ENHANCEMENTS**\n- Add comprehensive error handling and transaction management to prevent partial updates or data corruption.\n- Implement logging for failed checks, including timestamp, error details, and affected records.\n- Ensure scripts are compatible with the target SQL Server version and follow organizational security and performance best practices.\n- Include comments and documentation within the scripts for clarity and maintainability.\n\n**STEP 4: OUTPUT STRUCTURING**\n- Organize the final scripts into logical sections:\n  1. **Pre-Check Setup**: Variable declarations, temp tables, configuration.\n  2. **Quality Checks**: Each check clearly labeled and documented.\n  3. **Error Handling & Logging**: Centralized error capture and reporting.\n  4. **Summary Reporting**: Output summary of all checks and their results.\n- Provide a summary table (in markdown) listing each quality check, its purpose, and its status (optimized/merged/new/removed).\n\n**STEP 5: VALIDATION**\n- Review the consolidated scripts for completeness and correctness.\n- Ensure all original quality rules are represented and optimized.\n- Validate scripts for syntax, performance, and logical correctness.\n\n**OUTPUT FORMAT:**\n\n- **T-SQL Scripts**:  \n  - Format: Plain text, with clear section headers and inline documentation.\n  - Structure: As per Step 4 above.\n  - Quality: Must be production-ready, efficient, and maintainable.\n\n**QUALITY CRITERIA:**\n- No redundant or duplicate checks.\n- All scripts are idempotent and safe for production use.\n- Clear, maintainable, and well-documented code.\n- Comprehensive error handling and logging.\n- All original quality rules are accounted for.\n\n**KNOWLEDGE BASE**\n-Added knowledge base for better understanding on TSQL Optimization \n\n**INSTRUCTION FOR GITHUB TOOLS:**\n1.Read input from previous agent \"DI_ Create_T-SQLDQRules\"\n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_OptimiseTSQLScript\"\nInput\n{{github_credintials}} -for the user github credentials use this input from user\n",
                        "expectedOutput": "A single, production-ready T-SQL script (with inline documentation) "
                    },
                    "maxIter": 0,
                    "maxRpm": 0,
                    "maxExecutionTime": 300,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            },
            {
                "serial": 2,
                "agent": {
                    "id": 10045,
                    "name": "DI_UnitTesting_Tsql",
                    "role": "Automation Test Engineer",
                    "goal": "Develop a robust, automated framework for unit testing T-SQL scripts. The framework must validate query logic, ensure data integrity, and verify that outputs match predefined test cases. The solution should be designed to enhance code quality and reliability, enabling streamlined, error-free deployment of database code.\n",
                    "backstory": "T-SQL scripts are foundational to data-driven applications, powering business logic, reporting, and integrations. Manual testing of these scripts is time-consuming, error-prone, and often incomplete, leading to defects in production and costly rollbacks. An automated unit testing approach ensures that T-SQL code is reliable, maintainable, and meets business requirements before deployment, reducing risk and accelerating release cycles.\n",
                    "verbose": true,
                    "allowDelegation": true,
                    "updatedAt": "2025-10-27T06:50:16.243997",
                    "llm": {
                        "modelDeploymentName": "anthropic.claude-4_5-sonnet",
                        "model": "anthropic.claude-4-5-sonnet",
                        "modelType": "Generative",
                        "aiEngine": "AmazonBedrock",
                        "topP": 0.949999988079071,
                        "maxToken": 24000,
                        "temperature": 0.30000001192092896,
                        "bedrockModelId": "us.anthropic.claude-sonnet-4-5-20250929-v1:0",
                        "region": "us-east-1",
                        "accessKey": "****MASKED****",
                        "secretKey": "****MASKED****"
                    },
                    "task": {
                        "description": "Your task is to design and implement an automated unit testing approach for T-SQL scripts. The solution must systematically validate query logic, check data integrity, and compare actual outputs to expected results using predefined test cases. The process should be modular, repeatable, and easily extensible for future scripts.\n\nINSTRUCTIONS:\n\n**STEP 1: TEST CASE DEFINITION**\n- Receive the input script(s)  from previous agent.\nFor each T-SQL script to be tested, define a set of unit test cases.\n- Each test case must specify:\n  - Test case name/ID\n  - Input data (table state before execution)\n  - Expected output/result set\n  - Any expected changes to data (for DML scripts)\n  - Cleanup/rollback requirements\n\n**STEP 2: TEST HARNESS DESIGN**\n- Choose or implement a test harness (e.g., tSQLt, custom stored procedures, or PowerShell scripts).\n- The harness must:\n  - Set up test data in isolated schemas or temporary tables.\n  - Execute the T-SQL script under test.\n  - Capture outputs (result sets, table states, error codes).\n  - Compare outputs to expected results using assertions.\n  - Clean up test data after execution.\n\n**STEP 3: AUTOMATED EXECUTION**\n- Integrate the test harness with a CI/CD pipeline (e.g., Azure DevOps, GitHub Actions).\n- Ensure tests run automatically on code check-in or pull requests.\n- Capture and report test results in standard formats (e.g., JUnit XML, Markdown summary).\n\n**STEP 4: DATA INTEGRITY VALIDATION**\n- For each test, verify:\n  - No unintended data modifications occurred.\n  - Referential integrity is maintained.\n  - All constraints (PK, FK, unique, check) are respected.\n\n**STEP 5: RESULT VERIFICATION**\n- For SELECT queries, compare actual result sets to expected data (row-by-row and column-by-column).\n- For DML scripts, verify table state matches expectations post-execution.\n- For error cases, confirm that expected error codes/messages are raised.\n\n**STEP 6: REPORTING**\n- Generate a detailed test report including:\n  - Test case ID and description\n  - Pass/fail status\n  - Actual vs. expected results (with diffs for failures)\n  - Execution logs and error messages\n- Format reports for both human readability (Markdown/HTML) and machine consumption (JUnit XML/JSON).\n\n**STEP 7: QUALITY CRITERIA**\n- All tests must be isolated (no cross-test data leakage).\n- Tests must be idempotent and repeatable.\n- Coverage must include positive, negative, and edge cases.\n- All failures must be actionable and clearly reported.\n\n**SCOPE & CONSTRAINTS**\n- Focus on unit-level testing (not integration or performance).\n- Only T-SQL scripts (procedures, functions, views, ad-hoc queries) are in scope.\n- Solution must not require production data or impact production systems.\n- Prefer open-source or widely adopted frameworks when possible.\n\n**OUTPUT FORMAT:**\n\n- **Test Case Definitions:**  \n  - Format: CSV\n  - Structure:  \n    id: Unique identifier for the test case (e.g., TC001)\ndescription: Purpose of the test case (e.g., Validates correct aggregation in SalesReport)\nsetup_sql: SQL statements to insert or prepare test data\nscript_under_test: Name of the T-SQL script, function, or procedure being tested\nexpected_output: Expected result set, values, or table state\ncleanup_sql: SQL to clean up or remove test data after execution\n\n- **Test Harness Implementation:**  \n  - Format: T-SQL scripts, PowerShell scripts, or documentation in Markdown\n  - Structure:  \n    - Setup section\n    - Execution section\n    - Assertion section\n    - Cleanup section\n\n- **Test Report:**  \n  - Format: Markdown summary and JUnit XML  \n  - Structure:  \n    - Test case ID\n    - Description\n    - Status (Pass/Fail)\n    - Actual vs. Expected (table/diff)\n    - Error messages/logs\n\n- **Quality Checklist:**  \n  - Format: Markdown checklist  \n  - Structure:  \n    - [x] All test cases defined  \n    - [x] All tests isolated  \n    - [x] Data integrity validated  \n    - [x] Reports generated\n\n**QUALITY CRITERIA:**\n- All test cases are explicit and reproducible.\n- Test harness is modular and reusable.\n- Reports are clear, actionable, and standards-compliant.\n- No production data or schema is altered.\nINSTRUCTION FOR GITHUB TOOLS:\n1.Read input from previous agent \"DI_OptimiseTSQLScript\"\n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_UnitTestingReport\"\nInput\n{{github_credintials_unittest}} -for the user github credentials use this input from user\n",
                        "expectedOutput": "A fully automated, standards-compliant unit testing framework for T-SQL scripts, with sample test cases, harness code, and a sample test report."
                    },
                    "maxIter": 10,
                    "maxRpm": 0,
                    "maxExecutionTime": 600,
                    "tools": [],
                    "userTools": [
                        {
                            "toolId": 300,
                            "toolName": "DI_Github_File_Writer_Z",
                            "toolClassName": "GitHubFileWriterTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport urllib3\nimport logging\nimport re\nfrom typing import Type, Any\n\n# ---------------------------------\n# SSL & Logging Configuration\n# ---------------------------------\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"github_file_writer.log\",\n)\nlogger = logging.getLogger(\"GitHubFileWriterTool\")\n\n\n# ---------------------------------\n# Input Schema\n# ---------------------------------\nclass GitHubFileWriterSchema(BaseModel):\n    repo: str = Field(..., description=\"GitHub repository in 'owner/repo' format\")\n    branch: str = Field(..., description=\"Branch name (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub Personal Access Token\")\n    folder_name: str = Field(..., description=\"Name of the folder to create inside the repository\")\n    file_name: str = Field(..., description=\"Name of the file to create or update in the folder\")\n    content: str = Field(..., description=\"Text content to upload into the GitHub file\")\n\n\n# ---------------------------------\n# Main Tool Class\n# ---------------------------------\nclass GitHubFileWriterTool(BaseTool):\n    name: str = \"GitHub File Writer Tool\"\n    description: str = \"Creates or updates files in a GitHub repository folder\"\n    args_schema: Type[BaseModel] = GitHubFileWriterSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{path}\"\n\n    def _sanitize_path_component(self, component: str) -> str:\n        \"\"\"Remove invalid GitHub path characters.\"\"\"\n        sanitized = re.sub(r'[\\\\*?:\"<>|]', '_', component)\n        sanitized = re.sub(r'\\.\\.', '_', sanitized)\n        sanitized = sanitized.lstrip('./\\\\')\n        return sanitized if sanitized else \"default\"\n\n    def _validate_content(self, content: str) -> str:\n        \"\"\"Ensure valid string content within 10MB limit.\"\"\"\n        if not isinstance(content, str):\n            logger.warning(\"Content is not a string. Converting to string.\")\n            content = str(content)\n\n        max_size = 10 * 1024 * 1024  # 10 MB\n        if len(content.encode('utf-8')) > max_size:\n            logger.warning(\"Content exceeds 10MB limit. Truncating.\")\n            content = content[:max_size]\n\n        return content\n\n    def create_file_in_github(self, repo: str, branch: str, token: str,\n                              folder_name: str, file_name: str, content: str) -> str:\n        \"\"\"Create or update a file in GitHub repository.\"\"\"\n        sanitized_folder = self._sanitize_path_component(folder_name)\n        sanitized_file = self._sanitize_path_component(file_name)\n        validated_content = self._validate_content(content)\n\n        path = f\"{sanitized_folder}/{sanitized_file}\"\n        url = self.api_url_template.format(repo=repo, path=path)\n        headers = {\"Authorization\": f\"token {token}\", \"Content-Type\": \"application/json\"}\n\n        # Encode content\n        encoded_content = base64.b64encode(validated_content.encode()).decode()\n\n        # Check file existence to get SHA (for updating)\n        sha = None\n        try:\n            response = requests.get(url, headers=headers, params={\"ref\": branch}, verify=False)\n            if response.status_code == 200:\n                sha = response.json().get(\"sha\")\n        except Exception as e:\n            logger.error(f\"Failed to check file existence: {e}\", exc_info=True)\n\n        payload = {\"message\": f\"Add or update file: {sanitized_file}\",\n                   \"content\": encoded_content, \"branch\": branch}\n        if sha:\n            payload[\"sha\"] = sha  # Required for updating\n\n        # Upload or update file\n        try:\n            put_response = requests.put(url, json=payload, headers=headers, verify=False)\n            if put_response.status_code in [200, 201]:\n                logger.info(f\"\u2705 File '{sanitized_file}' uploaded successfully to {repo}/{sanitized_folder}\")\n                return f\"\u2705 File '{sanitized_file}' uploaded successfully to GitHub in folder '{sanitized_folder}'.\"\n            else:\n                logger.error(f\"GitHub API Error: {put_response.text}\")\n                return f\"\u274c Failed to upload file. GitHub API error: {put_response.text}\"\n        except Exception as e:\n            logger.error(f\"Failed to upload file: {e}\", exc_info=True)\n            return f\"\u274c Exception while uploading file: {str(e)}\"\n\n    # ------------------------------------------------------\n    # Required method for CrewAI Tool execution\n    # ------------------------------------------------------\n    def _run(self, repo: str, branch: str, token: str,\n             folder_name: str, file_name: str, content: str) -> Any:\n        \"\"\"Main execution method.\"\"\"\n        return self.create_file_in_github(repo, branch, token, folder_name, file_name, content)\n\n\n# ---------------------------------\n# Generalized Main (User-Parameterized)\n# ---------------------------------\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd27 GitHub File Writer Tool - Interactive Mode\\n\")\n    repo = input(\"Enter GitHub repository (owner/repo): \").strip()\n    branch = input(\"Enter branch name (e.g., main): \").strip()\n    token = input(\"Enter your GitHub Personal Access Token: \").strip()\n    folder_name = input(\"Enter folder name: \").strip()\n    file_name = input(\"Enter file name (e.g., example.txt): \").strip()\n    print(\"\\nEnter the content for your file (end with a blank line):\")\n    lines = []\n    while True:\n        line = input()\n        if line == \"\":\n            break\n        lines.append(line)\n    content = \"\\n\".join(lines)\n\n    tool = GitHubFileWriterTool()\n    result = tool._run(repo=repo, branch=branch, token=token,\n                       folder_name=folder_name, file_name=file_name, content=content)\n    print(\"\\nResult:\", result)\n",
                            "isApproved": false
                        },
                        {
                            "toolId": 344,
                            "toolName": "DI_GitHub_File_Reader_Z",
                            "toolClassName": "GitHubFileReaderTool",
                            "toolClassDef": "from crewai.tools import BaseTool\nfrom pydantic import BaseModel, Field\nimport base64\nimport requests\nimport logging\nfrom typing import Type, Any, List, Dict\n\n# Setup logging for the GitHub File Reader Tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='github_file_reader.log'\n)\nlogger = logging.getLogger('GitHubFileReaderTool')\n\nclass GitHubFileReaderSchema(BaseModel):\n    \"\"\"Input schema for the GitHubFileReaderTool.\"\"\"\n    repo: str = Field(..., description=\"GitHub repository in the format 'owner/repo'\")\n    file_paths: List[str] = Field(..., description=\"List of file paths in the repository\")\n    branch: str = Field(..., description=\"Branch name to read the files from (e.g., 'main')\")\n    token: str = Field(..., description=\"GitHub personal access token for authorization\")\n\nclass GitHubFileReaderTool(BaseTool):\n    name: str = \"GitHub File Reader Tool\"\n    description: str = \"Reads multiple files from a GitHub repository based on user inputs.\"\n    args_schema: Type[BaseModel] = GitHubFileReaderSchema\n\n    api_url_template: str = \"https://api.github.com/repos/{repo}/contents/{file_path}\"\n\n    def fetch_file_from_github(self, repo: str, file_path: str, branch: str, token: str) -> str:\n        \"\"\"Fetches a file content from GitHub.\"\"\"\n        url = self.api_url_template.format(repo=repo, file_path=file_path)\n        headers = {\n            \"Authorization\": f\"token {token}\",\n            \"Accept\": \"application/vnd.github.v3+json\"\n        }\n        params = {\"ref\": branch}\n\n        try:\n            logger.info(f\"Fetching file '{file_path}' from repo '{repo}' on branch '{branch}'\")\n            response = requests.get(url, headers=headers, params=params)\n            response.raise_for_status()\n\n            file_data = response.json()\n            if \"content\" not in file_data:\n                raise ValueError(f\"\u274c Error: Path '{file_path}' might be a directory or missing content.\")\n\n            decoded_content = base64.b64decode(file_data['content']).decode('utf-8')\n            logger.info(f\"\u2705 Successfully fetched file '{file_path}'.\")\n            return decoded_content\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch file '{file_path}': {str(e)}\", exc_info=True)\n            raise\n\n    def _run(self, repo: str, file_paths: List[str], branch: str, token: str) -> Dict[str, Any]:\n        \"\"\"Main execution logic.\"\"\"\n        all_files_content = {}\n        for file_path in file_paths:\n            try:\n                content = self.fetch_file_from_github(repo, file_path, branch, token)\n                all_files_content[file_path] = {\"status\": \"success\", \"content\": content}\n            except Exception as e:\n                all_files_content[file_path] = {\"status\": \"error\", \"message\": str(e)}\n\n        return all_files_content\n\n\n# Example Usage\nif __name__ == '__main__':\n    github_token = \"YOUR_GITHUB_TOKEN\"\n    github_repo = \"owner/repository-name\"\n    github_branch = \"main\"\n    github_files = [\n        \"path/to/file1.txt\",\n        \"path/to/file2.sql\",\n        \"path/to/file3.json\"\n    ]\n\n    if github_token == \"YOUR_GITHUB_TOKEN\":\n        print(\"\u26a0\ufe0f Please replace the placeholder values before running.\")\n    else:\n        reader_tool = GitHubFileReaderTool()\n        result = reader_tool.run(\n            repo=github_repo,\n            file_paths=github_files,\n            branch=github_branch,\n            token=github_token\n        )\n\n        for file, details in result.items():\n            print(f\"\\nFile: {file}\")\n            if details['status'] == 'success':\n                print(f\"Content:\\n{details['content'][:200]}...\")  # print first 200 characters\n            else:\n                print(f\"Error: {details['message']}\")\n",
                            "isApproved": false
                        }
                    ],
                    "allowCodeExecution": false,
                    "isSafeCodeExecution": true
                }
            }
        ],
        "enableAgenticMemory": false,
        "levelId": 99,
        "org": "Ascendion",
        "orgId": 1,
        "domain": "Data&Insights",
        "domainId": 96,
        "projectId": 98,
        "project": "AllProjects",
        "teamId": 99,
        "team": "AVA Team",
        "callbacks": []
    }
}