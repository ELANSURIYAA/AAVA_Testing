{
    "workflowId": 3666,
    "workflowName": "DI_SSIS_To_PySpark_Convert",
    "nodes": [
        {
            "agentName": "DI_SSIS_to_PySpark_Converter",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "The agent is tasked with converting SSIS logic into equivalent PySpark code. The conversion must replicate all functionalities, including transformations, aggregations, conditional splits, derived columns, and lookups, using PySpark functions. The output should be optimized for performance and adhere to PySpark standards.\n \n### **INSTRUCTIONS:**  \n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n\n1. **Context and Background Information:**  \n   - SSIS workflows often involve data flow tasks, transformations, and control flow logic.  \n   - PySpark provides equivalent functionality through its DataFrame API, SQL-like operations, and transformation methods.  \n   - The goal is to map SSIS components to PySpark constructs while preserving the logic and functionality.\n \n2. **Scope and Constraints:**  \n   - The conversion must include all SSIS components, such as:  \n     - **Conditional Splits:** Map to PySpark `filter()` or `when()` clauses.  \n     - **Derived Columns:** Use PySpark `withColumn()` and expressions.  \n     - **Lookups:** Implement using PySpark `join()` operations.  \n     - **Aggregations:** Use PySpark `groupBy()` and aggregation functions.  \n   - Ensure the PySpark code is modular, readable in Databricks Notebook  \n   - Handle edge cases such as null values, data type mismatches, and performance bottlenecks.\n \n3. **Process Steps to Follow:**  \n   - **Step 1:** Parse the SSIS logic and identify all components (e.g., transformations, splits, lookups).  \n   - **Step 2:** Map each SSIS component to its equivalent PySpark function or construct.  \n   - **Step 3:** Write PySpark code for each component, ensuring the logic is preserved.  \n   - **Step 4:** Combine all PySpark code snippets into a cohesive workflow.  \n   - **Step 5:** Optimize the PySpark code for performance (e.g., minimize shuffling, use caching where necessary).  \n   - **Step 6:** Validate the output by comparing results with the original SSIS workflow.\n \n4. **Output Format:**  \n   - The output should be Converted PySpark Code provided for Databricks notebook .  \n      \n \n5. **Quality Criteria:**  \n   - Code must be functional and error-free.  \n   - Ensure the logic is preserved accurately.  \n   - Use industry best practices for PySpark coding (e.g., modular functions, clear variable names).  \n   - Include comments in the code for clarity.\n \nOptimize Performance:\nApply partitioning, caching, and bucketing strategies for efficient execution.\nand use DDL file for understanding the source and destination delta table, so using that write pyspark code with this delta table source and destination instead of that actual source and destination present in that SSIS. importantly don't use SQL Server code anywhere in that converted PySpark code.\n\nDont use any other database SQL code any where use only the SQL complaint with the delta tables for the reading and writing the input and output\n\nINPUT:\nSSIS Package File: {{SSIS_File}}\nAnd include DDL file: {{DDL_File}} for understanding the source and destination and use the source and destination name from this file. \nEnsure all data processing happens within PySpark delta tables without using external databases.",
                "expectedOutput": "Expected Output\nmetadata requirements only in the top of the output once\nA working PySpark script that replicates the full functionality of the SSIS package .\nSuccess message\nInclude a statement of API cost consumed (e.g., \"API Cost Consumed in dollars \")."
            }
        },
        {
            "agentName": "DI_SSIS_to_PySpark_Unit_Tester",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "You are responsible for designing unit tests and writing Pytest scripts for the given PySpark DataFrame transformation logic derived from the SSIS package. Your expertise in PySpark testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\n\n**INSTRUCTIONS:**  \n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n1. Analyze the provided PySpark DataFrame transformations to identify key logic, joins, aggregations, and data manipulations.  \n2. Create a list of test cases covering:  \n   a. Happy path scenarios  \n   b. Edge cases (e.g., NULL values, empty DataFrames, boundary conditions)  \n   c. Error handling (e.g., invalid schema, unexpected data formats, type mismatches)  \n3. Design test cases using PySpark testing methodologies, ensuring validation of DataFrame outputs.  \n4. Implement the test cases using Pytest, leveraging PySpark\u2019s testing utilities.  \n5. Ensure proper setup and teardown for test datasets, including creating mock DataFrames.  \n6. Use appropriate assertions to validate expected results, comparing schema, row count, and specific transformations.  \n7. Organize the test cases logically, grouping related tests together for readability and maintainability.  \n8. Implement any necessary helper functions or reusable fixtures to support efficient test execution.  \n9. Ensure the Pytest script follows PEP 8 style guidelines and best practices for PySpark testing.  \n\ninput:\n* Use the previous DI_SSIS_to_PySpark_Converter agents converted PySpark script as input\n",
                "expectedOutput": "A comprehensive suite of Pytest scripts that thoroughly validate the converted PySpark code's functionality, accuracy, and performance.\nMetadata Requirements only once in the top of the output\n1. **Test Case List:**  \n   - Test case ID  \n   - Test case description  \n   - Expected outcome  \n2. **Pytest Script for each test case**  \n3. Include the cost consumed by the API for this call in the output."
            }
        },
        {
            "agentName": "DI_SSIS_to_PySpark_Conversion_Tester",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "As an Automation Test Engineer, you will create a robust testing framework to validate the SSIS-to-PySpark conversion. Take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input. Follow these detailed steps to accomplish your task:\n\nINSTRUCTIONS:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n1. Analyze the original SSIS package and the converted PySpark script to understand the workflow and data transformations.\n2. Identify key components and functionalities in the SSIS package that need to be tested in the PySpark version.\n3. Develop a comprehensive set of test cases covering various scenarios, including:\n   a. Data integrity checks\n   b. Transformation logic validation\n   c. Error handling and exception scenarios\n   d. Performance benchmarks\n4. Create a Pytest script that implements the test cases, following these guidelines:\n   a. Use appropriate Pytest fixtures for setup and teardown operations\n   b. Implement parameterized tests for different input scenarios\n   c. Use assertions to validate expected outcomes\n   d. Include error handling and logging mechanisms\n5. Develop helper functions to:\n   a. Load test data\n   b. Execute the PySpark script\n   c. Compare output data with expected results\n6. Implement data comparison logic to verify the consistency between SSIS and PySpark outputs\n7. Add performance testing components to measure and compare execution times\n8. Include documentation for each test case and function in the Pytest script\n9. Implement a reporting mechanism to generate detailed test results\nINPUT :\n* For the input SSIS code analysis use this file : {{SSIS_File}}\n*For the Analyser input use this file {{Analysis_File}}\n* And also take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input.\n",
                "expectedOutput": "Metadata requirements only once in the top of the output\nA comprehensive Pytest script and associated files for validating the SSIS-to-PySpark conversion."
            }
        },
        {
            "agentName": "DI_SSIS_To_PySpark_Recon_Tester",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "Create a robust testing framework that compares the output of SSIS packages with their PySpark equivalents. The framework should automate the process of running both SSIS and PySpark jobs, collecting their outputs, and performing detailed reconciliation checks. take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.\n\nINSTRUCTIONS:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n1. Analyze the existing SSIS packages and identify the corresponding PySpark scripts.\n2. Develop a test harness that can execute both SSIS packages and PySpark scripts with identical input data.\n3. Implement data extraction mechanisms for SSIS outputs.\n4. Create a comparison engine that can:\n   a. Compare row counts between SSIS and PySpark outputs.\n   b. Perform column-level comparisons, including data types and values.\n   c. Identify discrepancies in data transformations.\n   d. Handle complex data types and nested structures.\n5. Develop a reporting mechanism to highlight any inconsistencies found during the reconciliation process.\n6. Implement performance metrics collection to compare execution times between SSIS and PySpark.\n7. Create a configuration system to easily add new test cases and manage existing ones.\n8. Develop a logging system to track test executions and results.\n9. Implement error handling and retry mechanisms for failed tests.\n10. Create documentation for the testing framework, including setup instructions and usage guidelines.\n* For input SSIS file take from this file : {{SSIS_File}}\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.  ",
                "expectedOutput": "A comprehensive testing framework that automates the reconciliation process between SSIS and PySpark data transformations, providing detailed reports on data consistency, discrepancies, and performance improvements.\nMetadata Requiremensts only once in the top of the output\n1. **Test Cases Document:**  \n   - Test Case ID  \n   - Description  \n   - Input Data  \n   - Expected Output  \n   - Test Steps  \n   - Pass/Fail Criteria  \n2. **Pytest Script for each test case**  \n3. Include the cost consumed by the API for this call in the output."
            }
        },
        {
            "agentName": "DI_SSIS_To_PySpark_Reviewer",
            "model": "gpt-4",
            "tools": [],
            "task": {
                "description": "As a Senior Data Engineer, you will review the migrated PySpark scripts that were converted from SSIS packages. Your task is to ensure that the PySpark scripts accurately replicate the functionality of the original SSIS packages while leveraging PySpark's distributed computing capabilities.\n\nINSTRUCTIONS:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n1. Analyze the original SSIS package structure and data flow.\n2. Review the corresponding PySpark script for each SSIS package.\n3. Verify that all data sources and destinations are correctly mapped.\n4. Ensure that data transformations and business logic are accurately implemented in PySpark.\n5. Check for proper error handling and logging mechanisms.\n6. Validate that the PySpark script follows best practices for performance and scalability.\n7. Identify any potential improvements or optimizations in the PySpark implementation.\n8. Test the PySpark script with sample data to confirm expected results.\n9. Compare the output of the PySpark script with the original SSIS package output.\n10. Document any discrepancies, issues, or recommendations for improvement.\nINPUT:\n* For input SSIS file take from this file : {{SSIS_File}}\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input. \n\nOUTPUT FORMAT:\nProvide a detailed review report in the following structure:\n\n1. Overview\n   - SSIS Package Name\n   - PySpark Script Name\n   - Review Date\n\n2. Functionality Assessment\n   - Data Sources and Destinations\n   - Transformations and Business Logic\n   - Error Handling and Logging\n\n3. Performance and Scalability\n   - Resource Utilization\n   - Execution Time Comparison\n   - Scalability Considerations\n\n4. Code Quality and Best Practices\n   - Code Structure and Readability\n   - PySpark API Usage\n   - Adherence to Coding Standards\n\n5. Testing Results\n   - Sample Data Used\n   - Output Comparison\n   - Discrepancies (if any)\n\n6. Recommendations\n   - Suggested Improvements\n   - Optimization Opportunities\n\n7. Conclusion\n   - Migration Success Rating (1-10)\n   - Final Remarks\n",
                "expectedOutput": "Metadata requirements only once in the top of the ouput\n1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n7. Include the cost consumed by the API for this call in the output."
            }
        }
    ]
}