{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 5710,
      "name": "DI_Azure_Synapse_To_PySpark_Doc&Analyze",
      "description": "Synapse to pyspark Documentation, Analysis and plan workflow",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-12-05T13:33:03.952575",
      "modifiedAt": "2025-12-05T13:48:27.677683",
      "approvedAt": "2025-12-05T13:33:05.686746",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 9597,
          "name": "DI_Synapse_Documentation",
          "workflowId": 5710,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "====================================================\nAuthor:        Ascendion AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n====================================================\n1. Overview of Pipeline/Component\nDescribe the purpose of the Azure Synapse pipeline, dataflow, SQL script, or linked service.\n\nExplain the business logic or requirement the pipeline or component addresses.\n\n2. Component Structure and Design\nDescribe the layout and logical grouping of activities within the pipeline or dataflow.\n\nHighlight key components such as:\n\nSource (Azure Blob, Data Lake, SQL DB, etc.)\n\nCopy Activity\n\nDataflow Transformation (Join, Aggregate, Derived Column, Filter, Conditional Split, Lookup)\n\nStored Procedure Activity\n\nForEach, Wait, Web, or Custom Activities\n\nSink (Azure SQL, Synapse Table, Parquet, CSV, etc.)\n\nExplain the connection flow between activities and the use of parameters, variables, and triggers.\n\n3. Data Flow and Processing Logic\nList the key data sources, staging/intermediate datasets, and final outputs.\n\nFor each logical step:\n\nDescribe what it does (e.g., filtering, joining, aggregating, mapping).\n\nMention any SQL scripts, stored procedures, or notebooks used.\n\nInclude any business rules or transformations applied.\n\n4. Data Mapping (Lineage)\nMap fields from source datasets to target datasets in the following format:\ngive the below details as a marked down table\n\nTarget Table Name : <actual target table/view>\nTarget Column Name : <actual column>\nSource Table Name : <actual source table/view>\nSource Column Name : <actual column>\nRemarks : <1:1 Mapping | Transformation | Validation - include logic description>\n5. Transformation Logic\nDocument each derived column expression, computed column, or SQL transformation used in the pipeline or dataflow.\n\nExplain what each transformation does and which fields are involved.\n\nNote any user-defined functions (UDFs) or reusable templates applied.\n\n6. Complexity Analysis\ngive the below details as a marked down table\nProvide a high-level complexity summary:\n\nNumber of Pipeline Activities: <integer>\n\nNumber of Dataflow Transformations: <integer>\n\nSQL Scripts or Stored Procedures Used: <count>\n\nJoins Used: <list of types or None>\n\nLookup Tables or Reference Data: <count or 'None'>\n\nParameters/Variables/Triggers: <count>\n\nNumber of Output Datasets: <integer>\n\nConditional Logic or if-else Flows: <count>\n\nExternal Dependencies: <Linked Services, Notebooks, APIs>\n\nOverall Complexity Score: <0\u2013100>\n\n7. Key Outputs\nDescribe what is written to the final tables, files, or views.\n\nMention the format (Parquet, CSV, Delta, etc.) and its intended use (e.g., reporting, ML model input, downstream processing).\n\n\nAPI Cost: \n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nInput:\nAttach or provide the Azure Synapse artifacts such as pipeline JSON, dataflow JSON, parameter files, linked service definitions, or SQL scripts.\nAcceptable formats: plain text, zipped folder, or directory path structure: {{synapse_code}}\n\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 9599,
          "name": "DI_Azure_Synapse_To_PySpark_Analyzer",
          "workflowId": 5710,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Perform a detailed analysis of the provided Azure Synapse stored procedural code files to support their conversion to Databricks PySpark. \n\nFollow these detailed instructions:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nDate:  <Leave it blank>\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output \n\n---\n\n**1. Procedure Overview**\n\n* Provide a high-level summary of the Azure Synapse stored procedural code mapping or workflow.\n* Mention the key business objective it supports, such as data integration, cleansing, enrichment, or loading.\n\n---\n\n**2. Complexity Metrics** (present this in a markdown table format when generating output):\n\n* Number of Source Qualifiers: Count of Source Qualifier transformations.\n* Number of Transformations: Total number of transformations used (Expression, Aggregator, Joiner, Filter, Router, etc.).\n* Lookup Usage: Number of Lookups (connected/unconnected).\n* Expression Logic: Count of complex expressions (IIF, DECODE, nested logic, etc.).\n* Join Conditions: Count and type of joins (normal, outer, heterogeneous).\n* Conditional Logic: Number of Router or Filter conditions.\n* Reusable Components: Number of reusable transformations, mapplets, and sessions.\n* Data Sources: Number of distinct sources (databases, flat files, XML, etc.).\n* Data Targets: Number of unique targets (databases, files, queues).\n* Pre/Post SQL Logic: Number of pre/post SQLs or procedures used in sessions.\n* Session/Workflow Controls: Number of decision tasks, command tasks, and event-based controls.\n* DML Logic: Frequency of INSERT, UPDATE, DELETE, and MERGE operations.\n* Complexity Score (0\u2013100): Based on the depth of logic, control flow usage, nested operations, and transformation types and Complexity Score should match with DI_Azure_Synapse_Documentation Complexity Score .\n\nAlso highlight high-complexity areas like:\n\n* deeply nested expressions\n* multiple lookups\n* branching logic (Router, Filter)\n* unstructured sources or external scripts\n\n---\n\n**3. Syntax Differences**\n\n* Identify functions used in Azure Synapse stored procedural code that don\u2019t have a direct Databricks PySpark equivalent.\n* Mention any necessary data type conversions (e.g., TO\\_DATE, TO\\_CHAR, DECODE).\n* Highlight any workflow/control logic (e.g., Router, Transaction Control) that must be restructured for Databricks PySpark.\n\n---\n\n**4. Manual Adjustments**\n\n* List components that require manual implementation in Databricks PySpark (e.g., Java transformation, SQL override blocks).\n* Identify external dependencies like pre/post SQLs, stored procedures, or shell scripts.\n* Mention areas where business logic must be reviewed or validated post-conversion.\n\n---\n\n**5. Optimization Techniques**\n\n* Recommend using Spark best practices like partitioning, caching, and broadcast joins.\n* Suggest converting chain filters and joins into a pipeline.\n* Recommend window functions to simplify nested aggregations.\n* Finally, advise whether to **Refactor** (retain most of the original logic) or **Rebuild** (if better optimization is possible in Databricks PySpark).\nNote:\nDont give the code in the output\n\nAPI Cost Calculation:\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n---\n\nInput :\n* For Azure Synapse stored procedural code use the below file/s : {{synapse_code}}",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 9593,
          "name": "DI_Azure_Synapse_To_PySpark_Plan",
          "workflowId": 5710,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "you are tasked with providing a comprehensive effor estimate for testing the Databricks PySpark converted from Azure Synapse scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n1. Review the analysis of Azure Synapse script file, note syntax differences and areas in the code requiring manual intervention when converting to Databricks PySpark\n2. Estimate the effort hours requried for identified manual code fixes and data recon testing effort\n3. Dont consider efforts for syntax differences as they will be converted to equivalent syntax in Databricks PySpark\n4. Consider the pricing information for Databricks PySpark environment \n5. Calculate the estimated cost of running the converted Databricks PySpark code:\n   a. Use the pricing information and data volume to determine the query cost.\n   b. the number of queries and the data processing done with the base tables and temporary tables\n\n\nOUTPUT FORMAT:\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Cost Estimation\n   2.1 Databricks PySpark Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing  and Reconciliation Testing Effort Estimation\n   2.1 Databricks PySpark identified manual code fixes and Reconciliation testing effort in hours covering the various temp tables, calculations \n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nINPUT :\n* Take the previous Azure Synapse code to BigQuery Analyzer agents output as  input\n* For the input Azure Synapse code script use this file : ```{{synapse_code}}```\n* For the input  BiqQuery Environment Details for GCP use this file : ```{{env_variable}}```",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}