{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2147,
      "name": "DI DataSpecs Modelupdates SN",
      "description": "Includes tech spec creation, functional testing with SQL scripts, and DDL-based model updates.",
      "createdBy": "kiran.krishnakumar@ascendion.com",
      "modifiedBy": "kiran.krishnakumar@ascendion.com",
      "approvedBy": "kiran.krishnakumar@ascendion.com",
      "createdAt": "2025-11-05T10:48:51.806066",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:48:53.023443",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {
            "id": 34,
            "topP": 0.95,
            "maxToken": 12000,
            "temperature": 0.3,
            "modelDeploymentName": "gpt-4.1"
          }
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 3885,
          "name": "DI Data Technical Specification SN",
          "workflowId": 2147,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent must create a comprehensive technical specification document based on the provided inputs:  \n- JIRA story file  \n- Confluence context file  \n- DDL file for the new source table  \n- Existing source data model  \n- Existing target data model  \n\nThe specification should include:  \n1. **Code Changes Required for the Enhancement:**  \n   - Identify the specific areas in the codebase that need modification.  \n   - Detail the logic and functionality changes required to incorporate the new source table.  \n   - Include pseudocode or code snippets where applicable.  \n\n2. **Updates to the Data Models:**  \n   - Analyze the existing source and target data models.  \n   - Define the updates required to integrate the new source table into the models.  \n   - Ensure consistency and alignment between the source and target models.  \n\n3. **Source-to-Target Mapping:**  \n   - Provide a detailed mapping of fields from the new source table to the target data model.  \n   - Include any transformation rules or business logic required for the mapping.  \n\n**INSTRUCTIONS:**  \n1. **Context and Background Information:**  \n   - Review the JIRA story file to understand the business requirements and objectives.  \n   - Refer to the Confluence context file for additional project details and constraints.  \n   - Analyze the DDL file to understand the structure and schema of the new source table.  \n   - Examine the existing source and target data models to identify dependencies and relationships.  \n\n2. **Scope and Constraints:**  \n   - Ensure the specification aligns with the business requirements outlined in the JIRA story.  \n   - Maintain compatibility with existing systems and processes.  \n   - Adhere to data governance and security standards.  \n\n3. **Process Steps to Follow:**  \n   - Step 1: Extract relevant details from the provided files.  \n   - Step 2: Identify code changes required for the enhancement, including impacted modules and functions.  \n   - Step 3: Define updates to the source and target data models, ensuring logical consistency.  \n   - Step 4: Create a detailed source-to-target mapping, including transformation rules.  \n   - Step 5: Format the technical specification document as per industry standards.  \n4. **OUTPUT FORMAT:**  \n   - **Format:** Markdown  \n   - **Structure Requirements:**  \n- **Metadata Requirements:**\n-=============================================\n-Author: Ascendion AVA+\n-Date: <Leave it blank>\n-Description: <one-line description of the purpose >\n-============================================= \n     - Title: Technical Specification for [Enhancement Name]  \n     - Sections:  \n       - Introduction  \n       - Code Changes  \n       - Data Model Updates  \n       - Source-to-Target Mapping  \n       - Assumptions and Constraints  \n       - References  \n     - Use headings, subheadings, and bullet points for clarity.  \n   - **Quality Criteria:**  \n     - Clear and concise language.  \n     - Logical flow and organization.  \n     - Accurate and complete mapping and transformation rules.  \n   - **Formatting Needs:**  \n     - Use tables for source-to-target mapping.  \n     - Include diagrams for data model updates (if applicable).  \nPoints to Remember:\nRemember Must use the Github File Writer Tool to upload the File in the Github Repo for the Environment Details for github take that from the input\nRemember for the branch input use it as \"main\" and conent is what you give as output save the file as .md format\n\n\nAll the Inputs (Jira Stories, Confluence Documentation, Source Data Model, Target Data Model) are available in the zip folder that is uploaded. *****The input file names are provided in {{Technical_Specifications}}\n* GitHub repo details : {{GitHub_Repo_Details}}\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 3474,
          "name": "DI Delta Model Changes SN",
          "workflowId": 2147,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "DMEA accepts two primary inputs:\n**The existing data model (ER diagrams, DDLs, JSON schemas, etc.)\n**The new technical specifications (manually entered or taken as input from upstream agents like TSA - Technical Specification Agent)\nIt performs the following stages:\n1. **Model Ingestion\n**Parse and map existing model: tables, fields, constraints, relationships\n**Build internal representations for current schema using graph/tree formats\n**Supports relational (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB), and modern lakehouse (Delta Lake, Snowflake, BigQuery) models\n2. **Spec Parsing & Mapping\n    Normalize inputs from tech specs to structural requirements\n    Detect:\n         **Additions (new tables/columns/indexes)\n         **Modifications (type changes, nullable, constraints)\n         **Deprecations (dropping columns, soft deletes)\n    Infer indirect changes (e.g., changed business rule implies a column constraint update)\n3. **Delta Computation\n    Compare existing vs desired model\n    Categorize deltas:\n        **New tables/fields\n        **Changed column types/nullability\n        **Added/removed constraints\n        **Modified indexes/PKs\n    Compute version bump impact (patch/minor/major)\n4. **Impact Assessment\n    --Downstream break detection (views, ETL jobs, APIs)\n    --Data loss risk (e.g., narrowing column types, dropping constraints)\n    --Foreign key ripple effects\n    --Platform-specific caveats (e.g., PostgreSQL vs MySQL)\n5. **DDL/Alter Statement Generation\n    --Forward DDLs:\n        CREATE TABLE, ALTER TABLE, ADD CONSTRAINT, DROP COLUMN\n    --Rollback support\n    --Optional zero-downtime deployment (via COPY, rename strategies)\n    --Index rebalancing if necessary\n    --Optional data migration scripts (e.g., populate new tables from old ones)\n6. **Documentation\n    --Side-by-side diff of model (before vs after)\n    --Full DDL logs with change reasons\n    --Visual diagrams (ERD updates)\n    --Change traceability matrix (tech spec section \u2192 DDL line)\n\nThe inputs for this agent which is technical specification and existing DDL's :\n--existing DDL's file names are mentioned in {{Data_Model_Delta}}\n--Take the technical specification requirement from the first agent named ------\"\"\"\"DI_Data_Technical_Specification_SN\"\"\"\" ",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 3545,
          "name": "DI Functional Test Cases SN",
          "workflowId": 2147,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "****MASKED****",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}