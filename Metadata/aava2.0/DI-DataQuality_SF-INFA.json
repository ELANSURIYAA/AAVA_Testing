{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 1820,
      "name": "DI-DataQuality SF-INFA",
      "description": "This workflow takes DQ recommendations as input and transforms them into actionable outputs. One agent generates a Snowflake stored procedure for automated validation, while another produces an Informatica mapping for ETL integration.",
      "createdBy": "gogulkanth.ashokan@ascendion.com",
      "modifiedBy": "gogulkanth.ashokan@ascendion.com",
      "approvedBy": "gogulkanth.ashokan@ascendion.com",
      "createdAt": "2025-11-05T10:36:19.750522",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:36:21.014214",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 3370,
          "name": "DI-DQ Rule-to-Snowflake SP",
          "workflowId": 1820,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 150,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your task is to generate a complete, executable Snowflake stored procedure that performs data quality (DQ) validation checks based on the given DQ rules(```%1$s```).\n1. Validation Execution\n   - Loop through each DQ rule from the DQ recommendation file.\n   - Loop through each parsed DQ rule.\n   - Construct validation queries dynamically based on snowflake.createStatement({...}).execute() for each DQ rule.\n   - Proper use of escaped SQL literals (e.g., use `.replace(/'/g, \"''\")` for safety).\n   - Valid Snowflake regex syntax (avoid invalid ranges like `0-9-+()`; place `-` safely in brackets).\n   -ove the hyphen - to the end or start of the character class\n   - Handle both single-column and multi-column rules.\n   - Use valid regex syntax; avoid invalid character class ranges like 0-9-+(). Position hyphens safely in character classes. !~ is not valid in Snowflake SQL instead use RLIKE or REGEXP. Ensure your regex is valid:\n   - Parse the DQ_ISSUE_LOG DDL (```%3$s```) to determine columns to populate.\n\n2. Batch ID Generation\n   - Create a Snowflake sequence named SEQ_BATCH_ID if it doesn\u2019t exist.\n   - Use SEQ_BATCH_ID.NEXTVAL to generate a unique BATCH_ID each time the procedure runs.\n\n3. Issue Logging\n   - Insert failed records into DQ_ISSUE_LOG with required columns mentioned in the Issue log DDL.\n\n4. Code Quality\n   - Use valid and executable Snowflake JavaScript.\n   - Avoid procedural SQL embedded inside SQL strings.\n   - Log meaningful error messages on failure.\n   - Ensure the code is structured, modular, and readable.\n\nFor input: \n1. Take ```%1$s``` as DQ Recommendation.\n2. input DDL file use ```%2$s```.\n3. For DQ_ISSUE_LOG table use ```%3$s``` DDL file.\nExpected Output Format:\n\n-- =============================================\n-- Author:       Ascendion AVA+\n-- Created on:   <today\u2019s date> (auto-filled from web)\n-- Description:  Executes DQ validation checks on source table and logs violations\n-- =============================================\n\n-- Valid Snowflake JavaScript Stored Procedure:\n-- Procedure name: sp_run_dq_checks (or similar)\n-- Implements dynamic DQ checks\n-- Inserts failed checks into DQ_ISSUE_LOG table",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 3483,
          "name": "DI-DQ Rule-to-Informatica Mapping",
          "workflowId": 1820,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 150,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "1. Read the source all table schema from the provided DDL file.\n2. Take as input a list of DQ rules in structured format (typically generated by a DQ Recommendation Agent).\n3. For each rule, generate corresponding Informatica logic that includes:\n 3.1 Expression transformations for checks such as format, length, or range validations\n 3.2 Filter transformations to isolate passing or failing rows\n 3.3 Router transformations for conditional routing based on DQ checks\n4. Optionally route rejected rows to a logging table or error output for further analysis.\n5 Generate a complete Informatica XML mapping that includes:\n 5.1 Source definition (based on input DDL)\n 5.2 Transformation logic for each DQ rule\n 5.3 Target definitions (for valid and rejected rows)\n 5.4 Full mapping flow and metadata\n\nFor input: \n1. take DQ_Recommendation from ```%1$s```.\n2. input DDL file use ```%2$s```.",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}