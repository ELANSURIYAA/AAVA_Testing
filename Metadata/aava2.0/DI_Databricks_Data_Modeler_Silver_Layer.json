{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 4659,
      "name": "DI Databricks Data Modeler Silver Layer",
      "description": "Data Modeler for Databricks environment",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-11-05T12:09:22.654305",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T12:09:23.702654",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 6470,
          "name": "DI Databricks Silver Model Logical",
          "workflowId": 4659,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n\n#### **1. Standard Silver layer logical data model Workflow (Mode 1)**\n\nExecuted when:\n* The Conceptual data model, data constraints and bronze layer logical data model file exists in GitHub input directories and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the file name with 'Silver_Logical_Data_Model_(followed by a number)'), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (in the name Silver_Logical_Data_Model_(followed by a number)), proceed to create the logical Silver data model with the given Conceptual data model, data constraints and bronze layer logical data model file from the input directories. For generating the Silver logical data model instructions and structure are given below. Once generated, store the Silver logical data model in the output directory with the file name as 'Silver_Logical_Data_Model_1.md'.\n\nThe agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the Conceptual data model, data constraints and bronze layer logical data model file.\n* Create a detailed logical data model for a medallion architecture Silver Layer. This model will serve as the blueprint for implementing a scalable and efficient data platform. Follow these instructions carefully to ensure a comprehensive and well-structured output.\n* Review and analyze the provided Data Constraints.\n* Design the Silver layer:\n   a. Mirror the Bronze data structure exactly but remove primary and foreign key fields in the output.\n   b. Include all fields from the Bronze data structure without primary key and foreign key fields; just remove those fields and keep the rest.\n   c. Create a consistent naming convention for tables with the first 3 characters in the table name as 'Si_'.\n   d. Include the data structure to hold both error data from data quality checks and validation and process audit data from pipeline execution.\n   e. Implement data type standardization.  \n   f. Give brief descriptions for all the columns.\n   g. Don't include primary key, foreign key, unique identifiers, and ID fields.\n* Document relationships between tables across all layers.\n* Provide rationale for key design decisions and any assumptions made.\n* Create a visual representation of the conceptual data model (e.g., entity-relationship diagram). Clearly need to be mention one table is connected to another table by which key field \n* Generate Silver logical data model containing the sections listed in **Silver Logical data model Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be the Silver_Logical_Data_Model_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n\n#### **2. Update Silver layer logical data model Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n\nThe agent must:\n* Identify the Silver layer logical data model file in GitHub output directory with the Databricks_Silver_Logical_Data_Model_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Add or modify the following fields in the output Metadata \n```\n## *Version* : 2 or 3 or 4 etc...\n## *Changes*: \n## *Reason*: \n``` \n* Save the updated file to the same GitHub output directory with the with the Databricks_Silver_Logical_Data_Model_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n\n## **Input Sections**\n* GitHub Credentials, Conceptual data model and Source data structure file present in this github input directories: `{{Conceptual_File_Constraints_File_Databricks_For_Databricks_Silver_Model_Logical}}`\n\n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Silver_Logical_Data_Model}}`\n\n## **Silver Logical data model Structure**\n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n\n1. Silver Layer Logica Model\n   - Table Name with description\n   - Column Name with description\n   - Data Type\n2. Conceptual Data Model Diagram in tabular form by one tale is having a relationship with other table by which key field\n3. apiCost: float  // Cost consumed by the API for this call (in USD)\n\n## **Guidelines Policies (Both Modes)**\n* Assume source data structure, and sample data are provided unless explicitly stated otherwise.\n* Use the information exactly as provided without introducing new elements or assumptions.\n* If certain details in the inputs are ambiguous or missing, clearly state what can be inferred based on the available input without adding unnecessary disclaimers.\n* Give output in the markdown format\n* In the output add all the necessary Number bullet marks",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 7173,
          "name": "DI Databricks Silver Model Physical",
          "workflowId": 4659,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "\n \nBefore starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard  Databricks Silver Model Physical Workflow (Mode 1)**\n \nExecuted when:\n* The inputs file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks Silver Model Physical underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks Silver Model Physical underscore followed by a number), proceed to create the  Databricks Silver Model Physical for the input file from the input directory. The Databricks Silver Model Physical instructions and structure are given below. Once generated, store the Databricks Silver Model Physical in the output directory with the file name as tDatabricks_Silver_Model_Physical_1.md.\n \nThe agent must:\n* Parse the Input.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks Silver Model Physical containing the sections listed in **Databricks Silver Model Physical Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Silver_Model_Physical_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Silver Model Physical Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Identify the Databricks Silver Model Physical file in GitHub output directory with the Databricks_Silver_Model_Physical_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Silver_Model_Physical_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n* GitHub Credentials and Input File present in the github input directory: `{{GitHub_Details_For_Databricks_Silver_Model_Physical}}`\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Silver_Model_Physical_Test_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Silver Model Physical Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the Input or workflow does.\n \n---\n\n\nYou need to translate the provided logical data model into a comprehensive physical data model with id fields for the Silver layer of the Medallion architecture. Follow these detailed instructions to complete the task:\n\nINSTRUCTIONS:\n\nAnalyze the provided logical data model to understand the data entities, relationships, and attributes. And also analyze the provided Physical Model DDL script for Bronze layer because all the columns and tables mentioned in the Bronze layer physical DDL must also be present here, with an additional error data table.\n\nDesign tables for the Silver layer to store cleansed and conformed data, ensuring compatibility with Spark SQL.\n\nInclude an error data table to store details of errors encountered during data validation in both Silver and Gold layers.\n\nFor each table in the physical data model:\n\nGive DDL script if not exists for all the columns of the Bronze tables with id fields also.\n\nIf id fields are missing in the logical model, add them in the physical model DDL script.\n\nDefine appropriate data types for each column, considering Databricks PySpark compatibility.\n\nDetermine appropriate partitioning strategies based on business domain.\n\nDesign necessary indexes to optimize query performance.\n\nDo not include foreign keys, primary keys, or constraints incompatible with Spark SQL. Even if the input contains PK/FK, exclude them. Only keep field name with datatype.\n\nEnsure the DDL script has all the columns present in the Bronze layer.\n\nInclude metadata columns for each table: load_date, update_date, source_system.\n\nSpecify appropriate storage formats (Delta Lake).\n\nDefine data retention policies for the Silver layer.\n\nCreate the Data Definition Language (DDL) scripts for each table, ensuring compatibility with Databricks PySpark SQL.\n\nInclude an audit table in both Silver and Gold layers to store details of pipeline executions, including start and end times, status, and error messages if any.\n\nDocument any assumptions or design decisions made during the process.\n\nIn the attached Knowledge Base file, ensure that any limitations of Databricks SparkSQL are identified and not included in the final output.\n\nCreate a visual representation of the conceptual data model (e.g., ERD). Clearly mention which table connects to which table by which key field.\n\nOUTPUT FORMAT:\n\nProvide the physical data model and DDL scripts in the following structure:\nSilver Layer:\n\nDDL scripts\n\nError Data Table DDL script\n\nAudit Table DDL script\n\nUpdate DDL script\n\nData Retention Policies\n\nRetention periods for the Silver layer\n\nArchiving strategies\n\nConceptual Data Model Diagram in tabular form showing relationships (table-to-table with key field).\n\nGUIDELINES:\n\nEnsure all scripts are syntactically correct and adhere to SQL standards for Databricks.\n\nDo not include PK/FK constraints.\n\nIncorporate Databricks-specific features into the DDL scripts.\n\nOrganize the output clearly for implementation.\n\nEnsure DDL scripts for Silver layer are separated into distinct sections or files.\n\nDDL scripts must include code for the physical model and update scripts for model changes.\n\nUse CREATE TABLE IF NOT EXISTS.\n\nAdditionally, calculate and include the apiCost consumed for this call in USD, reported as a floating-point value with all decimals preserved (no rounding/truncation).\n\nEnsure cost computation considers different agents and execution parameters.\n\nExpected Output\n\nProvide the physical data model and DDL scripts in the following structure:\n\nSilver Layer : DDL scripts, Error Data Table DDL script, Audit Table DDL script, Update DDL script\n\nData Retention Policies\n\nRetention periods for Silver layer\n\nArchiving strategies\n\nConceptual Data Model Diagram in tabular form showing table-to-table relationships with key fields\n\napiCost: float // Cost consumed by the API for this call (in USD, full precision)\n\n",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 7183,
          "name": "DI Databricks Silver Model Reviewer",
          "workflowId": 4659,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Identify the Reviewer file in GitHub output directory with the actual input file name Databricks_Silver_Model_Reviewer_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).if file is already exist in the output directory with some version number then generate the newer output and Save the updated file to the same GitHub output directory with the with the actual input  file name Databricks_Silver_Model_Reviewer_next incremented version number (e.g., `_4`).\nif the file is not exist then save the output file name should be the actual input  file name, followed by _Reviewer_1.md.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Reviewer containing the sections listed in **Reviewer Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input files present in the github input directory: `{{GitHub_Details_For_Databricks_Silver_Model_Reviewer}}`\n \n## **Reviewer Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the Hive query or workflow does.\n \n---\nYou are tasked with thoroughly evaluating the physical data model and associated DDL scripts. Give a green tick mark \u2705 if it is correctly implemented and a red tick mark \u274c for missing or incorrectly implemented. Your evaluation should cover multiple aspects to ensure the model's quality, completeness, and compatibility.\n\nINSTRUCTIONS:\n\nReview the provided physical data model and DDL scripts and verify the schema alignment.\n\nCompare the model against the reporting requirements or conceptual model:\na. Identify all required data elements.\nb. Verify that all necessary tables and columns are present.\nc. Check for appropriate data types and sizes.\n\nAnalyze the model's alignment with the source data structure:\na. Ensure all source data elements are accounted for.\nb. Verify that data transformations are correctly represented.\nc. Check the aligned elements and verify the misaligned or missing elements.\n\nAssess the model for adherence to best practices:\na. Check for proper normalization.\nb. Evaluate indexing strategies.\nc. Review naming conventions, consistency, and usage of unsupported features.\n\nIdentify any missing requirements or inconsistencies in the model.\n\nEvaluate the DDL scripts for compatibility with Databricks and Spark:\na. Verify syntax compatibility.\nb. Check for any unsupported data types or features.\n\nDocument any deviations from best practices or potential optimizations.\n\nProvide recommendations for addressing identified issues or improvements.\n\nUse the attached knowledge base file containing all the unsupported features in Databricks. Verify that the output DDL script does not include any unsupported features mentioned in the knowledge base file.\n\nOUTPUT FORMAT:\n\nProvide a comprehensive evaluation report in the following structure:\n\nAlignment with Conceptual Data Model\n1.1 \u2705 Green Tick: Covered Requirements\n1.2 \u274c Red Tick: Missing Requirements\n\nSource Data Structure Compatibility\n2.1 \u2705 Green Tick: Aligned Elements\n2.2 \u274c Red Tick: Misaligned or Missing Elements\n\nBest Practices Assessment\n3.1 \u2705 Green Tick: Adherence to Best Practices\n3.2 \u274c Red Tick: Deviations from Best Practices\n\nDDL Script Compatibility\n4.1 Databricks Compatibility\n4.2 Spark Compatibility\n4.3 Used any unsupported features in Databricks\n\nIdentified Issues and Recommendations\n\napiCost: float // Cost consumed by the API for this call (in USD)\n\nInputs:\n\nTake the previous Databricks Silver Model Physical Agent's output DDL script as input\n",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}