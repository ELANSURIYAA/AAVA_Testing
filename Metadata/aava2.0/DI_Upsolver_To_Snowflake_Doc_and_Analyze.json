{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 6773,
      "name": "DI Upsolver To Snowflake Doc and Analyze",
      "description": "Upsolver to snowflake",
      "createdBy": "aarthy.jr@ascendion.com",
      "modifiedBy": "aarthy.jr@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2026-01-05T13:38:35.627320",
      "modifiedAt": "2026-01-05T14:25:56.084329",
      "approvedAt": "2026-01-05T14:25:56.074546",
      "status": "APPROVED",
      "comments": {
        "whatWentGood": "n",
        "whatWentWrong": "",
        "improvements": ""
      },
      "isDeleted": false,
      "parentId": 6262,
      "workflowConfigs": {
        "topP": null,
        "maxToken": null,
        "managerLlm": [],
        "temperature": null,
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 11765,
          "name": "DI Upsolver Documentation",
          "workflowId": 6773,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "INSTRUCTIONS:\n1. **Initial Assessment**:\n   - Analyze the provided Upsolver code from \n      \n      \n      \n      \n      \n      {{input1_string_true}}\n    \n    \n    \n    \n    \n    .\n   - Identify all ETL/ELT components, data sources, targets, transformations, and dependencies.\n   - Assess the complexity, structure, and technologies used.\n   - Research Upsolver best practices and enterprise documentation standards.\n\n2. **Strategic Planning**:\n   - Develop a documentation strategy aligned with the provided template, adapted for Upsolver.\n   - Identify all required sections: metadata, overview, structure, data flow, technology mapping, column mapping, complexity analysis, outputs, and API cost.\n   - Plan for handling large/complex jobs, reusable components, and external dependencies.\n   - Establish validation checkpoints for each documentation section.\n\n3. **Systematic Implementation**:\n   - Parse the Upsolver code to extract job structure, stages, sources, targets, and transformations.\n   - Identify all data sources and targets, including exact technologies (e.g., Amazon S3, Snowflake, Redshift, etc.).\n   - Map each transformation, join, filter, and aggregation step.\n   - Generate block-style markdown diagrams representing data flow and control logic.\n   - Create detailed column mapping tables, specifying source/target columns, data types, transformation logic, nullability, and defaults.\n   - Document all parameters, reusable components, and external dependencies.\n   - Ensure all metadata requirements are met at the top of the output.\n\n4. **Quality Assurance**:\n   - Validate that all required sections are present and correctly formatted.\n   - Check for accuracy in technology identification, mapping, and logic descriptions.\n   - Ensure markdown formatting is correct and diagrams are clear.\n   - Perform security and compliance checks on documentation content.\n\n5. **Optimization and Enhancement**:\n   - Optimize documentation for clarity, conciseness, and completeness.\n   - Ensure scalability for large/complex Upsolver jobs.\n   - Incorporate feedback and lessons learned from previous documentation projects.\n\n6. **Comprehensive Documentation**:\n   - Produce a single, pure markdown output with all required sections.\n   - Include troubleshooting and maintenance notes where relevant.\n   - Ensure knowledge transfer and continuity for future users.\n\n7. **Continuous Monitoring**:\n   - Track documentation quality metrics and user feedback.\n   - Plan for future updates as Upsolver jobs evolve.\n   - Maintain documentation versioning and change logs.\n\nINPUT PARAMETERS:\n- \n      \n      \n      \n      \n      \n      {{input1_string_true}}\n    \n    \n    \n    \n    \n    : Upsolver code (SQL, pipeline definition, or job metadata)\n\nOUTPUT FORMAT:\n- Metadata section at the top (author, created on, description)\n- Sectioned markdown documentation: Overview, Structure, Data Flow (with block diagrams), Technology Mapping, Column Mapping, Complexity Analysis, Key Outputs, API Cost Calculations\n- All diagrams in markdown text, no images\n- No code samples or original file content\n- No extra summary or recommendations\n- Strict adherence to formatting and section structure\n\nSAMPLE:\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:   \nDescription:   [Concise summary of Upsolver job purpose]\n=============================================\n\n# 1. Overview of Job\n[Detailed description of Upsolver job purpose, business process, and ETL/ELT alignment]\n\n# 2. Job Structure and Design\n[Explanation of job structure, stages, reusable components, design patterns, dependencies]\n\n# 3. Data Flow and Processing Logic\n[Block-style markdown diagram of data flow, with descriptions for each stage]\n\n# 4. Data Mapping\n## 4.1 Source and Target Technology\n**SOURCE TECHNOLOGY**: [e.g., Amazon S3, Snowflake, Redshift]\n**TARGET TECHNOLOGY**: [e.g., Snowflake, Redshift, S3]\n\n## 4.2 Detailed Column Mapping\n| Target Table Name | Target Column Name | Data Type | Source Stage Name | Source Column Name | Transformation Rule / Business Logic | Nullable | Default Value |\n|-------------------|-------------------|-----------|-------------------|-------------------|--------------------------------------|----------|---------------|\n[Complete mapping for each column]\n\n# 5. Complexity Analysis\n| Category | Measurement |\n|----------|-------------|\n| Number of Stages | [count] |\n| Source/Target Systems | [list technologies] |\n| Transformation Stages | [count] |\n| Parameters Used | [count] |\n| Reusable Components | [count] |\n| Control Logic | [description] |\n| External Dependencies | [list] |\n| Performance Considerations | [description] |\n| Volume Handling | [description] |\n| Error Handling | [description] |\n| Overall Complexity Score | [Score out of 100] |\n\n# 6. Key Outputs\n[Description of final outputs, storage formats, destinations, and downstream support]\n\n# 7. API Cost Calculations\n[ total cost in USD ]\n\nAdditionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n\n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value.\n\nleave created on as blank",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 12070,
          "name": "DI Upsolver to snowflake Analyzer",
          "workflowId": 6773,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "INSTRUCTIONS:\n1. **Initial Assessment**:\n   - Analyze the provided Upsolver code file (\n      \n      \n      \n      \n      \n      \n      {{input1_string_true}}\n    \n    \n    \n    \n    \n    \n    ) to extract all relevant metadata, including job stages, links, parameters, and transformation logic.\n   - Validate file format and ensure accurate extraction of job configuration details.\n   - Identify explicit and implicit migration requirements for Snowflake.\n\n2. **Strategic Planning**:\n   - Develop a comprehensive analysis strategy aligned with migration objectives.\n   - Identify dependencies, risks, and areas requiring manual intervention.\n   - Create a detailed implementation roadmap for migration, including quality gates and validation checkpoints.\n\n3. **Systematic Implementation**:\n   - Parse Upsolver code to extract job stages, source/target systems, transformation logic, parameters, reusable components, control logic, external dependencies, performance considerations, volume handling, and error handling.\n   - Generate complexity metrics and assign a numeric complexity score (0\u2013100) with conversion complexity rating (Low/Medium/High), justifying high-complexity areas.\n   - Map Upsolver constructs to Snowflake SQL syntax, providing relevant examples.\n   - Enumerate all manual adjustments required for migration, referencing actual job features.\n   - Recommend Snowflake optimization techniques, stating whether to refactor or rebuild, with reasoning.\n   - Calculate and display API cost for the analysis in full decimal precision.\n\n4. **Quality Assurance**:\n   - Validate all extracted metadata and metrics for accuracy and completeness.\n   - Ensure strict adherence to markdown formatting and section structure.\n   - Perform security, performance, and quality assessments.\n   - Document test results and validation outcomes.\n\n5. **Optimization and Enhancement**:\n   - Identify opportunities for performance improvement and future extensibility.\n   - Apply best practices for ETL migration and Snowflake optimization.\n   - Incorporate feedback and lessons learned.\n\n6. **Comprehensive Documentation**:\n   - Produce a markdown report with the following sections:\n     - Metadata block (Author, Created on, Description)\n     - #1 Job Overview\n     - #2 Complexity Metrics (table)\n     - #3 Syntax Differences\n     - #4 Manual Adjustments(table)\n     - #5 Optimization Techniques\n     - #6 API Cost\n   - Ensure all sections are populated with actual parsed values from the Upsolver code file.\n\n7. **Continuous Monitoring**:\n   - Establish monitoring and feedback mechanisms for future migration analysis.\n   - Track performance metrics and success indicators.\n   - Plan for ongoing maintenance and updates.\n\nINPUT PARAMETERS:\n- \n      \n      \n      \n      \n      \n      \n      {{input1_string_true}}\n    \n    \n    \n    \n    \n    \n    : Upsolver code file to be analyzed\n\nOUTPUT FORMAT:\n- Strict markdown report with all required sections and tables\n- All metrics and recommendations based on actual parsed Upsolver code\n- No summaries or assumptions; only direct evidence from the input file\n- API cost displayed in full decimal precision\n\nSAMPLE:\n\n=============================================\nAuthor: <parsed_author>\nCreated on:\nDescription: <parsed_description>\n\n=============================================\n\n# 1. Job Overview\n<parsed_job_overview>\n\n# 2. Complexity Metrics\n| Category | Measurement |\n|----------|------------|\n| Number of Stages | <parsed_value> |\n| Source/Target Systems | <parsed_value> |\n| Transformation Stages | <parsed_value> |\n| Parameters Used | <parsed_value> |\n| Reusable Components | <parsed_value> |\n| Control Logic | <parsed_value> |\n| External Dependencies | <parsed_value> |\n| Performance Considerations | <parsed_value> |\n| Volume Handling | <parsed_value> |\n| Error Handling | <parsed_value> |\n| Overall Complexity Score | <parsed_value> |\n| Conversion Complexity Rating | <parsed_value> |\n\n# 3. Syntax Differences\n<parsed_syntax_differences>\n\n# 4. Manual Adjustments\n<parsed_manual_adjustments>\n\n# 5. Optimization Techniques\n<parsed_optimization_techniques>\n\n# 6. API Cost\napiCost: <parsed_api_cost> USD\n\nleave created on blank\nfor author give Ascendion AAVA",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 13758,
          "name": "DI upsolver to snowflake plan",
          "workflowId": 6773,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "INSTRUCTIONS:\n1. **Initial Assessment**:\n   - Analyze Upsolver code (\n      \n      {{input1_string_true}}\n    \n    ), Snowflake table details (\n      \n      {{input2_string_true}}\n    \n    ), and DI Upsolver to Snowflake analyzer output (\n      \n      \n    \n    ).\n   - Identify explicit and implicit migration requirements, transformation complexity, and success criteria.\n   - Research relevant ETL migration standards, cost estimation methodologies, and validation frameworks.\n\n2. **Strategic Planning**:\n   - Develop a comprehensive migration strategy aligned with project objectives.\n   - Identify manual re-implementation tasks, dependencies, risks, and mitigation strategies.\n   - Create a detailed implementation roadmap with milestones and quality gates.\n\n3. **Systematic Implementation**:\n   - Exclude direct 1:1 mapping constructs; focus on logic-heavy transformations (aggregations, conditional derivations, lookups, joins, business rules).\n   - Estimate hours for rewriting complex Upsolver logic in Snowflake SQL, implementing/testing models, macros, incremental logic, and metadata tracking.\n   - Align schema and datatypes between Upsolver and Snowflake targets.\n   - Add validation steps for data equivalence and business rule consistency.\n   - Maintain detailed documentation throughout the process.\n\n4. **Quality Assurance**:\n   - Conduct thorough testing and validation of all outputs.\n   - Compare row counts, sample records, aggregations, joins, and derived columns between Upsolver and Snowflake outputs.\n   - Execute Snowflake tests (unique, not_null, accepted_values).\n   - Document and resolve discrepancies.\n\n5. **Optimization and Enhancement**:\n   - Identify optimization opportunities (incremental/partitioning, performance tuning).\n   - Ensure scalability, maintainability, and extensibility.\n   - Incorporate feedback and lessons learned.\n\n6. **Comprehensive Documentation**:\n   - Create detailed documentation covering all aspects of migration, effort estimation, cost breakdown, and validation.\n   - Include troubleshooting guides and maintenance procedures.\n   - Provide recommendations for future improvements.\n\n7. **Continuous Monitoring**:\n   - Establish monitoring and feedback mechanisms.\n   - Track performance metrics and success indicators.\n   - Plan for future updates, maintenance, and improvements.\n\nINPUT PARAMETERS:\n- \n      \n      {{input1_string_true}}\n    \n    : Upsolver code (ETL job definitions, transformation logic)\n- \n      \n      {{input2_string_true}}\n    \n    : Snowflake table details (schema, metadata, partitioning)\n- \n      \n      \n    \n    : DI Upsolver to Snowflake analyzer output (complexity score, metadata analysis)\n\nOUTPUT FORMAT:\n=============================================\nAuthor:        Ascendion AAVA\nCreated on: (leave it blank)\nDescription:   Cost and effort estimation for Upsolver to Snowflake migration\n=============================================\n\n1. Cost Estimation\n1.1 Snowflake Runtime Cost\nEstimate runtime cost\n\nCost Breakdown:\n- Compute: $X\n- Storage: $X\n\nTotal Estimated Runtime Cost per Job:\n$X USD per run\n\nJustification:\n- Percentage of data processed per run\n- Job complexity\n- Storage impact\n- Need (or lack) of incremental/partitioning optimizations\n\n2. Code Fixing and Recontesting Effort Estimation\n2.1 Manual Model Fixes and Recon Testing Effort\n- Parameter replacement (e.g., Upsolver variables \u2192 Snowflake SQL variables)\n- Stage-to-model conversion (Upsolver transformations \u2192 Snowflake SQL/CTEs)\n- Schema and datatype alignment\n- Adding Snowflake tests for null checks and row-count validation\n- Estimate time (hrs) for each activity and subtotal the effort\n\n2.2 Output Validation Effort\n- Compare row counts and sample records\n- Validate aggregations, joins, and derived columns\n- Execute Snowflake tests (unique, not_null, accepted_values)\n- Document and resolve discrepancies\n- Provide estimated effort (hours) for validation + documentation\n\n2.3 Total Estimated Effort in Hours\nSummarize all efforts in a table:\nTask\tEstimated Hours\tNotes\nManual Code Fixes & Recontesting\t\t\nOutput Validation\t\t\nTotal Estimated Effort\t\t\nJustify effort based on Analyzer complexity score:\nSimple (0-20) \u2192 \u2264 5 hrs\nModerate (21-50) \u2192 6\u201312 hrs\nComplex (51-100) \u2192 > 12 hrs\n\n3. API Cost Consumption\napiCost: X.XXXXX USD\n\nchange testing as recontesting",
          "modelName": "model"
        }
      ],
      "realmId": 79,
      "tags": [
        1
      ],
      "practiceArea": 6
    }
  },
  "status": "SUCCESS"
}