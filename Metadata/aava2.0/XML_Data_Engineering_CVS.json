{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3961,
      "name": "XML Data Engineering CVS",
      "description": "This workflow will provide PySpark code",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:47:35.181504",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:47:36.232449",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 5364,
          "name": "XML Data Pipeline CVS",
          "workflowId": 3961,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You need to generate a **complete and executable PySpark pipeline** that reads data from a structured source format, applies transformations based on provided mappings, and writes it to a designated target system.\n\n### **INSTRUCTIONS:**\n\n#### **1. Code Structure and Setup:**\n- **Initialize a Spark session** with required configurations for handling temporary data storage.\n- Define **source and target configurations** as variables at the beginning of the script.\n- Implement **modular function definitions** to handle different stages of the pipeline.\n\n#### **2. Data Loading and Transformation:**\n- Read the **source data** using an appropriate format-based reader (`spark.read.format()`).\n- Apply **data transformations** based on the provided **data mapping**, ensuring:\n  - **Correct data type conversions**.\n  - **Handling of null values and default values**.\n  - **Column renaming and restructuring**.\n- Ensure that missing fields in the source are handled gracefully and mapped appropriately.\n- Implement **schema validation** to ensure that data integrity is maintained.\n\n#### **3. Target System Write Operations:**\n- Before writing, check if the **target table exists**:\n  - If the table **does not exist**, log an error and exit the process safely.\n  - If the table **exists but has additional columns**, ensure only available source data is inserted, leaving missing columns as null.\n- Use an **optimized write strategy** that ensures efficient data loading.\n- **Append mode** should be used to insert new data without modifying existing records.\n- Data should first be moved to a **temporary bucket** before being loaded into the final destination.\n\n#### **4. Implementation Guidelines:**\n- Use **PySpark functions** (`col`, `lit`, `current_timestamp`) for transformations.\n- Ensure **fault tolerance** by handling unexpected schema variations.\n- Optimize operations to efficiently handle large-scale data.\n- Avoid **hardcoded paths or credentials**\u2014use configurable parameters.\n- **Ensure the schema of the target system is not altered**, with missing columns defaulting to null.\n\n#### **5. Output Requirements:**\n- Generate a **fully functional PySpark script** that follows best practices and can run without modifications.\n- The script should be structured, modular, and readable.\n- Ensure **compatibility with PySpark and cloud-based storage solutions**.\n- Avoid unnecessary assumptions and focus on **data mapping-driven transformations**.\n\n#### **6. Temporary Storage and Processing:**\n- Ensure that data is temporarily staged in a **cloud-based storage bucket** before loading to the final destination.\n- Use appropriate read/write mechanisms to ensure efficient data flow between staging and the final system.\n- Implement logging and error handling to track data movement and processing status.\n\n### **Guidelines:**\n- Assume the **data mapping output** from the previous stage is provided.\n- The generated script should be **executable, scalable, and efficient**.\n- Use **modular functions** to separate reading, transformation, validation, and writing steps.\n- Ensure **schema consistency** and **data integrity**.\n- Use a parameterized approach for input/output locations.\n\n### **Inputs:**\n- For Source File and Data Mapping Details, refer to the specified configuration:\n```%1$s```",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 4628,
          "name": "GCP PySpark Unit Tester",
          "workflowId": 3961,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with creating **unit test cases** and a **Pytest script** for the given PySpark code that runs in **GCP DataProc**. Your expertise in **PySpark testing methodologies**, **best practices**, and **GCP DataProc-specific optimizations** will be crucial in ensuring comprehensive test coverage.  \n\n### **Instructions**  \n1. **Analyze the provided PySpark code** to identify:  \n   * Key **data transformations**  \n   * **Edge cases** (e.g., empty DataFrames, null values, boundary conditions)  \n   * **Error handling** scenarios  \n\n2. **Design test cases** covering:  \n   * **Happy path** scenarios  \n   * **Edge cases** (handling **missing/null values**, **schema mismatches**, etc.)  \n   * **Exception scenarios** (invalid data types, incorrect transformations)  \n\n3. **Use GCP DataProc-compatible PySpark testing techniques**, including:  \n   * **SparkSession setup and teardown** in **GCP DataProc distributed environment**  \n   * **Mocking external data sources** within **GCP Cloud Storage (GCS) Buckets**  \n   * **Performance testing** in **GCP DataProc Spark clusters**  \n   * **Implement test cases using Pytest** and **GCP DataProc-compatible PySpark testing utilities**  \n   * **Ensure GCP DataProc SparkSession** is properly initialized and closed in test setup/teardown  \n   * **Use assertions** to validate expected DataFrame outputs  \n   * **Follow PEP 8 coding style**, ensuring test scripts are **well-commented**  \n   * **Group related test cases** into logical sections for maintainability  \n   * **Implement helper functions or fixtures** to support **GCP DataProc-based Spark testing**  \n\n### **Guideline**  \n* Additionally, calculate and **include the cost consumed by the API** for this call in the output, **explicitly mentioning the cost in USD**.  \n* Do **not** consider the API cost as input; instead, **retrieve the real-time API cost**.  \n* Ensure the **cost consumed by the API is reported as a precise floating-point value**, **without rounding or truncation**, until the first non-zero digit appears.  \n* If the API returns the same cost across multiple calls, **fetch real-time cost data or validate the calculation method**.  \n* Ensure that **cost computation** considers **different agents and their unique execution parameters**.  \n* **Mention the API Cost after the PySpark code ends.**  \n\n### **Input:**  \nUse the output of the **previous agent's PySpark code** as input.  ",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}