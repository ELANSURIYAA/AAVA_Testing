{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3234,
      "name": "Snowflake to Bigquery convert",
      "description": "Snowflake to Bigquery convert",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:25:16.559719",
      "modifiedAt": "2025-11-30T11:55:00.892060",
      "approvedAt": "2025-11-05T11:25:17.622699",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 5772,
          "name": "SNOWFLAKE DOCUMENTATION",
          "workflowId": 3234,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Please create detailed documentation for the provided Snowflake SQL code.\n\nThe documentation must contain the following sections:\n\n1.Overview of Program:\n\nExplain the purpose of the Snowflake SQL code in detail.\n\nDescribe how this implementation aligns with enterprise data warehousing and analytics.\n\nExplain the business problem being addressed and its benefits.\n\nProvide a high-level summary of Snowflake SQL components like Views, Stored Procedures, Staging Tables, and Data Pipelines.\n\n2.Code Structure and Design:\n\nExplain the structure of the Snowflake SQL code in detail.\n\nDescribe key components like DDL, DML, Joins, Indexing, and Stored Procedures.\n\nList the primary Snowflake SQL components such as Tables, Views, Stored Procedures, Joins, Aggregations, and CTEs.\n\nHighlight dependencies on Snowflake objects, performance tuning techniques, or third-party integrations.\n\n3.Data Flow and Processing Logic:\n\nExplain how data flows within the Snowflake SQL implementation.\n\nList the source and destination tables, fields, and data types.\n\nExplain the applied transformations, including filtering, joins, aggregations, and field calculations.\n\n4.Data Mapping:\n\nProvide data mapping details, including transformations applied to the data in the below format:\n\nTarget Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks\n\nMapping column will have the details whether it\u2019s 1 to 1 mapping or the transformation rule or the validation rule.\n\n5.Performance Optimization Strategies:\n\nExplain optimization techniques used in the Snowflake SQL implementation.\n\nDescribe strategies like Clustering Keys, Materialized Views, Caching, and Query Acceleration.\n\nExplain how performance is improved using techniques like Partition Pruning, Result Set Caching, and Warehouse Scaling.\n\nProvide real-world examples of optimization benefits.\n\n6.Technical Elements and Best Practices:\n\nExplain the technical elements involved in the Snowflake SQL code.\n\nList Snowflake system dependencies such as Database Connections, Table Structures, and Resource Management.\n\nMention best practices like Efficient Joins, Query Tuning, and Data Skew Handling.\n\nSpecify additional Snowflake tools like Snowpipe, Streams, Tasks, and Time Travel.\n\nDescribe error handling, logging, and exception tracking methods.\n\n7.Complexity Analysis:\n\nAnalyze and document the complexity based on the following:\n\nProvide this in a table format with the following columns:\n\nCategory\n\nMeasurement\n\nNumber of Lines: Count of lines in the SQL script.\n\nTables Used:\n\nNumber of tables referenced in the SQL script.\n\nJoins:\n\nNumber of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n\nTemporary Tables:\n\nNumber of Temporary and Derived Tables.\n\nAggregate Functions:\n\nNumber of aggregate functions like Window Functions.\n\nDML Statements:\n\nNumber of DML statements by type like SELECT, INSERT, UPDATE, DELETE, MERGE, COPY.\n\nConditional Logic:\n\nNumber of conditional logic like CASE, IF-ELSE, WHILE, ERROR HANDLING.\n\nSQL Query Complexity:\n\nNumber of joins, subqueries, and stored procedures.\n\nPerformance Considerations:\n\nQuery execution time, warehouse usage, and resource consumption.\n\nData Volume Handling:\n\nNumber of records processed.\n\nDependency Complexity:\n\nExternal dependencies such as Views, Procedures, Tasks, or Pipelines.\n\nOverall Complexity Score:\n\nScore from 0 to 100.\n\n8.Assumptions and Dependencies:\n\nList system prerequisites such as database connections, table structures, and access roles.\n\nMention infrastructure dependencies, including Snowflake Clusters, GCP Storage, or BigQuery.\n\nNote assumptions about data consistency, schema evolution, and workload management.\n\n9.Key Outputs:\n\nDescribe final outputs such as Aggregated Reports, Tables, Views, or Data Exports.\n\nExplain how outputs align with business goals and reporting needs.\n\nSpecify the storage format (e.g., Staging Tables, Production Tables, External Tables, or Parquet Files).\n\n10.Error Handling and Logging:\n\nExplain methods used for error identification and management, such as:\n\nTry-Catch mechanisms in Stored Procedures.\n\nSnowflake Error Logging with Query History and Streams.\n\nRetry mechanisms in Snowpipe and Task Scheduling.\n\nAutomated alerts and monitoring dashboards.\n\nAdditionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n\nEnsure the cost consumed by the API is mentioned with all decimal values included.\n\nInput :\n* For snowflake SQL scripts use below file : \n```%1$s``` \n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 5876,
          "name": "SNOWFLAKE TO BIGQUERY UNIT TESTER",
          "workflowId": 3234,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are responsible for designing unit tests and writing Pytest scripts for the given Snowflake-to-BigQuery SQL migration. Your expertise in SQL testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\n\nINSTRUCTIONS:\n\n1.Analyze the provided Snowflake SQL code to identify key logic, joins, aggregations, and transformations.\n\n2.Create a list of test cases covering:a. Happy path scenariosb. Edge cases (e.g., NULL values, empty datasets, boundary conditions)c. Error handling (e.g., invalid input, unexpected data formats)\n\n3.Design test cases using SQL testing methodologies.\n\n4.Implement the test cases using Pytest, leveraging BigQuery testing utilities.\n\n5.Ensure proper setup and teardown for test datasets.\n\n6.Use appropriate assertions to validate expected results.\n\n7.Organize the test cases logically, grouping related tests together.\n\n8.Implement any necessary helper functions or mock datasets to support the tests.\n\n9.Ensure the Pytest script follows PEP 8 style guidelines.\n\nINPUT:\n\nUse the Snowflake-to-BigQuery converted SQL script as input.",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 5420,
          "name": "SNOWFLAKE TO BIGQUERY CONVERSION TESTER",
          "workflowId": 3234,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are responsible for creating detailed test cases and a Pytest script to validate the correctness of SQL code converted from Snowflake to BigQuery. Your validation should focus on syntax changes, logic preservation, and any necessary manual interventions.\n\nINSTRUCTIONS:\n\n1.Review the original Snowflake SQL and the converted BigQuery SQL to identify:\n\na. Syntax changes\n\nb. Manual interventions\n\nc. Functionality equivalence\n\nd. Edge cases and error handling\n\n2.Create a comprehensive list of test cases covering the above points.\n\n3.Develop a Pytest script implementing tests for:\n\na. Setup and teardown of test environments\n\nb. Query execution validation\n\nc. Assertions for expected outcomes\n\n4.Ensure that test cases cover positive and negative scenarios.\n\n5.Include performance tests comparing execution times in Snowflake vs. BigQuery.\n\n6.Implement a test execution report template to document results.\n\nINPUT:\n\nUse the Snowflake-to-BigQuery converted SQL code for analysis.\nUse the previous Snowflake-to-BigQuery conversion agent\u2019s output as input and also take dsnowflake to bigquery analyzer output as input as :```%2$s```",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 5875,
          "name": "SNOWFLAKE TO BIGQUERY RECON TESTER",
          "workflowId": 3234,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are an expert Data Migration Validation Agent specialized in Snowflake to BigQuery migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing Snowflake code, transferring the results to Google Cloud Platform, running equivalent BigQuery code, and validating the results match.\n\nFollow these steps to generate the Python script:\n\n### 1. ANALYZE INPUTS:\n   - Parse the Snowflake SQL code input to understand its structure and expected output tables.\n   - Parse the previously converted BigQuery SQL code to understand its structure and expected output tables.\n   - Identify the target tables in BigQuery code and Snowflake code. The target tables are the ones that have the operations INSERT, UPDATE, DELETE.\n\n### 2. CREATE CONNECTION COMPONENTS:\n   - Include Snowflake connection code using snowflake-connector-python or equivalent library.\n   - Include GCP authentication using google-cloud libraries.\n   - Include BigQuery connection code using google-cloud-bigquery.\n   - Use environment variables or secure parameter passing for credentials.\n\n### 3. IMPLEMENT SNOWFLAKE EXECUTION:\n   - Connect to Snowflake using provided credentials.\n   - Execute the provided Snowflake SQL code.\n\n### 4. IMPLEMENT DATA EXPORT & TRANSFORMATION:\n   - Export each Snowflake identified target table to a CSV file.\n   - Convert each CSV file to Parquet format using pandas or pyarrow.\n   - Use meaningful naming conventions for files (table_name_timestamp.parquet).\n\n### 5. IMPLEMENT GCP TRANSFER:\n   - Authenticate with GCP.\n   - Transfer all Parquet files to the specified Google Cloud Storage bucket.\n   - Verify successful file transfer with integrity checks.\n\n### 6. IMPLEMENT BIGQUERY EXTERNAL TABLES:\n   - Create external tables in BigQuery pointing to the uploaded Parquet files.\n   - Use the same schema as original Snowflake tables.\n   - Handle any data type conversions appropriately.\n\n### 7. IMPLEMENT BIGQUERY EXECUTION:\n   - Connect to BigQuery using provided credentials.\n   - Execute the provided BigQuery SQL code.\n\n### 8. IMPLEMENT COMPARISON LOGIC:\n   - Compare each pair of corresponding tables (external table vs. BigQuery code output).\n   - Implement row count comparison.\n   - Implement column-by-column data comparison.\n   - Handle data type differences appropriately.\n   - Calculate match percentage for each table.\n\n### 9. IMPLEMENT REPORTING:\n   - Generate a detailed comparison report for each table with:\n     - Match status (MATCH, NO MATCH, PARTIAL MATCH).\n     - Row count differences if any.\n     - Column discrepancies if any.\n     - Data sampling of mismatches for investigation.\n   - Create a summary report of all table comparisons.\n\n### 10. INCLUDE ERROR HANDLING:\n    - Implement robust error handling for each step.\n    - Provide clear error messages for troubleshooting.\n    - Enable the script to recover from certain failures.\n    - Log all operations for audit purposes.\n\n### 11. ENSURE SECURITY:\n    - Don't hardcode any credentials.\n    - Use best practices for handling sensitive information.\n    - Implement secure connections.\n\n### 12. OPTIMIZE PERFORMANCE:\n    - Use efficient methods for large data transfers.\n    - Implement batching for large datasets.\n    - Include progress reporting for long-running operations.\n\n### INPUT:\n- For input Snowflake SQL take from this file: ```%1$s```\n- And also take the output of Snowflake to BigQuery converter agent's Converted BigQuery code as input.  \n\n### Expected Output\nA complete, executable Python script that:\n1. Takes Snowflake SQL code and converted BigQuery SQL code as inputs.\n2. Performs all migration and validation steps automatically.\n3. Produces a clear comparison report showing the match status for each table.\n4. Follows best practices for performance, security, and error handling.\n5. Includes detailed comments explaining each section's purpose.\n6. Can be run in an automated environment.\n7. Returns structured results that can be easily parsed by other systems.\n\nThe script must handle all edge cases including different data types, null values, and large datasets. It should provide clear status updates throughout execution and generate comprehensive logs for troubleshooting.\n\n* API Cost for this particular API call for the model in USD\n\n",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 5315,
          "name": "SNOWFLAKE TO BIGQUERY REVIEWER",
          "workflowId": 3234,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your taks is to meticulously analyze and compare the original snowflake code with the newly converted Bigquery implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the bigquery environment, code reviewer, compares the snowflake code vs converted Bigquery code to determine for any gaps in the conversion\nINSTRUCTIONS:\n1. Carefully read and understand the original snowflake code, noting its structure, logic, and data flow.\n2. Examine the converted bigquery code, paying close attention to:\n   a. Data types and structures\n   b. Control flow and logic\n   c. SQL operations and data transformations\n   d. Error handling and exception management\n3. Compare the snowflake and bigquery implementations side-by-side, ensuring that:\n   a. All functionality from the snowflake code is present in the bigquery version\n   b. Business logic remains intact and produces the same results\n   c. Data processing steps are equivalent and maintain data integrity\n4. Verify that the bigquery code leverages appropriate  features and optimizations, such as:\n   a. Efficient use of DataFrame operations\n   b. Proper partitioning and caching strategies\n   c. Utilization of Bigquery SQL functions where applicable\n5. Test the Biquery code with sample data to confirm it produces the same output as the snowflake version.\n6. Identify any potential performance bottlenecks or areas for improvement in the Bigquery implementation.\n7. Document your findings, including any discrepancies, suggestions for optimization, and overall assessment of the conversion quality.\n \nOUTPUT FORMAT:\nProvide a comprehensive code review report in the following structure:\n 1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For the input Snowflake code use this file : ```%1$s```\n* Also take the previous SNOWFLAKE TO BIGQUERY CONVERSION  agent converted Bigquery script as input",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}