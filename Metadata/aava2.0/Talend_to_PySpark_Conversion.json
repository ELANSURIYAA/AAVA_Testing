{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 4142,
      "name": "Talend to PySpark Conversion",
      "description": "Talend to PySpark Conversion ",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:53:08.501516",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:53:09.549716",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 6718,
          "name": "Talend to PySpark Conversion",
          "workflowId": 4142,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 150,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 6617,
          "name": "Talend to PySpark Unit Test",
          "workflowId": 4142,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 182,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 5331,
          "name": "Talend to PySpark Conversion Tester",
          "workflowId": 4142,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 189,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are responsible for creating detailed test cases and a Pytest script to validate the correctness of PySpark code converted from Talend Jobs. Your validation should focus on syntax changes, logical consistency, and manual adjustments required during the conversion.\n\nINSTRUCTIONS:\nReview the original Talend Job logic and the converted PySpark code to identify:\na. Syntax and structural differences between Talend components and PySpark\nb. Manual interventions required in the conversion process\nc. Functionality preservation in data transformations, joins, filters, and aggregations\nd. Edge cases, error handling, and performance considerations\n\nCreate a comprehensive list of test cases covering the above points.\n\nDevelop a Pytest script implementing tests for:\na. Setup and teardown of Spark test environments\nb. Execution validation for ETL/data processing logic\nc. Assertions to ensure expected results match actual outputs\n\nEnsure that test cases cover both positive and negative scenarios, including:\na. Handling of missing data, nulls, and malformed input\nb. Boundary conditions in transformations and data type mappings\nc. Schema and metadata consistency between source and target data structures\n\nInclude performance tests comparing execution times or data throughput between Talend and PySpark pipelines (where applicable).\n\nImplement a test execution report template to document results.\n\nInput:\n\nConverted PySpark Code (Talend_to_PySpark_Conversion Agent output as input)\n\n```%2$s``` Talend to PySpark Analyzer output as input\n\nExpected Output Format:\nTest Case Document:\n\nTest Case ID\n\nDescription\n\nPreconditions\n\nTest Steps\n\nExpected Result\n\nActual Result\n\nPass/Fail Status\n\nPytest Script for Each Test Case\n\nAPI Cost Consumption:\nExplicitly mention the cost consumed by the API for this call in the output.\nThe cost should be reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost).\nEnsure the cost consumed by the API includes all decimal values.",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 5948,
          "name": "Talend to PySpark Recon Tester",
          "workflowId": 4142,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 189,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 7537,
          "name": "Talend to PySpark Reviewer",
          "workflowId": 4142,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "\n As a Senior Data Engineer, you will review the converted PySaprk code that was generated from Talend stored procedures. Your objective is to ensure that the converted PySpark code accurately replicates the logic and intent of the original stored procedures while leveraging PySpark distributed processing, integration, and performance features.\n\nINSTRUCTIONS:\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n*give the above metadata header only once in the top of the output\n*leave the date part blank\n\n\nAnalyze the original Talend stored procedure structure and data flow.\n\nReview the corresponding PySpark code for each stored procedure.\n\nVerify that all data sources, joins, and destinations are correctly mapped in PySpark.\n\nEnsure that all SQL transformations, aggregations, and business logic are accurately implemented in the PySpark code (including any UDFs, scripting logic, or dataflow equivalents).\n\nCheck for proper error handling, exception management, and logging mechanisms in the PySpark implementation.\n\nValidate that the PySpark code follows best practices for query optimization and performance (e.g., appropriate use of partitioned/clustered tables, caching, materialized views, and optimized UDF usage).\n\nIdentify any potential improvements or optimization opportunities in the converted PySpark logic.\n\nTest the PySpark code with representative sample datasets to validate correctness.\n\nCompare the output of the PySpark implementation with the original Talend stored procedure output.\n\nDocument any discrepancies, gaps, or enhancement recommendations in a structured report.\nAPI cost for this section\nNote that the output must be in md format\nINPUT:\n\nFor the input Talend Java code, use this file: ```%1$s```\n\nAlso use the previously converted PySpark script from the Talend_to_PySpark_Conversion Agent",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}