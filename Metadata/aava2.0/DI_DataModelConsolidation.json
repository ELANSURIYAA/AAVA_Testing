{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 1894,
      "name": "DI DataModelConsolidation",
      "description": "Analyze DDLs of data pipeline layers across EDW, Lakehouse, determine the best model for enhancements ",
      "createdBy": "muneeswara.pandian@ascendion.com",
      "modifiedBy": "muneeswara.pandian@ascendion.com",
      "approvedBy": "muneeswara.pandian@ascendion.com",
      "createdAt": "2025-11-05T10:39:12.851427",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:39:14.064732",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4192,
          "name": "DI DataModelConsolidation Analyzer",
          "workflowId": 1894,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [
              4
            ],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent is tasked with analyzing the provided DDL (Data Definition Language) scripts for various data platforms to identify:  \n1. **Overlapping Tables:** Tables that exist across multiple platforms.  \n2. **Overlapping Columns:** Columns with identical names and data types that are present in tables across different platforms.  \n3. **Unique Tables:** Tables that are exclusive to a specific platform.  \n4. **Unique Columns:** Columns that are exclusive to a specific table or platform.  \n\nUse the FileWriterTool and give the output in the markdown format.\nEnsure listing all the data attributes. Do not give commments as ' (additional tables and columns mapped similarly)' and leave out listing all the data attributes in the mapping table.\n\nINSTRUCTIONS:\n\n#### **Context and Background Information:**  \n- The DDL scripts define the schema for tables and columns across the data platforms.  \n- Platforms include data lakes, lakehouses, data warehouses, and data marts, each serving distinct purposes but often sharing data.  \n- Overlaps may indicate shared business logic or redundant data storage, while unique elements highlight specialized data structures. \n\n####Scope and Constraints:\n-Focus only on schema-level analysis (tables and columns).\n-Ensure the analysis also looks at the domain function during analysis.\n-Use standardized terminology such as \"schema,\" \"attribute,\" \"data type,\" \"primary key,\" \"foreign key,\" etc.\n\n####Process Steps:\n\nStep 1: Load and parse the schema definitions for the data warehouse, data lake, and data mart models.\n\nStep 2: Identify all tables and columns in each model.\n\nStep 3: Compare tables across the models to identify:\n-Common tables (exist in all models).\n-Overlapping tables (exist in some models but not all).\n-Unique tables (exist in only one model).\n\nStep 4: For each table, compare column attributes to identify:\n-Common columns (exist in all models).\n-Overlapping columns (exist in some models but not all).\n-Unique columns (exist in only one model).\n\nStep 5: Highlight differences in column characteristics such as:\n-Data type (e.g., INT, VARCHAR, DATE).\n-Format (e.g., YYYY-MM-DD, MM/DD/YYYY).\n-Length (e.g., VARCHAR(50) vs. VARCHAR(100)).\n\nStep 6: Summarize findings in a structured markdown format, ensuring clarity and completeness.\n\n####Output Format:\n-Use Markdown for the report structure.\n-Provide tabular comparisons for tables and columns using Markdown tables.\n\n####Ensure the output adheres to the following quality criteria:\n-Clear and concise language.\n-Accurate use of domain-specific terminology.\n-Logical organization and formatting.\n*For \"Table Name\" give the exact table name , column from the DDL one after another in\nLike this \n| Table Name | Column Name | DataLake_1 | DataLake_2 | DataLake_3 | Remarks |  \n\nand tell in which the layer  the table-column is X (Present), Blank Space(Not Present)\n\nSample output:\n- Add the following metadata at the top of the file:\n=============================================    \nAuthor:       Ascendion AVA+   \nCreated on:    {leave it empty}   \nDescription:  {one-line description of the purpose}   \n=============================================  \n## Table-to-DataLake Mapping  \n| Table Name | Column Name | DataLake_1 | DataLake_2 | DataLake_3 | Remarks |  \n|------------|-------------|------------|------------|------------|---------|  \n\n##DataLake Uniqueness\n## DataLake_1 Uniqueness  \n| Table Name | Column Name | Data Type | Any Constraints |  \n|------------|-------------|-----------|------------------|  \n|            |             |           |                  |  \n \n## DataLake_2 Uniqueness  \n| Table Name | Column Name | Data Type | Any Constraints |  \n|------------|-------------|-----------|------------------|  \n|            |             |           |                  |  \n \n## DataLake_3 Uniqueness  \n| Table Name | Column Name | Data Type | Any Constraints |  \n|------------|-------------|-----------|------------------|  \n|            |             |           |                  | \nInput:\n{{DataModelFiles}}\n\nPoints to Remember for Handling Input files:\n- ensure table name and column names are listed in the tables 'AS IS' from the DDL statements\n-- Use FileWriter Tool to write all the output in the Markdown format\n\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 3575,
          "name": "DI DataModelConsolidation Recommender",
          "workflowId": 1894,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent is tasked with analyzing and improving the structure of data models by following the detailed instructions below:\n\nINSTRUCTIONS\n### Scope:\n\n-Compare table structures, column overlaps, and unique columns across the three data models.\n-Recommend a unified data model based on analysis.\n-Propose structural changes to the recommended model for integration and optimization.\n-Generate DDL updates for the proposed changes.\n\n### Constraints:\n-Ensure recommendations align with industry best practices for data modeling.\n-Maintain backward compatibility wherever possible.\n-Avoid introducing redundant tables or columns.\n\n### Process Steps to Follow:\nAnalysis:\n-Extract metadata for tables and columns from the Data Warehouse, Data Mart, and Data Lakehouse.\n-Identify common tables and columns across the models.\n-Highlight unique tables and columns in each model.\n\nRecommendation:\n-Determine which data model contains the most common tables and columns.\n-Recommend the most suitable data model for integration based on scalability, flexibility, and alignment with organizational needs.\n\nStructural Improvements:\n-Propose additions (new tables and columns) to the recommended data model based on unique elements from other models.\n-Suggest alterations to existing tables and columns to improve normalization, performance, and usability.\n-Provide clear reasons for each recommendation.\n\nDDL Updates:\n-Generate SQL DDL statements for the proposed changes, including:\n-CREATE TABLE statements for new tables.\n-ALTER TABLE statements for modifying existing tables.\n-ADD COLUMN statements for new columns.\n\n### Points to remember:\n-get the exact table name and coloumn name from the previous agent \n-Previous agent use the filewritertool to write the output as text file try to get that\n\nInput:\nFor input use the previous agent output as input 'DI_DataModelConsolidation_Analyzer'\n\n\nOUTPUT FORMAT:\n### **SAMPLE OUTPUT**\n```markdown\n### **Data Model Match Scores**\n| Model Pair          | Similarity Score (%) | Remarks |\n|---------------------|----------------------|---------------------------|\n-Model Match Score Summary: Recommend the best model for further enhancement with justification\n\n### **Summary of Datamodel updates with model which has highest match score**\nList of tables to be added:\nTables to be Added:\n| Table name | Column | Datatype |\n|------------|--------|----------|\n\n### **Column-Level Updates for Existing Tables**\n| Table name | Column Name | Addition Type (New, Alter) | Remarks |\n|------------|--------------|----------------------------|---------|\n### **DDL Update Statements**\n\n\n",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}