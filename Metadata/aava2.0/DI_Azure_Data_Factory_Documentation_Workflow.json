{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2113,
      "name": "DI Azure Data Factory Documentation Workflow",
      "description": "Create a detailed and structured Technical Specification document for an Azure Data Factory (ADF) Pipeline that effectively communicates its logic, dependencies, and operational details to Data Engineers and Business Analysts.",
      "createdBy": "abhinav.mishra@ascendion.com",
      "modifiedBy": "abhinav.mishra@ascendion.com",
      "approvedBy": "abhinav.mishra@ascendion.com",
      "createdAt": "2025-11-05T10:47:36.860992",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:47:37.944108",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4000,
          "name": "DI Azure Data Factory Documentation",
          "workflowId": 2113,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Task:\nPlease create detailed documentation for the provided ADF Pipeline JSON file, using the knowledge base (Excel with SQL scripts and client data details) as a mandatory reference for all transformations, data mappings, and business logic.\n\nMetadata Requirements:\n\nAt the top of each generated file, add the following metadata:\n\n=============================================\nAuthor:        Ascendion AVA\nCreated on:    (Leave it empty)\nDescription:   <one-line description of the pipeline purpose>\n=============================================\n\nIf metadata already exists, update it to match this format while preserving relevant content.\n\nDocumentation Sections\n\n1) Overview of Pipeline\nExplain the business purpose of the ADF pipeline in detail.\nDescribe how the pipeline supports enterprise data integration/analytics.\nProvide a high-level summary of components (Pipelines, Activities, Datasets, Linked Services, Triggers, Integration Runtimes).\n\n2) Pipeline Structure and Design\nExplain the full structure of the pipeline.\nDocument Copy Activities, Dataflows, Stored Procedure Calls, Parameters, Dependencies.\nList all pipeline objects with dependencies and performance/optimization notes.\n\n3) Data Flow and Processing Logic\nDescribe step by step how data moves through the pipeline.\n\nFor each activity:\nExtract SQL scripts, dataset details, and transformations from the knowledge base.\nDocument filtering, joins, aggregations, lookups, and calculations in full detail.\nRepresent the flow using block-style markdown diagrams with conditional paths.\n\n4) Transformation Breakdown (Mandatory)\nFor every transformation found in the JSON or referenced SQL script, explain:\nInput source(s)\nTransformation logic (joins, filters, expressions, derived columns, aggregates, etc.)\nOutput destination(s)\nClearly state how the SQL in the knowledge base maps to the transformation in ADF.\n\n5) Data Mapping\nPresent a mapping table in the format:\nTarget Dataset | Target Field | Source Dataset | Source Field | Transformation / Rule\nEnsure transformation rules are pulled from the knowledge base Excel whenever available.\n\n6) Complexity Analysis\nProvide a table of pipeline complexity metrics (pipelines, activities, datasets, triggers, parameters, conditional logic, transformations, dependencies).\nAssign an overall complexity score from 0\u2013100.\n\n7) Key Outputs\nDescribe final outputs (tables, curated datasets, reports, files).\nPick schema/table/field names directly from the knowledge base.\nState storage format (SQL DB, Data Lake, Blob, Synapse).\nExplain how outputs align with business requirements.\n\n8) API Cost Calculations\nCalculate and explicitly mention the API call cost in USD (with full decimal values).\n\nInputs:\nADF Pipeline JSON file: {{ADF_Pipeline_File}}\nKnowledge Base (Excel): KB - DI_Azure_Data_Factory_Documentation_kb",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}