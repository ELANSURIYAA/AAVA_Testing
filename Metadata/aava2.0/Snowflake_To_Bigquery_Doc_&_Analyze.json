{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3879,
      "name": "Snowflake To Bigquery Doc & Analyze",
      "description": "Snowflake_To_Bigquery_Doc_&_Analyze",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:45:07.405538",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:45:08.457073",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 5772,
          "name": "SNOWFLAKE DOCUMENTATION",
          "workflowId": 3879,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Please create detailed documentation for the provided Snowflake SQL code.\n\nThe documentation must contain the following sections:\n\n1.Overview of Program:\n\nExplain the purpose of the Snowflake SQL code in detail.\n\nDescribe how this implementation aligns with enterprise data warehousing and analytics.\n\nExplain the business problem being addressed and its benefits.\n\nProvide a high-level summary of Snowflake SQL components like Views, Stored Procedures, Staging Tables, and Data Pipelines.\n\n2.Code Structure and Design:\n\nExplain the structure of the Snowflake SQL code in detail.\n\nDescribe key components like DDL, DML, Joins, Indexing, and Stored Procedures.\n\nList the primary Snowflake SQL components such as Tables, Views, Stored Procedures, Joins, Aggregations, and CTEs.\n\nHighlight dependencies on Snowflake objects, performance tuning techniques, or third-party integrations.\n\n3.Data Flow and Processing Logic:\n\nExplain how data flows within the Snowflake SQL implementation.\n\nList the source and destination tables, fields, and data types.\n\nExplain the applied transformations, including filtering, joins, aggregations, and field calculations.\n\n4.Data Mapping:\n\nProvide data mapping details, including transformations applied to the data in the below format:\n\nTarget Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks\n\nMapping column will have the details whether it\u2019s 1 to 1 mapping or the transformation rule or the validation rule.\n\n5.Performance Optimization Strategies:\n\nExplain optimization techniques used in the Snowflake SQL implementation.\n\nDescribe strategies like Clustering Keys, Materialized Views, Caching, and Query Acceleration.\n\nExplain how performance is improved using techniques like Partition Pruning, Result Set Caching, and Warehouse Scaling.\n\nProvide real-world examples of optimization benefits.\n\n6.Technical Elements and Best Practices:\n\nExplain the technical elements involved in the Snowflake SQL code.\n\nList Snowflake system dependencies such as Database Connections, Table Structures, and Resource Management.\n\nMention best practices like Efficient Joins, Query Tuning, and Data Skew Handling.\n\nSpecify additional Snowflake tools like Snowpipe, Streams, Tasks, and Time Travel.\n\nDescribe error handling, logging, and exception tracking methods.\n\n7.Complexity Analysis:\n\nAnalyze and document the complexity based on the following:\n\nProvide this in a table format with the following columns:\n\nCategory\n\nMeasurement\n\nNumber of Lines: Count of lines in the SQL script.\n\nTables Used:\n\nNumber of tables referenced in the SQL script.\n\nJoins:\n\nNumber of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n\nTemporary Tables:\n\nNumber of Temporary and Derived Tables.\n\nAggregate Functions:\n\nNumber of aggregate functions like Window Functions.\n\nDML Statements:\n\nNumber of DML statements by type like SELECT, INSERT, UPDATE, DELETE, MERGE, COPY.\n\nConditional Logic:\n\nNumber of conditional logic like CASE, IF-ELSE, WHILE, ERROR HANDLING.\n\nSQL Query Complexity:\n\nNumber of joins, subqueries, and stored procedures.\n\nPerformance Considerations:\n\nQuery execution time, warehouse usage, and resource consumption.\n\nData Volume Handling:\n\nNumber of records processed.\n\nDependency Complexity:\n\nExternal dependencies such as Views, Procedures, Tasks, or Pipelines.\n\nOverall Complexity Score:\n\nScore from 0 to 100.\n\n8.Assumptions and Dependencies:\n\nList system prerequisites such as database connections, table structures, and access roles.\n\nMention infrastructure dependencies, including Snowflake Clusters, GCP Storage, or BigQuery.\n\nNote assumptions about data consistency, schema evolution, and workload management.\n\n9.Key Outputs:\n\nDescribe final outputs such as Aggregated Reports, Tables, Views, or Data Exports.\n\nExplain how outputs align with business goals and reporting needs.\n\nSpecify the storage format (e.g., Staging Tables, Production Tables, External Tables, or Parquet Files).\n\n10.Error Handling and Logging:\n\nExplain methods used for error identification and management, such as:\n\nTry-Catch mechanisms in Stored Procedures.\n\nSnowflake Error Logging with Query History and Streams.\n\nRetry mechanisms in Snowpipe and Task Scheduling.\n\nAutomated alerts and monitoring dashboards.\n\nAdditionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n\nEnsure the cost consumed by the API is mentioned with all decimal values included.\n\nInput :\n* For snowflake SQL scripts use below file : \n```%1$s``` \n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 5613,
          "name": "SNOWFLAKE TO BIGQUERY ANALYZER",
          "workflowId": 3879,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "\"Please analyze the following Snowflake SQL query and provide an overview of its structure and elements while assessing the effort required to convert it to BigQuery\"\n\nNote:\n* Identify all tables explicitly mentioned in the query.\n* List all joins explicitly mentioned in the query. Include join types (e.g., INNER JOIN, LEFT JOIN) and the conditions used (e.g., ON, USING).\n* List all functions (e.g., aggregate, string, date functions) used in the query.\n* Assess syntax differences between the input Snowflake SQL query and its equivalent BigQuery code.\n* Identify any Snowflake-specific features such as QUALIFY, WINDOW, or ARRAY handling, and describe how they will need to be adapted to BigQuery.\n* Provide a clear and concise remark describing the notable differences or challenges in conversion.\n* Ensure accurate complexity assessment with proper justification in the description.\n* Ensure that the analysis strictly follows the format below.\n\nthe input is mentioned below:\n\n%1$s",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 5251,
          "name": "SNOWFLAKE TO BIGQUERY PLAN",
          "workflowId": 3879,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 284,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive effort estimate for testing the Bigquery converted from snowflake scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n1. Review the analysis of snowflake script file, noting syntax differences when converting to Bigquery and areas requiring manual intervention.\n2. Consider the pricing information for GCP Bigquery environment \n4. Calculate the estimated cost of running the converted Bigquery code:\n   a. Use the pricing information and data volume to determine the code cost.\n   b. the number of code and the data processing done with the base tables and temporary tables\n5. Estimate the code fixing and data recon testing effort required:\n\nINPUT :\n* Take the previous SNOWFLAKE_TO_BIGQUERY_ANALYZER agent output as  input\n* For the input Snowflake script use this file : ```%1$s```\n* For the input  Bigquery Environment Details for GCP  use this file : ```%2$s```",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}