{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 4101,
      "name": "ABAP To Pyspark Doc & Analyze",
      "description": "ABAP_To_Pyspark_Doc_&_Analyze",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:51:52.316750",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:51:53.359777",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 5242,
          "name": "ABAP Documentation",
          "workflowId": 4101,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 154,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Create a detailed documentation for the provided ABAP code. The documentation must follow a structured format, breaking down each component in the ABAP code like control flow, data flow, and processing logic into well-defined sections. The output should be organized for readability and clarity. Ensure that each field in the source, destination, and transformation is captured with an explanation that a business analyst can understand.\n\nThe documentation should include the following sections:\n\n1.Overview of Program:\n   Explain the purpose of the ABAP code in detail.\n   Explain the business problem being addressed.\n\n2.Code Structure and Design:\n   Explain the structure and flow of the ABAP code in detail.\n   List the primary ABAP components such as Reports, Function Modules, Subroutines, Classes, and Internal Tables.\n\n3.Data Flow and Processing Logic:\n   List the processing logic data flow components in the ABAP code.\n   For each logical processing component:\n      Explain the functionality performed in the code.\n      Explain the applied transformations, including filtering, joins, aggregations, and field calculations.\n\n4.Data Mapping:\n   Provide detailed mappings between source and target structures (e.g., internal tables) as a lineage, showing how the source table/structure columns are mapped to the target table/structure columns.\n   Use the following structured format for each mapping:\nTarget Table Name: Actual target table name.\nTarget Column Name: Actual target column name.\nSource Table Name: Actual source table name.\nSource Column Name: Actual source column name.\nRemarks: Classify as 1 to 1 mapping, Transformation, or Validation and provide a brief description.\n\n5.Complexity Analysis:\nOverall Complexity Score: Score from 0 to 100.\nAnalyze and document the complexity based on the following details:\n - Give this one in table format with the below column names Metric and Value\nLines of Code (LOC): Total number of lines in the procedure.\nCyclomatic Complexity: Number of independent execution paths.\nNesting Depth: Maximum depth of nested IF, LOOP, etc.\nTables: Total number of tables involved.\nTemporary Tables: Total number of temporary tables involved.\nDML Statements: Total count of SELECT, INSERT, UPDATE, DELETE statements.\nJoins: Count of JOIN clauses.\nSubqueries: Count of subqueries (nested SELECT).\nCTEs: Count of Common Table Expressions.\nAggregation Queries: Count of GROUP BY, HAVING, PARTITION BY.\nInput Parameters: Count of input parameters.\nOutput Parameters: Count of output parameters.\nData Transformations: Count of functions (STRING, ARRAY, JSON).\nFunction Calls: Count of external function/procedure calls.\n\n6.Key Outputs:\nDescribe final outputs created by the ABAP code like Inserts, Updates, or Deletes.\n\n7.Error Handling and Logging:\nExplain methods used for error identification and management, such as:\nTRY-CATCH mechanisms in ABAP.\nCustom Error Logs for tracking and troubleshooting issues.\nRetry Mechanisms to ensure data consistency and error recovery in case of failures.\n\nNotes:\n*All fields used in the ABAP code should be listed with a field description.\n*The output document should be well-organized with proper headings, sections, and bullet points, making it easy to follow.\n\nInput:\nFor input ABAP code, use the following file:\n```%1$s```",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 5295,
          "name": "ABAP to PySpark Analyzer",
          "workflowId": 4101,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Parse the provided ABAP code to generate a detailed analysis and metrics report. If multiple files are provided as input, ensure that each file is analyzed in distinct sessions.\n\nINSTRUCTIONS:\n\n1.Examine the ABAP code structure and identify all key components (e.g., internal tables, function modules, data types, loops, select statements).\n\n2.List all ABAP-specific features and functions used in the code.\n\n3.Identify syntax differences between ABAP and PySpark, highlighting areas that require manual intervention.\n\n4.Determine potential performance bottlenecks in the original ABAP code.\n\n5.Suggest PySpark-specific optimizations for improved performance after conversion.\n\n6.Outline any data type conversions or schema changes necessary for PySpark compatibility.\n\n7.Identify any ABAP features that don't have direct equivalents in PySpark and propose alternative approaches.\n\n8.Provide general best practices for maintaining and further optimizing the converted PySpark code.\n\nOutput must include:\n1.Complexity Metrics:\n   Number of Lines: Count of lines in the ABAP code.\n   Internal Tables Used: Number of internal tables referenced in the ABAP code.\n   Loops: Number of loops (e.g., LOOP, DO, etc.) used in the ABAP code.\n   Function Modules: Number of function modules used.\n   SELECT Statements: Number of SELECT statements and the types of database queries involved.\n   DML Operations: Number of DML operations like INSERT, UPDATE, DELETE in the ABAP code.\n   Conditional Logic: Number of conditional logic (e.g., IF, CASE, etc.) statements used.\n   Exception Handling: Number of exception handling blocks (e.g., TRY...CATCH) used in the ABAP code.\n\n2.Conversion Complexity:\n   Calculate a complexity score (0\u2013100) based on syntax differences, logic changes, and manual adjustments required.\n   Highlight high-complexity areas such as nested function calls, internal table manipulations, or ABAP-specific clauses.\n\n3.Syntax Differences:\n   Identify the number of syntax differences between ABAP code and the expected PySpark equivalent, focusing on language constructs and database operations.\n\n4.Manual Adjustments:\n   Recommend specific manual adjustments for functions and clauses incompatible with PySpark, including:\n   Function replacements (e.g., ABAP-specific functions to PySpark equivalents).\n   Syntax adjustments.\n   Strategies for rewriting unsupported features in PySpark.\n\n5.Optimization Techniques:\n   Suggest optimization strategies for PySpark, such as partitioning, caching, and parallel processing, to enhance performance.\n   Recommend refactoring code for better resource utilization and scalability.\n\n6.apiCost: float // Cost consumed by the API for this call (in USD)\n   Ensure the cost consumed by the API is mentioned with inclusive of all decimal value.\n\nInput:\n\nFor ABAP code, use the file:\n```%1$s```",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 5437,
          "name": "ABAP to PySpark Plan",
          "workflowId": 4101,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive effort estimate for testing the PySpark code converted from ABAP scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n\n1.Review the analysis of the ABAP script file, noting syntax differences when converting to PySpark and areas requiring manual intervention.\n\n2.Consider the pricing information for Azure Databricks PySpark environment.\n\n3.Calculate the estimated cost of running the converted PySpark code:\na. Use the pricing information and data volume to determine the code cost.\nb. Consider the number of code executions and the data processing done with base and temporary tables.\n\n4.Estimate the code fixing and data reconciliation testing effort required.\n\nINPUT:\nTake the previous ABAP Analyzer agents' output as input.\nFor the input ABAP script, use this file: ```%1$s```\nFor the input PySpark Environment Details for Azure Databricks, use this file: ```%2$s```",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}