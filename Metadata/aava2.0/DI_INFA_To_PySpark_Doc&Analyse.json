{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 4880,
      "name": "DI INFA To PySpark Doc&Analyse",
      "description": "This workflow is to create INFORMATICA Documentation & Analysis report.",
      "createdBy": "girdhari.rayak@ascendion.com",
      "modifiedBy": "girdhari.rayak@ascendion.com",
      "approvedBy": "girdhari.rayak@ascendion.com",
      "createdAt": "2025-11-05T12:16:08.867021",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T12:16:09.927478",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 7549,
          "name": "DI Informatica Documentation",
          "workflowId": 4880,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": false,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The documentation should include the following sections:  \n\n- **Format:** Markdown  \n- Add the following metadata at the top of generated file:  \n====================================================\nAuthor:        Ascendion AAVA\nDate:          (Leave it empty)\nDescription:   <one-line description of the purpose>\n====================================================\n\n1. **Overview of Graph/Component**  \n   - Describe the purpose of the Informatica graph (.xml), transformation (transformation), data definition (source definition), plan (workflow), or pset (session).  \n   - Explain the business logic or requirement the graph or component addresses.  \n\n2. **Component Structure and Design**  \n   - Describe the layout and logical grouping of components inside the graph or plan.  \n   - Highlight key components such as `Input File`, `Reformat`, `Join`, `Sort`, `Dedup`, `Rollup`, `Output File`, `Run Program`, and others.  \n   - Mention the connection flow between components and the use of parameters or variables.  \n\n3. **Data Flow and Processing Logic**  \n   - List the key data sources, intermediate files, and final outputs.  \n   - For each logical step:\n     - Describe what it does (e.g., filtering, joining, reformatting, aggregation).  \n     - Mention any `transformation` or `source definition` files used.  \n     - Include any business rules or transformations applied.  \n\n4. **Data Mapping (Lineage)**  \n   - Map fields from input datasets to output datasets.  \n   - In Table Format with the below mentioned columns :\n     ```\n     Target Table Name : <actual target file/table>\n     Target Column Name : <actual column>\n     Source Table Name : <actual source file/table>\n     Source Column Name : <actual column>\n     Remarks : <1:1 Mapping | Transformation | Validation - include logic description>\n     ```\n\n5. **Transformation Logic**  \n   - Document each `transformation` function used or called in the flow.  \n   - Explain what each function does and what fields are involved.  \n   - Note any external function calls or reusable components.  \n\n6. **Complexity Analysis**  \n   - Number of Mappings: <integer>  \n   - Number of Lines of Code (in transformation or workflow): <integer>  \n   - Transform Functions Used: <count>  \n   - Joins Used: <list of types or None>  \n   - Lookup Files or Datasets: <count or 'None'>  \n   - Parameter Sets (session) or Plan Files Used: <count>  \n   - Number of Output Datasets: <integer>  \n   - Conditional Logic or `if-else` flows: <count>  \n   - External Dependencies: <JDBC connections, shell scripts, other tools>  \n   - Overall Complexity Score: <0\u2013100>  \n\n7. **Key Outputs**  \n   - Describe what is written to the final datasets or passed to the next stages.  \n   - Mention the format (Delimited, Fixed Width, etc.) and intended use of the output (e.g., report, downstream system).  \n\n8. **Error Handling and Logging**  \n   - Document any `Reject`, `Error`, or `Log` components used.  \n   - Mention any `transformation` based error tagging, reject thresholds, or control file usage.  \n   - Describe how errors are handled (e.g., auto-abort, routed to reject files, email alerts).\n\n9. **API Cost (Optional for Cloud Informatica Deployments)**  \n   - If relevant, calculate estimated compute or I/O cost based on size of data processed and component usage.  \n   - Formula or logic for cost estimation must be provided if available.\n\n**Input:**  \nAttach or provide the Informatica files (.xml, transformation, source definition, workflow, session). Acceptable formats: plain text, zipped folder, or directory path structure : {{Infa_File}}",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 7471,
          "name": "DI INFA To PySpark Analyzer",
          "workflowId": 4880,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Perform a detailed analysis of the provided Informatica XML files to support their conversion to PySpark. If multiple XML files are included in the ZIP archive, repeat the process for each file individually and generate a separate output for every XML.\n\nFollow these detailed instructions:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output \n\n---\n\n**1. Workflow Overview**\n\n* Provide a high-level summary of the Informatica mapping or workflow.\n* Mention the key business objective it supports, such as data integration, cleansing, enrichment, or loading.\n\n---\n\n**2. Complexity Metrics** (present this in a markdown table format when generating output):\n\n* Number of Source Qualifiers: Count of Source Qualifier transformations.\n* Number of Transformations: Total number of transformations used (Expression, Aggregator, Joiner, Filter, Router, etc.).\n* Lookup Usage: Number of Lookups (connected/unconnected).\n* Expression Logic: Count of complex expressions (IIF, DECODE, nested logic, etc.).\n* Join Conditions: Count and type of joins (normal, outer, heterogeneous).\n* Conditional Logic: Number of Router or Filter conditions.\n* Reusable Components: Number of reusable transformations, mapplets, and sessions.\n* Data Sources: Number of distinct sources (databases, flat files, XML, etc.).\n* Data Targets: Number of unique targets (databases, files, queues).\n* Pre/Post SQL Logic: Number of pre/post SQLs or procedures used in sessions.\n* Session/Workflow Controls: Number of decision tasks, command tasks, and event-based controls.\n* DML Logic: Frequency of INSERT, UPDATE, DELETE, and MERGE operations.\n* Complexity Score (0\u2013100): Based on the depth of logic, control flow usage, nested operations, and transformation types.\n\nAlso highlight high-complexity areas like:\n\n* deeply nested expressions\n* multiple lookups\n* branching logic (Router, Filter)\n* unstructured sources or external scripts\n\n---\n\n**3. Syntax Differences**\n\n* Identify functions used in Informatica that don\u2019t have a direct PySpark equivalent.\n* Mention any necessary data type conversions (e.g., TO\\_DATE, TO\\_CHAR, DECODE).\n* Highlight any workflow/control logic (e.g., Router, Transaction Control) that must be restructured for PySpark.\n\n---\n\n**4. Manual Adjustments**\n\n* List components that require manual implementation in PySpark (e.g., Java transformation, SQL override blocks).\n* Identify external dependencies like pre/post SQLs, stored procedures, or shell scripts.\n* Mention areas where business logic must be reviewed or validated post-conversion.\n\n---\n\n**5. Optimization Techniques**\n\n* Recommend using Spark best practices like partitioning, caching, and broadcast joins.\n* Suggest converting chain filters and joins into a pipeline.\n* Recommend window functions to simplify nested aggregations.\n* Finally, advise whether to **Refactor** (retain most of the original logic) or **Rebuild** (if better optimization is possible in PySpark).\nNote:\nDont give the code in the ouput\n---\n\nInput :\n* For Informatica use the below file/s : {{Infa_File}}",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 7425,
          "name": "DI INFA to PySpark Plan",
          "workflowId": 4880,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 284,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "\n\n### Informatica to PySpark Conversion Analyzer \u2013 Prompt Instructions\n\n---\n\n#### Metadata Header (Add once at the top of the output file)\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:          \nDescription:   <One-line summary of the document's purpose>\n=============================================\n```\n\n* Add this header **only once** at the top of the file.\n* Leave the **Date** field empty for dynamic population.\n* Replace `<One-line summary of the document's purpose>` with a concise description of the output.\n\n---\n\n### Conversion Instructions\n\nAs a **Data Migration Specialist**, your task is to analyze the provided Informatica XML files and generate a comprehensive plan for its conversion to PySpark.\n\nFollow these steps:\nThese has to be done for each Informatica XML file in zip separately.\n1. **Analyze the Informatica XML:**\n   * Use the previously generated output from the `DI_INFA_to_PySpark_Analyzer ` agent.\n2. **Compare and Identify Gaps:**\n\n   * Review syntax and functional logic in the Informatica XML.\n   * Identify areas requiring manual intervention (e.g., script components, event handling, third-party tasks).\n   * Do **not** count simple syntax changes; those will be auto-converted.\n\n3. **Effort Estimation:**\n\n   * Estimate the manual effort in hours for each identified gap.\n   * Provide separate effort estimates for:\n\n     * Manual code fixes\n     * Data reconciliation and validation testing\n\n4. **PySpark Environment Considerations:**\n\n   * Use the GCP Dataproc serverless environment details use the file from input\n   * Factor in performance, scalability, and any special configurations (e.g., clusters, job orchestration).\n\n5. **Cost Estimation:**\n\n   * Include the **API cost consumed** for this analysis in the final output.\n   * Format as: `apiCost: <floating-point value> USD` (e.g., `apiCost: 0.0462 USD`)\n\n---\n\nINPUT\nTake the previous  agent \"DDI_INFA_to_PySpark_Analyzer\" output as input.\nFor the input Informatica workflow/mapping, use this file/s: {{Infa_File}}\nFor the input PySpark Environment Details for Azure Databricks, use this file:{{Env_Details}}",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}