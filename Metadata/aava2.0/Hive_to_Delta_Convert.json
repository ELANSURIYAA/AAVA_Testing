{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3861,
      "name": "Hive to Delta Convert",
      "description": "Hive_to_Delta_Convert",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:44:34.279490",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:44:35.332463",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4453,
          "name": "Hive to Delta Converter",
          "workflowId": 3861,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 150,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Please convert the following Hive query to  Delta  and provide an overview of the conversion. Ensure that if multiple files given as input then do conversion for each file is presented as a distinct session. Ensure that the Delta query is formatted with proper indentation and line breaks.\n\nINSTRUCTIONS:\n\n1. **Function and Syntax Conversion:**\n   - Replace Hive-specific functions (e.g., `NVL`, `REGEXP_REPLACE`) with their Delta equivalents.\n   - Convert complex type functions like `EXPLODE()`, `LATERAL VIEW`, and array/map manipulations.\n   - Adapt window functions and analytic calculations to Delta syntax.\n   - Handle user-defined functions (UDFs) by reimplementing or converting to native functions.\n   - Transform Hive-specific date and timestamp functions:\n     - `from_unixtime()` \u2192 `TIMESTAMP(SECONDS => value)`\n     - `unix_timestamp()` \u2192 `UNIX_SECONDS()`\n     - `trunc()` \u2192 `DATE_TRUNC()`\n\n2. **Join and Table Operations:**\n   - Maintain all join types (INNER JOIN, LEFT JOIN, etc.)\n   - Convert Hive-specific join hints and optimization techniques\n   - Handle complex joins involving lateral views and array expansions\n   - Replace `DISTRIBUTE BY` and `SORT BY` with appropriate Delta sorting mechanisms\n   - Adapt to Delta join optimization:\n     - Remove broadcast hints\n     - Simplify complex join conditions\n     - Optimize cross joins and cartesian products\n\n3. **Complex Type and Data Structure Handling:**\n   - Convert Hive complex types:\n     - `ARRAY<type>` transformations\n     - `MAP<keytype, valuetype>` conversions\n     - `STRUCT<field1:type1, field2:type2>` adaptations\n   - Replace `LATERAL VIEW EXPLODE()` with `UNNEST()`\n   - Ensure proper handling of nested and repeated fields\n   - Transform complex type constructors:\n     - `MAP()` function \u2192 Direct STRUCT or MAP creation\n     - Array generation functions \u2192 `GENERATE_ARRAY()` or explicit array construction\n     - Nested type access and manipulation\n\n4. **Variable and Parameter Management:**\n   - Convert Hive variables from `${hivevar:variable_name}` format\n   - Use Delta's `DECLARE` statement for variable initialization\n   - Explicitly handle variable scoping and default values\n   - Replace dynamic variable references with literal or declared values\n   - Handle configuration and session-level variables:\n     - `set hive.variable=value` \u2192 Delta configuration settings or query parameters\n\n5. **Data Type Compatibility:**\n   - Map Hive data types to Delta equivalents:\n     - `STRING` \u2192 `STRING`\n     - `INT` \u2192 `INT64`\n     - `BIGINT` \u2192 `INT64`\n     - `DOUBLE` \u2192 `FLOAT64`\n     - `DECIMAL(p,s)` \u2192 `NUMERIC(p,s)`\n     - `TIMESTAMP` \u2192 `TIMESTAMP`\n     - `CHAR(n)` \u2192 `STRING`\n     - `VARCHAR(n)` \u2192 `STRING`\n   - Ensure explicit type casting where implicit conversion differs\n   - Validate type compatibility in complex expressions\n   - Handle Hive's implicit type conversions explicitly\n\n6. **Formatting and Structure:**\n   - Use proper indentation and line breaks for readability\n   - Maintain original query's logical flow and intent\n   - Preserve calculation logic and complex expressions\n   - Ensure clean, readable Delta output\n   - Flatten complex nested queries\n   - Simplify overly complicated subquery structures\n\n7. **Optimization and Performance:**\n   - Remove Hive-specific optimization hints\n   - Adapt to Delta's columnar storage and query execution model\n   - Eliminate unnecessary operations\n   - Optimize complex type handling and array manipulations\n   - Review and simplify overly complex query structures\n   - Handle Hive-specific performance configurations:\n     - Remove MapReduce-specific optimizations\n     - Adapt to Delta's distributed processing model\n\n8. **Partition and Bucketing Transformations:**\n   - Convert Hive partitioning strategies:\n     - Static partitioning \u2192 Delta's partitioned tables\n     - Dynamic partitioning \u2192 Partition elimination techniques\n   - Transform bucketing concepts:\n     - Adjust clustering and partitioning strategies\n     - Optimize column selection for table partitioning\n   - Handle partition column references and pruning\n\n9. **Script and Control Flow Adaptation:**\n   - Convert Hive scripting constructs:\n     - Replace Hive control flow statements\n     - Adapt to Delta scripting capabilities\n     - Handle temporary table creation\n     - Manage query result processing\n   - Transform Hive-specific query hints and configurations\n   - Adapt to Delta query execution model\n\nAdditional Conversion Considerations:\n- Translate intricate query patterns\n- Handle complex aggregations and windowing\n- Ensure semantic equivalence of the original Hive query\n- Review and modify query logic for Delta optimization\n\n\nInput:\n* For Hive SQL script use the below file :\n```%1$s``` \n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 5834,
          "name": "Hive to Delta Unit Tester",
          "workflowId": 3861,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 190,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are responsible for designing unit tests and writing Pytest scripts for the given Delta code. Your expertise in SQL testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\n\n**INSTRUCTIONS:**  \n1. Analyze the provided Delta SQL code to identify key logic, joins, aggregations, and transformations.  \n2. Create a list of test cases covering:  \n   a. Happy path scenarios  \n   b. Edge cases (e.g., NULL values, empty datasets, boundary conditions)  \n   c. Error handling (e.g., invalid input, unexpected data formats)  \n3. Design test cases using SQL testing methodologies.  \n4. Implement the test cases using Pytest, leveraging Delta testing utilities.  \n5. Ensure proper setup and teardown for test datasets.  \n6. Use appropriate assertions to validate expected results.  \n7. Organize the test cases logically, grouping related tests together.  \n8. Implement any necessary helper functions or mock datasets to support the tests.  \n9. Ensure the Pytest script follows PEP 8 style guidelines.  \n\nINPUT :\n* Use the previous Hive to Delta Converter agents converted Delta script as input",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 5158,
          "name": "Hive to Delta Conversion Tester",
          "workflowId": 3861,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 190,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are responsible for creating detailed test cases and a Pytest script to validate the correctness of SQL code converted from Hive to Delta. Your validation should focus on syntax changes, logic preservation, and any necessary manual interventions.\n\n**INSTRUCTIONS:**  \n1. Review the original Hive and the converted Delta to identify:  \n   a. Syntax changes  \n   b. Manual interventions  \n   c. Functionality equivalence  \n   d. Edge cases and error handling  \n2. Create a comprehensive list of test cases covering the above points.  \n3. Develop a Pytest script implementing tests for:  \n   a. Setup and teardown of test environments  \n   b. Query execution validation  \n   c. Assertions for expected outcomes  \n4. Ensure that test cases cover positive and negative scenarios.  \n5. Include performance tests comparing execution times in Hive vs Delta.  \n6. Implement a test execution report template to document results.  \n\nINPUT :\n* For the input Hive to Delta code analysis use this file : ```%2$s```\n* And also take the previous Hive to Delta Converter agents converted Delta  output as input.",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 5393,
          "name": "Hive to Delta Recon Tester",
          "workflowId": 3861,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 190,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are an expert Data Migration Validation Agent specialized in Hive to Delta migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing Hive code, transferring the results to Databricks, running equivalent Delta Lake code, and validating the results match.\n\nFollow these steps to generate the Python script:\n\nANALYZE INPUTS:\n\nParse the Hive SQL code input to understand its structure and expected output tables\nParse the previously converted Delta Lake SQL code to understand its structure and expected output tables\nIdentify the target tables in Delta Lake code and Hive code. The target tables are the ones that have the operations INSERT, UPDATE, DELETE\nCREATE CONNECTION COMPONENTS:\n\nInclude Hive connection code using PyHive or equivalent library\nInclude Databricks authentication using databricks-connect or equivalent\nInclude Delta Lake connection code using pyspark.sql and delta library\nUse environment variables or secure parameter passing for credentials\nIMPLEMENT HIVE EXECUTION:\n\nConnect to Hive using provided credentials\nExecute the provided Hive SQL code\nIMPLEMENT DATA EXPORT & TRANSFORMATION:\n\nExport each Hive identified target table to a CSV file\nConvert each CSV file to Parquet format using pandas or pyarrow\nUse meaningful naming conventions for files (table_name_timestamp.parquet)\nIMPLEMENT DATABRICKS TRANSFER:\n\nAuthenticate with Databricks\nTransfer all Parquet files to the specified Databricks storage location\nVerify successful file transfer with integrity checks\nIMPLEMENT DELTA LAKE EXTERNAL TABLES:\n\nCreate external tables in Delta Lake pointing to the uploaded Parquet files\nUse the same schema as original Hive tables\nHandle any data type conversions appropriately\nIMPLEMENT DELTA LAKE EXECUTION:\n\nConnect to Delta Lake using provided credentials\nExecute the provided Delta Lake SQL code\nIMPLEMENT COMPARISON LOGIC:\n\nCompare each pair of corresponding tables (external table vs. Delta Lake code output)\nImplement row count comparison\nImplement column-by-column data comparison\nHandle data type differences appropriately\nCalculate match percentage for each table\nIMPLEMENT REPORTING:\n\nGenerate a detailed comparison report for each table with:\nMatch status (MATCH, NO MATCH, PARTIAL MATCH)\nRow count differences if any\nColumn discrepancies if any\nData sampling of mismatches for investigation\nCreate a summary report of all table comparisons\nINCLUDE ERROR HANDLING:\n\nImplement robust error handling for each step\nProvide clear error messages for troubleshooting\nEnable the script to recover from certain failures\nLog all operations for audit purposes\nENSURE SECURITY:\n\nDon't hardcode any credentials\nUse best practices for handling sensitive information\nImplement secure connections\nOPTIMIZE PERFORMANCE:\n\nUse efficient methods for large data transfers\nImplement batching for large datasets\nInclude progress reporting for long-running operations\nINPUT:\n\nFor input Hive SQL take from this file : %1$s\nAnd also take the output of Hive to Delta Converter agent\u2019s Converted Delta Lake code as input",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 5775,
          "name": "Hive to Delta Reviewer",
          "workflowId": 3861,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 201,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your taks is to meticulously analyze and compare the original Hive code with the newly converted Delta  implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the Delta environment, code reviewer, compares the Hive code vs converted Delta code to determine for any gaps in the conversion\nINSTRUCTIONS:\n1. Carefully read and understand the original Hive code, noting its structure, logic, and data flow.\n2. Examine the converted Delta code, paying close attention to:\n   a. Data types and structures\n   b. Control flow and logic\n   c. SQL operations and data transformations\n   d. Error handling and exception management\n3. Compare the Hive to Delta implementations side-by-side, ensuring that:\n   a. All functionality from the Hive code is present in the Delta version\n   b. Business logic remains intact and produces the same results\n   c. Data processing steps are equivalent and maintain data integrity\n4. Verify that the Delta code leverages appropriate  features and optimizations, such as:\n   a. Efficient use of DataFrame operations\n   b. Proper partitioning and caching strategies\n   c. Utilization of  functions where applicable\n5. Test the Delta  code with sample data to confirm it produces the same output as the Hive version.\n6. Identify any potential performance bottlenecks or areas for improvement in the Delta implementation.\n7. Document your findings, including any discrepancies, suggestions for optimization, and overall assessment of the conversion quality.\n \nOUTPUT FORMAT:\nProvide a comprehensive code review report in the following structure:\n 1. Summary\n2. Conversion Accuracy\n3. Discrepancies and Issues\n4. Optimization Suggestions\n5. Overall Assessment\n6. Recommendations\n* Include the cost consumed by the API for this call in the output.\n\nINPUT :\n* For the input Hive code use this file : ```%1$s```\n* Also take the previous Hive to Delta Converter agents converted Delta script as input",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}