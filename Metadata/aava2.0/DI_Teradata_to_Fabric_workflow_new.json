{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 9151,
      "name": "DI Teradata to Fabric workflow new",
      "description": "Teradata to Fabric",
      "createdBy": "aarthy.jr@ascendion.com",
      "modifiedBy": "aarthy.jr@ascendion.com",
      "createdAt": "2026-02-03T09:36:51.775970",
      "modifiedAt": "2026-02-03T09:36:52.299667",
      "approvedAt": "2026-02-03T09:36:52.297838",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "topP": null,
        "maxToken": null,
        "managerLlm": [],
        "temperature": null,
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 18829,
          "name": "DI Teradata SQL or Python Embedded SQL to Microsoft Fabric SQL Conversion",
          "workflowId": 9151,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "------------------------------------------------------------\n\n\nINPUT\n\n\n------------------------------------------------------------\n\n\n- A single input file will be provided.\n\n\n- The input file may be:\n\n\n\u00a0 - Teradata SQL / BTEQ script\n\n\n\u00a0 \u00a0 (.txt, .sql, .json, .yaml)\n\n\n\u00a0 - Python source file containing embedded Teradata SQL\n\n\n\u00a0 \u00a0 (.py)\n\n\n\u00a0 - Teradata Stored Procedure\n\n\n\u00a0 \u00a0 (.txt, .sql)\n{{input1_string_true}} \u200b\n\u200b\n\n\n------------------------------------------------------------\n\n\nFILE TYPE DETECTION (MANDATORY)\n\n\n------------------------------------------------------------\n\n\n1. Inspect BOTH the input file extension AND file content.\n\n\n2. Determine input type using the following deterministic rules:\n\n\n\u00a0 \u00a0A. IF the file contains:\n\n\n\u00a0 \u00a0 \u00a0 - BTEQ commands\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 (.LOGON, .LOGOFF, .IF, .GOTO, .LABEL)\n\n\n\u00a0 \u00a0 \u00a0 - Standalone Teradata SQL statements\n\n\n\u00a0 \u00a0 \u00a0 - NO Python syntax\n\n\n\u00a0 \u00a0 \u00a0 - NO CREATE PROCEDURE / REPLACE PROCEDURE blocks\n\n\n\u00a0 \u00a0 \u00a0 THEN classify input as:\n\n\n\u00a0 \u00a0 \u00a0 \u2192 \"Teradata SQL Input\"\n\n\n\u00a0 \u00a0B. IF the file contains:\n\n\n\u00a0 \u00a0 \u00a0 - Python syntax\n\n\n\u00a0 \u00a0 \u00a0 - cursor.execute(), executemany(), or similar calls\n\n\n\u00a0 \u00a0 \u00a0 - Embedded SQL strings inside Python code\n\n\n\u00a0 \u00a0 \u00a0 - teradatasql / pyodbc / DBAPI usage\n\n\n\u00a0 \u00a0 \u00a0 THEN classify input as:\n\n\n\u00a0 \u00a0 \u00a0 \u2192 \"Python-Embedded Teradata SQL Input\"\n\n\n\u00a0 \u00a0C. IF the file contains:\n\n\n\u00a0 \u00a0 \u00a0 - CREATE PROCEDURE or REPLACE PROCEDURE\n\n\n\u00a0 \u00a0 \u00a0 - DECLARE variable blocks\n\n\n\u00a0 \u00a0 \u00a0 - CALL DBC.SysExecSQL\n\n\n\u00a0 \u00a0 \u00a0 - MERGE, GET DIAGNOSTICS, ROW_COUNT\n\n\n\u00a0 \u00a0 \u00a0 - Procedural control-flow\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 (BEGIN / END, IF, loops)\n\n\n\u00a0 \u00a0 \u00a0 THEN classify input as:\n\n\n\u00a0 \u00a0 \u00a0 \u2192 \"Teradata Stored Procedure Input\"\n\n\n3. Based on the detected input type, execute the corresponding\n\n\n\u00a0 \u00a0execution path defined below.\n\n\n------------------------------------------------------------\n\n\nEXECUTION PATH 1\n\n\n------------------------------------------------------------\n\n\n### IF INPUT TYPE = \"Teradata SQL Input\"\n\n\n### APPLY: Teradata SQL \u2192 Microsoft Fabric SQL (Spark SQL)\n\n\nINSTRUCTIONS:\n\n\n1. Initial Assessment:\n\n\n- Parse Teradata SQL / BTEQ scripts.\n\n\n- Validate UTF-8 encoding and structural integrity.\n\n\n- Identify:\n\n\n\u00a0 - Source and target tables\n\n\n\u00a0 - Joins, filters, aggregations\n\n\n\u00a0 - Implicit business logic\n\n\n\u00a0 - Teradata-specific constructs:\n\n\n\u00a0 \u00a0 PRIMARY INDEX, UPI, SAMPLE, TOP, QUALIFY,\n\n\n\u00a0 \u00a0 ACTIVITYCOUNT, ERRORCODE, BTEQ commands.\n\n\n2. Strategic Planning:\n\n\n- Define deterministic mappings from Teradata SQL to Spark SQL.\n\n\n- Replace unsupported procedural constructs with declarative SQL.\n\n\n- Design idempotent, restart-safe execution logic.\n\n\n- Explicitly list all table dependencies.\n\n\n3. Systematic Conversion:\n\n\n- Generate a SINGLE Fabric-compatible Spark SQL script.\n\n\n- Use ONLY declarative, set-based SQL.\n\n\n- Prefer:\n\n\n\u00a0 - DROP TABLE IF EXISTS\n\n\n\u00a0 - CREATE OR REPLACE TABLE\n\n\n\u00a0 - Atomic CTAS (CREATE TABLE AS SELECT)\n\n\n- DO NOT generate:\n\n\n\u00a0 - Stored procedures\n\n\n\u00a0 - SQL control-flow\n\n\n\u00a0 - TRY/CATCH, WHILE, GOTO, PRINT, WAITFOR.\n\n\n4. Quality Assurance:\n\n\n- Ensure SQL executes in Fabric Lakehouse or Warehouse.\n\n\n- Ensure safe re-runs with no partial writes.\n\n\n- Validate behavior for empty and non-empty source tables.\n\n\n5. Optimization & Documentation:\n\n\n- Use Delta tables where applicable.\n\n\n- Apply partitioning and Z-Ordering when relevant.\n\n\n- Include SQL comments for:\n\n\n\u00a0 - Conversion log\n\n\n\u00a0 - Troubleshooting guide\n\n\n\u00a0 - Maintenance recommendations\n\n\n\u00a0 - Fabric execution & API cost estimation.\n\n\n------------------------------------------------------------\n\n\nEXECUTION PATH 2\n\n\n------------------------------------------------------------\n\n\n### IF INPUT TYPE = \"Python-Embedded Teradata SQL Input\"\n\n\n### APPLY: Python + Teradata SQL \u2192 Microsoft Fabric PySpark\n\n\nINSTRUCTIONS:\n\n\n1. Initial Assessment (Python + SQL Parsing):\n\n\n- Parse Python file(s) and identify:\n\n\n\u00a0 - Embedded Teradata SQL strings\n\n\n\u00a0 - SQL executed via cursor.execute(), executemany()\n\n\n\u00a0 - Dynamically constructed SQL\n\n\n\u00a0 - Python control-flow influencing SQL execution\n\n\n- Ignore non-SQL Python logic unless required for semantics.\n\n\n- Detect Teradata-specific SQL constructs:\n\n\n\u00a0 PRIMARY INDEX, MERGE, QUALIFY, SAMPLE, TOP,\n\n\n\u00a0 Teradata data types, ACTIVITYCOUNT patterns.\n\n\n- Identify all source and target tables.\n\n\n2. Strategic Planning:\n\n\n- Replace Teradata SQL with Spark SQL executed via PySpark.\n\n\n- Replace procedural Python DB logic with:\n\n\n\u00a0 - spark.sql()\n\n\n\u00a0 - PySpark DataFrame transformations.\n\n\n- Decide between:\n\n\n\u00a0 - Pure spark.sql()\n\n\n\u00a0 - Hybrid DataFrame + SQL approach.\n\n\n- Design deterministic, idempotent execution.\n\n\n3. Systematic Conversion:\n\n\n- Generate a SINGLE Fabric-compatible PySpark script.\n\n\n- Use ONLY:\n\n\n\u00a0 - spark.sql(...)\n\n\n\u00a0 - PySpark DataFrame APIs\n\n\n\u00a0 - Delta Lake patterns.\n\n\n- Apply mappings:\n\n\n\u00a0 CHAR / VARCHAR / NVARCHAR \u2192 STRING\n\n\n\u00a0 INTEGER / SMALLINT \u2192 INT\n\n\n\u00a0 MERGE \u2192 Delta MERGE INTO\n\n\n\u00a0 TOP \u2192 LIMIT\n\n\n\u00a0 SAMPLE \u2192 LIMIT or removal\n\n\n- Remove completely:\n\n\n\u00a0 - Teradata DB connections\n\n\n\u00a0 - Cursor-based execution\n\n\n\u00a0 - Python exception-driven SQL control-flow.\n\n\n4. Quality Assurance:\n\n\n- Ensure script runs in Fabric Notebook or Pipeline.\n\n\n- Ensure deterministic, re-runnable behavior.\n\n\n- Validate logical equivalence with original Python logic.\n\n\n- Document assumptions and limitations explicitly.\n\n\n5. Optimization & Documentation:\n\n\n- Use Delta tables.\n\n\n- Avoid unnecessary intermediate tables.\n\n\n- Include:\n\n\n\u00a0 - Conversion log\n\n\n\u00a0 - Troubleshooting guide\n\n\n\u00a0 - Maintenance notes\n\n\n\u00a0 - Fabric execution & API cost estimation.\n\n\n------------------------------------------------------------\n\n\nEXECUTION PATH 3\n\n\n------------------------------------------------------------\n\n\n### IF INPUT TYPE = \"Teradata Stored Procedure Input\"\n\n\n### APPLY: Teradata Stored Procedure \u2192 Microsoft Fabric PySpark\n\n\nINSTRUCTIONS:\n\n\n1. Initial Assessment (Procedure Decomposition):\n\n\n- Parse stored procedure and identify:\n\n\n\u00a0 - Variable declarations\n\n\n\u00a0 - Procedural control-flow\n\n\n\u00a0 - DDL, DML, MERGE logic\n\n\n\u00a0 - Audit logging patterns\n\n\n\u00a0 - Temporary / work tables\n\n\n\u00a0 - GET DIAGNOSTICS / ROW_COUNT usage\n\n\n- Identify all source, staging, and target tables.\n\n\n2. Strategic Planning:\n\n\n- Decompose the procedure into ordered logical steps.\n\n\n- Replace procedural flow with sequential PySpark execution.\n\n\n- Replace:\n\n\n\u00a0 MERGE \u2192 Delta MERGE INTO\n\n\n\u00a0 Temporary tables \u2192 DataFrames or Delta staging tables\n\n\n\u00a0 ROW_COUNT \u2192 DataFrame.count()\n\n\n- Identify orchestration needs via Fabric Pipelines if required.\n\n\n3. Systematic Conversion:\n\n\n- Generate a SINGLE Fabric-compatible PySpark script.\n\n\n- DO NOT generate:\n\n\n\u00a0 - Stored procedures\n\n\n\u00a0 - SQL procedural logic\n\n\n\u00a0 - GOTO / IF / WHILE constructs.\n\n\n- Use ONLY:\n\n\n\u00a0 - spark.sql()\n\n\n\u00a0 - PySpark DataFrame transformations\n\n\n\u00a0 - Delta Lake MERGE patterns.\n\n\n- Implement audit logging using:\n\n\n\u00a0 - INSERT INTO audit tables via spark.sql()\n\n\n\u00a0 - Or documented Fabric logging alternatives.\n\n\n4. Quality Assurance:\n\n\n- Preserve execution order and business semantics.\n\n\n- Ensure safe re-runs with no partial writes.\n\n\n- Validate behavior for empty and non-empty datasets.\n\n\n- Explicitly document semantic deviations, if any.\n\n\n5. Optimization & Documentation:\n\n\n- Apply Delta Lake best practices.\n\n\n- Use partitioning and Z-Ordering where applicable.\n\n\n- Include:\n\n\n\u00a0 - Conversion log\n\n\n\u00a0 - Troubleshooting guide\n\n\n\u00a0 - Maintenance recommendations\n\n\n\u00a0 - Fabric execution & API cost estimation.\n\n\n------------------------------------------------------------\n\n\nGLOBAL OUTPUT REQUIREMENTS\n\n\n------------------------------------------------------------\n\n\n- Execution Path 1:\n\n\n\u00a0 \u2192 Output MUST be a single Fabric-compatible SQL file.\n\n\n- Execution Path 2 & 3:\n\n\n\u00a0 \u2192 Output MUST be a single Fabric-compatible PySpark script.\n\n\n- Never emit Teradata SQL in final output.\n\n\n- Never emit SQL Server\u2013specific syntax.\n\n\n- Never simulate procedural logic in SQL.\n\n\n- Assume source tables exist; document explicitly.\n\n\n- If logic cannot be safely converted, explicitly document\n\n\n\u00a0 the need for Fabric Pipelines or orchestration.\n\n\n------------------------------------------------------------\n\n\nMANDATORY HEADER (MUST APPEAR IN OUTPUT)\n\n\n------------------------------------------------------------\n\n\n=============================================\n\n\nAuthor: Ascendion AAVA\n\n\nCreated on: (leave it empty)\n\nDescription: Microsoft Fabric\u2013compatible implementation converted\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0from Teradata input using self-healing, idempotent patterns.\n\n\n=============================================\n\n\n------------------------------------------------------------\n\n\nIMPORTANT RULES\n\n\n------------------------------------------------------------\n\n\n- Never emit Teradata SQL in final output.\n\n\n- Never emit SQL Server syntax.\n\n\n- Use Spark SQL and PySpark ONLY.\n\n\n- Always include Fabric execution & API cost estimation.",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 18910,
          "name": "DI Teradata to Fabric Unit Test ",
          "workflowId": 9151,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 4000,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "INSTRUCTIONS:\n1. **Initial Assessment**:\n   - Analyze the provided Teradata code\n      \n      {{input1_string_true}}\n    \n    \u00a0.\n   - Identify source and target tables, transformation logic, joins, filters, aggregations, data type conversions, and dependencies.\n   - Document all transformation rules and relationships.\n2. **Strategic Planning**:\n   - Develop a comprehensive test strategy covering transformation validation, join correctness, aggregation accuracy, filtering, derived columns, null handling, schema validation, boundary/data type tests, data volume consistency, and error handling.\n   - Limit to 6-8 logically required test cases for full coverage.\n   - Establish quality gates and validation checkpoints.\n3. **Systematic Implementation**:\n   - Generate a standardized metadata block at the top :\n     =============================================\n     Author:        Ascendion AAVA\n     Created on:   (Leave it empty)\n     Description:   \n     =============================================\nEnsure the header is emitted exactly once at the top of the output file. The header must not appear again under any circumstances.\u200b\n   - For each test case, provide Test Case ID, Description, and Expected Result.\n   - Create modular, executable Pytest scripts for all test cases, using mock-based testing (Pandas DataFrames/in-memory tables), fixtures for setup/teardown, and function calls to validate Fabric transformations.\n   - Add detailed logging for every test step using Python's logging module.\n   - Ensure no hardcoded credentials; use environment variables/config files.\n   - Mask any sensitive information in logs.\n4. **Quality Assurance**:\n   - Validate all outputs against requirements and standards.\n   - Perform security, performance, and quality assessments.\n   - Document test results and validation outcomes in Markdown and JSON formats.\n5. **Optimization and Enhancement**:\n   - Use vectorized Pandas operations and cache outputs to optimize performance.\n   - Support parallel test execution (pytest-xdist).\n   - Ensure CI/CD compatibility and proper exit codes.\n6. **Comprehensive Documentation**:\n   - Provide a test case document and execution summary report (Markdown and JSON).\n   - Include troubleshooting guides and maintenance procedures.\n   - Store reports in /test_reports or similar folder.\n7. **Continuous Monitoring**:\n   - Establish monitoring and feedback mechanisms for ongoing improvement.\n   - Track performance metrics and plan for future updates.\n\nOUTPUT FORMAT:\n- Metadata block at the top of every file\n- Test Case Document: Test IDs, Descriptions, Expected Results\n- Full Pytest Script Implementations\n- Test Execution Report: Markdown and JSON\n- API Cost in output (e.g., apiCost: 0.021 USD)\n\nSAMPLE:\n=============================================\nAuthor:        Ascendion AAVA\nCreated on:\u00a0\nDescription:   Pytest script for validating Teradata-to-Fabric ETL transformation logic\n=============================================\n\nTest Case Document (Markdown):\n| Test Case ID | Description | Expected Result |\n|--------------|-------------|----------------|\n| TC01         | Validate join logic between CUSTOMER and ORDERS | All joined records match expected keys |\n| TC02         | Validate aggregation of TOTAL_AMOUNT | Aggregated values match expected sums |\n| ...          | ...         | ...            |\n\nPytest Script (test_etl_validation.py):\nimport pytest\nimport pandas as pd\nimport logging\nfrom fabric_utils import run_fabric_transformation\n\n@pytest.fixture\ndef mock_input_data():\n    # Setup mock data for Teradata sources\n    ...\n    yield {...}\n    # Teardown\n\ndef test_join_logic(mock_input_data):\n    ...\n\ndef test_aggregation_accuracy(mock_input_data):\n    ...\n\nTest Execution Report (test_reports/summary.md, summary.json):\n- Test Case ID\n- Description\n- Expected Result\n- Actual Result\n- Pass/Fail\n\napiCost: 0.021 USD\n\u200b\n(=============================================\r\n\nAuthor: Ascendion AAVA\r\n\nCreated on: (Leave it empty)\r\n\nDescription:\r\n\n=============================================)\nEnsure the header is emitted exactly once at the top of the output file. The header must not appear again under any circumstances.\u200b\u200b",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 18911,
          "name": "DI Teradata to Fabric Conversion Test",
          "workflowId": 9151,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 4000,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "INSTRUCTIONS:\n1. **Initial Assessment**:\n   - Analyze \n      \n      {{input1_string_true}}\n    \n     (Teradata code) and \n      \n      {{input2_string_true}}\n    \n     (Teradata analyzer output) to extract transformation logic, mappings, filters, joins, aggregations, and business rules.\n   - Review corresponding Fabric models or SQL scripts to identify target logic and data structures.\n   - Identify explicit and implicit validation requirements, edge cases, and manual interventions.\n   - Research best practices for ETL validation, Pytest automation, and Fabric integration.\n\n2. **Strategic Planning**:\n   - Develop a validation strategy covering transformation logic, join conditions, filters, aggregations, data mappings, and exception handling.\n   - Define comprehensive test cases covering positive, negative, and boundary scenarios.\n   - Plan for modular Pytest script generation, metadata normalization, and multi-format reporting.\n   - Establish quality gates, validation checkpoints, and error handling protocols.\n\n3. **Systematic Implementation**:\n   - Normalize or insert metadata header.\n   - Design a test case document (table) with fields: Test Case ID, Description, Preconditions, Test Steps, Expected Result, Actual Result, Pass/Fail.\n   - Generate modular Pytest scripts for automated validation between Teradata and Fabric outputs, including setup, teardown, data fetching, transformation execution, and assertions.\n   - Implement logging, error handling, and environment variable-based credential management.\n   - Maintain detailed documentation and code comments throughout.\n\n4. **Quality Assurance**:\n   - Execute all test cases and validate results against expected outputs.\n   - Verify compliance with metadata, security, and reporting standards.\n   - Perform performance and scalability assessments (e.g., parallel test execution).\n   - Document test results, mismatches, and summary statistics.\n\n5. **Optimization and Enhancement**:\n   - Identify opportunities for test reusability, modularization, and performance tuning.\n   - Ensure scalability for large datasets and extensibility for new transformation types.\n   - Apply feedback and lessons learned to refine validation logic and reporting.\n\n6. **Comprehensive Documentation**:\n   - Produce detailed documentation covering test case design, Pytest usage, reporting formats, troubleshooting, and maintenance procedures.\n   - Include recommendations for future improvements and knowledge transfer.\n\n7. **Continuous Monitoring**:\n   - Establish mechanisms for ongoing validation, regression testing, and feedback collection.\n   - Track validation metrics, test coverage, and success rates.\n   - Plan for future updates as Teradata or Fabric logic evolves.\n\nINPUT PARAMETERS:\n- \n      \n      {{input1_string_true}}\n    \n    : Teradata code (SQL, Python, or configuration)\n- \n      \n      {{input2_string_true}}\n    \n    : Teradata analyzer output (JSON, YAML, or text)\n\nOUTPUT FORMAT:\n- Test Case Document: Table with all required fields.\n- Pytest Script(s): Modular, well-documented Python code for validation.\n- Execution Report: Console, CSV, JSON, and optional HTML summary.\n- API Usage Cost: Estimated cost for execution.\n- All outputs must include normalized metadata headers.\n\nSAMPLE:\n=============================================\nAuthor: Ascendion AAVA\nCreated on: (leave it blank)\nDescription: Pytest script to validate Teradata-to-Fabric transformation integrity\n=============================================\nTest Case Table Example:\n| Test Case ID | Description | Preconditions | Test Steps | Expected Result | Actual Result | Pass/Fail |\n|--------------|-------------|---------------|------------|-----------------|--------------|-----------|\n| TC-001 | Validate record count equivalence | Fabric connection, Teradata job run | 1. Run Teradata job 2. Query Fabric target table 3. Compare record counts | Counts match | [Captured] | [Result] |\nPytest Script Example:\nimport os\nimport pytest\n# ... (setup, teardown, test functions, logging, reporting)\nExecution Report Example:\n{\n  \"test_case_id\": \"TC-001\",\n  \"description\": \"Validate record count equivalence\",\n  \"validation_type\": \"Aggregation\",\n  \"row_count_match\": \"100%\",\n  \"column_match_summary\": \"All columns match\",\n  \"failed_rows\": [],\n  \"overall_status\": \"PASS\",\n  \"execution_time\": \"12.3s\"\n}\nGenerate only 5 testcases.\r\n\n\n\n\n\r\u200b\n(=============================================\nAuthor: Ascendion AAVA\nCreated on: (Leave it empty)\nDescription: Pytest script to validate Teradata-to-Fabric transformation integrity\n=============================================)\n\u200bEnsure the header is emitted exactly once at the top of the output file. The header must not appear again under any circumstances.\u200b\u200b",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 18912,
          "name": "DI Teradata to Fabric Recon test",
          "workflowId": 9151,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "INSTRUCTIONS:\n1. **Initial Assessment**:\n   - Analyze \n      \n      \n      \n      {{input1_string_true}}\n    \n    \n    \n     (Teradata code) and \n      \n      \n      \n      {{input2_string_true}}\n    \n    \n    \n     (conversion agent output) to identify all target datasets and output tables requiring validation.\n   - Extract Fabric connection details and schema from configuration files or environment variables.\n   - Identify explicit and implicit validation requirements, edge cases, and success criteria.\n   - Research relevant standards, best practices, and methodologies for ETL validation.\n\n2. **Strategic Planning**:\n   - Develop a validation strategy that orchestrates execution of both Teradata and Fabric pipelines in a controlled environment.\n   - Identify dependencies, risks (e.g., schema drift, data volume), and mitigation strategies.\n   - Create a detailed implementation roadmap with milestones for data extraction, comparison, and reporting.\n   - Establish quality gates and validation checkpoints for each stage.\n\n3. **Systematic Implementation**:\n   - Execute the Teradata pipeline using provided code or API, capturing output datasets in a standardized format (CSV/Parquet/Fabric table).\n   - Execute the converted Fabric pipeline, ensuring all transformations are run and outputs are materialized in Fabric.\n   - Extract output data from both sources using Python (pandas, pyodbc, adlfs, or appropriate connectors).\n   - Perform row-level and column-level comparisons, handling nulls, case sensitivity, ordering, datatype conversions, and decimal precision.\n   - Implement comprehensive error handling, logging, and masking of sensitive information.\n   - Maintain detailed documentation and logs throughout the process.\n\n4. **Quality Assurance**:\n   - Validate all outputs against success criteria (match status, row/column counts, sample mismatches).\n   - Generate detailed reconciliation reports for each table, including match status, differences, and metrics.\n   - Verify compliance with security, performance, and quality standards.\n   - Document test results and validation outcomes for auditability.\n\n5. **Optimization and Enhancement**:\n   - Optimize data extraction and comparison for large datasets (chunking, parallelization, Fabric temp tables).\n   - Ensure scalability, maintainability, and extensibility for future migrations.\n   - Apply performance tuning and resource optimization.\n   - Incorporate feedback and lessons learned for continuous improvement.\n\n6. **Comprehensive Documentation**:\n   - Create detailed documentation covering setup, configuration, execution, troubleshooting, and maintenance.\n   - Provide recommendations for future improvements and knowledge transfer.\n\n7. **Continuous Monitoring**:\n   - Establish monitoring and feedback mechanisms for ongoing validation jobs.\n   - Track performance metrics and plan for future updates and enhancements.\n\nINPUT PARAMETERS:\n- \n      \n      \n      \n      {{input1_string_true}}\n    \n    \n    \n    : Teradata code or pipeline definition (SQL, JSON, or API reference)\n- \n      \n      \n      \n      {{input2_string_true}}\n    \n    \n    \n    : DI Teradata-to-Fabric conversion agent output (converted pipeline, mapping, or metadata)\n\nOUTPUT FORMAT:\n**Executive Summary**:\n- Project Overview: High-level description of validation performed\n- Key Achievements: List of validated tables, match status, and metrics\n- Success Metrics: Quantitative outcomes (match %, row/column diffs)\n- Recommendations: Next steps and optimization suggestions\n\n**Detailed Analysis**:\n- Requirements Assessment: Analysis of Teradata and Fabric outputs\n- Technical Approach: Execution and comparison methodology\n- Implementation Details: Step-by-step process\n- Quality Assurance: Validation and error handling\n\n**Deliverables**:\n- Python Validation Script: Complete, modular, CLI-executable script with metadata header\n- Reconciliation Reports: Console, CSV, JSON, and optional HTML outputs\n- Logs: Detailed log files with timestamps and error messages\n- Documentation: Setup, usage, and troubleshooting guide\n\nSAMPLE:\n=============================================\nAuthor: Ascendion AAVA\nCreated on: (leave it blank)\nDescription: Python validation suite for reconciling Teradata and Fabric ETL outputs\n=============================================\n# Usage Example\npython validate_recon.py --teradata-code teradata_pipeline.sql --conversion-output conversion_agent_output.json --fabric-config fabric.env\n# Output\n- reconciliation_report.csv\n- reconciliation_report.json\n- validation.log\n- Console summary\n# Sample Report (CSV)\n| Table Name | Match Status | Row Diff | Col Diff | Match % | Sample Mismatches |\n|------------|-------------|----------|----------|---------|-------------------|\n| orders     | MATCH       | 0        | 0        | 100%    | []                |\n| customers  | PARTIAL     | 2        | 1        | 98.7%   | [row_id: 123, ...]|\n* API Cost for this particular API call for the model in USD\nCompare the output of agent DI Teradata to Fabric SQL conversion and Raw_ingestion.txt file. That should be the recon testing here.\n\n\u200b(=============================================\r\n\nAuthor: Ascendion AAVA\r\n\nCreated on: (leave it blank)\r\n\nDescription: Python validation suite for reconciling Teradata and Fabric ETL outputs\r\n\n=============================================)\u200b\n\nEnsure the header is emitted exactly once at the top of the output file. The header must not appear again under any circumstances.\u200b\u200b\u200b",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 18913,
          "name": "DI Teradata to Fabric Reviewer",
          "workflowId": 9151,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Verbose",
            "maxIter": 8000,
            "temperature": 0.6,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 300,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "INSTRUCTIONS:\n1. Initial Assessment:\n   - Analyze all provided inputs: \n      \n      \n      {{input1_string_true}}\n    \n    \n     (Teradata code), \n      \n      \n      {{input2_string_true}}\n    \n    \n     (conversion output).\n   - Identify explicit and implicit requirements, transformation logic, business rules, and metadata needs.\n   - Assess complexity, constraints, and success criteria for ETL/ELT workflows.\n   - Research relevant standards, best practices, and methodologies for Teradata and Fabric.\n\n2. Strategic Planning:\n   - Develop a comprehensive review strategy aligned with objectives.\n   - Identify dependencies, risks, and mitigation strategies for transformation validation.\n   - Create a detailed review roadmap with milestones and quality gates.\n   - Establish validation checkpoints for logic, code quality, and metadata compliance.\n\n3. Systematic Implementation:\n   - Enforce metadata requirements at the top:\n     =============================================\n     Author: Ascendion AAVA\n     Created on: (Leave it empty)\n     Description: \n     =============================================\nEnsure the header is emitted exactly once at the top of the output file. The header must not appear again under any circumstances.\u200b\n   - Analyze Teradata code for job type, input/output stages, transformations, lookup logic, joins, aggregations, business rules, and parameterization.\n   - Review Fabric conversion output for transformation accuracy, SQL logic, joins, lookups, aggregations, column derivations, filters, macros, and modular design.\n   - Compare Teradata and Fabric logic to ensure all functional components are covered, transformations match, and data types/formats are preserved.\n   - Assess code quality, readability, modularity, efficiency, and best practices for large dataset handling and incremental models.\n   - Validate correctness using sample input/output datasets if available.\n   - Identify gaps, risks, missing logic, and suggest improvements in performance, readability, maintainability, and business logic replication.\n   - Flag transformation errors, SQL anti-patterns, and incorrect logic replication.\n\n4. Quality Assurance:\n   - Conduct thorough validation of review outputs.\n   - Verify compliance with metadata, transformation logic, and enterprise standards.\n   - Perform security, performance, and quality assessments.\n   - Document test results and validation outcomes.\n\n5. Optimization and Enhancement:\n   - Identify optimization opportunities and implement improvements.\n   - Ensure scalability, maintainability, and future extensibility.\n   - Apply performance tuning and resource optimization.\n   - Incorporate feedback and lessons learned.\n\n6. Comprehensive Documentation:\n   - Create detailed documentation covering all aspects of the review.\n   - Include troubleshooting guides and maintenance procedures.\n   - Provide recommendations for future improvements.\n   - Ensure knowledge transfer and continuity.\n\n7. Continuous Monitoring:\n   - Establish monitoring and feedback mechanisms.\n   - Track performance metrics and success indicators.\n   - Plan for future updates, maintenance, and improvements.\n   - Ensure long-term sustainability and effectiveness.\n\nINPUT PARAMETERS:\n- \n      \n      \n      {{input1_string_true}}\n    \n    \n    : Teradata code (ETL/ELT pipeline source)\n- \n      \n      \n      {{input2_string_true}}\n    \n    \n    : Fabric conversion output (SQL/ELT code)\n\nOUTPUT FORMAT:\n**Executive Summary**:\n- Project Overview: High-level description of review scope and objectives\n- Key Achievements: Major findings and deliverables\n- Success Metrics: Quantifiable outcomes and performance indicators\n- Recommendations: Strategic recommendations for next steps\n\n**Detailed Analysis**:\n- Requirements Assessment: Comprehensive analysis of Teradata and Fabric transformation requirements\n- Technical Approach: Methodology and technical decisions for review\n- Implementation Details: Step-by-step review process and logic comparison\n- Quality Assurance: Validation, compliance, and quality measures\n\n**Deliverables**:\n- Review Report: Comprehensive assessment of Teradata code and Fabric conversion output\n- Gap Analysis: Identification of missing logic, risks, and improvement opportunities\n- Optimization Recommendations: Performance, readability, maintainability tips\n- Metadata Compliance: Verification of metadata headers and descriptions\n- Supporting Documentation: Additional materials and resources\n- Test Results: Validation outcomes and performance metrics\n\n**Implementation Guide**:\n- Setup Instructions: How to use the agent for review\n- Configuration Steps: Customization guidelines\n- Usage Guidelines: Operational procedures\n- Maintenance Procedures: Ongoing support requirements\n\n**Quality Assurance Report**:\n- Testing Summary: Validation results\n- Performance Metrics: Analysis and benchmarks\n- Security Assessment: Security validation\n- Compliance Verification: Standards and regulatory compliance\n\n**Troubleshooting and Support**:\n- Common Issues: Potential problems and solutions\n- Diagnostic Procedures: Issue identification and resolution\n- Support Resources: Documentation and help contacts\n- Escalation Procedures: When and how to escalate issues\n\n**Future Considerations**:\n- Enhancement Opportunities: Potential improvements and upgrades\n- Scalability Planning: Growth and expansion considerations\n- Technology Evolution: Future adoption strategies\n- Maintenance Schedule: Update planning\n\nSAMPLE:\nExecutive Summary:\n- Project Overview: Reviewed Teradata ETL pipeline and Fabric conversion output for transformation accuracy, code quality, and metadata compliance.\n- Key Achievements: Identified missing lookup logic, improved aggregation performance, enforced metadata standards.\n- Success Metrics: 100% transformation coverage, 30% performance improvement, full metadata compliance.\n- Recommendations: Refactor incremental models, optimize SQL joins, enhance documentation.\n\nDetailed Analysis:\n- Requirements Assessment: Teradata pipeline includes aggregations, filters, joins; Fabric output covers most transformations but misses some lookup logic.\n- Technical Approach: Compared transformation logic, validated code quality, assessed metadata compliance.\n- Implementation Details: Step-by-step review, logic comparison, gap analysis.\n- Quality Assurance: Validated outputs against sample datasets, ensured compliance.\n\nDeliverables:\n- Review Report: Detailed findings, gap analysis, optimization tips.\n- Metadata Compliance: Verified headers and descriptions.\n- Supporting Documentation: Review methodology, troubleshooting guide.\n- Test Results: Validation outcomes.\n\nExpected Output: Comprehensive review report with actionable recommendations, gap analysis, optimization tips, and metadata compliance verification.\n\n\u200b(=============================================\r\n\nAuthor: Ascendion AAVA\r\n\nCreated on: (Leave it empty)\r\n\nDescription:\r\n\n=============================================)\n\nEnsure the header is emitted exactly once at the top of the output file. The header must not appear again under any circumstances.\u200b\u200b",
          "modelName": "model"
        }
      ],
      "realmId": 79,
      "tags": [
        2,
        11,
        12
      ],
      "practiceArea": 6
    }
  },
  "status": "SUCCESS"
}