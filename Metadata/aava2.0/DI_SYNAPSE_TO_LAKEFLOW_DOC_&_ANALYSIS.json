{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2516,
      "name": "DI SYNAPSE TO LAKEFLOW DOC & ANALYSIS",
      "description": "Analyzing and Documenting the Synapse Code",
      "createdBy": "jahnavi.lingutla@ascendion.com",
      "modifiedBy": "jahnavi.lingutla@ascendion.com",
      "approvedBy": "jahnavi.lingutla@ascendion.com",
      "createdAt": "2025-11-05T11:02:32.067907",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:02:33.180421",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4268,
          "name": "DI Synapse SQL Documentation",
          "workflowId": 2516,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 4328,
          "name": "DI Synapse To Lakeflow Analyzer",
          "workflowId": 2516,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your task is to process the provided Synapse SQL pipeline file and produce a detailed analysis and metrics report, strictly adhering to the specified markdown format and section structure. The report must be tailored for a Synapse SQL-to-Lakeflow migration context.\nAll content must be derived directly from the actual Synapse SQL pipeline file; do not make assumptions or provide summaries without verifiable evidence from the file.\n\nINSTRUCTIONS:\n\n1. **File Parsing**  \n   - Parse the provided Synapse SQL pipeline file to extract all relevant metadata, including pipeline activities, datasets, linked services, parameters, and transformation logic.\n   - Ensure that all extracted details are accurate and reflect the actual Synapse pipeline configuration.\n\n2. **Section Generation**  \n   - Follow the required output format exactly, maintaining section numbering, headings, and markdown layout as shown.\n   - For each section, populate content based on the parsed Synapse SQL pipeline file:\n     - **Pipeline Overview:** : Summarize the pipeline\u2019s data flow logic, business purpose, and conceptual mapping to Lakeflow layers (ingest, transform, publish).\n     - **Complexity Metrics:** - Provide a complexity breakdown in the table format:\nCategory  |  Measurement\n* Number of Activities\n* Source/Target Systems\n* Transformation Logic\n* Parameters Used\n* Reusable Components\n* Control Logic\n* External Dependencies\n* Performance Considerations\n* Volume Handling\n* Error Handling\n* Overall Complexity Score.\n*Conversion complexity rating(this is also the part of the table)-Assign a numeric complexity score (0\u2013100) and a conversion complexity rating (Low/Medium/High) with justification. List high-complexity areas with specifics from the job.\n     - **Syntax Differences:** Clearly explain how Synapse SQL constructs (e.g., stored procedures, activities, data flows) map to Lakeflow components (dataflows, transformations, task orchestration), using examples relevant to the pipeline.\n     - **Manual Adjustments:** Enumerate all manual interventions required for migration, referencing actual pipeline features (e.g., stored procedure calls, parameterization, dependency chains).\n     - **Optimization Techniques:** Recommend Lakeflow-specific performance optimizations, referencing pipeline scenarios. Indicate whether the pipeline should be refactored or rebuilt, with reasoning.\n     - **API Cost:** Calculate and display the API cost for this analysis, in full decimal precision (e.g., `apiCost: 0.0182 USD`).\n\n3. **Formatting & Quality Criteria**  \n   - Use markdown headers and tables exactly as specified.\n   - Write in clear, enterprise-style English.\n   - Every section must appear in order, with no omissions or reordering.\n   - Replace all `<placeholders>` with actual parsed values from the txt file.\n   - Do not provide summaries or assumptions; only use details present in the Synapse SQL pipeline file.\n4. **Output Structure**  \n   - The report must begin with the metadata block (Author, Created on, Description) and follow with sections #1 through #6 as shown. Do not mention any date in front of the Created on block; leave it empty.\n   - All metrics must be presented in table format.\n   - All recommendations and mappings must be specific to Databricks Lakeflow SQL.\nAll content must be derived from the actual Synapse SQL pipeline file; do not make assumptions or provide summaries without direct evidence from the file.\nInput :\nFor Synapse SQL pipeline designs, use the below file: {{synapse_code}}",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 4329,
          "name": "DI Synapse To Lakeflow Plan",
          "workflowId": 2516,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "****MASKED****",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with reviewing the Synapse source files (e.g., .sql, .pipeline.json, .notebook, .dml) during conversion to Lakeflow pipelines.\nYour responsibilities include identifying logic gaps requiring manual SQL or flow adjustments, estimating developer and tester effort, and forecasting Lakeflow pipeline and compute execution costs based on data volume, transformation complexity, and orchestration logic.\n\nINSTRUCTIONS:\n\nAnalyze the Synapse codebase and its conversion implications to Lakeflow pipelines:\n\nFocus on logic gaps (e.g., transformation complexity, branching, and error handling).\n\nExclude syntactic equivalents already handled by auto-conversion or Lakeflow optimization utilities.\n\nEstimate developer/tester effort (in hours) for:\n\nFlow logic rewrites (e.g., custom transformation steps, dynamic SQL or control flow adjustments)\n\nMetadata alignment (schema transformation, data type mapping, catalog and dataset consistency)\n\nEdge case handling (error paths, fallback dataflows, or recovery mechanisms)\n\nData reconciliation and validation testing\n\nEstimate Databricks Compute/Query Cost using:\n\nExpected data volume processed per pipeline run (in GB)\n\nNumber of pipeline executions per day/week/month\n\nLakeflow pricing model: e.g., \u201c$0.50 per compute unit (Standard) + storage and orchestration cost\u201d\n\nCalculate Total Developer Cost using a default hourly rate (e.g., $50/hr)\n\nPresent cost metrics, effort estimation, and guidance in a structured Markdown report.\n\nOUTPUT FORMAT:\n\nUse Markdown format with the following metadata header:\n\n==============================================================================\nAuthor:        Ascendion AAVA\nCreated on:    (Leave it empty)\nDescription:   Cost & Effort Planning for Synapse SQL to Lakeflow Conversion\n==============================================================================\n\n1. Lakeflow Cost Estimation\n1.1 Pipeline/Compute Cost Breakdown\n\nData Volume Processed per Run: <estimated_gb> GB\n\nExecution Frequency: <runs/day>\n\nMonthly Volume Processed: <calculated> GB\n\nLakeflow Pricing Used: $0.50 per compute unit (Standard Tier)\n\nEstimated Monthly Cost (USD): <calculated_cost>\n\nCost Formula Used:\nMonthly Cost = (Estimated Compute Units \u00d7 $0.50) + (Storage & Orchestration GB \u00d7 $/GB)\n\n2. Manual Flow Adjustment and Data Reconciliation Effort\n2.1 Estimated Effort (Hours)\n\nFlow Logic Adjustments (e.g., transformation steps, task dependencies): <integer> hrs\n\nSchema/Data Type Alignment (e.g., dataset structure, catalog mapping): <integer> hrs\n\nError/Exception Handling Logic: <integer> hrs\n\nOutput Validation & Data Reconciliation Testing: <integer> hrs\n\nTotal Estimated Effort: <sum> hrs\n\n2.2 Developer Cost\n\nDeveloper Hourly Rate: $50/hr\n\nTotal Developer Cost: <effort_hrs \u00d7 50> USD\n\n3. API Processing Cost\n\napiCost: <actual_cost> USD\ndo not give summary \ndo not give guidence and notes\nInput:\n\nSynapse Source File: {{synapse_code}}\nEnvironment File: {{Synapse_Env}}",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}