{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 6160,
      "name": "DI REDSHIFT TO SNOWFLAKE  CONVERT",
      "description": "REDSHIFT TO SNOWFLAKE WORKFLOW",
      "createdBy": "harish.kumaresan@ascendion.com",
      "modifiedBy": "harish.kumaresan@ascendion.com",
      "approvedBy": "karthikeyan.iyappan@ascendion.com",
      "createdAt": "2025-12-17T06:33:05.055093",
      "modifiedAt": "2026-01-05T10:05:16.339499",
      "approvedAt": "2026-01-05T10:05:16.342265",
      "status": "APPROVED",
      "comments": {
        "whatWentGood": "good",
        "whatWentWrong": "",
        "improvements": ""
      },
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "topP": null,
        "maxToken": null,
        "managerLlm": [],
        "temperature": null,
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 11714,
          "name": "DI REDSHIFT TO SNOWFLAKE CONVERSION",
          "workflowId": 6160,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 10,
            "temperature": 0.3,
            "guardrailIds": [],
            "allowDelegation": true,
            "maxExecutionTime": 1731,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "Convert the following Redshift SQL to Snowflake SQL.\n\nEnsure that if multiple files are given as input, the conversion for each file is presented as a distinct session.Ensure that the Snowflake query is formatted with proper indentation and line breaks so that it is ready to be stored as a .sql file.\n\nINSTRUCTIONS\n\n1. Metadata Requirements\n\nAdd the following metadata at the top of each converted/generated file:\n\n==================================================================\n\nAuthor: AAVA\n\nCreated on: \n\nDescription: \n\n==================================================================\n\nThe Created on field must always reflect the current date when the agent runs.\n\nIf the source SQL already contains metadata headers, update them to match this format.\n\n2. SQL Conversion\n\nConvert all Redshift SQL syntax, functions, and constructs into valid Snowflake SQL syntax.\n\nEnsure:\n\nLogical equivalence between source and target queries\n\nCorrect handling of date, time, numeric, and string operations\n\nProper translation of joins, subqueries, and window functions\n\nCompatibility with Snowflake\u2019s execution model\n\n3. Query Correctness\n\nThe converted Snowflake SQL must:\n\nExecute successfully in a Snowflake environment\n\nPreserve the original query\u2019s intent and results\n\nUse Snowflake-supported syntax and functions only\n\n4. Formatting and Readability\n\nApply consistent indentation and line breaks\n\nFormat complex expressions, CASE statements, and calculations clearly\n\nEnsure the query is production-ready and readable\n\nOUTPUT FORMAT:\n\nGenerate the converted output for each input file independently in separate sessions.\n\nEach session must include:\n\nConverted Snowflake SQL Code\n\nThe fully converted and formatted Snowflake SQL, ready to be saved as a .sql file.\n\nCost Information:\n\nReport the API cost consumed for this operation in USD with all decimal places.\n\nFor each converted SQL file, include a mandatory SQL comment\u00a0 in the end of every converted queries\u00a0 in 10-20 words that describes the conversion details for each and every queries present in the file.\n\nThis comment must:\n\nBe short and simple,dont need to be long\n\nBe included in the end of every converted queries in the file and in every per converted SQL file\n\nUse standard SQL comment syntax (/* ... */ or --)\n\nApply to all SQL files and all converted SQL queries without exception\n\nThe conversion details comment must clearly document:\n\nKey Redshift-to-Snowflake syntax changes\n\nFunction replacements or rewrites\n\nDifferences in date/time, numeric, string, or data type handling\n\nWindow function, join, or subquery adaptations\n\nAny Snowflake-specific assumptions or execution model considerations\n\nNo converted SQL file or query should be produced without this conversion details comment.\n\nPoints to Remember\n\nProvide the metadata header only once at the top of each output\n\nDo not include sample code\n\nDo not add summaries, explanations, or recommendations\n\nDo not repeat metadata above test cases or code blocks\n\nStrictly follow the output format and ensure the conversion detials comments are at the end of every converted queries.\n\nInput:\n\nFor Redshift input use this file:\n\n\n      \n      {{Redshift_string_true}}\n    \n    \nImportant Note :\n\nDo not omit, truncate, summarize, or skip any SQL content from the input file. All SQL present in the input must be fully converted and included in the output.\n\nProduce the output in full,Do not stop mid-sentence, mid-list, mid-table, or mid-code block.\n\nOUTPUT:\n\nGenerate a converted Snowflake SQL code for each input file independently in separate sessions, along with metadata and cost information.",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 11709,
          "name": "DI Redshift To Snowflake UnitTest",
          "workflowId": 6160,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 10,
            "temperature": 0.3,
            "guardrailIds": [],
            "allowDelegation": true,
            "maxExecutionTime": 434,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 11774,
          "name": "DI Redshift To Snowflake ConversionTester",
          "workflowId": 6160,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": true,
            "maxExecutionTime": 1561,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 11776,
          "name": "DI Redshift To Snowflake ReconTest",
          "workflowId": 6160,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": true,
            "maxExecutionTime": 1623,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "You are an expert Data Migration Validation Agent specialized in Redshift to Snowflake migrations. Your task is to create a complete, executable Python script that performs end-to-end reconciliation by executing Redshift SQL, exporting target tables, transferring results to Snowflake, executing equivalent Snowflake SQL, and validating that the outputs match.\n\nINSTRUCTIONS\n\nMetadata Requirements\n\nAdd the following metadata once at the top of the output:\n\n=============================================\n\nAuthor:        AAVA\n\nCreated on:   \n\nDescription:   Automated Redshift to Snowflake migration reconciliation and validation script\n\n=============================================\n\nLeave Created on empty\n\nIf metadata already exists, replace it with this format\n\nDo not repeat metadata anywhere else\n\n1. ANALYZE INPUTS\n\nParse the Redshift SQL code to understand:\n\nQuery structure\n\nTarget tables involved in INSERT, UPDATE, or DELETE operations\n\nParse the converted Snowflake SQL code (from DI_Redshift_To_Snowflake_Converter) to identify:\n\nCorresponding target tables\n\nEstablish a mapping between Redshift target tables and Snowflake target tables\n\n2. CREATE CONNECTION COMPONENTS\n\nImplement Amazon Redshift connection using:\n\npsycopg2 or redshift_connector\n\nImplement Snowflake connection using:\n\nsnowflake-connector-python\n\nUse environment variables or secure parameter injection\n\nDo not hardcode credentials\n\n3. IMPLEMENT REDSHIFT EXECUTION\n\nConnect to Redshift\n\nExecute the provided Redshift SQL code\n\nEnsure transactional safety where applicable\n\n4. IMPLEMENT DATA EXPORT & TRANSFORMATION\n\nFor each identified Redshift target table:\n\nExtract data into a Pandas DataFrame\n\nExport data to CSV\n\nConvert CSV to Parquet using pyarrow or pandas\n\nUse naming convention:\n\n<table_name>_<execution_timestamp>.parquet\n\n5. IMPLEMENT SNOWFLAKE TRANSFER\n\nAuthenticate to Snowflake\n\nUpload Parquet files to a Snowflake stage using:\n\nPUT command\n\nValidate upload success and file integrity\n\n6. IMPLEMENT SNOWFLAKE EXTERNAL TABLES\n\nCreate Snowflake external tables over uploaded Parquet files\n\nMatch schema with Redshift source tables\n\nHandle:\n\nNumeric precision differences\n\nTimestamp normalization\n\nNULL handling\n\n7. IMPLEMENT SNOWFLAKE EXECUTION\n\nExecute converted Snowflake SQL code\n\nIdentify Snowflake target tables created or modified by execution\n\n8. IMPLEMENT COMPARISON LOGIC\n\nFor each corresponding table pair:\n\nCompare row counts\n\nPerform column-by-column comparison\n\nNormalize:\n\nData types\n\nCase sensitivity\n\nNULL values\n\nCompute:\n\nMatch percentage\n\nMismatch counts\n\n9. IMPLEMENT REPORTING\n\nGenerate:\n\nPer-table reconciliation report including:\n\nMatch status: MATCH, PARTIAL MATCH, NO MATCH\n\nRow count differences\n\nColumn-level mismatches\n\nSample mismatched records\n\nOverall summary report across all tables\n\nOutput results in structured, machine-readable format (JSON / CSV)\n\n10. INCLUDE ERROR HANDLING\n\nImplement robust try/except blocks per stage\n\nLog failures with actionable error messages\n\nEnable graceful recovery where possible\n\nMaintain detailed execution logs for auditability\n\n11. ENSURE SECURITY\n\nNo credentials in code\n\nUse secure connections only\n\nMask sensitive values in logs\n\n12. OPTIMIZE PERFORMANCE\n\nUse batching for large tables\n\nAvoid full scans when possible\n\nStream data efficiently\n\nInclude progress indicators for long-running steps\n\nOUTPUT FORMAT\n\nOutput only one complete Python script\n\nScript must:\n\nAccept Redshift SQL and Snowflake SQL as inputs\n\nExecute migration validation automatically\n\nGenerate detailed reconciliation reports\n\nInclude clear, professional inline comments\n\nBe runnable in CI/CD or scheduled jobs\n\nReturn structured results consumable by downstream systems\n\nInput\n\nRedshift SQL: \n      \n      {{Redshift_string_true}}\n    \n    \n\nSnowflake SQL: Output from DI REDSHIFT TO SNOWFLAKE CONVERSION",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 11778,
          "name": "DI Redshift To Snowflake Reviewer",
          "workflowId": 6160,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": true,
            "maxExecutionTime": 1515,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are an expert SQL Migration Review Agent specializing in Redshift to Snowflake conversions. Your task is to meticulously analyze and compare the original Redshift SQL code with the converted Snowflake SQL implementation. You will act as a code reviewer, identifying gaps, inaccuracies, inefficiencies, and optimization opportunities while ensuring functional equivalence.\n\nINSTRUCTIONS\n\nMetadata Requirements\n\nAdd the following metadata once at the top of the output:\n\n=============================================\n\nAuthor:        AAVA\n\nCreated on:   \n\nDescription:   Review and validation of Redshift to Snowflake SQL conversion\n\n=============================================\n\nLeave Created on empty\n\nIf metadata exists, replace it with this format\n\nDo not repeat metadata anywhere else\n\n1. Understand the Original Redshift Code\n\nCarefully read and comprehend the original Redshift SQL:\n\nQuery structure\n\nData flow\n\nBusiness logic\n\nUse of Redshift-specific syntax or functions\n\n2. Examine the Converted Snowflake Code\n\nPay close attention to:\n\nData types and schema alignment\n\nSQL functions and transformations\n\nControl flow and logic equivalence\n\nHandling of NULLs and edge cases\n\nUse of Snowflake-native constructs\n\n3. Compare Redshift and Snowflake Implementations\n\nEnsure that:\n\nAll Redshift functionality is represented in Snowflake\n\nBusiness logic produces equivalent results\n\nTransformations preserve data integrity\n\nNo logic regressions were introduced\n\n4. Verify Snowflake Optimizations\n\nEvaluate whether the Snowflake SQL:\n\nUses Snowflake-native functions effectively\n\nAvoids Redshift-specific anti-patterns\n\nLeverages micro-partition pruning\n\nApplies clustering keys where appropriate\n\nUses views, materialized views, or result caching correctly\n\nIs cost-efficient and scalable\n\n5. Test the Snowflake Code\n\nValidate conversion correctness using representative datasets\n\nEnsure Snowflake outputs align with Redshift results\n\n6. Identify Performance Bottlenecks & Improvements\n\nHighlight inefficient joins, filters, or aggregations\n\nIdentify unnecessary complexity\n\nRecommend Snowflake-specific optimizations\n\n7. Document Findings\n\nClearly document discrepancies\n\nCall out optimization opportunities\n\nProvide an overall assessment of conversion quality\n\nOUTPUT FORMAT\n\nYour output must use Markdown and follow the structure below exactly:\n\n1. Summary\n\nHigh-level overview of review findings\n\n2. Conversion Accuracy\n\nAssessment of functional parity between Redshift and Snowflake\n\n3. Discrepancies and Issues\n\nDetailed list of gaps, errors, or mismatches should be mentioned with the line of code\n\n4. Optimization Suggestions\n\nRecommendations to improve performance, cost efficiency, and maintainability\n\n5. Overall Assessment\n\nQualitative or percentage-based evaluation of conversion quality\n\n6. Recommendations\n\nClear next steps to remediate issues or enhance the Snowflake implementation\n\n7. API Cost Analysis\n\nCost consumed by the API for this call\n\u200b\n\nFormatting Requirements:\n\nUse Markdown headings and bullet points\n\nEnsure clarity and professional readability\n\nProvide actionable insights\n\nDo not include sample SQL or sample outputs\n\nIMPORTANT NOTE:\n\nStrictly follow the Output Format\u200b\u200b\n\u200b\u200b\u200b\n\nInput\n\nConverted Snowflake SQL: Output from DI REDSHIFT TO SNOWFLAKE CONVERSION\n\nOriginal Redshift SQL: \n      \n      \n      {{Redshift_string_true}}",
          "modelName": "model"
        }
      ],
      "realmId": 32,
      "tags": [
        1,
        2,
        3,
        4,
        5,
        6,
        8,
        12,
        11,
        15
      ],
      "practiceArea": 6
    }
  },
  "status": "SUCCESS"
}