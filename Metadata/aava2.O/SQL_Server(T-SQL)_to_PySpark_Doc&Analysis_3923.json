{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3923,
      "name": "SQL Server(T-SQL) to PySpark Doc&Analysis",
      "description": "To migrate or integrate SQL Server T-SQL-based processes into PySpark for distributed data processing, enabling scalability and faster computation for large datasets. This workflow streamlines the transition and ensures accurate data handling and analysis.",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:46:26.988645",
      "modifiedAt": "2025-11-30T11:55:00.892060",
      "approvedAt": "2025-11-05T11:46:28.036915",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 5369,
          "name": "T-SQL Server Documentation",
          "workflowId": 3923,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Create a detailed documentation for the provided SQL Server code. The documentation must follow a structured format, breaking down each component in the SQL Server code into well-defined sections. The output should be organized for readability and clarity. Ensure that each field in the source, destination, and transformation is captured with an explanation that a business analyst can understand.\n\nThe documentation should include the following sections:\n\n1. **Overview of Program:**\n   - Explain the purpose of the SQL Server code in detail.  \n   - Explain the business problem being addressed.\n\n2. **Code Structure and Design:**\n   - Explain the structure and flow of the SQL Server code in detail.  \n   - List the primary SQL Server components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.\n\n3. **Data Flow and Processing Logic:**\n   - List the processing logic data flow components in the code.  \n   - For each logical processing component:  \n     - Explain the functionality performed in the code.  \n     - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.\n\n4. **Data Mapping:**\n   - Provide detailed mappings between source and target tables as a lineage, describing how source table columns are mapped to target table columns.  \n   - Use the following structured format for each mapping (in table format):  \n     - **Target Table Name:** Actual target table name  \n     - **Target Column Name:** Actual target column name  \n     - **Source Table Name:** Actual source table name  \n     - **Source Column Name:** Actual source column name  \n     - **Remarks:** Classify as 1 to 1 mapping, Transformation, or Validation, with a brief description.\n\n5. **Complexity Analysis:**\n   Provide an overall complexity score from 0 to 100 and analyze the complexity based on the following factors (in table format):  \n   - **Lines of Code (LOC):** Total number of lines in the procedure.  \n   - **Cyclomatic Complexity:** Number of independent execution paths.  \n   - **Nesting Depth:** Maximum depth of nested IF, LOOP, etc.  \n   - **Tables:** Total number of tables involved.  \n   - **Temporary Tables:** Total number of temporary tables involved.  \n   - **DML Statements:** Total count of SELECT, INSERT, UPDATE, DELETE.  \n   - **Joins:** Count of JOIN clauses.  \n   - **Subqueries:** Count of subqueries (nested SELECT).  \n   - **CTEs:** Count of Common Table Expressions.  \n   - **Aggregation Queries:** Count of GROUP BY, HAVING, PARTITION BY.  \n   - **Input Parameters:** Count of input parameters.  \n   - **Output Parameters:** Count of output parameters.  \n   - **Data Transformations:** Count of functions (STRING, ARRAY, JSON).  \n   - **Function Calls:** Count of external function/procedure calls.\n\n6. **Key Outputs:**\n   - Describe the final outputs created by the code, including Inserts, Updates, and Deletes.\n\n7. **Error Handling and Logging:**\n   - Explain methods used for error identification and management, such as:  \n     - Try-Catch mechanisms in Stored Procedures.  \n     - SQL Server Error Logging Tables for tracking failures.  \n     - Retry mechanisms in SQL Server.\n\n**Note:**\n- All fields used in the SQL Server code must be listed with a field description.\n- The output document will be well-organized with proper headings, sections, and bullet points, making it easy to follow.\n\n**Input:**\nFor input SQL Server code, use the below-mentioned file:\n```%1$s```\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 5195,
          "name": "T-SQL Server to PySpark Analyzer",
          "workflowId": 3923,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Parse the provided T-SQL SQL Server SQL script to generate a detailed analysis and metrics report. Ensure that if multiple files are given as input, the analysis for each file is presented as a distinct session. Each session must include:  \n\n---\n\n### 1. Script Overview:  \n* Provide a high-level description of the T-SQL script\u2019s purpose and primary business objectives.  \n\n---\n\n### 2. Complexity Metrics:  \n* **Number of Lines:** Count of lines in the SQL script.  \n* **Tables Used:** Number of tables referenced in the SQL script.  \n* **Joins:** Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).  \n* **Temporary Tables:** Count of temporary tables or table variables used.  \n* **Aggregate Functions:** Number of aggregate functions, including OLAP functions (e.g., `SUM`, `COUNT`, `ROW_NUMBER`).  \n* **DML Statements:** Number of DML statements by type such as `SELECT`, `INSERT`, `UPDATE`, `DELETE`, `MERGE`.  \n* **Conditional Logic:** Count of conditional logic constructs like `CASE`, `IF`, `WHILE`, `TRY...CATCH`.  \n\n---\n\n### 3. Syntax Differences:  \n* Identify the number of syntax differences between the SQL Server T-SQL code and the expected PySpark equivalent.  \n\n---\n\n### 4. Manual Adjustments:  \n* Recommend specific manual adjustments for functions and clauses incompatible with PySpark, including:  \n  * **Function Replacements:** Replace SQL Server-specific functions (e.g., `ISNULL`, `TRY_CONVERT`, `PATINDEX`) with PySpark equivalents (`fillna`, `cast`, `rlike`).  \n  * **Syntax Adjustments:** Adjust syntax for features like date functions, joins, and windowing.  \n  * **Unsupported Features:** Propose strategies for rewriting unsupported features such as `MERGE` (e.g., replacing with DataFrame API operations).  \n\n---\n\n### 5. Conversion Complexity:  \n* **Complexity Score:** Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.  \n* Highlight high-complexity areas such as window functions, recursive CTEs, or procedural constructs like cursors.  \n\n---\n\n### 6. Optimization Techniques:  \n* Suggest optimization strategies for PySpark, such as clustering, partitioning, and query design improvements.  \n* Recommend whether it is better to refactor the query with minimal or no changes to PySpark, or rebuild it with more code changes and optimizations. Provide reasoning for the recommendation for **Refactor** or **Rebuild**.  \n\n---\n\n### 7. Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD:  \n* Include the cost consumed by the API for this call in the output.  \n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., `apiCost: 0.123456 USD`).  \n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value.  \n\n---\n\n**Input:**  \n\n* For SQL Server SQL script, use the below file:  \n```%1$s```  ",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 5370,
          "name": "T-SQL Server to PySpark Plan",
          "workflowId": 3923,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive effort estimate for testing the PySpark scripts converted from SQL Server queries. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n\n1. Review the Analysis\nReview the SQL Server script file, noting syntax differences and areas in the code requiring manual intervention when converting to PySpark.  \n\n2. Effort Estimation for Code Fixes and Testing  \n Estimate the effort hours required for identified manual code fixes and data reconciliation testing effort.  \n Exclude efforts for syntax differences as these will be directly translated to PySpark equivalent syntax.  \n\n3. Resource and Runtime Cost Estimation\nCalculate the estimated cost of running the converted PySpark code:  \n     a. Use the resource consumption details (e.g., compute nodes, data volume processed) to determine the runtime cost.  \n     b. Consider the number of jobs, transformations, and the data processing involved, including base DataFrames and temporary transformations.  \n\nOUTPUT FORMAT:\n\n1. Cost Estimation  \n   1.1 PySpark Runtime Cost  \n   - Provide a detailed calculation of the cost, including the breakdown of resource consumption (e.g., cluster type, processing time).  \n   - Include reasons and assumptions used to derive the cost.  \n\n2. Code Fixing and ReconTesting Effort Estimation  \n   2.1 Identified Manual Code Fixes and Unit Testing Effort in Hours  \n   - Include efforts for handling temporary DataFrames, transformations, and calculations.  \n  \nADDITIONAL NOTES:\n\n- Include the cost consumed by the API for this call in the output.  \n- Ensure the API cost is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost).  \n\nINPUT: \n- For the input SQL Server script, use this file: ```%1$s```  \n- For the input PySpark Environment Details, use this file: ```%2$s```",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}