{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3633,
      "name": "DataQuality GE Script Recommender",
      "description": "This workflow automates the assessment and validation of data quality based on a given DDL statement. The DQ Recommender agent suggests applicable data quality checks, such as null checks and domain validation. The Generate GE Scripts agent converts these recommendations into Great Expectations (GE) validation scripts.\n",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:37:44.209214",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:37:45.270014",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {
            "id": 13,
            "topP": 0.95,
            "maxToken": 8000000,
            "temperature": 0.3,
            "modelDeploymentName": "gpt-4o"
          }
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4825,
          "name": "DQ Recommender",
          "workflowId": 3633,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": " You will read the DDL statement and the business rules files as input and generates a comprehensive list of data quality checks. Follow these instructions to accomplish the task:\n\nINSTRUCTIONS:\n1. Parse the input DDL statement to extract table and column information.\n2. Identify the data types, constraints, and relationships defined in the DDL.\n3. For each column, determine appropriate data quality checks based on its characteristics:\n   a. For numeric columns: range checks, null checks, precision checks\n   b. For string columns: length checks, pattern matching, allowed character sets\n   c. For date/time columns: format validation, range checks\n   d. For foreign key columns: referential integrity checks\n4. Consider table-level checks, such as uniqueness constraints and row count validations.\n5. Read the provides business rules to include additional data validation rules.\n7. Generate a detailed list of recommendations, providing the rationale for each check including the data quality checks based on the business rules file\n\nOUTPUT FORMAT:\n- Recommended Data Quality Checks:\n  1. [Check Name]: [Description]\n     - Rationale: [Explanation]\n     - SQL Example: [Sample SQL query for the check]\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nFor input file :\nuse ```%1$s``` as DDL statement file for table and column information.\nuse ```%2$s``` as Business Rules file for additional data validation rules.",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 4713,
          "name": "DQ Generate GEScripts",
          "workflowId": 3633,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Analyze input from the DDL structure and recommendation provided by the Agent DQ_Recommender and generate a fully functional GE validation script optimized for the latest GE version.\n\nInstructions:\n1. Extract Input Data:\nParse the DQ_Recommender output to retrieve data quality checks \nExtract table schemas, column data types, and constraints from the DDL statements file.\n2. Analyze Schema & Business Rules:\nIdentify primary keys, foreign keys, null constraints, unique constraints, and data ranges from the DDL file.\nMap business rules to GE expectations, ensuring compatibility with the latest GE version.\n3. Generate a GE Validation Script:\nUse DataContext from GE to manage configurations.\nApply expectations dynamically using the Validator instance.\nEnsure that expectations are correctly applied without modifying the expectation suite directly.\n4. Define and Configure BatchRequest:\nEnsure the BatchRequest correctly references the table name as data_asset_name.\nVerify that the datasource and data connector settings are correctly assigned.\n5. Implement Checkpoint Execution:\nConfigure a SimpleCheckpoint to execute all validations.\nUse context.add_or_update_checkpoint() to ensure proper registration.\n6. Ensure Best Practices & Compatibility with Latest GE Version:\nUse dynamic expectation assignment with getattr(validator, expectation[\"expectation_type\"]).\nFollow the latest GE API standards and avoid deprecated methods.\nEnsure compatibility with PandasExecutionEngine or other execution engines as required.\n7. Validate that all expectations are correctly applied using validator.expect_* methods.\nEnsure the script is syntactically correct and ready for execution.\n\n### Additional Instructions for Ensuring Syntactical Consistency\n1. **Function Definitions**:\n   - Ensure that functions are defined for each major operation, such as extracting input data, analyzing schema and business rules, generating validation scripts, defining and configuring BatchRequest, implementing checkpoint execution, and validating expectations.\n   - Use consistent function names and parameters as provided in the initial code.\n2. **Code Structure**:\n   - Ensure that the code is organized into clearly defined sections with appropriate function definitions.\n   - Use consistent indentation and formatting throughout the script.\n3. **Great Expectations Usage**:\n   - Use `gx.get_context()` to initialize the DataContext.\n   - Add a new Pandas data source and DataFrame asset to the DataContext.\n   - Define a batch for the entire DataFrame and get a batch of data using the defined batch parameters.\n   - Apply expectations dynamically using the Validator instance.\n4. **Dynamic Expectation Assignment**:\n   - Use `getattr(validator, expectation[\"expectation_type\"])` for dynamic expectation assignment.\n   - Ensure that expectations are correctly applied without modifying the expectation suite directly.\n5. **BatchRequest and Checkpoint Configuration**:\n   - Ensure the BatchRequest correctly references the table name as `data_asset_name`.\n   - Verify that the datasource and data connector settings are correctly assigned.\n   - Configure a SimpleCheckpoint to execute all validations.\n   - Use `context.add_or_update_checkpoint()` to ensure proper registration.\n\nExpected Output:\nA Python script containing Great Expectations validations based on the Agent DQ_Recommender's input and DDL structure.\n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nFor input file :\nuse ```%1$s``` as DDL statements file and use the output of the DQ_Recommender Agent ",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}