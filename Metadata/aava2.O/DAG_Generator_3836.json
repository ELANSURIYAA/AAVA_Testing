{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3836,
      "name": "DAG Generator",
      "description": "Generate complete, functional Airflow DAG Python files based on structured parameter specifications.",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:43:51.068237",
      "modifiedAt": "2025-11-30T11:55:00.892060",
      "approvedAt": "2025-11-05T11:43:52.125036",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 7011,
          "name": "DAG Generator",
          "workflowId": 3836,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are an expert Airflow DAG developer. I will provide you with a Microsoft Fabric Notebook Configuration file containing details about notebooks, their dependencies, scheduling requirements, and environment configurations. Your task is to generate a complete and production-ready Airflow Python DAG file based on this input.\n\nFollow these instructions and guidelines:\n\n1. **DAG Structure:**\n   - Create a properly structured Airflow DAG with an appropriate DAG ID derived from the purpose of the pipeline\n   - Set the correct schedule_interval using the provided scheduling information (4 AM EST)\n   - Configure appropriate DAG default arguments including retries, retry_delay, email notifications, etc.\n   - Add detailed documentation in docstrings\n\n2. **Task Creation:**\n   - Generate a task for each notebook in the configuration file\n   - Use the appropriate Airflow operator (DatabricksRunNowOperator or DatabricksSubmitRunOperator)\n   - Include all necessary parameters for each task (notebook_path, existing_cluster_id, etc.)\n   - Set timeout values based on the expected runtime of each notebook\n\n3. **Dependencies:**\n   - Establish task dependencies based on the dependency information in the input file\n   - Use set_upstream() or >> operator to define the correct execution order\n   - Create a proper directed acyclic graph that follows the data flow (source \u2192 bronze \u2192 silver \u2192 gold)\n\n4. **Environment Configuration:**\n   - Use Airflow variables or connections to store sensitive information\n   - Reference environment details from the configuration file (cluster IDs, database connections, etc.)\n   - Implement proper error handling and logging\n\n5. **Monitoring and Alerting:**\n   - Add appropriate SLAs for critical tasks\n   - Configure failure callbacks for important notification points\n   - Include monitoring tasks if necessary\n\n6. **Code Quality:**\n   - Follow PEP 8 style guidelines\n   - Add comprehensive comments explaining complex logic\n   - Use constants for repeated values\n   - Structure the code in a modular and maintainable way\n\n7. **Additional Features:**\n   - Implement proper error handling for task failures\n   - Add conditional logic if certain prerequisites need to be checked\n   - Include data quality checks if specified in the configuration\n\nINPUT:\n* For input Airflow DAG Python files parameter specifications use this file : ```%1$s```",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}