{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3188,
      "name": "Postgres To Pyspark Doc & Analyze",
      "description": "Postgres_To_Pyspark_Doc_&_Analyze",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:23:52.029061",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:23:53.077449",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 5253,
          "name": "Postgres Documentation",
          "workflowId": 3188,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 154,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Create a detailed documentation for the provided postgres code. The documentation must follow a structured format, breaking down each component in the postgres code like control flow, data flow into well-defined sections. The output should be   organized for readability and clarity. Ensure that each field in the source, destination and transformation are captured with an  explanation that a business analyst can understand.\n\nThe documentation should include the following sections:\n1. Overview of Program:  \n   Explain the purpose of the PostgreSQL code in detail.  \n   Explain the business problem being addressed .  \n\n2. Code Structure and Design:  \n   Explain the structure and flow of the PostgreSQL code in detail.  \n   List the primary PostgreSQL components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.  \n\n3. Data Flow and Processing Logic:  \n   - List the processing logic data flow components in the code.  \n   - For each logical processing component\n           -Explain the functionality performed in the code.  \n          - Explain the applied transformations, including filtering, joins, aggregations, and field calculations.  \n\n4. Data Mapping:  \n   - Provide detailed mappings between source and target tables as a lineage in terms how the source table.columns are mapped \n     to the target table.columns\n   - Use the following structured format for each mapping:  \n   - Give this one in table format with the below column names\n     Target Table Name : Give actual target table name\n     Target Column Name : Give actual target column name\n     Source Table Name : Give actual source table name\n     Source Column Name : Give actual source column name\n     Remarks : Classify as 1 to 1 mapping or Transformation or Validation and provide a brief description\n\n5. Complexity Analysis:  \n    Overall Complexity Score: Score from 0 to 100.  \n   - Analyze and document the complexity based on the below mentioned details:  \n   - Give this one in table format with the below column names\n     Lines of Code (LOC) :Total number of lines in the procedure.\n     Cyclomatic Complexity :Number of independent execution paths.\n     Nesting Depth :Maximum depth of nested IF, LOOP, etc.\n     Tables :Total number of tables involved\n     Temporary tables :Total number of temporary tables involved\n     DML Statements :Total count of SELECT, INSERT, UPDATE,  DELETE.\n     Joins :\tCount of JOIN clauses.\n     Subqueries :Count of subqueries (nested SELECT).\n     CTEs :Count of Common Table Expressions.\n     Aggregation Queries :Count of GROUP BY, HAVING, PARTITION BY.\n     Input Parameters :Count of input parameters.\n     Output Parameters :Count of output parameters.\n     Data Transformations :Count of functions (STRING, ARRAY, JSON).\n     Function Calls :Count of external function/procedure calls.\n  \n6. Key Outputs:  \n    Describe final outputs created by the code like Inserts, Updates, Deletes.\n    \n7. Error Handling and Logging:  \n    Explain methods used for error identification and management, such as:  \n     - Try-Catch mechanisms in Stored Procedures.  \n     - PostgreSQL Error Logging Tables for tracking failures.  \n     - Retry mechanisms in PostgreSQL.  \n\nNote: \n* All fields used in the postgres must be listed with a field description \n* The output document will be well-organized with proper headings, sections, and bullet points, making it easy to follow.\n\nInput: \nFor input Postgres code use the below mentioned file:\n```%1$s```",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 5110,
          "name": "Postgres to PySpark Analyzer",
          "workflowId": 3188,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Parse the provided postgres code to generate a detailed analysis and metrics report. Ensure that if multiple files given as input then do analysis for each file is presented as a distinct session. \n\nINSTRUCTIONS:\n1. Examine the PostgreSQL code structure and identify all SQL elements (e.g., tables, views, functions, stored procedures).\n2. List all PostgreSQL-specific features and functions used in the code.\n3. Identify syntax differences between PostgreSQL and PySpark SQL, highlighting areas that require manual intervention.\n4. Determine potential performance bottlenecks in the original PostgreSQL code.\n5. Suggest PySpark-specific optimizations for improved performance after conversion.\n6. Outline any data type conversions or schema changes necessary for PySpark compatibility.\n7. Identify any PostgreSQL features that don't have direct equivalents in PySpark and propose alternative approaches.\n9. Provide general best practices for maintaining and further optimizing the converted PySpark code.\n\nOutput must include:\n\n1. Complexity Metrics:\n* Number of Lines: Count of lines in the postgres code.\n* Tables Used: number of tables referenced in the postgres code.\n* Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n* Temporary tables: Number of Volatile, derived tables\n*Aggregate Functions : Number of aggregate functions\n* DML Statements: Number of DML statements by type like SELECT, INSERT, UPDATE, DELETE, CALL, LOCK , Export, Import operations present in the SQL script.\n*Conditional Logic: Number of conditional logic like .IF, .\n\n2. Conversion Complexity:\n* Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.\n* Highlight high-complexity areas such as postgres clauses.\n\n3. Syntax Differences:\n* Identify the number of syntax differences between the postgres code and the expected Pyspark equivalent.\n\n4. Manual Adjustments:\n* Recommend specific manual adjustments for functions and clauses incompatible with Pyspark, including:\n    * Function replacements.\n    * Syntax adjustments.\n    * Strategies for rewriting unsupported features.\n\n5. Optimization Techniques:\n* Suggest optimization strategies for Pyspark, such as clustering, partitioning, and query design improvements.\n\n6. Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n\nInput :\n\n* For postgres code use the below file :\n```%1$s``` ",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 4645,
          "name": "Postgres to PySpark Plan",
          "workflowId": 3188,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive effort estimate for testing the PySpark converted from Postgres scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n1. Review the analysis of Postgres script file, noting syntax differences when converting to PySpark and areas requiring manual intervention.\n2. Consider the pricing information for Azure Databricks PySpark environment \n4. Calculate the estimated cost of running the converted PySpark code:\n   a. Use the pricing information and data volume to determine the code cost.\n   b. the number of code and the data processing done with the base tables and temporary tables\n5. Estimate the code fixing and data recon testing effort required:\n\nINPUT :\n* Take the previous Postgres Analyzer agents output as  input\n* For the input Postgres script use this file : ```%1$s```\n* For the input  PySpark Environment Details for Azure Databricks  use this file : ```%2$s```",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}