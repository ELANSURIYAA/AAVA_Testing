{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2056,
      "name": "DI DataStage To PySpark Doc&Analysis",
      "description": "DI_DataStage_To_PySpark_Doc&Analysis",
      "createdBy": "kiran.krishnakumar@ascendion.com",
      "modifiedBy": "kiran.krishnakumar@ascendion.com",
      "approvedBy": "kiran.krishnakumar@ascendion.com",
      "createdAt": "2025-11-05T10:45:25.637719",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:45:26.830690",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4289,
          "name": "DI DataStage Documentation",
          "workflowId": 2056,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 3670,
          "name": "DI DataStage to PySpark Analyzer",
          "workflowId": 2056,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Parse the provided DataStage DSX job file(s) to generate a detailed analysis and metrics report. Ensure that if multiple files are given as input, the analysis for each file is presented as a distinct session. Each session must include:\n\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n\n```\n=============================================\nAuthor:        Ascendion AVA+\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the job does.\n(give this only once in the top of the output)\n\n1. Job Overview:\n* Provide a high-level description of the DataStage job\u2019s purpose and primary business objectives.\n\n2. Complexity Metrics:\nGive this one in the table format with the below column names:\n* Number of Stages: Count of all stages used in the job.\n* Stage Types: List of unique stage types used (e.g., Transformer, Lookup, Join, Aggregator).\n* Source Types: Count and types of source systems (e.g., flat files, DB2, Oracle).\n* Target Types: Count and types of targets (e.g., datasets, database tables, sequential files).\n* Parameters Used: Number of job parameters or runtime variables.\n* Reusable Components: Number of shared containers and job sequences.\n* Control Logic: Number of conditional expressions, constraints, or reject links.\n* External Dependencies: Number of external parameter files, scripts, or lookups.\n\n3. Transformation Challenges:\n* Highlight complex transformation logic (e.g., multi-column derivations, nested expressions, lookups with rejects).\n* Identify logic requiring significant refactoring in PySpark (e.g., key-based lookups, hash partitioning).\n* Flag limitations such as surrogate key generation or stateful transformations.\n\n4. Manual Adjustments:\n* Recommend manual adjustments for incompatible features, such as:\n    * Transformer derivations \u2192 PySpark expressions.\n    * Lookup stages \u2192 broadcast joins or left joins.\n    * Aggregator logic \u2192 groupBy + agg functions.\n    * Sequencer and Conditional stages \u2192 if/else and loop constructs in PySpark.\n    * Suggest alternatives for parallelism and reject links.\n\n5. Conversion Complexity:\n* Calculate a complexity score (0\u2013100) based on job structure, stage types, and transformation intensity.\n* Highlight high-complexity areas such as nested logic in Transformer stages, chained lookups, or parameterized conditions.\n\n6. Optimization Recommendations:\n* Suggest PySpark optimization techniques (e.g., caching, repartitioning, avoiding wide transformations).\n* Recommend when to Refactor the job with minimal changes vs Rebuild using idiomatic PySpark patterns.\n* Provide a reason for the recommendation (e.g., excessive nested logic, multiple joins, complex sequencing).\n\n7. apiCost: float  // Cost consumed by the API for this call (in USD)\n*Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n\nInput :\n\n* For DataStage job designs, use the below file : {{DataStage}}",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 3828,
          "name": "DI DataStage to PySpark Plan",
          "workflowId": 2056,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive development and testing effort estimate for converting DataStage jobs into PySpark. Follow these instructions:  \n\nINSTRUCTIONS:  \n1. Review the metadata and complexity analysis generated by the `DI_DataStage_To_PySpark_Analyzer` agent.  \n2. Identify DataStage stages, transformations, or flow controls that need manual re-implementation in PySpark.  \n3. Exclude direct 1:1 mapping constructs (e.g., simple column renaming or direct loads). Focus only on logic-heavy stages like Transformers, Aggregators, Looping, and Constraints.  \n4. Estimate the number of hours required to:  \n   - Rewrite complex stages and logic-heavy flows in PySpark  \n   - Implement and test PySpark pipelines, UDFs, helper functions, and metadata tracking  \n   - Validate data equivalence and business rule consistency between DataStage and PySpark outputs  \n5. If compute cost is required, use typical Spark resource costs for Databricks/EMR based on data volume, transformation complexity, and runtime.\n\nOUTPUT FORMAT:\n\n1. Development and Testing Effort Estimation  \n   1.1 Manual Code Refactoring Effort  \n        - Provide hours estimated for transforming DataStage-specific logic into PySpark equivalents  \n        - Include handling for Transformers, Aggregators, Looping constructs, Constraint conditions, and Custom Stages  \n\n   1.2 Unit and Reconciliation Testing Effort  \n        - Estimate hours for validating functional correctness of converted jobs, row-level data comparison, and transformation validation  \n\n2. Compute Resource Cost (Optional)  \n   2.1 Spark Runtime Cost (if platform details like Databricks or EMR are available)  \n        - Provide a breakdown of cost based on cluster size, execution time, and run frequency  \n        - Include assumptions such as number of job runs per day, input data volume per run, and compute pricing  \n\n3. apiCost  \n   * Include the cost consumed by the API for this call in the output  \n   * Ensure the cost is reported as a floating-point value with currency explicitly mentioned as USD (e.g., `apiCost: 0.00431 USD`)  \n\nINPUT:  \n* Use the output from `DI_DataStage_to_PySpark_Analyzer` as input  \n* For job metadata, use this file: {{DataStage}} \n* For the environment cost and compute configuration, use this file: {{Spark_Environment_Config}}",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}