{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2662,
      "name": "DB2 to Oracle Doc&Analyze",
      "description": "Analyzing and Documenting the DB2 Code",
      "createdBy": "saiprakash.r@ascendion.com",
      "modifiedBy": "saiprakash.r@ascendion.com",
      "approvedBy": "saiprakash.r@ascendion.com",
      "createdAt": "2025-11-05T11:11:31.632320",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:11:32.687409",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4683,
          "name": "DB2 DOCUMENTATION",
          "workflowId": 2662,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 154,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "### INSTRUCTIONS:\n1. **Input Processing**:\n   - Accept a single input file containing Db2 database schema information (SQL DDL statements, catalog query results, or JSON/XML exports)\n   - Validate the input file format and structure before processing\n   - Extract all relevant database objects and their relationships\n\n2. **Schema Analysis**:\n   - Identify and categorize all database objects (tables, views, stored procedures, functions, triggers, etc.)\n   - Analyze table structures including columns, data types, constraints, and indexes\n   - Map relationships between tables (primary/foreign keys)\n   - Document partitioning schemes and tablespaces\n   - Extract stored procedure and function logic\n\n3. **Documentation Generation**:\n   - Create a hierarchical documentation structure organized by schemas/collections\n   - Generate detailed descriptions for each database object\n   - Include data dictionary with complete column definitions\n   - Document constraints, indexes, and relationships\n   - Include execution flow diagrams for stored procedures and triggers\n   - Generate entity-relationship diagrams (ERDs) for related tables\n   - Add performance considerations for complex objects\n\n4. **Metadata Enhancement**:\n   - Extract and incorporate any existing comments from the schema\n   - Flag potential issues or areas needing attention (e.g., missing indexes, nullable foreign keys)\n   - Include statistics on object counts and complexity metrics\n   - Add timestamps and versioning information\n\n5. **Quality Assurance**:\n   - Verify all objects from input are documented\n   - Check for consistency in naming and formatting\n   - Validate all cross-references between objects\n   - Ensure documentation is complete for all critical elements\n\n* For DB2 documentation, use the below file:  \n```%1$s```\n\n### OUTPUT FORMAT:\nGenerate documentation in Markdown format with the following structure:\n\n1. **Title Page**:\n   - Database name and version\n   - Documentation generation date\n   - Documentation version\n\n2. **Table of Contents**:\n   - Hyperlinked sections for easy navigation\n\n3. **Database Overview**:\n   - High-level description\n   - Schema count and names\n   - Object count by type\n   - Key statistics\n\n4. **Schema Details** (for each schema):\n   - Schema name and description\n   - List of contained objects by type\n\n5. **Table Documentation** (for each table):\n   ```markdown\n   ## Table: [TABLE_NAME]\n   \n   **Description**: [TABLE_DESCRIPTION]\n   \n   **Schema**: [SCHEMA_NAME]\n   \n   ### Columns\n   \n   | Column Name | Data Type | Nullable | Default | Description |\n   |-------------|-----------|----------|---------|-------------|\n   | column1     | VARCHAR(50) | NO     | NULL    | Primary identifier |\n   | column2     | INTEGER    | YES     | 0       | Count of items |\n   \n   ### Constraints\n   \n   #### Primary Key\n   - PK_[TABLE_NAME] ([COLUMN_NAMES])\n   \n   #### Foreign Keys\n   - FK_[NAME] ([COLUMN_NAMES]) \u2192 [REFERENCED_TABLE]([REFERENCED_COLUMNS])\n   \n   #### Other Constraints\n   - [CONSTRAINT_NAME]: [CONSTRAINT_DEFINITION]\n   \n   ### Indexes\n   \n   | Index Name | Columns | Type | Unique |\n   |------------|---------|------|--------|\n   | IDX_[NAME] | col1, col2 | BTREE | YES |\n   \n   ### Relationships\n   \n   #### Parent Tables\n   - [TABLE_NAME] ([RELATIONSHIP_DESCRIPTION])\n   \n   #### Child Tables\n   - [TABLE_NAME] ([RELATIONSHIP_DESCRIPTION])\n   ```\n\n6. **Views, Procedures, Functions, and Triggers**:\n   - Similar detailed documentation for each object type\n   - Include SQL definitions in code blocks\n   - Document parameters and return values for procedures/functions\n\n7. **Entity-Relationship Diagrams**:\n   - Visual representations of table relationships\n   - Grouped by functional area\n\n8. **Appendices**:\n   - Data dictionary\n   - Naming conventions\n   - Special considerations\n\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 6934,
          "name": "DB2 to Oracle Analyzer",
          "workflowId": 2662,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Parse the provided DB2 SQL script to generate a detailed analysis and metrics report. Ensure that if multiple files are given as input, then the analysis for each file is presented as a distinct session. Each session must include:\n\n1. Script Overview:\n\t- Provide a high-level description of the SQL script\u2019s purpose and primary business objectives.\n\n2. Complexity Metrics:\n\t- Number of Lines: Count of lines in the SQL script.\n\t- Tables Used: Number of tables referenced in the SQL script.\n\t- Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n\t- Temporary Tables: Number of declared global temporary tables and derived tables.\n\t- Aggregate Functions: Number of aggregate functions, including OLAP/analytical functions.\n\t- DML Statements: Number of DML statements by type such as SELECT, INSERT, UPDATE, DELETE, CALL, LOCK, LOAD, IMPORT, and EXPORT operations present in the SQL script.\n\t- Conditional Logic: Number of conditional control flow elements like IF, CASE, GOTO, LABEL, etc.\n\n3. Syntax Differences:\n\t- Identify the number of syntax differences between the DB2 SQL code and the expected Oracle Exadata equivalent.\n\n4. Manual Adjustments:\n\t- Recommend specific manual adjustments for functions and clauses incompatible with Oracle Exadata, including:\n\t- Function replacements (e.g., DB2-specific functions replaced with Oracle equivalents).\n\t- Syntax adjustments for date, string, and window functions.\n\t- Strategies to rewrite unsupported constructs such as FETCH FIRST N ROWS ONLY or recursive SQL.\n\n5. Conversion Complexity:\n\t- Calculate a complexity score (0\u2013100) based on syntax differences, query logic, and the level of manual adjustments required.\n\t- Highlight high-complexity areas such as recursive CTEs, OLAP functions, or DB2-specific procedural logic.\n\n6. Optimization Techniques:\n\t- Suggest optimization strategies for Oracle Exadata, such as using parallel execution plans, partitioning, indexing strategies, and rewriting for Oracle hints.\n\t- Recommend whether to Refactor the query with minimal changes or Rebuild with significant code changes and optimization.\n\t- Provide justification for the chosen recommendation (Refactor vs. Rebuild).\n\n7. API Cost Calculation:\n\t- Include the cost consumed by the API for this call in the output.\n\t- Report the API cost as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: 0.0382 USD).\n\t- Ensure that all decimal values are included and accurate to the actual consumption.\n\nInput:\n\t* For DB2 SQL script, use the below file:```%1$s``` ",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 4678,
          "name": "DB2 to Oracle Plan",
          "workflowId": 2662,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 284,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive effort estimate for testing the Oracle Exadata SQL converted from DB2 scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\nReview the analysis of the DB2 script file and identify areas requiring manual intervention when converting to Oracle SQL. Pay close attention to procedural logic, data type differences, and use of DB2-specific features.\nEstimate the effort hours required for:\nManual code fixes\nData reconciliation and validation testing effort\nDo not consider efforts for basic syntax translation, as they will be handled through automated conversion tools.\nConsider the pricing information for the Oracle Exadata environment, including compute, storage, and licensing where applicable.\nCalculate the estimated cost of running the converted Oracle SQL code:\na. Use the pricing and resource usage information (e.g., CPU, storage I/O) to determine the runtime cost.\nb. Factor in the number of queries, data volume processed, and use of base and temporary tables.\n\nINPUT:\nTake the previous DB2 to Oracle Analyzer Agent's output as input\nFor the input DB2 script, use this file: %1$s\nFor the input Oracle Exadata Environment Details, use this file: %2$s",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}