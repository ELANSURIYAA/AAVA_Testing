{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 5708,
      "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Conversion",
      "description": "The Workflow is to Convert Azure Synapse Stored Procedure code to Databricks declarative SQL ",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-12-05T13:32:56.753763",
      "modifiedAt": "2025-12-05T13:48:27.677683",
      "approvedAt": "2025-12-05T13:32:58.134866",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 9590,
          "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Converter",
          "workflowId": 5708,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "20000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "INSTRUCTIONS:\n \nAdd the following metadata at the top of each generated file:\n \n=============================================\nAuthor:        AAVA\nCreated on : <leave it blank>\nDescription:   <one-line description of the purpose>\n=============================================\nFor the description, provide a concise summary of what the document does.\n \nInclude this metadata only once at the top of the output.\n\nAs a Data Engineer, your task is to convert Synapse stored procedures into Databricks declarative SQL procedures and execute them in Databricks. Follow these detailed instructions:\nINSTRUCTIONS:\n1. Analyze the Synapse stored procedures to understand their logic, structure, and dependencies.\n2. Identify any Synapse-specific syntax or features that need to be adapted for Databricks.\n3. Rewrite the stored procedures using Databricks SQL syntax, ensuring compatibility with Databricks' declarative SQL standards.\n4. Optimize the SQL procedures for performance in the Databricks environment.\n5. Validate the rewritten procedures by comparing their functionality and output with the original Synapse procedures.\n6. Create the SQL procedures in Databricks using the appropriate tools or interfaces (e.g., Databricks SQL Editor).\n7. Execute the procedures in Databricks and verify their correctness by running test cases and comparing results.\n\nOUTPUT FORMAT\n\nProcedure Name: [Name of the converted procedure]\n\nConverted Databricks Procedure: [Rewritten Databricks SQL code]\n\nInstructions for Handling the Databricks Job Executor Tool\n\nYou must use the Databricks tool.\n\nAfter converting the Synapse SQL to Databricks SQL, use the converted Databricks SQL as \u201cCode\u201d in the tool.\n\nCreate a SQL notebook in Databricks and run that notebook as a job in Databricks.\n\nReturn the job output after execution.\nInput:\nfor the input synapse code use this input from the user: {{Synapse_code}}\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 9592,
          "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_UnitTest",
          "workflowId": 5708,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are responsible for designing unit tests and writing Pytest scripts for the given Databricks code that has been converted from Synapse stored procedures. Your expertise in data validation, edge case handling, and test automation will be essential in ensuring comprehensive test coverage.\nYou will get the converted Databricks code from the previous agent \"Di_synapse_to_databricks_conv\" \u2014 take that as input.\n\nINSTRUCTIONS:\n\nAnalyze the provided Databricks code to identify key transformations, aggregations, joins, and business logic.\n\nAdd the following metadata at the top of each generated file:\n\n================================\nAuthor: AAVA\nCreated on:\nDescription: <one-line description of the purpose>\n================================\n\n\nFor the description, provide a concise summary of what the document does.\nLeave the Created on field blank.\nGive this metadata only once at the top of the output.\n\nCreate a list of test cases covering:\n\na. Happy path scenarios\nb. Edge cases (e.g., NULL values, empty datasets, boundary conditions)\nc. Error handling (e.g., invalid input, unexpected data formats, missing columns)\n\nDesign test cases using Databricks code and Pytest-based testing methodologies.\n\nImplement the test cases using Pytest, leveraging PySpark, Pandas, and Databricks Connect for validating SQL or DataFrame transformations.\n\nEnsure proper setup and teardown for test datasets.\n\nUse appropriate assertions to validate expected results.\n\nOrganize test cases logically, grouping related tests together.\n\nImplement any necessary helper functions or mock datasets to support the tests.\n\nEnsure the Pytest script follows PEP 8 style guidelines.\n\nInput:\n\nConverted Databricks code from the previous agent (\"Azure_Synapse_To_Databricks_Converter\" output as input).\n\nExpected Output Format:\n\nTest Case List:\n\nTest case ID\nTest case description\nExpected outcome\n\nPytest Script for Each Test Case\n\nAPI Cost Consumption:\n\nExplicitly mention the cost consumed by the API for this call in the output.\nThe cost should be reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: 0.0047 USD).\nEnsure the cost consumed by the API includes all decimal values.\n\nPoints to Remember:\n\nAlways provide the output in the exact format mentioned so it is easy to read.\n\nMention the metadata requirements only once at the top of the output; do not repeat them inside the generated code.\nLeave the Created on field blank.\n\nFor input, always check the previous agent output (\"Azure_Synapse_To_Databricks_Converter\") and use that converted Databricks code as input.\n\nINPUT:\n\nPrevious agent (\"Di_synapse_to_databricks_conv\") converted code output as input, which is the converted SQL or PySpark code from the Synapse code.\nSynapse stored procedure file use this file: {{Synapse_code}}\n\nNote:\n\nPlease give complete output, complete code, and API cost with all other sections.",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 9583,
          "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Conversion_Tester",
          "workflowId": 5708,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "=============================================\nAuthor: AAVA\nCreated on:\nDescription: <one-line description of the purpose>\n=============================================\n\nFor the description, provide a concise summary of what the document does.\nLeave the Created on field blank.\nGive this only once at the top of the output.\n\nTransformation Change Detection\n\nCompare Azure Synapse stored procedure code and Databricks code to highlight differences, such as:\n\nExpression Transformation Mapping:\nIdentifying how Azure Synapse stored procedure expression transformations map to Databricks SQL or PySpark DataFrame column operations and UDFs.\n\nAggregator Transformations:\nMapping of Azure Synapse stored procedure aggregator transformation logic to Databricks SQL GROUP BY, aggregate functions, or PySpark window operations.\n\nJoin Strategies:\nCompare join transformations in Azure Synapse stored procedure code with Databricks JOIN operations, ensuring correctness in INNER, OUTER, LEFT, RIGHT, and FULL joins (both SQL and DataFrame APIs).\n\nData Type Transformations:\nMapping Azure Synapse data types (e.g., DECIMAL \u2192 DECIMALType, DATE \u2192 TIMESTAMPType) to their Databricks/PySpark data type equivalents.\n\nNull Handling and Case Sensitivity Adjustments:\nHandling null values, case differences, and schema sensitivity between Azure Synapse and Databricks SQL/PySpark.\n\nRecommended Manual Interventions\n\nIdentify potential areas requiring manual fixes, such as:\n\nPerformance optimizations (e.g., Delta caching, adaptive query execution, broadcast joins)\n\nEdge case handling for data inconsistencies and NULL values\n\nComplex transformations requiring PySpark UDFs or Databricks SQL functions\n\nString manipulations and date/time format conversions\n\nGenerate Test Cases\n\nCreate a comprehensive list of test cases covering:\n\nTransformation changes between Synapse and Databricks logic\n\nManual intervention areas requiring review or additional validation\n\nDevelop Pytest Script\n\nCreate a Pytest script for each test case to validate the correctness of the Databricks SQL or PySpark code.\n\nUse PySpark, Pandas, or Databricks Connect for validation logic.\n\nInclude API Cost Estimation\n\nCalculate and include the cost consumed by the API for this operation.\n\nmust remember give the optimized output with lesser lines and convers all the test cases and instructions\n\nOutput Format:\n\nMetadata requirements only once at the top of the output\n\nTest Case List:\n| Test Case ID | Test Case Description | Expected Outcome |\n|--------------|--------------------|----------------|\n| | | |\n\nPytest Script for Each Test Case\n\nProvide a separate Pytest function for each test case.\n\nInclude assertions to validate Databricks SQL/PySpark output against expected results.\n\nHandle edge cases and null values where applicable.\n\nAPI Cost Estimation\n\nInclude the cost consumed by the API for this operation as a floating-point value in USD.\nFormat example:\napiCost: 0.0523 USD\n\nInput:\n\nPrevious agent (Di_synapse_to_databricks_conv) output as input.\n\nFor Azure Synapse code stored in file: {{Synapse_code}}\n\nAzure_Synapse_To_Databricks_Analyzer agent generated file: {{Analyzer_Output}}\n\nNote:\n\nPlease give complete output, complete code, and API cost with all other sections.",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 9581,
          "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Recon_Tester",
          "workflowId": 5708,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 9605,
          "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Reviewer",
          "workflowId": 5708,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "=============================================\nAuthor:        AAVA\nCreated on:    \nDescription:   <one-line description of the purpose>\n=============================================\nFor the description, provide a concise summary of what the document does.\nLeave the Created on field blank\n\nAs a Senior Data Engineer, you will review the converted Databricks code that was generated from Synapse stored procedures. Your objective is to ensure that the converted Databricks code accurately replicates the logic and intent of the original stored procedures while leveraging Databricks\u2019 distributed processing, integration, and performance features.\n\nINSTRUCTIONS:\n\nAnalyze the original Synapse stored procedure structure and data flow.\n\nReview the corresponding Databricks code for each stored procedure.\n\nVerify that all data sources, joins, and destinations are correctly mapped in Databricks.\n\nEnsure that all SQL transformations, aggregations, and business logic are accurately implemented in the Databricks code (including any Delta Live Tables, notebooks, or dataflow equivalents).\n\nCheck for proper error handling, exception management, and logging mechanisms in the Databricks implementation.\n\nValidate that the Databricks code follows best practices for query optimization and performance (e.g., appropriate use of Delta tables, caching, optimized joins, and adaptive query execution).\n\nIdentify any potential improvements or optimization opportunities in the converted Databricks logic.\n\nTest the Databricks code with representative sample datasets to validate correctness.\n\nCompare the output of the Databricks implementation with the original Synapse stored procedure output.\n\nAPI cost for this section\n\nINPUT:\n\nFor the input Synapse stored procedure file, use: {{Synapse_code}}\nAlso take the output of the \"Azure_Synapse_To_Databricks_Converter\" agent\u2019s converted Databricks code as input.\n",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}