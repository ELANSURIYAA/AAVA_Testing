{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3999,
      "name": "MSTR Metrics to Power BI Prompts",
      "description": "MSTR Metric to power bi prompts",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:48:43.800195",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:48:44.867508",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4479,
          "name": "RFP MSTR PRJCT REPORT DASHJSON",
          "workflowId": 3999,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "As a Senior Business Intelligence Developer, you will create an agent that processes MSTR project metadata and dashboard requirement documents to generate JSON files for MSTR reports and dashboards. This agent will streamline the development process and ensure that the output adheres to MicroStrategy's standards and best practices.\n\nINSTRUCTIONS:\n1. Parse the MSTR project metadata, extracting relevant information such as data sources, attributes, metrics, and existing report structures.\n2. Analyze the dashboard requirement document to identify key components, visualizations, and data elements needed for the new dashboard.\n3. Map the requirements to the available metadata, identifying any gaps or necessary data transformations.\n4. Generate a JSON structure for the MSTR report, including:\n   a. Report details (name, description, folder location)\n   b. Data source connections\n   c. Attributes and metrics used\n   d. Filters and prompts\n   e. Report layout and formatting\n5. Generate a JSON structure for the MSTR dashboard, including:\n   a. Dashboard details (name, description, folder location)\n   b. Layout and grid structure\n   c. Visualization components (charts, tables, selectors)\n   d. Data binding for each component\n   e. Interactivity settings (filters, drill-downs)\n6. Validate the generated JSON files against MSTR's schema and best practices.\n7. Provide a summary of any potential issues or missing elements that may require manual intervention.\n\nInput:\nThe MSTR Metadata project and dashboard requirement will use the following file as input: '''%2$s'''",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 4661,
          "name": "Prompting Agent ",
          "workflowId": 3999,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "INSTRUCTIONS:\n1. Review the provided data model carefully and the report requirements.\n2. Analyze the list of needed visuals to be used.\n3. Study the data model on which the Power BI semantic model is created.\n4. Based on available fields and tables in Power BI semantic model and data model:\n   a. Identify the most appropriate visual(s) from the needed list.\n   b. Determine the relevant data fields from the data model.\n   c. Craft a clear and concise prompt in three or four sentences that specifies:\n      - The type of visual to create\n      - The data fields to use\n      - Any necessary calculations or aggregations\n      - Formatting requirements (colors, labels, titles, etc.)\t\n5. Ensure each prompt follows Power BI best practices for data visualization.\n6. Review and refine the prompts for clarity and effectiveness. \n7. For first visuals, the prompt should start with \"Create\". Rest all visuals should start with \"Add\" statement.\n\nInput:\n* For Data Model take use the below file: ```%1$s```\n* For the reporting requirements and Chart Types use the below file: ```%2$s```\n",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 5409,
          "name": "Prompts Unit Testing",
          "workflowId": 3999,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "A comprehensive suite of unit tests and SQL Queries that is created based on the Copilot prompts which is used for the dashboard visuals. Tests should cover various aspects of the prompts, including the types of visuals, data fields, and calculations specified.\nINSTRUCTIONS:\n1. Review the provided Power BI Copilot prompts for dashboard visuals.\n2. For each prompt, identify the following elements:\n   a. Type of visual (e.g., bar chart, line graph, pie chart)\n   b. Data fields used\n   c. Calculations or aggregations required\n3. Develop unit test cases that cover:\n   a. Input validation (data types, ranges, required fields)\n   b. Edge cases (null values, extreme values, empty datasets)\n   c. Calculation accuracy\n   d. Visual type appropriateness\n4. Must Create only SQL Queries to:\n   a. Retrieve the necessary data for each visual\n   b. Perform required calculations or aggregations\n   c. Validate the results against expected outcomes\n5. Document each test case and SQL Queries with:\n   a. A unique identifier\n   b. Description of the test purpose\n   c. Expected results\n   d. Actual results (to be filled during execution)\n6. Organize the test cases and SQL Queries in a logical, easy-to-follow structure.\n\nOUTPUT FORMAT:\n 1. Test Case List:\n   - Test ID\n   - Test Description\n   - Expected Output\n\n2. SQL Queries to validate each test cases \n* Include the cost consumed by the API for this call in the output.\n\nInput:\n* Use the previous agent's Output as input\n* For Data Model and Physical and Logical Attributes take the output of Power BI Metadata Agent as Input.\n* For the reporting requirements and Chart Types take the output of MicroStrategy Reports (RFP_MSTR_PRJCT_REPORT_DASHJSON) as input\n",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}