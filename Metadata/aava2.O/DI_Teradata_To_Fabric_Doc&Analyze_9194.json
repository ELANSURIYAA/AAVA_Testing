{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 9194,
      "name": "DI Teradata To Fabric Doc&Analyze",
      "description": "Teradata to fabric migration",
      "createdBy": "jahnavi.lingutla@ascendion.com",
      "modifiedBy": "jahnavi.lingutla@ascendion.com",
      "createdAt": "2026-02-03T14:53:19.464466",
      "modifiedAt": "2026-02-03T14:53:19.935501",
      "approvedAt": "2026-02-03T14:53:19.930498",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": 9049,
      "workflowConfigs": {
        "topP": null,
        "maxToken": null,
        "managerLlm": [],
        "temperature": null,
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 18861,
          "name": "DI Teradata Document",
          "workflowId": 9194,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 2000,
            "temperature": 0.6,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 736,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Please create detailed documentation for the provided Teradata SQL code.\n\n\nThe documentation must contain the following sections:\n\n\n**Metadata Requirements:**\n\n\n- Add the following metadata at the top of each converted/generated file:\n\n\n```\n\n\n=============================================\n\n\nAuthor: Ascendion AVA+\n\n\nCreated on: (Leave it empty)\n\n\nDescription:\n\n\n=============================================\n\n\n```\n\n\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n\n\n- For the description, provide a concise summary of what the code does.\n\n\n(give this only once in the top of the output)\n\n\n1. Overview of Program:\n\n\n- Explain the purpose of the Teradata SQL code in detail.\n\n\n- Describe how this implementation aligns with enterprise data warehousing and analytics.\n\n\n- Explain the business problem being addressed and its benefits.\n\n\n- Provide a high-level summary of Teradata SQL components like BTEQ scripts, Macros, Stored Procedures, Views, and Tables.\n\n\n2. Code Structure and Design:\n\n\n- Explain the structure of the Teradata SQL code in detail.\n\n\n- Describe key components like DDL, DML, Joins, Indexing, and Macros.\n\n\n- List the primary Teradata SQL components such as Tables, Views, Stored Procedures, Joins, Aggregations, and Subqueries.\n\n\n- Highlight dependencies on Teradata objects, performance tuning techniques, or third-party integrations.\n\n\n3. Data Flow and Processing Logic:\n\n\n- Explain how data flows within the Teradata SQL implementation.\n\n\n- List the source and destination tables, fields, and data types.\n\n\n- Explain the applied transformations, including filtering, joins, aggregations, and field calculations.\n\n\n* Break down the code into logical components and represent the control/data flow using a **block-style diagram in plain markdown**.\n\n\n* Follow these instructions:\n\n\n* Analyze the input script and identify each logical component, such as:\n\n\n* Variable Declarations\n\n\n* SQL Operations (SELECT, INSERT, UPDATE, DELETE, MERGE)\n\n\n* Loops (WHILE, FOR, cursor loops)\n\n\n* Conditionals (IF/ELSE, CASE)\n\n\n* Exception or Error Handling\n\n\n* Cursor handling (DECLARE, OPEN, FETCH, CLOSE)\n\n\n* Procedure or macro calls\n\n\n*. For each component, create a **markdown block** using the following format:\n\n\n```\n\n\n+--------------------------------------------------+\n\n\n| [Block Title] |\n\n\n| Description: 1\u20132 line summary of the operation |\n\n\n+--------------------------------------------------+\n\n\n```\n\n\n* Connect the blocks using arrows to represent flow:\n\n\n```\n\n\n[Block A]\n\n\n\u2193\n\n\n[Block B]\n\n\n\u2193\n\n\n[Block C]\n\n\n```\n\n\n* Use branching arrows for conditional logic:\n\n\n```\n\n\n[IF v_status = 'ACTIVE']\n\n\n\u2193 Yes\n\n\n[Insert into ACTIVE_TABLE]\n\n\n\u2193\n\n\n[Next Cursor Row]\n\n\n\u2191\n\n\n[No] \u2190 [Skip and Continue]\n\n\n```\n\n\n* Use proper indentation and line breaks for sub-flows or nested logic.\n\n\n* Do not include \u201cStart\u201d or \u201cEnd\u201d blocks unless they are explicitly present in the input.\n\n\n* Do not display the original code\u2014only show the **logic block diagram** in pure markdown format, accurately representing the structure and flow of the script.\n\n\n* A markdown-formatted block diagram that clearly illustrates the logic and structure of the input script. Example:\n\n\n```\n\n\n+-------------------------------+\n\n\n| [DECLARE VARIABLES] |\n\n\n| v_id, v_status, v_count |\n\n\n+-------------------------------+\n\n\n\u2193\n\n\n+-------------------------------+\n\n\n| [OPEN CURSOR] |\n\n\n| Select from EMP_TABLE |\n\n\n+-------------------------------+\n\n\n\u2193\n\n\n+-------------------------------+\n\n\n| [FETCH ROW] |\n\n\n| Loop through each employee |\n\n\n+-------------------------------+\n\n\n\u2193\n\n\n+-------------------------------+\n\n\n| [IF v_status = 'ACTIVE'] |\n\n\n| Conditional check |\n\n\n+-------------------------------+\n\n\n\u2193Yes \u2193No\n\n\n+----------------+ +--------------------+\n\n\n| Insert ACTIVE | | Skip current row |\n\n\n+----------------+ +--------------------+\n\n\n\u2193 \u2193\n\n\n+-------------------------------+\n\n\n| [UPDATE LOG TABLE] |\n\n\n| Insert audit trail |\n\n\n+-------------------------------+\n\n\n```\n\n\n4. Data Mapping:\n\n\n* Provide data mapping details, including transformations applied to the data in the below format:\n\n\n* Target Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks\n\n\n* Mapping column will have the details whether its 1 to 1 mapping or the transformation rule or the validation rule\n\n\n5. Complexity Analysis:\n\n\n- Analyze and document the complexity based on the following:\n\n\n- Give this one in the table format with below two columns for the below data\n\n\nCategory | Measurement\n\n\n* Number of Lines: Count of lines in the SQL script.\n\n\n* Tables Used: number of tables referenced in the SQL script.\n\n\n* Joins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\n\n\n* Temporary tables: Number of Volatile, derived tables\n\n\n* Aggregate Functions : Number of aggregate functions like OLAP functions\n\n\n* DML Statements: Number of DML statements by type like SELECT, INSERT, UPDATE, DELETE, CALL, LOCK , Export, Import operations present in the SQL script.\n\n\n* Conditional Logic: Number of conditional logic like .IF, .GOTO, .LABEL\n\n\n* SQL Query Complexity: Number of joins, subqueries, and stored procedures.\n\n\n* Performance Considerations: Query execution time, spool space usage, and memory consumption.\n\n\n* Data Volume Handling: Number of records processed.\n\n\n* Dependency Complexity: External dependencies such as Macros, Stored Procedures, or Load Scripts.\n\n\n* Overall Complexity Score: Score from 0 to 100.\n\n\n6. Key Outputs:\n\n\n- Describe final outputs such as Aggregated Reports, Tables, Views, or Data Exports.\n\n\n- Explain how outputs align with business goals and reporting needs.\n\n\n- Specify the storage format (e.g., Staging Tables, Production Tables, Flat Files, External Data Sources).\n\n\n7. API Cost Calculations:\n\n\n* Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD\n\n\n* Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n\n\nPoints to Remember:\n\n\n- give the metadata requirements in the top of the output only once and also leave the created on field in the metadata requirements empty\n\n\n- don't give the sample code any where and i strictly follow the output format no extra summary or recommendation needed\n\n\n-don't give the metadata above the test case code or converted code give only once in top of the output is enough\n\n\nInput :\n\n\n* For Teradata SQL scripts use below file : {{Teradata_string_true}}",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 18840,
          "name": "DI Teradata To Fabric Analyzer",
          "workflowId": 9194,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 8000,
            "temperature": 0.2,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 300,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your task is to process the provided Teradata job file (SQL, BTEQ script, macro, or stored procedure) and produce a detailed analysis and metrics report, strictly adhering to the specified markdown format and section structure.\n\nThe report must be tailored for a Teradata-to-Microsoft Fabric migration context.\n\nAll content must be derived exclusively from the actual input job file.\n\nDo not make assumptions, infer logic, or provide summaries without direct evidence from the file.\n\nINSTRUCTIONS\n\n1. File Parsing\n\nParse the provided Teradata input job file to extract all relevant metadata, including:\n\nSQL statements\n\nSource and target tables\n\nJoins, filters, aggregations\n\nVolatile tables, macros, procedures\n\nBTEQ control logic (if present)\n\nStatistics, indexing, and partitioning logic\n\nEnsure that all extracted details are accurate and directly reflect the job file contents.\n\n2. Section Generation\n\nFollow the required output format exactly, maintaining section numbering, headings, and markdown layout as specified.\n\nFor each section, populate content only using parsed input file details:\n\nJob Overview\n\nDescribe what the Teradata job does, based strictly on SQL and script logic\n\nExplain the ETL or ELT flow implemented\n\nIdentify the business purpose only if explicitly evident in the job\n\nMap the logic conceptually to Microsoft Fabric layers, such as:\n\nBronze (raw ingestion)\n\nSilver (transformed data)\n\nGold (serving / reporting)\n\nComplexity Metrics\n\nProvide a complexity breakdown in the following table format:\n\nCategory\tMeasurement\n\nNumber of SQL Statements\t\n\nSource Systems\t\n\nTarget Systems\t\n\nTables Involved\t\n\nJoin Count\t\n\nSubqueries / Nested SQL\t\n\nVolatile / Temporary Tables\t\n\nMacros / Procedures\t\n\nControl Logic (BTEQ)\t\n\nExternal Dependencies\t\n\nPerformance Optimizations\t\n\nVolume Handling Indicators\t\n\nError Handling Logic\t\n\nOverall Complexity Score\t\n\nConversion Complexity Rating\t\n\nAssign a numeric complexity score (0\u2013100)\n\nProvide a conversion complexity rating (Low / Medium / High) with explicit justification\n\nList high-complexity areas, referencing exact SQL constructs or features found in the file\n\nSyntax Differences\n\nExplain how Teradata-specific constructs map to Microsoft Fabric equivalents, using job-specific examples, such as:\n\nTeradata joins \u2192 Spark SQL joins\n\nVolatile tables \u2192 Temporary views / Delta tables\n\nQUALIFY \u2192 Window function filters\n\nBTEQ scripting \u2192 Fabric Pipelines / Notebooks\n\nDo not include generic mappings unless they are used in the job\n\nManual Adjustments\n\nEnumerate all manual migration steps required, referencing actual job features, including:\n\nTeradata-specific SQL rewrites\n\nMacro or stored procedure refactoring\n\nIndex and statistics redesign\n\nVolatile table behavior changes\n\nTransaction control differences\n\nAny logic not directly portable to Fabric\n\nOptimization Techniques\n\nRecommend Fabric-specific performance optimizations, grounded in job logic, such as:\n\nDelta table design\n\nPartitioning strategies\n\nCaching or materialization\n\nIncremental load patterns\n\nClearly state whether the job should be:\n\nRefactored, or\n\nRebuilt\n\nProvide explicit reasoning based on complexity and structure found in the job\n\nAPI Cost\n\nCalculate and display the API cost for this analysis in full decimal precision, for example:\n\napiCost: 0.0182 USD\n\n3. Formatting & Quality Criteria\n\nUse markdown headers and tables exactly as specified\n\nWrite in clear, enterprise-grade English\n\nEvery section must appear in order, with no omissions\n\nReplace all  with actual parsed values\n\nDo not include assumptions, inferred logic, or generalized explanations\n\n4. Output Structure\n\nThe report must begin with the following metadata block:\n\nAuthor: Ascendion AAVA\n\nCreated on:\n\nDescription:\n\n\u26a0\ufe0f Do not mention any date next to Created on \u2014 leave it empty\n\nFollow with Sections #1 through #6, exactly as defined\n\nAll metrics must be in table format\n\nAll mappings and recommendations must be specific to Microsoft Fabric\n\n5. Fabric Service Recommendation (Additional Section)\n### Mandatory Analysis Steps\n\n1. Determine the Teradata asset type (SQL Script, View, BTEQ, Stored Procedure, Macro, Python embedded Teradata SQL) using syntax patterns in the script\n\n2. Detect and explicitly identify the following features from the script:\n\n\u00a0 \u00a0- Declarative SQL vs procedural logic\n\n\u00a0 \u00a0- Control flow (IF, LOOP, GOTO, error handling)\n\n\u00a0 \u00a0- Temp / volatile tables\n\n\u00a0 \u00a0- Parameterization and reuse\n\n\u00a0 \u00a0- Teradata-specific constructs (QUALIFY, macros, statistics, session commands)\n\n\u00a0 \u00a0- Python logic, libraries, or external integrations\n\n3. Classify the dominant workload pattern:\n\n\u00a0 \u00a0- Ingestion-centric\n\n\u00a0 \u00a0- Data Quality-centric\n\n\u00a0 \u00a0- Transformation-centric\n\n\u00a0 \u00a0- Orchestration-centric\n\n\u00a0 \u00a0- Procedural Business logic\u2013heavy\n\n\u00a0 \u00a0- Python-driven programming\n\n4. Determine the most appropriate Microsoft Fabric technologies.\n\n\u00a0\n\n### Fabric Technologies Allowed\n\n- Fabric SQL (Warehouse or Lakehouse)\n\n- Fabric Spark (PySpark / Spark SQL)\n\n- Fabric Notebooks (Python + SQL)\n\n- Fabric Data Pipelines\n\n\u00a0\n\n### Recommendation Rules\n\n- You MAY recommend multiple Fabric technologies.\n\n- You MUST clearly identify ONE as **Best Fit**.\n\n- Secondary recommendations must include trade-offs.\n\n- Do NOT recommend SQL stored procedures (Fabric does not support procedural SQL procs).\n\n- All recommendations MUST be justified using evidence from the script.\n\n\u00a0\n\n### Output Format (STRICT)\n\n\u00a0\n\nTeradata Asset Name           : \n\n\u00a0\n\nFabric Recommendations:\n\n1.  (Best Fit)\n\n\u00a0 \u00a0- Reason 1: \n\n\u00a0 \u00a0- Reason 2: \n\n\u00a0 \u00a0- Reason 3: \n\n\u00a0\n\n2.  (Alternative)\n\n\u00a0 \u00a0- Reason 1: \n\n\u00a0 \u00a0- Reason 2: \n\n\u00a0\n\n\u00a0\n\nDo NOT:\n\n- Rename the asset\n\n- Provide generic Fabric explanations\n\n- Suggest non-Fabric technologies\n\n- Rewrite or optimize the script\u200b\n\nInput\u00a0\n\n      \u00a0\n      \n      \n      \n      \n      \n      {{Teradata_string_true}}\n    \n    \n    \n    \n    \n     \n\n\u26a0\ufe0f STRICT RULE\n\nAll content must be fully traceable to the actual Teradata input file.\n\nIf information is not present in the file, it must not appear in the report. tion.",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 18449,
          "name": "DI Teradata to Fabric Plan",
          "workflowId": 9194,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 8000,
            "temperature": 0.2,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 300,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive development and testing effort estimate for converting Teradata workloads into Microsoft Fabric\u2013based implementations. Follow these instructions exactly:\n\n\nINSTRUCTIONS:\n\n\nReview the metadata and complexity analysis generated by the\n\n\nDI Teradata To Fabric Analyzer agent.\n\n\nIdentify Teradata SQL constructs, BTEQ logic, macros, procedures, or query patterns that require manual re-implementation in Microsoft Fabric using Spark SQL, Fabric Notebooks, and Pipelines.\n\n\nExclude direct 1:1 mapping constructs (e.g., simple SELECT * loads, basic column projection, or straightforward INSERT-SELECT statements).\n\n\nFocus only on logic-heavy transformations, such as:\n\n\nAggregations and GROUP BY logic\n\n\nConditional derivations (CASE, IF, DECODE)\n\n\nLookups and reference joins\n\n\nMulti-table joins and nested queries\n\n\nWindow functions and QUALIFY logic\n\n\nBusiness rules embedded in SQL or macros\n\n\nEstimate the number of hours required to:\n\n\nRewrite complex Teradata SQL, macros, or procedures into Spark SQL / Fabric-compatible SQL\n\n\nImplement and test Fabric Lakehouse tables, Notebooks, reusable logic, and Pipeline orchestration\n\n\nValidate data equivalence and business rule consistency between Teradata outputs and Fabric outputs\n\n\nIf compute cost is required, use Microsoft Fabric capacity costs, based on:\n\n\nData volume\n\n\nTransformation complexity\n\n\nExpected runtime\n\n\nOUTPUT FORMAT:\n\n\n=============================================\n\n\nAuthor:        Ascendion AAVA\n\n\nCreated on:     \n\n\nDescription:   Cost and effort estimation for Microsoft Fabric conversion\n\n\n=============================================\n\n\n1. Cost Estimation\n\n\n1.1 Microsoft Fabric Runtime Cost\n\n\nEstimate runtime cost.\n\n\nCost Breakdown:\n\n\n(give the final cost only, do not include calculations)\n\n\nCompute: $X\n\n\nStorage: $X\n\n\nTotal Estimated Runtime Cost per Job:\n\n\n$X USD per run\n\n\nJustification:\n\n\n(Stick to theory only. Do not include calculations. Use bullet points instead of paragraphs.)\n\n\nPercentage of data processed per run\n\n\nJob complexity\n\n\nStorage impact\n\n\nNeed (or lack) of incremental / partitioning optimizations\n\n\n2. Code Fixing and Testing Effort Estimation\n\n\n2.1 Manual Code Fixes and Recon Testing Effort\n\n\nDescribe manual tasks required to convert Teradata logic into Fabric implementations, such as:\n\n\nParameter replacement\n\n\n(e.g., :Audit_Year \u2192 Notebook parameters / Pipeline parameters)\n\n\nSQL refactoring\n\n\n(Teradata SQL \u2192 Spark SQL)\n\n\nMacro / stored procedure refactoring\n\n\nVolatile table redesign\n\n\n(\u2192 temporary views or Delta tables)\n\n\nSchema and datatype alignment between Teradata and Fabric Lakehouse\n\n\nAdding validation logic (row counts, null checks, aggregates)\n\n\nEstimate time (hours) for each activity and subtotal the effort.\n\n\n2.2 Output Validation Effort\n\n\nOutline tasks for validating Fabric outputs against Teradata results:\n\n\nCompare row counts and sample records\n\n\nValidate aggregations, joins, and derived columns\n\n\nExecute validation checks (counts, uniqueness, nullability)\n\n\nDocument and resolve discrepancies\n\n\nProvide estimated effort (hours) for validation and documentation.\n\n\n2.3 Total Estimated Effort in Hours\n\n\nSummarize all efforts in a table:\n\n\nTask\tEstimated Hours\tNotes\n\n\nManual Code Fixes & Testing\t\t\n\n\nOutput Validation\t\t\n\n\nTotal Estimated Effort\t\t\n\n\nJustify effort based on Analyzer complexity score:\n\n\nSimple (0\u201320) \u2192 \u2264 5 hrs\n\n\nModerate (21\u201350) \u2192 6\u201312 hrs\n\n\nComplex (51\u2013100) \u2192 > 12 hrs\n\n\n3. API Cost Consumption\n\n\napiCost: X.XXXXX USD\n\n\nInclude the cost consumed by the API for this call in the output.\n\n\nEnsure the cost is reported as:\n\n\nFloating-point value\n\n\nCurrency explicitly mentioned as USD\n\n\n(e.g., apiCost: 0.00431 USD)\n\n\nINPUT:\n\n\nUse the output from DI Teradata To Fabric Analyzer as input\n\n      {{Teradata_string_true}}\u00a0 \u00a0 \u00a0 {{Fabric Env_string_true}} \u200b",
          "modelName": "model"
        }
      ],
      "realmId": 79,
      "tags": [
        2,
        12
      ],
      "practiceArea": 6
    }
  },
  "status": "SUCCESS"
}