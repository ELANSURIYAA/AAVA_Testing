{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3788,
      "name": "Hive To Delta Doc & Analyze",
      "description": "Hive_To_Delta_Doc_&_Analyze",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:42:24.973339",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:42:26.033580",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 5702,
          "name": "Hive to Delta Documentation",
          "workflowId": 3788,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 154,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Please create a detailed documentation for the given Hive code. The documentation must follow a well-organized structure, thoroughly explaining the purpose, logic, and technical aspects of the code.\n\n1. Script Overview\nPurpose: Provide a high-level explanation of the SQL script's purpose, objectives, and the business problem it addresses.\nData Sources: Outline the key tables, views, and macros that are present.\nKPI Definitions: Clearly define key metrics (e.g., revenue, churn rate) and their calculation logic.\n\n2. Query Breakdown\nBreak down each query, subquery, or section of the script step-by-step.\nExplain the logic implemented in a way that can be understood by a Business Analyst, including joins, filters, aggregations, or conditions used.\nHighlight syntax differences between Hive and Delta where applicable.\n\n3. Data Mapping Details:\n*Provide data mapping details, including transformations applied to the data in the below format:  \n*Destination Table | Destination Column | Source Table | Source Column | Mapping\n* Mapping column will have the details whether its 1 to  1 mapping or the transformation rule or the validation rule  \n\n4. Complexity Metrics\nNumber of Lines: Count of lines in the script.\nTables Used: Number of tables referenced in the script.\nJoins: Number of joins and the types of joins used (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\nTemporary Tables: Number of Common Table Expressions (CTEs) or derived tables used.\nAggregate Functions: Number of aggregate functions (e.g., COUNT, SUM, AVG, OLAP functions).\nDML Statements: Count of DML statements by type (SELECT, INSERT, UPDATE, DELETE, MERGE).\nConditional Logic: Number of conditional statements (CASE, IF, ELSE, etc.).\n\n5. Key Outputs\nExplain the outputs generated by the SQL script, such as result sets, updated tables, or reports.\nMention the final tables or views where data is stored.\n\n6. Error Handling and Optimization\nDocument any error-handling mechanisms, such as conditional logic or exception handling used in the script.\nInclude details of query optimizations, such as partitioning, statistics collection, or execution plan improvements.\nHighlight any performance improvements made when migrating from Hive to Delta.\n\n7. Additionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD\n \n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n*Ensure the cost consumed by the API is mentioned with inclusive of all decimal value\n \n\nInput :\n* For Hive code use the below file :\n```%1$s``` ",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 5728,
          "name": "Hive to Delta Analyser",
          "workflowId": 3788,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 152,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Parse and analyze each Hive file independently to generate a comprehensive report. Ensure that if multiple files given as input then do analysis for each file is presented as a distinct session. Each session must include:\n\n1. Script Overview:\n* Provide a high-level description of the script\u2019s purpose and primary business objectives.\n\n2. Complexity Metrics\n\nNumber of Lines: Count the total number of lines in the Hive script.\nTables Used: Count the number of tables referenced in the script.\nJoins: Identify and count the number of joins used, categorizing them by type (e.g., INNER JOIN, LEFT JOIN, CROSS JOIN).\nTemporary Tables: Count the number of Common Table Expressions (CTEs), subqueries, and temporary tables created using WITH or INSERT OVERWRITE LOCAL DIRECTORY.\nAggregate Functions: Identify and count the number of aggregate functions such as SUM(), COUNT(), AVG(), MAX(), MIN(), and OLAP functions (RANK(), DENSE_RANK(), NTILE()).\nDML Statements: Count the number of DML statements by type (SELECT, INSERT, UPDATE, DELETE, MERGE, LOAD, EXPORT).\nConditional Logic: Count the occurrences of conditional statements like CASE, IF, WHEN, and COALESCE.\n\n3. Syntax Differences\n\nIdentify Hive-specific syntax differences that require modifications when converting to Delta.\nCommon differences include:\nPartitioning Syntax (DISTRIBUTE BY in Hive vs. PARTITION BY in Delta).\nWindow Functions (Delta uses OVER(PARTITION BY ORDER BY), while Hive might use DISTRIBUTE BY and SORT BY).\nLateral Views & Explode (Delta has UNNEST() instead of LATERAL VIEW EXPLODE()).\nData Types (e.g., STRING in Hive vs. STRING in Delta, BIGINT vs. INT64).\nDate Functions (e.g., from_unixtime() in Hive vs. TIMESTAMP_SECONDS() in Delta).\n\n4. Manual Adjustments\n\nProvide recommendations for converting Hive to Delta, including:\nFunction replacements (e.g., UNIX_TIMESTAMP() \u2192 TIMESTAMP_SECONDS(), REGEXP_EXTRACT() \u2192 SAFE.REGEXP_EXTRACT()).\nSyntax Adjustments:\nINSERT OVERWRITE to Delta\u2019s MERGE.\nMAP data types converted to ARRAYS or STRUCTS.\nWorkarounds for unsupported features, such as:\nLATERAL VIEW EXPLODE() \u2192 Use UNNEST().\nDISTRIBUTE BY \u2192 Use PARTITION BY.\n\n5. Conversion Complexity\n\nCompute a complexity score (0\u2013100) based on:\nNumber of syntax differences.\nAmount of manual adjustments required.\nComplexity of query logic (e.g., CTEs, nested subqueries, advanced window functions).\nHighlight high-complexity areas such as:\nLateral Views and Explode functions.\nRecursive CTEs.\nPartitioned table queries.\nWindow functions.\n\n6. Optimization Techniques\n\nSuggest optimization strategies for Delta:\nPartitioning & Clustering: Identify opportunities to use Delta\u2019s native partitioning and clustering to improve performance.\nQuery Restructuring: Recommend changes in join order, materialized views, or denormalization for cost efficiency.\nStorage Format Adjustments: If Hive stores data in ORC/Parquet, recommend best storage practices in Delta.\nRefactor vs. Rebuild Decision:\nRefactor: Minimal changes to work efficiently in Delta.\nRebuild: Major restructuring for improved performance.\nProvide a reason for choosing Refactor or Rebuild.\n\n7. API Cost Estimation\n\nCalculate and report the cost consumed by the API for this analysis.\nEnsure:\nThe cost is reported as a floating-point value.\nThe cost is explicitly mentioned in USD (e.g., apiCost: 0.1234 USD).\nInclude all decimal values to maintain accuracy.\nInput :\n* For Hive code use the below file :\n```%1$s```",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 5147,
          "name": "Hive to Delta Plan",
          "workflowId": 3788,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 284,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive effort estimate for testing the Delta converted from Hive scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n1. Review the analysis of Hive script file, noting syntax differences when converting to Delta and areas requiring manual intervention.\n2. Consider the pricing information for Azure Databricks Delta environment \n4. Calculate the estimated cost of running the converted Delta code:\n   a. Use the pricing information and data volume to determine the code cost.\n   b. the number of code and the data processing done with the base tables and temporary tables\n5. Estimate the code fixing and data recon testing effort required:\n\nINPUT :\n* Take the previous Hive to Delta Analyser agent output as  input\n* For the input Hive script use this file : ```%1$s```\n* For the input  Delta Environment Details for Azure Databricks  use this file : ```%2$s```",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}