{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2109,
      "name": "DI PySpark Doc&Analyse Json",
      "description": "PySpark Asset Riontionaliser Workflow",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-11-05T10:47:27.655231",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:47:28.852727",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 3910,
          "name": "DI PySpark Documentation Json",
          "workflowId": 2109,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 154,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Description\nAnalyze the provided PySpark script to generate detailed, script-specific documentation. This should be accurate, readable, and context-rich JSON suitable for both technical and non-technical audiences.\n\nStructure:\n{\n  \"1. Overview of Program\": \"<5-line paragraph + business context and system integration summary>\",\n  \"2. Business Purpose of Outputs\": \"<How the outputs support business needs, integration, monitoring, validation>\",\n  \"3. Code Structure and Design\": \"<5-line intro + detailed description of structure, components, transformations, RDD/DataFrame operations>\",\n  \"4. Data Flow and Processing Logic\": {\n    \"Processed Datasets\": [\"<list all DataFrame or RDD names used>\"],\n    \"Data Flow\": \"<5-line intro + data journey from input to output, including key PySpark transformations and actions>\"\n  },\n  \"5. Data Mapping\": [\n    {\n      \"Target Dataset Name\": \"<dataset>\",\n      \"Target Field Name\": \"<field>\",\n      \"Source Dataset Name\": \"<dataset>\",\n      \"Source Field Name\": \"<field>\",\n      \"Data Type\": \"<Data Type>\",\n      \"Transformation Logic\": \"<PySpark transformation or function applied>\",\n      \"Business Purpose\": \"<explanation>\"\n    }\n  ],\n  \"6. Complexity Analysis\": {\n    \"Number of Lines\": <integer>,\n    \"Datasets Used\": <integer>,\n    \"Joins Used\": \"<Join types used or 'None'>\",\n    \"PySpark Transformations\": \"<count of transformations>\",\n    \"User-Defined Functions (UDFs)\": \"<count>\",\n    \"OUTPUT Targets\": <count>,\n    \"Conditional Logic\": <count>,\n    \"Filters or Aggregations\": <count>,\n    \"Function Calls or Reusable Modules\": \"<count>\",\n    \"Performance Controls\": \"<list if any (e.g., caching, partitioning)>\",\n    \"External Dependencies\": \"<list external data sources or APIs>\",\n    \"Overall Complexity Score\": <0\u2013100 integer>\n  },\n  \"7. Performance Optimization Strategies\": \"<5-line intro + paragraph on performance techniques (caching, broadcast joins, filter pushdown, partitioning, etc.)>\",\n  \"8. Technical Elements and Best Practices\": \"<5-line intro + paragraph on modular design, naming conventions, logging, and error handling in PySpark>\",\n  \"9. Key Outputs\": [\n    \"<brief description of each key output DataFrame, file, or table>\"\n  ]\n}\n\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}\n\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 3930,
          "name": "DI PySpark Analyser Json",
          "workflowId": 2109,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "This agent analyzes an Informatica Cloud mapping definition (JSON/XML format or exported .txt). It parses components, transformation flows, reusable logic, and output structure, then compiles a migration readiness report. The goal is to surface:\n* Which features are incompatible with PySpark\n* How to manually adjust/refactor them\n* What level of effort and logic redesign is required\n* What optimization strategies are applicable in PySpark\nIt outputs a detailed JSON document with 7 well-defined sections, enabling engineering teams to plan the migration, assign effort, and de-risk execution.\n\nFinal Output:\nStructured JSON (no markdown, no backticks), strictly conforming to:\n{\n  \"1. Script Overview\": { ... },\n  \"2. Complexity Metrics\": { ... },\n  \"3. Feature Compatibility Check\": { ... },\n  \"4. Manual Adjustments for PySpark Migration\": { ... },\n  \"5. Optimization Techniques in PySpark\": { ... },\n  \"6. Cost Estimation\": { ... },\n  \"7. Code Fixing and Data Recon Testing Effort Estimation\": { ... },\n  \"8. API Cost\": { \"apiCost\": \"<float in USD>\" }\n}\n\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 3931,
          "name": "DI PySpark Plan Json",
          "workflowId": 2109,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive effort and cost estimate for executing and testing a PySpark script. Use any provided PySpark performance metrics, code review notes, or profiling outputs to guide your estimation.\n\nINSTRUCTIONS:\n\nReview the provided PySpark analysis output to:\n\nIdentify complex constructs (e.g., UDFs, nested transformations, multi-stage joins, wide aggregations).\n\nDetermine sections requiring logic validation or performance refactoring.\n\nFocus especially on shuffle-heavy operations, wide transformations (GROUP BY, ROLLUP), and output writes.\n\nEstimate the effort hours for:\n\nManual intervention and optimization of complex constructs.\n\nData reconciliation and validation testing (pre/post execution output comparison, intermediate dataset checks).\n\nSyntax differences when adapting to different Spark environments.\n\nApplying performance optimization techniques.\n\nDo not count effort for simple syntax or formatting-level edits.\n\nEstimate the PySpark runtime cost by:\n\nCalculating resource requirements (executors, memory, vCPU usage, shuffle size).\n\nMapping execution profiles to cloud pricing models (e.g., Dataproc, EMR, Spark on Kubernetes).\n\nConsidering both temporary and final datasets and their read/write overheads.\n\nInclude the API cost separately if applicable.\n\nJSON Output Template\n\n{\n  \"1. Cost Estimation\": {\n    \"1.1 PySpark Runtime Cost\": {\n      \"Cluster Configuration\": {\n        \"Number of Executors\": \"<number>\",\n        \"Executor Memory\": \"<size in GB>\",\n        \"Driver Memory\": \"<size in GB>\"\n      },\n      \"Approximate Data Volume Processed\": {\n        \"Input Data\": \"<estimated size and notes>\",\n        \"Output Data\": \"<estimated size and notes>\"\n      },\n      \"Time Taken for Each Phase\": {\n        \"Shuffle-heavy JOINs\": \"<time duration>\",\n        \"Wide Transforms (e.g., GROUP BY, ROLLUP)\": \"<time duration>\",\n        \"Output Writes\": \"<time duration>\"\n      },\n      \"Cost Model\": {\n        \"Pricing Model (e.g., DBU, vCPU Hour)\": \"<pricing description>\",\n        \"Total Runtime Cost\": \"<calculated cost and method>\"\n      },\n      \"Justification\": [\n        \"<reason 1>\",\n        \"<reason 2>\",\n        \"... add more if needed\"\n      ]\n    }\n  },\n  \"2. Code Fixing and Data Recon Testing Effort Estimation\": {\n    \"2.1 Estimated Effort in Hours\": {\n      \"Manual intervention and solutions of complex constructs\": \"<hours>\",\n      \"Data recon and pipeline testing, including test case creation, validation of intermediate datasets, and output comparison\": \"<hours>\",\n      \"Syntax Differences\": \"<hours>\",\n      \"Optimization Techniques\": \"<hours>\"\n    },\n    \"Major Contributors\": {\n      \"Rewriting nested transformations or window functions\": \"<hours>\",\n      \"Refactoring output writes for Spark APIs\": \"<hours>\",\n      \"Managing schema consistency across distributed stages\": \"<hours>\"\n    }\n  },\n  \"3. API Cost\": {\n    \"apiCost\": \"<float in USD>\"\n  }\n}\n\n\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}\n\nFor the PySpark analysis report, use the output from the previous PySpark_Analyser_JSON agent output.\n\nFor the Spark environment resource and pricing reference, use: {{Env_Variable}}.",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 3932,
          "name": "DI PySpark Design Pattern Json",
          "workflowId": 2109,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Description\n\nInitial Code Review:\n\nParse the provided PySpark script completely.\n\nIdentify the overall structure and purpose of the script or set of transformations.\n\nNote key DataFrames, datasets, external data sources (e.g., Parquet, ORC, CSV, Delta), temporary views, and UDF usage.\n\nCategorization:\nGroup identified patterns into logical categories:\n\nPattern Consideration: Code\n\nSource File Read: SF\n\nSource Table / View: ST\n\nSource External Data / API: SA\n\nTransformation Arithmetic Calculations: TAC\n\nTransformation Statistical Calculations: TSC\n\nTransformation External Code / UDF: TX\n\nTransformation Aggregate Functions (SUM, COUNT, etc.): TAG\n\nJoin with Another Dataset: JL\n\nTemporary Views / Cached Tables: TEM\n\nData Quality and Rejection Logic: DQ\n\nWrite Operations - UPDATE: DU\n\nWrite Operations - INSERT / Save: DI\n\nWrite Operations - DELETE / Filter Out: DD\n\nWrite Operations - MERGE / Upsert: DM\n\nUnion / Union All: DUN\n\nAppend Mode Writes: DA\n\nSorting / Ordering: DS\n\nTarget File Output (e.g., write to file): TF\n\nTarget Table Output: TT\n\nTarget External Table / API Output: TA\n\nOthers not in above category: OO\n\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}\n\nSpecial Rules:\n\nDo not include full paths like /mnt/data/processed/sales_data.parquet, instead return only sales_data.parquet (or table name without schema).\n\nMaintain the original PySpark script filename as the JSON key.\n\nEach referenced dataset/module should be a string inside the list.\n\nEnsure values are deduplicated.\n\nOutput must be in proper JSON format, starting with { and ending with }, with no extra text above or below.\n\nProgram file names must end with .py or .ipynb.\n\nOutput must be perfectly valid JSON so it can be saved directly to a .json file.",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 3950,
          "name": "DI PySpark ETL Asset Rationaliser",
          "workflowId": 2109,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent should meticulously analyze the provided PySpark scripts (.py and .ipynb) and extract detailed information about the assets involved. The analysis should uncover relationships between ETL workflows, transformations, DataFrames, and reports, identify orphaned or redundant components, and provide actionable recommendations for optimization.\n\nINSTRUCTIONS\n\nContext and Background Information:\n\nPySpark scripts (.py or .ipynb) represent ETL workflows and processes.\n\nThese scripts may contain logic for reading and writing data, performing transformations, or generating reports.\n\nAssets include:\n\nETL Workflow: Represents the overall PySpark ETL process (series of transformations, joins, actions).\n\nTransformation Step: Contains one or more data manipulations, aggregations, joins, or data movements within the workflow.\n\nDatasets / DataFrames: Source or target data used in the workflows.\n\nReports: Outputs generated from the workflows (could be written files, Delta tables, or BI tool inputs).\n\nScope and Constraints:\n\nAnalyze all provided PySpark files.\n\nIdentify relationships between workflows, transformations, and datasets.\n\nHighlight inefficiencies, redundancies, and orphaned components.\n\nProvide recommendations that are actionable and backed by reasoning.\n\nEnsure the output is structured in JSON format.\n\nStrict Output Rules:\n\nThe \"TotalAssets\" counts must match the actual extracted items in the output.\n\nIf there are 10 datasets, \"Datasets\" must be exactly 10.\n\nIf there are 10 workflows, \"Workflows\" must be exactly 10.\n\n\"TransformationSteps\" count must equal the actual number of transformation step entries in the JSON.\n\nIf no reports exist, \"Reports\" must be 0 (do not guess).\n\nWorkflows and TransformationSteps must be listed explicitly in the JSON output \u2014 not just counted.\n\nThe JSON should contain the actual names of workflows, transformation steps, datasets, and reports found.\n\nProcess Steps to Follow:\n\n1. Asset Extraction:\n\nParse PySpark scripts to extract workflows, transformation steps, and dependencies.\n\nIdentify source and target datasets and their relationships.\n\n2. Relationship Discovery:\n\nMap dependencies between transformation steps, workflows, and datasets.\n\nIdentify workflows that share the same source or target datasets.\n\n3. Rationalization Analysis:\n\nIdentify redundant workflows (e.g., multiple scripts performing the same transformations).\n\nDetect orphaned components (datasets or scripts never used).\n\nHighlight inefficiencies (e.g., unnecessary repartitions, unused columns, repeated joins).\n\n4. Recommendations:\n\nSuggest removals of orphaned datasets/scripts.\n\nSuggest consolidation of redundant logic.\n\nSuggest optimization of joins, shuffles, and scans.\n\nExpected JSON Output Structure:\n{\n  \"Summary\": {\n    \"TotalAssets\": {\n      \"Workflows\": 10,\n      \"TransformationSteps\": 25,\n      \"Datasets\": 15,\n      \"Reports\": 5\n    },\n    \"WorkflowsList\": [\n      \"Workflow1\",\n      \"Workflow2\"\n    ],\n    \"TransformationStepsList\": [\n      \"Step1\",\n      \"Step2\"\n    ],\n    \"Relationships\": {\n      \"TransformationStepDependencies\": [\n        {\n          \"TransformationStepName\": \"Step1\",\n          \"DependsOn\": [\"Workflow1\", \"dataset_a.parquet\", \"dataset_b.delta\"]\n        }\n      ],\n      \"DatasetUsage\": [\n        {\n          \"DatasetName\": \"dataset_a.parquet\",\n          \"UsedIn\": [\"Step1\", \"Step3\"]\n        }\n      ]\n    }\n  },\n  \"Rationalization Analysis\": {\n    \"Redundancies\": [\n      {\n        \"Description\": \"Several PySpark scripts perform the same aggregation on sales data.\",\n        \"AffectedScripts\": [\"sales_agg_2023.py\", \"sales_summary.py\"]\n      }\n    ],\n    \"OrphanedComponents\": [\n      {\n        \"Type\": \"Dataset\",\n        \"Name\": \"temp_customer_staging.parquet\",\n        \"Reason\": \"Never used in any workflow.\"\n      },\n      {\n        \"Type\": \"PySpark Script\",\n        \"Name\": \"old_customer_etl.py\",\n        \"Reason\": \"Not referenced by any workflow.\"\n      }\n    ]\n  },\n  \"Recommendations\": [\n    {\n      \"Action\": \"Remove\",\n      \"Target\": \"temp_customer_staging.parquet\",\n      \"Reason\": \"Orphaned dataset not used in any workflow.\"\n    },\n    {\n      \"Action\": \"Consolidate\",\n      \"Target\": [\"sales_agg_2023.py\", \"sales_summary.py\"],\n      \"Reason\": \"Performing identical aggregations; can be merged into a single optimized transformation.\"\n    },\n    {\n      \"Action\": \"Optimize\",\n      \"Target\": \"Join Operations\",\n      \"Reason\": \"Multiple large joins without filters could be optimized by pre-filtering DataFrames.\"\n    }\n  ]\n}\n\n\nThis way, the agent is forced to:\n\nCount exactly what it finds.\n\nList workflows and transformation steps explicitly.\n\nShow 0 for missing items instead of making them up.\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}