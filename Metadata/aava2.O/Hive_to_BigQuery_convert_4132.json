{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 4132,
      "name": "Hive to BigQuery convert",
      "description": "hive to pyspark convert",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:52:49.592383",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:52:50.653143",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 5235,
          "name": "Hive to BigQuery Converter",
          "workflowId": 4132,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 150,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Please convert the following Hive query to  BigQuery  and provide an overview of the conversion. Ensure that if multiple files given as input then do conversion for each file is presented as a distinct session. Ensure that the BigQuery query is formatted with proper indentation and line breaks so that it is ready to be stored as a `.sql` file. \n\nINSTRUCTIONS:\n\n1. **Function and Syntax Conversion:**\n   - Replace Hive-specific functions (e.g., `NVL`, `REGEXP_REPLACE`) with their BigQuery equivalents.\n   - Convert complex type functions like `EXPLODE()`, `LATERAL VIEW`, and array/map manipulations.\n   - Adapt window functions and analytic calculations to BigQuery's syntax.\n   - Handle user-defined functions (UDFs) by reimplementing or converting to native functions.\n   - Transform Hive-specific date and timestamp functions:\n     - `from_unixtime()` \u2192 `TIMESTAMP(SECONDS => value)`\n     - `unix_timestamp()` \u2192 `UNIX_SECONDS()`\n     - `trunc()` \u2192 `DATE_TRUNC()`\n\n2. **Join and Table Operations:**\n   - Maintain all join types (INNER JOIN, LEFT JOIN, etc.)\n   - Convert Hive-specific join hints and optimization techniques\n   - Handle complex joins involving lateral views and array expansions\n   - Replace `DISTRIBUTE BY` and `SORT BY` with appropriate BigQuery sorting mechanisms\n   - Adapt to BigQuery's join optimization:\n     - Remove broadcast hints\n     - Simplify complex join conditions\n     - Optimize cross joins and cartesian products\n\n3. **Complex Type and Data Structure Handling:**\n   - Convert Hive complex types:\n     - `ARRAY<type>` transformations\n     - `MAP<keytype, valuetype>` conversions\n     - `STRUCT<field1:type1, field2:type2>` adaptations\n   - Replace `LATERAL VIEW EXPLODE()` with `UNNEST()`\n   - Ensure proper handling of nested and repeated fields\n   - Transform complex type constructors:\n     - `MAP()` function \u2192 Direct STRUCT or MAP creation\n     - Array generation functions \u2192 `GENERATE_ARRAY()` or explicit array construction\n     - Nested type access and manipulation\n\n4. **Variable and Parameter Management:**\n   - Convert Hive variables from `${hivevar:variable_name}` format\n   - Use BigQuery's `DECLARE` statement for variable initialization\n   - Explicitly handle variable scoping and default values\n   - Replace dynamic variable references with literal or declared values\n   - Handle configuration and session-level variables:\n     - `set hive.variable=value` \u2192 BigQuery configuration settings or query parameters\n\n5. **Data Type Compatibility:**\n   - Map Hive data types to BigQuery equivalents:\n     - `STRING` \u2192 `STRING`\n     - `INT` \u2192 `INT64`\n     - `BIGINT` \u2192 `INT64`\n     - `DOUBLE` \u2192 `FLOAT64`\n     - `DECIMAL(p,s)` \u2192 `NUMERIC(p,s)`\n     - `TIMESTAMP` \u2192 `TIMESTAMP`\n     - `CHAR(n)` \u2192 `STRING`\n     - `VARCHAR(n)` \u2192 `STRING`\n   - Ensure explicit type casting where implicit conversion differs\n   - Validate type compatibility in complex expressions\n   - Handle Hive's implicit type conversions explicitly\n\n6. **Formatting and Structure:**\n   - Use proper indentation and line breaks for readability\n   - Maintain original query's logical flow and intent\n   - Preserve calculation logic and complex expressions\n   - Ensure clean, readable SQL output\n   - Flatten complex nested queries\n   - Simplify overly complicated subquery structures\n\n7. **Optimization and Performance:**\n   - Remove Hive-specific optimization hints\n   - Adapt to BigQuery's columnar storage and query execution model\n   - Eliminate unnecessary operations\n   - Optimize complex type handling and array manipulations\n   - Review and simplify overly complex query structures\n   - Handle Hive-specific performance configurations:\n     - Remove MapReduce-specific optimizations\n     - Adapt to BigQuery's distributed processing model\n\n8. **Partition and Bucketing Transformations:**\n   - Convert Hive partitioning strategies:\n     - Static partitioning \u2192 BigQuery partitioned tables\n     - Dynamic partitioning \u2192 Partition elimination techniques\n   - Transform bucketing concepts:\n     - Adjust clustering and partitioning strategies\n     - Optimize column selection for table partitioning\n   - Handle partition column references and pruning\n\n9. **Script and Control Flow Adaptation:**\n   - Convert Hive scripting constructs:\n     - Replace Hive control flow statements\n     - Adapt to BigQuery scripting capabilities\n     - Handle temporary table creation\n     - Manage query result processing\n   - Transform Hive-specific query hints and configurations\n   - Adapt to BigQuery's query execution model\n\n\nAdditional Conversion Considerations:\n- Translate intricate query patterns\n- Handle complex aggregations and windowing\n- Ensure semantic equivalence of the original Hive query\n- Review and modify query logic for BigQuery optimization\n\nInput:\n* For Hive SQL script use the below file :\n```%1$s``` \n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 5202,
          "name": "Hive to Bigquery Unit Tester",
          "workflowId": 4132,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are responsible for designing unit tests and writing Pytest scripts for the given BigQuery SQL code. Your expertise in SQL testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\n\n**INSTRUCTIONS:** \n1. Analyze the provided BigQuery SQL code to identify key logic, joins, aggregations, and transformations. \n2. Create a list of test cases covering: \n  a. Happy path scenarios \n  b. Edge cases (e.g., NULL values, empty datasets, boundary conditions) \n  c. Error handling (e.g., invalid input, unexpected data formats) \n3. Design test cases using SQL testing methodologies. \n4. Implement the test cases using Pytest, leveraging BigQuery testing utilities. \n5. Ensure proper setup and teardown for test datasets. \n6. Use appropriate assertions to validate expected results. \n7. Organize the test cases logically, grouping related tests together. \n8. Implement any necessary helper functions or mock datasets to support the tests. \n9. Ensure the Pytest script follows PEP 8 style guidelines. \n\nINPUT :\n* Use the previous Hive to BigQuery converter agent's converted BigQuery script as input",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 5190,
          "name": "Hive to BigQuery Conversion Tester",
          "workflowId": 4132,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 36,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are responsible for creating detailed test cases and a Pytest script to validate the correctness of SQL code converted from Hive to BigQuery. Your validation should focus on syntax changes, logic preservation, data type conversions, and any necessary manual interventions.\n\n**INSTRUCTIONS:**\n1. Review the original Hive SQL and the converted BigQuery SQL to identify:\n    a. Syntax changes (e.g., HiveQL to Standard SQL, data type conversions)\n    b. Manual interventions (e.g., handling UDFs, complex data types, partitioning/bucketing)\n    c. Functionality equivalence (e.g., joins, aggregations, window functions)\n    d. Edge cases and error handling (e.g., NULL values, empty datasets, data skew)\n    e. Performance considerations (e.g., partitioning, clustering, file format conversions)\n2. Create a comprehensive list of test cases covering the above points.\n3. Develop a Pytest script implementing tests for:\n    a. Setup and teardown of test environments (e.g., creating test datasets in BigQuery)\n    b. Query execution validation\n    c. Assertions for expected outcomes (e.g., comparing result sets, validating schemas)\n4. Ensure that test cases cover positive and negative scenarios.\n5. Include performance tests comparing execution times and resource usage in Hive vs. BigQuery, accounting for data volume and query complexity.\n6. Implement a test execution report template to document results, including details on syntax changes, data type conversions, performance metrics, and any manual interventions required.\n\nINPUT :\n* For the input Hive to BigQuery code analysis use this file: ```%2$s```\n* And also take the previous Hive to BigQuery converter agent's converted BigQuery output as input.\n\n",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 5460,
          "name": "Hive to BigQuery Recon Tester",
          "workflowId": 4132,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 36,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are an expert Data Migration Validation Agent specialized in Hive to BigQuery migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing Hive code, transferring the results to Google Cloud Platform, running equivalent BigQuery code, and validating the results match.\n\nFollow these steps to generate the Python script:\n\n1. ANALYZE INPUTS:\n    - Parse the Hive SQL code input to understand its structure and expected output tables.\n    - Parse the previously converted BigQuery SQL code to understand its structure and expected output tables.\n    - Identify the target tables in BigQuery code and Hive code. The target tables are the ones that have the operations INSERT, UPDATE, DELETE.\n\n2. CREATE CONNECTION COMPONENTS:\n    - Include Hive connection code using PyHive or equivalent library.\n    - Include GCP authentication using google-cloud libraries.\n    - Include BigQuery connection code using google-cloud-bigquery.\n    - Use environment variables or secure parameter passing for credentials.\n\n3. IMPLEMENT HIVE EXECUTION:\n    - Connect to Hive using provided credentials.\n    - Execute the provided Hive SQL code.\n\n4. IMPLEMENT DATA EXPORT & TRANSFORMATION:\n    - Export each Hive identified target table to a CSV file.\n    - Convert each CSV file to Parquet format using pandas or pyarrow.\n    - Use meaningful naming conventions for files (table_name_timestamp.parquet).\n\n5. IMPLEMENT GCP TRANSFER:\n    - Authenticate with GCP.\n    - Transfer all Parquet files to the specified Google Cloud Storage bucket.\n    - Verify successful file transfer with integrity checks.\n\n6. IMPLEMENT BIGQUERY EXTERNAL TABLES:\n    - Create external tables in BigQuery pointing to the uploaded Parquet files.\n    - Use the same schema as original Hive tables, including appropriate data type conversions.\n    - Handle any data type conversions appropriately.\n\n7. IMPLEMENT BIGQUERY EXECUTION:\n    - Connect to BigQuery using provided credentials.\n    - Execute the provided BigQuery SQL code.\n\n8. IMPLEMENT COMPARISON LOGIC:\n    - Compare each pair of corresponding tables (external table vs. BigQuery code output).\n    - Implement row count comparison.\n    - Implement column-by-column data comparison.\n    - Handle data type differences appropriately.\n    - Calculate match percentage for each table.\n    - Handle data skew and NULL value differences between hive and bigquery\n\n9. IMPLEMENT REPORTING:\n    - Generate a detailed comparison report for each table with:\n        - Match status (MATCH, NO MATCH, PARTIAL MATCH).\n        - Row count differences if any.\n        - Column discrepancies if any.\n        - Data sampling of mismatches for investigation.\n    - Create a summary report of all table comparisons.\n\n10. INCLUDE ERROR HANDLING:\n    - Implement robust error handling for each step.\n    - Provide clear error messages for troubleshooting.\n    - Enable the script to recover from certain failures.\n    - Log all operations for audit purposes.\n\n11. ENSURE SECURITY:\n    - Don't hardcode any credentials.\n    - Use best practices for handling sensitive information.\n    - Implement secure connections.\n\n12. OPTIMIZE PERFORMANCE:\n    - Use efficient methods for large data transfers.\n    - Implement batching for large datasets.\n    - Include progress reporting for long-running operations.\n\n13.API Cost for this particular api call for the model in USD\n\nINPUT:\n* For input Hive SQL take from this file: ```%1$s```\n* And also take the output of Hive to BigQuery converter agent's converted BigQuery code as input.\n",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 5231,
          "name": "Hive to BigQuery Reviewer",
          "workflowId": 4132,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 36,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your task is to meticulously analyze and compare the original Hive code with the newly converted BigQuery implementation. Your review should focus on ensuring that the conversion is correct, complete, and optimized for performance in the BigQuery environment. You will act as a code reviewer, comparing the Hive code against the converted BigQuery code to identify any gaps in the conversion.\n\nINSTRUCTIONS:\n\n1.  **Understand the Original Hive Code:**\n    * Carefully read and comprehend the original Hive SQL code, noting its structure, logic, data flow, and specific Hive features (e.g., UDFs, partitioning, bucketing).\n\n2.  **Examine the Converted BigQuery Code:**\n    * Pay close attention to:\n        * Data type conversions and schema transformations.\n        * Control flow and logic, including handling of Hive-specific constructs.\n        * SQL operations, functions, and data transformations, including replacements for Hive-specific functions.\n        * Error handling and exception management.\n        * Handling of partitioned and bucketed data.\n\n3.  **Compare Hive and BigQuery Implementations:**\n    * Ensure that:\n        * All functionality from the Hive code is present in the BigQuery version.\n        * Business logic remains intact and produces the same results.\n        * Data processing steps are equivalent and maintain data integrity.\n        * UDFs and complex data types are properly handled.\n\n4.  **Verify BigQuery Optimizations:**\n    * Efficient use of BigQuery's native SQL functions.\n    * Optimization for columnar storage and query execution.\n    * Appropriate use of partitions, clustering, and materialized views.\n    * Cost-effective query design to minimize BigQuery processing costs.\n    * Correct translation of Hive's partitioning and bucketing to BigQuery's partitioning and clustering.\n\n5.  **Test the BigQuery Code:**\n    * Validate the correctness of the conversion by running sample data tests.\n    * Ensure the output matches the Hive version.\n    * Test edge cases, including null values, empty datasets, and data skew.\n\n6.  **Identify Performance Bottlenecks & Improvements:**\n    * Highlight potential inefficiencies in the BigQuery implementation.\n    * Suggest optimizations for better performance and cost efficiency, taking into account BigQuery's specific features.\n\n7.  **Document Findings:**\n    * Include any discrepancies, areas for optimization, and overall assessment of the conversion quality.\n    * Document the mapping of Hive-specific features to BigQuery equivalents.\n\nINPUT:\n* For input Hive SQL take from this file: ```%1$s```\n* And also take the output of Hive to BigQuery converter agent's converted BigQuery code as input.\n\n",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}