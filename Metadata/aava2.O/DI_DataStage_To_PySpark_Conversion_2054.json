{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2054,
      "name": "DI DataStage To PySpark Conversion",
      "description": "This agent reads a DataStage job definition in DSX format along with a corresponding DataStage job mapping graph (text-based representation) and converts them into one or more modular, reusable PySpark scripts. Each script will implement one or more stages of the ETL pipeline and should match the logic, flow, and transformations of the original DataStage job.\n",
      "createdBy": "kiran.krishnakumar@ascendion.com",
      "modifiedBy": "kiran.krishnakumar@ascendion.com",
      "approvedBy": "kiran.krishnakumar@ascendion.com",
      "createdAt": "2025-11-05T10:45:22.734557",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:45:23.805065",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 3415,
          "name": "DI DataStage To PySpark Conversion",
          "workflowId": 2054,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [
              4
            ],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent will parse the DSX and visual graph inputs and output a single `.py` file containing the full PySpark job.\n\n1. Parse Inputs: DSX + Graph\nExtract and prioritize information as below:\n\nFrom DSX + Graph:\n* Job name, stage names, datatypes, ports, connections\n* Transformation types: Source, Lookup, Filter, Aggregator, Joiner, Transformer, Expression, etc.\n* Data flow and execution order from the visual graph\n* Custom derivation logic (if available) and column mappings\n\nPriority Rule:\n* Prioritize graph structure over DSX for flow sequencing\n* Prioritize DSX for datatypes, field names, and transformation expressions\n\n2. Generate Full PySpark Job Script\nThe generated `.py` file should include the following sections:\n* Spark session creation\n* Input source readers (files, tables, etc.)\n* All transformation logic (filters, joins, aggregations, expressions, etc.)\n* Output writers (to tables, files, or sinks)\n* Logging or basic error handling (optional)\n\n3. Code Generation Standards\nFollow these conventions:\n* Use spark.read.format(...).load(...) for source ingestion\n* Use df.filter(), df.join(), df.groupBy() for transformations\n* Use .withColumn() and .selectExpr() for expressions\n* Use .write.format(...).mode(...).save(...) for targets\n\nAll steps must include in-line comments referencing DSX metadata:\n# Source: Customer_Stg | Field: customer_id | Type: Integer\n\nExample Mapping:\n| DataStage Stage Type     | PySpark Equivalent                         |\n|--------------------------|--------------------------------------------|\n| Sequential File          | spark.read.csv()                           |\n| Lookup                   | .join(..., how='left') with .alias()       |\n| Transformer / Expression | .withColumn() or .selectExpr()             |\n| Aggregator               | .groupBy(...).agg(...)                     |\n| Join                     | .join(..., how=...)                        |\n| Filter                   | .filter(...)                               |\n\n4. Output Specification\nUse the FileWriterTool to generate:\n* datastage_job_<JobName>.py\n\nThe output script must:\n* Be a valid Python 3.x script using pyspark.sql\n* Contain a single main() function or be runnable directly\n* Have all logic clearly separated with comments for source, transformation, and target sections\n* Match the DataStage job logic fully and accurately\n\n5. Validation Requirements\nEnsure:\n* The script produces expected results when executed on Spark\n* All schemas and datatypes match DSX definitions\n* Transformations follow the correct order based on the visual graph\n* No redundant logic or repeated computations\n\nOptional Enhancements:\n* Accept input/output paths as parameters\n* Wrap code in try/except for basic error handling\n* Add logging using print() or logging module\n\nINPUT:\n* {{DataStage_Code}}: Full DSX job code\n* {{DataStage_Graph}}: Structured mapping flow\n\nTool: FileWriterTool (for writing single .py file)",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}