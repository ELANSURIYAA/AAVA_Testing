{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 5707,
      "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Doc&Analyze",
      "description": "Document , Analyse and provide the plan for the Synapse Code",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-12-05T13:32:56.547199",
      "modifiedAt": "2025-12-05T13:48:27.677683",
      "approvedAt": "2025-12-05T13:32:58.134866",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 9597,
          "name": "DI_Synapse_Documentation",
          "workflowId": 5707,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "====================================================\nAuthor:        Ascendion AAVA\nDate:          <leave it blank>\nDescription:   <one-line description of the purpose>\n====================================================\n1. Overview of Pipeline/Component\nDescribe the purpose of the Azure Synapse pipeline, dataflow, SQL script, or linked service.\n\nExplain the business logic or requirement the pipeline or component addresses.\n\n2. Component Structure and Design\nDescribe the layout and logical grouping of activities within the pipeline or dataflow.\n\nHighlight key components such as:\n\nSource (Azure Blob, Data Lake, SQL DB, etc.)\n\nCopy Activity\n\nDataflow Transformation (Join, Aggregate, Derived Column, Filter, Conditional Split, Lookup)\n\nStored Procedure Activity\n\nForEach, Wait, Web, or Custom Activities\n\nSink (Azure SQL, Synapse Table, Parquet, CSV, etc.)\n\nExplain the connection flow between activities and the use of parameters, variables, and triggers.\n\n3. Data Flow and Processing Logic\nList the key data sources, staging/intermediate datasets, and final outputs.\n\nFor each logical step:\n\nDescribe what it does (e.g., filtering, joining, aggregating, mapping).\n\nMention any SQL scripts, stored procedures, or notebooks used.\n\nInclude any business rules or transformations applied.\n\n4. Data Mapping (Lineage)\nMap fields from source datasets to target datasets in the following format:\ngive the below details as a marked down table\n\nTarget Table Name : <actual target table/view>\nTarget Column Name : <actual column>\nSource Table Name : <actual source table/view>\nSource Column Name : <actual column>\nRemarks : <1:1 Mapping | Transformation | Validation - include logic description>\n5. Transformation Logic\nDocument each derived column expression, computed column, or SQL transformation used in the pipeline or dataflow.\n\nExplain what each transformation does and which fields are involved.\n\nNote any user-defined functions (UDFs) or reusable templates applied.\n\n6. Complexity Analysis\ngive the below details as a marked down table\nProvide a high-level complexity summary:\n\nNumber of Pipeline Activities: <integer>\n\nNumber of Dataflow Transformations: <integer>\n\nSQL Scripts or Stored Procedures Used: <count>\n\nJoins Used: <list of types or None>\n\nLookup Tables or Reference Data: <count or 'None'>\n\nParameters/Variables/Triggers: <count>\n\nNumber of Output Datasets: <integer>\n\nConditional Logic or if-else Flows: <count>\n\nExternal Dependencies: <Linked Services, Notebooks, APIs>\n\nOverall Complexity Score: <0\u2013100>\n\n7. Key Outputs\nDescribe what is written to the final tables, files, or views.\n\nMention the format (Parquet, CSV, Delta, etc.) and its intended use (e.g., reporting, ML model input, downstream processing).\n\n\nAPI Cost: \n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nInput:\nAttach or provide the Azure Synapse artifacts such as pipeline JSON, dataflow JSON, parameter files, linked service definitions, or SQL scripts.\nAcceptable formats: plain text, zipped folder, or directory path structure: {{synapse_code}}\n\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 9604,
          "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Analyzer",
          "workflowId": 5707,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "=============================================\nAuthor:        AAVA\nDate:   <Leave it blank>\nDescription:   <one-line description of the purpose>\n=============================================\nleave the Created on field empty.\nFor the description, provide a concise summary of what the document does.\n\nGive this only once at the top of the output.\n\n**1. Procedure Overview**\nProvide a high-level summary of the Azure Synapse stored procedure.\n\nMention the key business objective it supports, such as data integration, cleansing, enrichment, transformation, or reporting.\n\n**2. Complexity Metrics**\n(present this in a markdown table format when generating output, make this metric specific to the provided input synapse stored procedure code)\n\nMetric\tDescription\nNumber of Input Tables\tCount of distinct source tables used in the procedure.\nNumber of Output Tables\tCount of target or intermediate tables modified or populated.\nVariable Declarations\tNumber of declared variables and their usage complexity.\nConditional Logic\tNumber of IF, CASE, or nested conditional blocks.\nLoop Constructs\tNumber of WHILE or FOR loops, if present.\nJoin Conditions\tCount and types of joins (INNER, LEFT, RIGHT, FULL).\nAggregations\tNumber of aggregation operations (SUM, COUNT, AVG, etc.).\nSubqueries / CTEs\tNumber of subqueries or Common Table Expressions used.\nProcedural Calls\tNumber of stored procedure or function invocations.\nDML Operations\tFrequency of INSERT, UPDATE, DELETE, MERGE operations.\nTemporary Tables / Table Variables\tNumber of temp tables or table variables created and used.\nTransaction Handling\tCount of BEGIN TRAN, COMMIT, ROLLBACK statements.\nError Handling Blocks\tPresence and count of TRY...CATCH logic.\nComplexity Score (0\u2013100)\tBased on nested logic, control flow, DML count, and procedural depth.\n\nAlso highlight high-complexity areas like:\n\n- deeply nested conditional logic  \n- multiple joins or subqueries  \n- dynamic SQL execution  \n- dependency on procedural control flow  \n\n**3. Syntax Differences**\nIdentify T-SQL constructs in Synapse that don\u2019t have direct Databricks declarative SQL equivalents.\n\nMention necessary syntax changes \u2014 e.g., variable declarations (DECLARE), procedural loops, or transactions (BEGIN...END).\n\nHighlight control-flow logic (IF, WHILE, TRY...CATCH) that must be replaced with equivalent declarative SQL sequences or view-based logic.\n\nSpecify data type conversions where required (e.g., DATETIME \u2192 TIMESTAMP, DECIMAL precision adjustments).\n\n**4. Manual Adjustments**\nList components that require manual implementation in Databricks declarative SQL \n\nIdentify external dependencies such as pre/post job scripts, external function calls, or system procedures.\n\nMention any areas where business rules or conditional logic need validation post-conversion to ensure parity with Synapse execution.\n\n**5. Optimization Techniques**\nFollow Databricks declarative SQL best practices only (not Delta or Delta Live Tables).\n\nSimplify deeply nested logic into sequential CTEs or modular queries.\n\nOptimize large joins through partitioning or predicate pushdown where applicable.\n\nCombine redundant queries and reduce intermediate materializations.\n\nFocus on implementing effective SQL optimization techniques.\n\n### 6. API Cost Consumption\n \nInclude the cost consumed by the API during this call in the final output.\nEnsure the cost is clearly reported in **floating-point format** with currency, like this:\n \n```(for example : apiCost: 0.0523 USD)\n```\n**Important Notes:**\n-  Do **not** include any reference to Delta Lake, Delta Live Tables, or Delta-specific optimizations.  \n-  Focus purely on **Databricks declarative SQL syntax**\n- Do not generate any SQL code in the output.  \n\n\n\n**Input Files:**\n- For the converted Databricks SQL code: use output from the `Azure_Synapse_To_Databricks_Converter`\n- For the original Azure Synapse stored procedure: use `{{synapse_code}}`\n",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 9586,
          "name": "DI_Azure_Synapse_To_Databricks_DeclarativeSQL_Plan",
          "workflowId": 5707,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 284,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": null,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Metadata Header (Add once at the top of the output file)\n=============================================\nAuthor:        AAVA\nCreated on:      <leave it blank>\nDescription:    <one-line description of the purpose>\n=============================================\n\n* Add this header **only once** at the top of the file.  \n* leave the Created on field empty.\n* Replace `<Description>` with a concise summary of the output purpose.  \n\n---\n\n\n* Add this header **only once** at the top of the file.\n* Leave the **Date** field empty for dynamic population.\n* Replace `<One-line summary of the document's purpose>` with a concise description of the output.\n\n---\n\n### Conversion Instructions\n\nAs a **Senior Software Engineer**, your task is to analyze the provided Synapse workflow/mapping and generate a comprehensive plan for its conversion to Databricks.\n\nFollow these steps:\n\n1. **Analyze the Synapse Workflow/Mapping:**  \n\n   * Use the previously generated output from the `Azure_Synapse_To_Databricks_Analyzer` agent.\n\n2. **Compare and Identify Gaps:**  \n\n   * Review syntax and functional logic in the Synapse workflow/mapping.  \n   * Identify areas requiring manual intervention (e.g., script components, event handling, third-party tasks).  \n   * Do **not** count simple syntax changes; those will be auto-converted.\n\n3. **Effort Estimation:**  \n\n   * Estimate the manual effort in hours for each identified gap.  \n   * Provide separate effort estimates for:  \n\n     * Manual code fixes  \n     * Data reconciliation and validation testing\n\n4. **Databricks Environment Considerations:**  \n\n   * Use the Azure Databricks environment details from the input file.  \n   * Factor in performance, scalability, and any special configurations (e.g., clusters, job orchestration).\n\n5. **Cost Estimation:**  \n\n   * Include the **API cost consumed** for this analysis in the final output.  \n   * Format as: `apiCost: <floating-point value> USD` (e.g., `apiCost: 0.0462 USD`)\n\n---\n\nINPUT  \nTake the previous agent \"Azure_Synapse_To_Databricks_Analyzer\" output as input.  \nFor the input Synapse workflow/mapping, use this file: {{synapse_code}}  \nFor the input Databricks Environment Details, use this file: {{Env_Details}}\n",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}