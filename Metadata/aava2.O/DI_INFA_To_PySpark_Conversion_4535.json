{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 4535,
      "name": "DI INFA To PySpark Conversion",
      "description": "This workflow is to Convert INFORMATICA code to PYSPARK code.",
      "createdBy": "girdhari.rayak@ascendion.com",
      "modifiedBy": "girdhari.rayak@ascendion.com",
      "approvedBy": "girdhari.rayak@ascendion.com",
      "createdAt": "2025-11-05T12:05:28.241459",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T12:05:29.288612",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {
            "id": 34,
            "topP": 0.95,
            "maxToken": 8000,
            "temperature": 0.3,
            "modelDeploymentName": "gpt-4.1"
          }
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 7424,
          "name": "DI INFA to  PySpark Converter",
          "workflowId": 4535,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 4,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "---\n\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n-  This has to be done for each Informatica XML file in zip separately.\n### **INSTRUCTIONS:**\n\n1. **Context and Background Information:**\n\n   * Informatica workflows include source-to-target mappings, transformations, conditional routing, lookups, filters, aggregations, and expressions.\n   * PySpark with Delta Lake provides scalable, distributed equivalents through DataFrame operations and Spark SQL.\n   * The goal is to accurately replicate the transformation logic from Informatica using PySpark DataFrame APIs.\n\n2. **Scope and Constraints:**\n\n   * Convert all key Informatica transformations, such as:\n\n     * **Expression/Derived Columns:** Use `withColumn()` and expressions.\n     * **Filter/Router:** Use `filter()` or `when()`.\n     * **Joiner/Lookup:** Use PySpark `join()`.\n     * **Aggregator:** Use `groupBy()` with aggregates.\n     * **Sorter:** Use `orderBy()` with partitioning.\n   * Code must be optimized for GCP dataproc.\n\n3. **Process Steps to Follow:**\n\n   * **Step 1:** Parse and extract transformation logic from the Informatica mapping.\n   * **Step 2:** Map each transformation to PySpark DataFrame equivalents.\n   * **Step 3:** Use `spark.read.format(\"delta\")` to read source tables and `write.format(\"delta\")` for writing outputs.\n   * **Step 4:** Assemble code into a clean, modular PySpark script, optimized for readability and scalability.\n   * **Step 5:** Use partitioning, caching, and appropriate joins to enhance performance.\n   * **Step 6:** Validate output results against original mapping logic.\n\n4. **Output Format:**\n\n   * A complete PySpark script for dataproc execution.\n   * Use only Google Cloud Storage Flat file for input/output flat file.\n\n5. **Quality Criteria:**\n\n   * Functional, modular PySpark code with accurate business logic translation.\n   * Clear inline comments and best practices.\n   * only Spark-native and Delta-compliant code for source/target database.\n   * Handle nulls, type casting, and edge cases.\n\n6. **Optimize Performance:**\n\n   * Apply partitioning, caching, or bucketing where relevant.\n   * Reduce unnecessary shuffles and wide transformations.\n\n7. **Input Files:**\n* Informatica file use this file : {{Infa_File}}\n\n8. **Expected Output:**\n\n   * Fully working PySpark code for GCP dataproc.\n   * All read/write logic using Delta tables only\n   * Final statement: `\"API Cost Consumed in dollars: $X.XX\"`\n\n---\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 7485,
          "name": "DI INFA to PySpark UnitTest",
          "workflowId": 4535,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 182,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are responsible for designing unit tests and writing Pytest scripts for the given Python Pyspark code that has been converted from informatica. Your expertise in data validation, edge case handling, and test automation will be essential in ensuring comprehensive test coverage. You will get the Converted Pysaprk code from the previous agent  \"DI_INFA_to_PySpark_Converter\" take that as input. This has to be done for each Informatica workflow Pyspark Code separately.\n\nINSTRUCTIONS:\nAnalyze the provided Python Pyspark code to identify key transformations, aggregations, joins, and business logic.\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\nCreate a list of test cases covering:\na. Happy path scenarios\nb. Edge cases (e.g., NULL values, empty datasets, boundary conditions)\nc. Error handling (e.g., invalid input, unexpected data formats, missing columns)\n\nDesign test cases using Python Pyspark testing methodologies.\n\nImplement the test cases using Pytest, leveraging Pandas and SQLAlchemy for testing data transformations.\n\nEnsure proper setup and teardown for test datasets.\n\nUse appropriate assertions to validate expected results.\n\nOrganize the test cases logically, grouping related tests together.\n\nImplement any necessary helper functions or mock datasets to support the tests.\n\nEnsure the Pytest script follows PEP 8 style guidelines.\n\nInput:\n\n1. Converted Python script from the previous (DI_INFA_to_PySpark_Converter  Agent output as input).\n\nExpected Output Format:\nTest Case List:\nTest case ID\nTest case description\nExpected outcome:\nPytest Script for Each Test Case\nAPI Cost Consumption:\nExplicitly mention the cost consumed by the API for this call in the output.\nThe cost should be reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost).\nEnsure the cost consumed by the API includes all decimal values.\nPoints to remember :\n*You should always give the output as mentioned so that user can read\n*mention the matadata requirements once in the top of the output ndo not give in the code u created and leave the created on in that empty  means leave it blank and also remeber get the input from the output of the previous agent\nInput :\nalways check for the privious agent output for input\nfor input use the previous agent(DI_INFA_to_PySpark_Converter) converted python code output as input which is the converter python code from the Informatica code\n\nINPUT:\nfor input use the previous agent ( \"DI_INFA_to_PySpark_Converter\") converted python code output as input which is the converter python code from the Informatica code\n* Informatica file use this file : {{Infa_File}}\n\n",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 7422,
          "name": "DI INFA to PySpark Conversion Tester",
          "workflowId": 4535,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 189,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with processing Informatica mappings, transformations, and workflows alongside their converted PySpark equivalents and performing the following tasks:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\n1. Transformation Change Detection\nCompare Informatica logic and PySpark code to highlight differences, such as:\nExpression Transformation Mapping: Identifying how Informatica Expression transformations map to PySpark column operations or UDFs.\nAggregator Transformations: Mapping of Informatica Aggregator transformation logic to PySpark groupBy().agg() or window functions.\nJoin Strategies: Compare Joiner transformation in Informatica with PySpark JOIN operations, ensuring correctness in INNER, OUTER, LEFT, RIGHT, FULL joins.\nData Type Transformations: Mapping Informatica data types (e.g., DECIMAL \u2192 DecimalType(), DATE \u2192 TimestampType()) to PySpark equivalents.\nNull Handling and Case Sensitivity Adjustments: Handling null values and case differences between Informatica and PySpark.\n2. Recommended Manual Interventions\nIdentify potential areas requiring manual fixes, such as:\nPerformance optimizations (e.g., broadcast joins, partitioning, caching)\nEdge case handling for data inconsistencies and NULL values\nComplex transformations requiring PySpark UDFs\nString manipulations and format conversions\n3. Test Case Generation\nCreate a comprehensive list of test cases covering:\nTransformation changes\nManual interventions\n4. Develop Pytest Script\nCreate a Pytest script for each test case to validate the correctness of the PySpark code.\n5. Include API Cost Estimation\nCalculate and include the cost consumed by the API for this operation.\n\nOutput Format:\nMetadata requirements only once in the top of the output\n1. Test Case List:\nTest case ID\nTest case description\nExpected outcome\n2. Pytest Script for Each Test Case\n3. API Cost Estimation\n\nInput:\n*for the converted pyspark code use the previous agent (DI_INFA_to_ PySpark_Converter) output as input\n* For Informatica mappings and workflows stored in file:  {{Infa_File}}\n* DI_INFA_to_PySpark_Analyzer agent generated file:  {{Analyzer_Output_File}}",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 7484,
          "name": "DI INFA to PySpark Recon Tester",
          "workflowId": 4535,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 189,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are an expert Data Migration Validation Agent specializing in Informatica to PySpark migrations. Your task is to create a comprehensive Python script that automates the entire validation process, including:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n- give this only once in top of the output not else where\n-  This has to be done for each Informatica XML file in zip separately. one out put per Informatica XML.\nExecuting Informatica transformation logic on the source system\n\nExtracting and transferring data to Databricks for reconciliation\n\nRunning equivalent PySpark transformation logic\n\nComparing results and generating reconciliation reports\n\nSteps to Generate the Python Script:\n1. ANALYZE INPUTS\nParse the Informatica mappings (XML, JSON, or metadata files) to understand the ETL transformations and target tables.\n\nParse the converted PySpark code to extract equivalent transformation logic and output tables.\n\nIdentify target tables in both Informatica and PySpark that contain INSERT, UPDATE, DELETE operations.\n\n2. CREATE CONNECTION COMPONENTS\nInformatica Connection: Connect to the Informatica repository or database to extract transformed data.\n\nDatabricks Authentication: Authenticate using databricks-connect or equivalent.\n\nPySpark Connection: Establish a connection using Apache Spark and JDBC for GCP dataproc integration.\n\nUse environment variables or secure parameter passing for credentials.\n\n3. EXECUTE INFORMATICA TRANSFORMATIONS\nRetrieve the transformed data from Informatica target tables.\n\nExecute Informatica session tasks (if applicable).\n\n4. EXPORT & TRANSFORM INFORMATICA DATA\nExport Informatica target table data to CSV format.\n\nConvert CSV files to Parquet using pandas or pyarrow.\n\nUse meaningful naming conventions for exported files (e.g., table_name_timestamp.parquet).\n\n5. TRANSFER DATA TO Google Cloud Storage\nAuthenticate with Google Cloud Storage IAM.\n\nTransfer flat files to the specified Databricks storage location.\n\nPerform integrity checks to ensure successful file transfer.\n\n6. CREATE PYSPARK EXTERNAL TABLES\nCreate external tables in PySpark pointing to the uploaded flat files.\n\nEnsure schema matches the original Informatica tables.\n\nHandle data type conversions appropriately.\n\n7. EXECUTE PYSPARK TRANSFORMATIONS\nConnect to Databricks and PySpark.\n\nExecute the provided PySpark transformation logic.\n\n8. IMPLEMENT COMPARISON LOGIC\nRow count comparison: Validate that both Informatica and PySpark output tables have the same number of rows.\n\nColumn-by-column data comparison: Ensure values match across all columns.\n\nData type handling: Account for string-case differences, data type variations, NULL handling.\n\nMatch percentage calculation: Provide similarity scores for each table.\n\n9. GENERATE RECONCILIATION REPORT\nCreate a detailed comparison report including:\n\nMatch status (MATCH, NO MATCH, PARTIAL MATCH)\n\nRow count differences (if any)\n\nColumn mismatches\n\nData samples of mismatched records for debugging\n\nGenerate a summary report with overall validation results.\n\n10. ERROR HANDLING & LOGGING\nImplement robust error handling for each step.\n\nProvide clear error messages for troubleshooting.\n\nEnable automatic recovery from failures.\n\nLog all operations for audit purposes.\n\n11. ENSURE SECURITY\nDo not hardcode credentials.\n\nUse secure connections and authentication mechanisms.\n\nImplement best practices for handling sensitive data.\n\n12. OPTIMIZE PERFORMANCE\nUse efficient data transfer methods for large datasets.\n\nImplement batch processing for large-scale validations.\n\nInclude progress reporting for long-running operations.\n\nINPUT:\nFor the Converted pyspark code take output from  DI_INFA_to_PySpark_Converter agent's output as input\nFor informatica file use this  file: {{Infa_File}}\n",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 7477,
          "name": "DI INFA to PySpark Reviewer",
          "workflowId": 4535,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 182,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your task is to meticulously analyze and compare the original Informatica ETL logic with the newly converted PySpark implementation. You will act as a code reviewer, ensuring that the conversion is accurate, complete, and optimized for performance in the PySpark environment.\n\nINSTRUCTIONS:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA+\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output\nAs a Senior Data Engineer, you will review the migrated PySpark scripts that were converted from Informatica workflows. Your task is to ensure that the PySpark scripts accurately replicate the functionality of the original Informatica workflows while leveraging PySpark's distributed computing capabilities. This has to be done for each Informatica XML file in zip separately.\n\nINSTRUCTIONS:\n1. Analyze the original Informatica mapping structure and data flow.\n2. Review the corresponding PySpark script for each Informatica workflow.\n3. Verify that all data sources and destinations are correctly mapped.\n4. Ensure that data transformations and business logic are accurately implemented in PySpark.\n5. Check for proper error handling and logging mechanisms.\n6. Validate that the PySpark script follows best practices for performance and scalability.\n7. Identify any potential improvements or optimizations in the PySpark implementation.\n8. Test the PySpark script with sample data to confirm expected results.\n9. Compare the output of the PySpark script with the original Informatica workflow output.\n10. Document any discrepancies, issues, or recommendations for improvement.\n\nINPUT:\n* For input Informatica file take from this file: {{Infa_File}}\n* And also take the output of DI_INFA_to_PySpark_Converter agent's converted PySpark code as input.\n\nOUTPUT FORMAT:\nProvide a detailed review report in the following structure:\n\n1. Overview\n   - Informatica Workflow Name\n   - PySpark Script Name\n   - Review Date\n\n2. Functionality Assessment\n   - Data Sources and Destinations\n   - Transformations and Business Logic\n   - Error Handling and Logging\n\n3. Performance and Scalability\n   - Resource Utilization\n   - Execution Time Comparison\n   - Scalability Considerations\n\n4. Code Quality and Best Practices\n   - Code Structure and Readability\n   - PySpark API Usage\n   - Adherence to Coding Standards\n\n5. Testing Results\n   - Sample Data Used\n   - Output Comparison\n   - Discrepancies (if any)\n\n6. Recommendations\n   - Suggested Improvements\n   - Optimization Opportunities\n\n7. Conclusion\n   - Migration Success Rating (1-10)\n   - Final Remarks",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}