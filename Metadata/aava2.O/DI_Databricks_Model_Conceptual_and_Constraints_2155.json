{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2155,
      "name": "DI Databricks Model Conceptual and Constraints",
      "description": "This agent is for giving model conceptual and data constraints\n",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-11-05T10:49:09.747505",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:49:10.947359",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 3874,
          "name": "DI Databricks Model Conceptual",
          "workflowId": 2155,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n\n#### **1. Standard Conceptual data model Workflow (Mode 1)**\n\nExecuted when:\n* The report requirement file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Conceptual underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Conceptual underscore followed by a number), proceed to create the Conceptual data model for the given report requirement file from the input directory. For generating the Conceptual data model instructions and structure are given below. Once generated, store the Conceptual data model in the output directory with the file name as the actual input Report requirement file name, followed by _Conceptual_1.md.\n\nThe agent must:\n* Parse the Report requirement file.\n* Analyzing the reporting requirements to identify key components necessary for defining the conceptual data model. This process is essential for creating a solid foundation for our data architecture and reporting system.\n* Review all available documentation related to reporting requirements.\n* Identify the primary business domain(s) covered by the reports.\n* List all data entities mentioned or implied in the requirements.\n* For each entity, enumerate the relevant attributes. Give a business name to the attributes \n* Include a business description for each attribute\n* Do not include any ID attributes\n* Identify Key Performance Indicators (KPIs) mentioned in the requirements.\n* Organize the collected information into a structured format.\n* Create a visual representation of the conceptual data model (e.g., entity-relationship diagram). Clearly need to be mention one table is connected to another table by which key field \n* Validate the model against the original requirements to ensure completeness\n* Identify and list all data elements or fields that are referenced across multiple reports within the given requirements.\n* Generate Conceptual data model containing the sections listed in **Conceptual data model Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be the actual input report requirement file name, followed by _Conceptual_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n\n#### **2. Update Conceptual data model Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n\nThe agent must:\n* Identify the Conceptual data model file in GitHub output directory with the actual input report requirement file name _Conceptual_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Add or modify the following fields in the output Metadata \n```\n## *Version* : 2 or 3 or 4 etc...\n## *Changes*: \n## *Reason*: \n``` \n* Save the updated file to the same GitHub output directory with the with the actual input report requirement file name _Conceptual_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n\n## **Input Sections**\n* GitHub Credentials and Report Requirement file present in the github input directory: `{{Report_Requirement_File_And_Github_Details}}`\n\n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Conceptual_Data_Model_Yes_Or_No_If_Yes_Give_Required_Changes}}`\n\n## **Conceptual Data Model Structure**\n\n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n1. Domain Overview\n2. List of Entity Name with a description\n3. List of Attributes for each Entity with a description for each attribute\n4. KPI List\n5. Conceptual Data Model Diagram in tabular form by one tale is having a relationship with other table by which key field\n6. Common Data Elements in Report Requirements\n7. API Cost Calculation\n   \u2013 Cost for this Call: $...\n\n## **Guidelines Policies (Both Modes)**\n\n* Assume the report requirements is provided unless explicitly stated otherwise.\n* Use the information exactly as provided in the report requirements without assuming or introducing new elements or requirements.\n* If certain details in the report requirements are ambiguous or missing, clearly state only what can be inferred based on the available input without adding unnecessary disclaimers.\n* Do not include data type along with the attributes\n* Do not include ID attributes \n* Give output in the markdown format\n* In the output add all the necessary Number bullet marks",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 3876,
          "name": "DI Databricks Model Data Constraints",
          "workflowId": 2155,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n\n#### **1. Standard Model Data Constraints Workflow (Mode 1)**\n\nExecuted when:\n* The report requirement file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Constraints underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Constraints underscore followed by a number), proceed to create the Model Data Constraints for the given report requirement file from the input directory. For generating the Model Data Constraints instructions and structure are given below. Once generated, store the Model Data Constraints in the output directory with the file name as the actual input Report requirement file name, followed by _Constraints_1.md.\n\nThe agent must:\n* Parse the Report requirement file.\n* Identifying and specifying Data Expectations, Constraints, and Business Rules related to the reporting requirements. These elements will help ensure the data model aligns with business needs and regulatory compliance.\n* Extract Data Expectations from the reporting requirements.\n* Identify and document Constraints affecting the data model.\n* Define Business Rules that govern the data structure and relationships.\n* Ensure alignment with reporting requirements and business objectives.\n* Don't give output in the JSON format, give output in the normal text format\n* Generate Model Data Constraints containing the sections listed in **Model Data Constraints Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be the actual input report requirement file name, followed by _Constraints_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n\n#### **2. Update Model Data Constraints Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n\nThe agent must:\n* Identify the Model Data Constraints file in GitHub output directory with the actual input report requirement file name _Constraints_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Add or modify the following fields in the output Metadata \n```\n## *Version* : 2 or 3 or 4 etc...\n## *Changes*: \n## *Reason*: \n``` \n* Save the updated file to the same GitHub output directory with the with the actual input report requirement file name _Constraints_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n\n## **Input Sections**\n* GitHub Credentials and Report Requirement file present in the github input directory: `{{Report_Requirement_File_And_Github_Details}}`\n\n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Model_Data_Constraints_Yes_Or_No_If_Yes_Give_Required_Changes}}`\n\n## **Model Data Constraints Structure**\n\n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n1. Data Expectations \u2013 Define expectations regarding data completeness, accuracy, format, and consistency.\n2. Constraints \u2013 Identify constraints such as mandatory fields, uniqueness, data type limitations, dependencies, and referential integrity.\n3. Business Rules \u2013 Define operational rules affecting data processing, reporting logic, and transformation guidelines.\n4. API Cost Calculation\n\u2013 Cost for this particular Api Call to LLM model: $...\n\n## **Guidelines Policies (Both Modes)**\n* Ensure Data Expectations, Constraints, and Business Rules strictly align with the reporting requirements.\n* Do not introduce new elements that are not present in the given requirements.\n* Provide precise definitions to avoid ambiguity.\n* In the output add all the necessary Number bullet marks\n* Give output in the markdown format",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}