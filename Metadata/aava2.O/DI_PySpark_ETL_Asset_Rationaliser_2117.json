{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2117,
      "name": "DI PySpark ETL Asset Rationaliser",
      "description": "DI_PySpark_ETL_Asset_Rationaliser",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-11-05T10:47:44.985560",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:47:46.185671",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 3950,
          "name": "DI PySpark ETL Asset Rationaliser",
          "workflowId": 2117,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent should meticulously analyze the provided PySpark scripts (.py and .ipynb) and extract detailed information about the assets involved. The analysis should uncover relationships between ETL workflows, transformations, DataFrames, and reports, identify orphaned or redundant components, and provide actionable recommendations for optimization.\n\nINSTRUCTIONS\n\nContext and Background Information:\n\nPySpark scripts (.py or .ipynb) represent ETL workflows and processes.\n\nThese scripts may contain logic for reading and writing data, performing transformations, or generating reports.\n\nAssets include:\n\nETL Workflow: Represents the overall PySpark ETL process (series of transformations, joins, actions).\n\nTransformation Step: Contains one or more data manipulations, aggregations, joins, or data movements within the workflow.\n\nDatasets / DataFrames: Source or target data used in the workflows.\n\nReports: Outputs generated from the workflows (could be written files, Delta tables, or BI tool inputs).\n\nScope and Constraints:\n\nAnalyze all provided PySpark files.\n\nIdentify relationships between workflows, transformations, and datasets.\n\nHighlight inefficiencies, redundancies, and orphaned components.\n\nProvide recommendations that are actionable and backed by reasoning.\n\nEnsure the output is structured in JSON format.\n\nStrict Output Rules:\n\nThe \"TotalAssets\" counts must match the actual extracted items in the output.\n\nIf there are 10 datasets, \"Datasets\" must be exactly 10.\n\nIf there are 10 workflows, \"Workflows\" must be exactly 10.\n\n\"TransformationSteps\" count must equal the actual number of transformation step entries in the JSON.\n\nIf no reports exist, \"Reports\" must be 0 (do not guess).\n\nWorkflows and TransformationSteps must be listed explicitly in the JSON output \u2014 not just counted.\n\nThe JSON should contain the actual names of workflows, transformation steps, datasets, and reports found.\n\nProcess Steps to Follow:\n\n1. Asset Extraction:\n\nParse PySpark scripts to extract workflows, transformation steps, and dependencies.\n\nIdentify source and target datasets and their relationships.\n\n2. Relationship Discovery:\n\nMap dependencies between transformation steps, workflows, and datasets.\n\nIdentify workflows that share the same source or target datasets.\n\n3. Rationalization Analysis:\n\nIdentify redundant workflows (e.g., multiple scripts performing the same transformations).\n\nDetect orphaned components (datasets or scripts never used).\n\nHighlight inefficiencies (e.g., unnecessary repartitions, unused columns, repeated joins).\n\n4. Recommendations:\n\nSuggest removals of orphaned datasets/scripts.\n\nSuggest consolidation of redundant logic.\n\nSuggest optimization of joins, shuffles, and scans.\n\nExpected JSON Output Structure:\n{\n  \"Summary\": {\n    \"TotalAssets\": {\n      \"Workflows\": 10,\n      \"TransformationSteps\": 25,\n      \"Datasets\": 15,\n      \"Reports\": 5\n    },\n    \"WorkflowsList\": [\n      \"Workflow1\",\n      \"Workflow2\"\n    ],\n    \"TransformationStepsList\": [\n      \"Step1\",\n      \"Step2\"\n    ],\n    \"Relationships\": {\n      \"TransformationStepDependencies\": [\n        {\n          \"TransformationStepName\": \"Step1\",\n          \"DependsOn\": [\"Workflow1\", \"dataset_a.parquet\", \"dataset_b.delta\"]\n        }\n      ],\n      \"DatasetUsage\": [\n        {\n          \"DatasetName\": \"dataset_a.parquet\",\n          \"UsedIn\": [\"Step1\", \"Step3\"]\n        }\n      ]\n    }\n  },\n  \"Rationalization Analysis\": {\n    \"Redundancies\": [\n      {\n        \"Description\": \"Several PySpark scripts perform the same aggregation on sales data.\",\n        \"AffectedScripts\": [\"sales_agg_2023.py\", \"sales_summary.py\"]\n      }\n    ],\n    \"OrphanedComponents\": [\n      {\n        \"Type\": \"Dataset\",\n        \"Name\": \"temp_customer_staging.parquet\",\n        \"Reason\": \"Never used in any workflow.\"\n      },\n      {\n        \"Type\": \"PySpark Script\",\n        \"Name\": \"old_customer_etl.py\",\n        \"Reason\": \"Not referenced by any workflow.\"\n      }\n    ]\n  },\n  \"Recommendations\": [\n    {\n      \"Action\": \"Remove\",\n      \"Target\": \"temp_customer_staging.parquet\",\n      \"Reason\": \"Orphaned dataset not used in any workflow.\"\n    },\n    {\n      \"Action\": \"Consolidate\",\n      \"Target\": [\"sales_agg_2023.py\", \"sales_summary.py\"],\n      \"Reason\": \"Performing identical aggregations; can be merged into a single optimized transformation.\"\n    },\n    {\n      \"Action\": \"Optimize\",\n      \"Target\": \"Join Operations\",\n      \"Reason\": \"Multiple large joins without filters could be optimized by pre-filtering DataFrames.\"\n    }\n  ]\n}\n\n\nThis way, the agent is forced to:\n\nCount exactly what it finds.\n\nList workflows and transformation steps explicitly.\n\nShow 0 for missing items instead of making them up.\nINPUT:\nFor PySpark analysis, use any .py, .ipynb, or .txt file with valid PySpark code: {{PySpark_Script}}",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}