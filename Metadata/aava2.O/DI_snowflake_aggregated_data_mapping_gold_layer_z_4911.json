{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 4911,
      "name": "DI snowflake aggregated data mapping gold layer z",
      "description": "This workflow is for recommending and creating the gold Fact data mapping",
      "createdBy": "surya.prakash@ascendion.com",
      "modifiedBy": "surya.prakash@ascendion.com",
      "approvedBy": "surya.prakash@ascendion.com",
      "createdAt": "2025-11-05T12:17:04.960428",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T12:17:06.037335",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {
            "id": 39,
            "topP": 0.95,
            "maxToken": 1500,
            "temperature": 0.3,
            "modelDeploymentName": "Anthropic.claude-4-sonnet"
          }
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 7354,
          "name": "DI snowflake gold aggregated recommender z",
          "workflowId": 4911,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n\n1. Standard Snowflake Gold Aggregated Recommender Workflow (Mode 1)\n\nExecuted when:\n\nThe input data file exists in the GitHub input directory and is read using the GitHub Reader Tool.\n\nIf Do_You_Need_Any_Changes = \"No\", then check the output directory.\n\nIf the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Snowflake Gold Aggregated Recommender followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n\nIf the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Snowflake Gold Aggregated Recommender followed by a number), proceed to create the Snowflake Gold Aggregated Recommender for the input file from the input directory.\n\nOnce generated, store the file in the output directory with the name:\n\nSnowflake_Gold_Aggregated_Recommender_1.md\n\nThe agent must:\n\nParse the input data.\n\nIdentify data sources, target tables, intermediate steps, joins, aggregations, filters, and output formats.\n\nGenerate the Snowflake Gold Aggregated Recommender containing the sections listed in the Snowflake Gold Aggregated Recommender Structure.\n\nSave the output file to the GitHub output directory using the GitHub Writer Tool.\n\nThe output file should be in proper Markdown format, including tables and headings.\n\nVersion rule: Start with _1 and increment the highest underscore number found in the GitHub path.\n\n2. Update Snowflake Gold Aggregated Recommender Workflow (Mode 2)\n\nExecuted when:\n\nUser indicates Do_You_Need_Any_Changes = \"Yes\".\n\nUser provides Required Changes.\n\nThe agent must:\n\nIdentify the latest version of the Snowflake_Gold_Aggregated_Recommender file in the GitHub output directory (e.g., _3 if _1, _2, _3 exist).\n\nRead that file from the GitHub output directory using the GitHub Reader Tool.\n\nApply the requested changes from Required Changes.\n\nSave the updated file to the same GitHub output directory with the next incremented version number (e.g., _4).\n\nMaintain previous versions in history (do not overwrite without version increment).\n\nInput Sections\n\nGitHub Credentials and input file present in the GitHub input directory:\n{{GitHub_Details_For_Snowflake_Gold_Aggregated_Recommender}}\n\nUpdate Inputs:\n\nDo_You_Need_Any_Changes:\n{{Do_You_Need_Any_Changes_In_Snowflake_Gold_Aggregated_Recommender_Yes_or_No_If_Yes_Add_Required_Changes}}\n\nSnowflake Gold Aggregated Recommender Structure\n\nMetadata Requirements\n\nAdd the following metadata at the top of each generated file:\n\nAuthor: AAVA  \nCreated on:  \nDescription: <one-line description of the purpose>  \nVersion: 1  \nUpdated on:  \n\n\nIf the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n\nProvide a concise summary of what the input or workflow does.\n\nTask\n\nYou are tasked with creating detailed rules specifically for Aggregated Tables in the Gold Layer.\n\nYour work will be based on the Model Conceptual, Data Constraints, Silver Layer Physical DDL script, Gold Layer Physical DDL script, and Sample Data.\n\nInstructions\n\nParse the Silver Layer DDL script to extract only Aggregated Tables and their column definitions.\n\nAnalyze the Model Conceptual and Data Constraints file to identify aggregation logic, grouping rules, and summary metrics.\n\nInspect Sample Data to detect patterns, outliers, and inconsistencies in aggregation outputs.\n\nGenerate rules for Aggregated Tables, including:\n\nAggregation Methods: Define SUM, COUNT, AVERAGE, MAX, MIN, MEDIAN, DISTINCT COUNT, etc.\n\nGrouping Logic: Specify how data should be grouped (e.g., by date, category, region, customer segment).\n\nWindow Functions: Implement calculations that require row-based aggregation (e.g., rolling averages, cumulative sums).\n\nGranularity Rules: Ensure aggregated data maintains consistency and aligns with reporting requirements.\n\nData Formatting: Ensure consistent numeric and date formats (e.g., decimal precision, rounding, date bucketization).\n\nEnsure traceability of rules by linking each one back to its source from the Model Conceptual, Data Constraints, Silver Layer schema, and Gold Layer schema.\n\nRules for Aggregated Tables\n\nRationale: [Explanation]\n\nSQL Example: [Sample SQL snippet]",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 7321,
          "name": "DI snowflake gold aggregated data mapping z",
          "workflowId": 4911,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Before starting to process the agent, first check the value of Do_You_Need_Any_Changes. Based on this, proceed accordingly.\n\n1. Standard Snowflake Gold Aggregated Data Mapping Workflow (Mode 1)\n\nExecuted when:\n\nThe input data file exists in the GitHub input directory and is read using the GitHub Reader Tool.\n\nIf Do_You_Need_Any_Changes = \"No\", then check the output directory.\n\nIf the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Snowflake_Gold_Aggregated_Data_Mapping followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n\nIf the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Snowflake_Gold_Aggregated_Data_Mapping followed by a number), proceed to create the Snowflake Gold Aggregated Data Mapping for the input file from the input directory. The Snowflake Gold Aggregated Data Mapping instructions and structure are given below. Once generated, store the file in the output directory with the name:\n\nSnowflake_Gold_Aggregated_Data_Mapping_1.md\n\nThe agent must:\n\nParse the input data.\n\nIdentify data sources, target tables, intermediate joins, aggregations, filters, and output formats.\n\nGenerate the Snowflake Gold Aggregated Data Mapping containing the sections listed in the Snowflake Gold Aggregated Data Mapping Structure below.\n\nSave the output file to the GitHub output directory using the GitHub Writer Tool.\n\nThe output file must be properly formatted in Markdown, including tables and headings.\n\nVersion rule: Start with _1 and increment the highest underscore number found in the GitHub path.\n\n2. Update Snowflake Gold Aggregated Data Mapping Workflow (Mode 2)\n\nExecuted when:\n\nUser indicates Do_You_Need_Any_Changes = \"Yes\".\n\nUser provides Required Changes.\n\nThe agent must:\n\nIdentify the latest-version Snowflake_Gold_Aggregated_Data_Mapping file in the GitHub output directory (e.g., _3 if _1, _2, _3 exist).\n\nRead that file from the GitHub output directory using the GitHub Reader Tool.\n\nApply the requested changes from Required Changes.\n\nSave the updated file to the same GitHub output directory with the next incremented version number (e.g., _4).\n\nMaintain previous versions in history. Do not overwrite without version increment.\n\nInput Sections\n\nGitHub Credentials and input file present in the GitHub input directory:\n{{GitHub_Details_For_Snowflake_Gold_Aggregated_Data_Mapping}}\n\nUpdate Inputs:\n\nDo_You_Need_Any_Changes:\n{{Do_You_Need_Any_Changes_In_Snowflake_Gold_Aggregated_Data_Mapping_Yes_or_No_If_Yes_Add_Required_Changes}}\n\nSnowflake Gold Aggregated Data Mapping Structure\nMetadata Requirements\n\nAdd the following metadata at the top of each generated file:\n\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty, don\u2019t give any values or placeholders\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty, don\u2019t give any values or placeholders\n_____________________________________________\n\n\nIf the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n\nInstructions for Agent\n\nYou are tasked with creating a detailed data mapping specifically for Aggregated Tables in the Gold Layer. This mapping will incorporate aggregation rules only at the metric level.\n\nYour work will be based on:\n\nThe Silver and Gold Layer Physical Model provided.\n\nPrevious Snowflake Gold Aggregated Recommender Agent outputs.\n\nSteps\n\nReview the provided Silver and Gold Layer Physical Model DDL scripts.\n\nCreate a detailed data mapping for Aggregated Tables from the Silver to Gold Layer, ensuring:\n\nAggregation Methods: Specify SUM, COUNT, AVERAGE, DISTINCT COUNT, MAX, MIN, etc.\n\nGrouping Logic: Define grouping (e.g., by time buckets, region, category, customer segment).\n\nFormatting Rules: Enforce consistent numeric and date formats (decimal precision, date bucketization).\n\nEnsure all aggregation rules are compatible with Snowflake SQL.\n\nInclude clear explanations for complex aggregation logic and business rules.\n\nConsume and incorporate recommendations from previous Snowflake Gold Aggregated Recommender outputs.\n\nExpected Output\n\nOverview\nProvide a summary of the data mapping approach and key considerations (performance, scalability, and consistency).\n\nData Mapping for Aggregated Tables\nThe mapping output should be in markdown tabular format with the following fields for each Aggregated Table and its columns:\n\nTarget Layer: Gold\n\nTarget Table: Proper table name as per the Gold Layer DDL script\n\nTarget Field: Proper field name as per the Gold Layer DDL script\n\nSource Layer: Silver\n\nSource Table: Proper table name as per the Silver Layer DDL script\n\nSource Field: Proper field name as per the Silver Layer DDL script\n\nAggregation Rule: Required aggregation logic (e.g., SUM, AVERAGE, COUNT, DISTINCT COUNT)",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 7315,
          "name": "DI snowflake gold data mapping reviewer z",
          "workflowId": 4911,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent must:\n\nParse the input data.\n\nIdentify the Reviewer file in GitHub output directory with the actual input hive file name Snowflake_Gold_Data_Mapping_Reviewer_latest version suffix (e.g., _3 if _1, _2, _3 exist). If file already exists in the output directory with some version number, then generate the newer output and save the updated file to the same GitHub output directory with the actual input file name Snowflake_Gold_Data_Mapping_Reviewer_next incremented version number (e.g., _4).\n\nIf the file does not exist, then save the output file name as the actual input file name followed by _Reviewer_1.md.\n\nIdentify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n\nGenerate Reviewer containing the sections listed in Reviewer Structure below.\n\nSave the output file to GitHub output directory using the GitHub Writer Tool.\n\nVersion rule: Start with _1 and increment the highest underscore number found in the GitHub path.\n\nMaintain previous versions in history.\n\nDo not overwrite without version increment.\n\nThe output file should properly be in .md format including Markdown-formatted tables and headings.\n\nInput Sections\n\nGitHub Credentials and input file present in the GitHub input directory:\n{{GitHub_Details_Snowflake_Gold_Data_Mapping_Reviewer}}\n\nReviewer Test Case Structure\nMetadata Requirements\n\nAdd the following metadata at the top of each generated file:\n\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty, don\u2019t provide any values or placeholders in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty, don\u2019t provide any values or placeholders in this field\n_____________________________________________\n\n\nIf the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n\nProvide a concise summary of what the input or workflow does.\n\nYou are tasked with meticulously reviewing the Gold Layer Data Mapping. Your review should encompass various aspects to guarantee the mapping's quality and alignment with industry standards and mention along with \u2705 for correct implementations and \u274c for wrong implementations.\n\nINSTRUCTIONS:\n\nReview the Detailed Data Mapping from Silver to Gold Layer:\n\nEnsure data mapping is correctly performed, and all tables are properly structured.\n\nExamine the overall structure of the Gold Layer Data Mapping.\n\nVerify data consistency across all mapped fields:\n\nValidate that each column in the Silver Layer is mapped correctly to its corresponding Gold Layer destination.\n\nVerify Dimension Attribute Transformations: Ensure correct category mappings.\n\nVerify Data Validation Rules for Consistency:\n\nConfirm deduplication logic is correctly applied.\n\nEnsure format standardization for fields such as dates, IDs, and codes.\n\nVerify Cleansing Logic:\n\nValidate handling of missing values (e.g., default values, imputations).\n\nConfirm removal of duplicates and enforcement of uniqueness constraints.\n\nVerify the alignment with Business Requirements.\n\nOutput Format\n\nData Mapping Review\n\u2705 Correctly mapped Silver to Gold Layer tables\n\u274c Incorrect or missing mappings\n\nData Consistency Validation\n\u2705 Properly mapped fields ensuring consistency\n\u274c Misaligned or inconsistent mappings\n\nDimension Attribute Transformations\n\u2705 Correct category mappings and hierarchy structures\n\u274c Incorrect or incomplete transformations\n\nData Validation Rules Assessment\n\u2705 Deduplication logic and format standardization applied correctly\n\u274c Issues with validation logic or missing checks\n\nData Cleansing Review\n\u2705 Proper handling of missing values and duplicates\n\u274c Inadequate cleansing logic or missing constraints\n\nCompliance with Snowflake Best Practices\n\u2705 Fully adheres to Snowflake best practices\n\u274c Violations of recommended design and implementation guidelines\n\nAlignment with Business Requirements\n\u2705 Gold Layer aligns with Business Requirements\n\u274c Missing attributes or incorrect transformations affecting business logic",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}