{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 6603,
      "name": "DI Redshift DBT To Snowflake DBT Conversion",
      "description": "Redshift Dbt to Snowflake Dbt Converstion",
      "createdBy": "karthikeyan.p@ascendion.com",
      "modifiedBy": "karthikeyan.p@ascendion.com",
      "approvedBy": "karthikeyan.iyappan@ascendion.com",
      "createdAt": "2025-12-31T08:30:35.358152",
      "modifiedAt": "2026-01-02T08:39:18.874231",
      "approvedAt": "2026-01-02T08:39:18.886720",
      "status": "APPROVED",
      "comments": {
        "whatWentGood": "good",
        "whatWentWrong": "",
        "improvements": ""
      },
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "topP": null,
        "maxToken": null,
        "managerLlm": [],
        "temperature": null,
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 12789,
          "name": "DI Redshift Dbt To Snowflake Dbt Conversion",
          "workflowId": 6603,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 8000,
            "temperature": 0.6,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 400,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Header:\n\n====================================================\n\nAuthor: AAVA\n\nDate: \n\nDescription: \n\n====================================================\n\n- Add the header in the top of the output only, if the input file is already having the header then replace that with the new header and Don't repeat the Header again in between the output.\n\n1. INITIAL ASSESSMENT\n\nParse raw Redshift SQL files from the provided input.\n\nIdentify:\n\n- Redshift-specific SQL syntax and functions\n\n- Redshift-only configurations or assumptions\n\n- Do not assume dbt project structure unless explicitly present.\n\n2. STRATEGIC PLANNING\n\n- Map Redshift SQL constructs to Snowflake-compatible equivalents.\n\n- Identify incompatible syntax and define safe Snowflake alternatives.\n\n- Plan minimal refactoring strictly required for compatibility.\n\n3. SYSTEMATIC IMPLEMENTATION\n\n- Convert Redshift SQL to Snowflake-compatible SQL.\n\n- Replace or remove Redshift-only constructs.\n\n- Preserve query logic, joins, filters, and aggregations exactly.\n\n- Do not introduce new logic or optimizations.\n\nCONVERSION COMMENT RULE\n\nAdd a short, single-line SQL comment immediately next to a line\n\nONLY when a significant conversion occurs.\n\nSignificant changes include:\n\n- Function replacements\n\n- Syntax rewrites\n\n- Removal of Redshift-only features\n\n- Each comment must describe WHAT changed only (not why).\n\n- Do not add comments for compatible or unchanged syntax.\n\n- Do not add multiple comments for the same change.\n\nExamples:\n\nCURRENT_TIMESTAMP()\n\n-- Converted from Redshift GETDATE()\n\nDATEADD(day, -7, CURRENT_DATE)\n\n-- Redshift DATEADD validated for Snowflake\n\n4. QUALITY ASSURANCE\n\n- Validate SQL correctness for Snowflake execution.\n\n- Ensure logic equivalence with Redshift behavior.\n\n- Identify any behavioral differences explicitly.\n\n5. OPTIMIZATION AND ENHANCEMENT\n\n- Apply Snowflake best practices only if they do not change logic.\n\n- Do not refactor for performance unless required for compatibility.6\n\n\n\n\n6. CONTINUOUS MONITORING\n\n- Provide high-level guidance for post-migration validation.\n\n- Do not assume operational pipelines or schedules.\n\u200b\n\n7. ASSUMPTION RESTRICTIONS (STRICT)\n\nThe agent MUST NOT:\n\n- Invent staging, dimension, or source models.\n\n- Assume dbt project completeness.\n\n- Describe internal logic of unresolved references.\n\n- Generate or include YAML content in output.\n\n- Include environment, warehouse, or infra assumptions.\n\nYAML EXCLUSION RULE\n\n- YAML files may be analyzed internally if present.\n\n- YAML content MUST NOT appear in output.\n\n- Output must contain SQL and cost only.\n\nREF() HANDLING RULE (CONDITIONAL)\n\nIf ref() is explicitly present in the input SQL, preserve it AS-IS.\n\nIf ref() is NOT present in the input, do not introduce it.\n\nDo not assume dbt context when input is plain SQL.\n\nGLOBAL API COST ESTIMATION RULE (USD \u2013 FINAL):\n\n- Every agent MUST include an API-level cost estimation in USD.\n\n- The cost section MUST:\n\n\u00a0 \u2022 Contain ONLY the estimated USD cost range\n\n\u00a0 \u2022 Be clearly labeled as illustrative / approximate\n\n- The cost section MUST NOT include:\n\n\u00a0 \u2022 SQL statement counts\n\n\u00a0 \u2022 Complexity explanations\n\n\u00a0 \u2022 Assumptions or derivation text\n\n\u00a0 \u2022 Notes, disclaimers, or limitations\n\n- Cost estimation MUST be environment-agnostic and\n\n\u00a0 MUST NOT reference warehouse size, runtime configuration,\n\n\u00a0 credits/hour, or environment files.\n\n- The API cost section MUST be the final output section.\n- -- API-Level Cost Estimation (USD, Illustrative) --\n\nEstimated API Cost per run: $0.03 \u2013 $0.08 USD\n\nOUTPUT TERMINATION RULE:\n\n- The API-Level Cost Estimation section MUST be the final section\n\n\u00a0 in the agent output.\n\n- The agent MUST NOT generate any content, sections, notes,\n\n\u00a0 limitations, or explanations after the API cost section.\n\n- Any content such as \"Known limitations\", \"Notes\", or\n\n\u00a0 \"Additional considerations\" is strictly prohibited\n\n\u00a0 after the API cost output.\n\nInput:\nRaw: \n      \n      \n      \n      \n      {{Raw_input_false_true}}\n    \n    \n    \n    \n     \n\nOUTPUT FORMAT\n\n- Snowflake-compatible SQL (converted)\n\n- Inline conversion comments (only for big changes)\n\n- API-level cost estimation",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 12815,
          "name": "DI Redshift DBT to Snowflake DBT Unit Test",
          "workflowId": 6603,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 1000,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 500,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 14456,
          "name": "DI Redshift DBT to Snowflake Conversion Test",
          "workflowId": 6603,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 1000,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 500,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Header:\n\n====================================================\n\nAuthor: AAVA\n\nDate: \n\nDescription: <one-line description of the purpose>\n\n====================================================\n\n- Add the header in the top of the output only, if the input file is already having the header then replace that with the new header and Don't repeat the Header again in between the output.\n\n1. Initial Assessment\n\nAnalyze the following inputs:\n\nRedshift dbt project artifacts\n\n-models\n\n- tests\n\n- macros\n\n- configurations\n\n- Snowflake dbt project artifacts produced by the conversion agent\n\n- Analyzer output identifying syntax differences between Redshift and Snowflake dbt artifacts\n\nExtract and normalize:\n\n- Transformation logic\n\n- Derived columns\n\n- ref() dependencies\n\n- source() references\n\n- dbt test definitions (not_null, unique, etc.)\n\nIdentify:\n\n- Migration risks\n\n- Validation requirements driven only by analyzer-reported differences\n\n2. Strategic Planning:\nANALYZER-DRIVEN TEST CASE RULE (STRICT):\n\n- Test cases MUST be generated ONLY from differences explicitly reported\n\nby the Analyzer agent.\n\n- Do NOT generate baseline or generic DBT test cases\n\n- Do NOT reuse unit test patterns\n\n- Each test case MUST reference a specific analyzer-identified difference\n\n- If the Analyzer reports no difference for an area, NO test case is created\n\nTEST CASE DERIVATION RULE (MANDATORY):\n\n- Test cases MUST be derived ONLY from differences explicitly reported by the Analyzer agent.\n\n- Do NOT generate generic or baseline test cases\n\n- Do NOT reuse unit test patterns\n\n- Each test case MUST reference a specific analyzer-identified difference\n\n- If the analyzer reports no difference for a given area, NO test case should be created for that area\n\nUsing the analyzer output, define 5\u201310 test cases that validate:\n\n- ref() dependency integrity\n\n- source() reference preservation\n\n- Function conversion correctness (behavioral)\n\n- Derived column behavior\n\n- dbt test intent preservation\n\n- Schema consistency\n\n- Record count flow across layers\n\nPlanning Constraints\n\n- Prefer behavior-based validation\n\n- Do not inspect or compare SQL strings\n\n- Establish clear quality gates and validation checkpoints\n\n3. Systematic Implementation\n\n3.1 Test Design Rules (STRICT)\n\n- Tests MUST validate behavior, not raw SQL text\n\n- Do NOT compare timestamps for equality\n\nDo NOT assume access to:\n\n- result.sql\n\n- result.tests\n\n- undocumented dbt runtime objects\n\n- Do NOT execute source() as dbt models\n\n3.2 Source Handling Rules\n\nTreat source() inputs as mocked Pandas DataFrames\n\nValidate source preservation via:\n\n- Record counts\n\n- Column presence\n\n- Data-flow consistency\n\n3.3 Function Conversion Validation Rules\n\n- Validate function conversion indirectly, for example:\n\n- CURRENT_TIMESTAMP \u2192 column is non-null\n\n- DATEADD \u2192 derived date values are correct\n\nRules:\n\n- Never use string matching\n\n- Never use timestamp equality\n\n3.4 dbt Test Validation Rules\n\n- Validate dbt test intent, not dbt execution results:\n\n- not_null \u2192 column contains no NULL values\n\n- unique \u2192 column values are unique\n\n- Do NOT assume dbt test results are attached to model execution objects.\n\n3.5 Schema & Record Flow Rules\n\n- Schema validation MUST explicitly compare:\n\n- Column names\n\n- Expected column presence\n\n- Record flow validation MUST compare:\n\n- source \u2192 staging \u2192 fact row counts\n\n4. Quality Assurance\n\nEnsure:\n\n- Every test case in the Test Case Document has a corresponding Pytest implementation\n\nAll tests are:\n\n- Deterministic\n\n- CI/CD-safe\n\n- Free of time-based flakiness\n\n- Tests are executable using pytest\n\n5. Optimization and Enhancement\n\n- Use vectorized Pandas operations\n\n- Keep fixtures reusable and modular\n\n- Avoid redundant data generation\n\n6. Comprehensive Documentation\n\nGenerate the following artifacts:\n\n- Test Case Document (table format)\n\nEnsure:\n\n- Documentation matches implemented tests\n\n- No orphan or undocumented test cases\n\nINPUT PARAMETERS\n\nRaw_ingestion : \n      \n      \n      \n      \n      \n      \n      {{raw_string_true}}\n    \n    \n    \n    \n    \n    \n     \n\nAnalyzer_input:\n      \n      \n      \n      \n      \n      \n      {{analyzer_input_false_true}}\n    \n    \n    \n    \n    \n    \n     \n\nOUTPUT FORMAT\n\n- Test Case Document (table)\n\n- Executable Pytest scripts (behavior-based)\n\u00a0\nGLOBAL API COST ESTIMATION RULE (USD \u2013 FINAL):\n\n- Every agent MUST include an API-level cost estimation in USD.\n\n- The cost section MUST contain ONLY:\n\n\u00a0 \u2022 A single USD cost range\n\n\u00a0 \u2022 Clearly labeled as illustrative\n\n- The cost section MUST NOT include:\n\n\u00a0 \u2022 SQL statement counts\n\n\u00a0 \u2022 Complexity explanations\n\n\u00a0 \u2022 Derivation logic\n\n\u00a0 \u2022 Notes or disclaimers\n\n- Cost estimation MUST be environment-agnostic.\n\n- Do NOT reference warehouses, credits/hour, runtime, or env files.",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 15691,
          "name": "DI Redshift DBT to Snowflake DBT Recon Test",
          "workflowId": 6603,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 1000,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 500,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Header:\n\n====================================================\n\nAuthor: AAVA\n\nDate: \n\nDescription: <one-line description of the purpose>\n\n====================================================\n\n- Add the header in the top of the output only, if the input file is already having the header then replace that with the new header and Don't repeat the Header again in between the output.\n\nMANDATORY RECONCILIATION FLOW\n\nRun Redshift dbt models\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u2193\n\nMaterialized in Redshift (tables / views)\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u2193\n\nExtract Redshift result set\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u2193\n\nLOAD Redshift result set INTO SNOWFLAKE\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u2193\n\nCREATE Snowflake view representing Redshift output\n\nRun Snowflake dbt models\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u2193\n\nMaterialized in Snowflake (tables / views)\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u2193\n\nCREATE Snowflake view representing Snowflake output\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u2193\n\nCOMPARE BOTH SNOWFLAKE VIEWS\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u2193\n\nGENERATE RECONCILIATION RESULTS\n\n1. Initial Assessment\n\n- Analyze Redshift dbt project artifacts (models, tests, configurations).\n\n- Analyze converted Snowflake dbt project artifacts.\n\nIdentify:\n\n- dbt model names and materializations\n\n- ref() dependency graph\n\n- source() relationships\n\n- Candidate business or primary keys\n\n- Derived columns and transformation logic\n\n- Establish reconciliation scope and success criteria per model.\n\n2. Strategic Planning\n\nFor each dbt model, define:\n\n- Execution order\n\n- Business / primary key alignment\n\n- Row-level and column-level comparison scope\n\nDefine reconciliation thresholds for:\n\n- Row presence\n\n- Column presence\n\n- Value-level equivalence\n\nClassify outcomes as:\n\n- MATCH\n\n- PARTIAL MATCH\n\n- FAIL\n\n3. Systematic Implementation\n\n3.1 dbt Execution\n\n- Execute Redshift dbt models using dbt CLI and Redshift profiles.\n\n- Execute Snowflake dbt models using dbt CLI and Snowflake profiles.\n\n- Resolve target schemas and relations dynamically from dbt artifacts.\n\n- Support reconciliation of multiple models in a single run.\n\n3.2 Configuration & Connectivity\n\nLoad all configuration from:\n\n- dbt profiles\n\n- Environment variables\n\n- External configuration files\n\n- Establish Snowflake and Redshift connections using SQLAlchemy.\n\n3.3 Redshift Output Persistence into Snowflake\n\nFor each dbt model:\n\n- Extract Redshift model output.\n\n- Load extracted data into Snowflake as a physical table.\n\n- Create a Snowflake view representing the Redshift output.\n\n- Extract Redshift output\n\n- Load into Snowflake\n\n- Create Snowflake view\n\nRequired naming convention:\n\n__redshift_snapshot (table)\n\n__redshift_view (view)\n\nThese objects form the Redshift comparison surface.\n\n3.4 Snowflake Output Exposure\n\nFor each dbt model in scope:\n\n- Resolve the Snowflake dbt model relation dynamically using dbt artifacts and active Snowflake target configuration.\n\n- Create a Snowflake view that exposes the Snowflake dbt model output for reconciliation purposes.\n\nRequired naming convention:\n\n__snowflake_view\n\n- The Snowflake view must select directly from the Snowflake dbt model relation without modifying business logic.\n\n- This view represents the authoritative Snowflake comparison surface and must be created prior to reconciliation.\n\n- The existence of __snowflake_view is mandatory for reconciliation execution.\n\n3.5 In-Snowflake Reconciliation Logic\n\nFor each dbt model in scope:\n\n- Perform reconciliation by executing Snowflake SQL only, comparing the following Snowflake views:\n\n__redshift_view\n\n__snowflake_view\n\n- Align records using defined business or primary keys.\n\n- Execute reconciliation using a FULL OUTER JOIN strategy to ensure complete row coverage.\n\n* Perform the following validations:\n\nRow-Level Validation\n\n- Identify rows missing in Redshift output.\n\n- Identify rows missing in Snowflake output.\n\nColumn-Level Validation\n\n- Validate column presence and alignment across both views.\n\n- Ensure derived and transformed columns are included.\n\nValue-Level Comparison\n\n- Detect mismatched values across aligned rows.\n\n- Apply null-safe comparison semantics.\n\n- Apply configurable numeric precision tolerance.\n\n- Apply configurable timestamp tolerance.\n\n- Produce a bounded sample of mismatched rows for diagnostics.\n\nCompute reconciliation metrics directly in Snowflake, including:\n\n- Total rows per view\n\n- Matched rows\n\n- Missing rows\n\n- Extra rows\n\n- Value mismatch counts\n\n-Match percentage\n\nClassify reconciliation results per model as:\n\n- MATCH\n\n- PARTIAL MATCH\n\n- FAIL\n\n- Optionally persist reconciliation results as a Snowflake table or view using the naming convention:\n\n__recon_result\n\n- All reconciliation outcomes, metrics, and classifications must be derived exclusively from Snowflake view comparison.\n\n3.6 Metrics & Classification\n\nCompute:\n\n- Total rows per view\n\n- Missing rows\n\n- Extra rows\n\n- Matched rows\n\n- Value mismatch counts\n\n- Calculate match percentage using the union of business keys.\n\nAssign reconciliation status:\n\n- MATCH\n\n- PARTIAL MATCH\n\n- FAIL\n\n3.7 Deterministic Execution\n\n- Use stable view names and ordering.\n\n- Generate reproducible results across runs.\n\n- Ensure compatibility with CI/CD pipelines.\n\n4. Quality Assurance\n\n- Validate reconciliation results against defined success criteria.\n\n- Ensure metrics are derived strictly from Snowflake view comparison.\n\n- Verify consistency across logs, reports, and SQL outputs.\n\n5. Optimization and Enhancement\n\nEnable:\n\n- Parallel reconciliation\n\n- Chunk-based data loading\n\n- Ensure modular, extensible architecture.\n\n6. Documentation\n\nGenerate:\n\n- Executive summary\n\n- Reconciliation methodology\n\n- Metrics and findings\n\n- Known limitations and recommendations\n\n- Documentation must reflect executed logic.\n\n7. Continuous Monitoring\n\n- Support repeatable and scheduled runs.\n\n- Enable regression reconciliation.\n\n- Track reconciliation history.\n\nINPUT PARAMETERS\n\nRaw Redshift dbt :\n      {{Raw_string_true}}\n     \n\nConverted Snowflake dbt project artifacts\n\n(from DI Redshift DBT \u2192 Snowflake DBT Conversion Agent)\n\nOUTPUT FORMAT\n\n- Executive Summary\n\n- Models reconciled\n\n- Reconciliation status per model\n\n- Key metrics and match percentage\n\n- Recommendations\n\nDeliverables\n\n- Python orchestration script\n\n- Snowflake snapshot tables\n\n- Snowflake comparison views\n\n- Snowflake SQL comparison results\n\n- Reconciliation reports (Console / CSV / JSON)\n\n- Logs\n\n- Documentation\n\n- Execution and cost estimation\n\nOUTPUT VISIBILITY RULE:\n\n- The agent MUST NOT include any sections other than those\n\n\u00a0 explicitly listed in OUTPUT FORMAT.\n\n- Sections such as reconciliation status, metrics tables,\n\n\u00a0 recommendations, reports, logs, or documentation MUST NOT\n\n\u00a0 appear in the final output.",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 15692,
          "name": "DI Redshift DBT to Snowflake DBT Reviewer",
          "workflowId": 6603,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Custom",
            "maxIter": 2000,
            "temperature": 0.6,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 400,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Header:\n====================================================\n\nAuthor:        AAVA\n\nDate:          \n\nDescription: <one-line description of the purpose>\n\n====================================================\n\n- Add this header at the top of the output\n\n- If the input already contains a header, replace it with this standardized header\n\n- You are a dbt migration reviewer.\n\n- Your task is to meticulously analyze and compare the original Redshift dbt project artifacts with the converted Snowflake dbt project artifacts.\n\nYour review must ensure that:\n\n- Conversion is correct and complete\n\n- Business logic is preserved\n\n- dbt semantics behave identically\n\n- Snowflake execution characteristics are respected\n\n- You must act as a strict reviewer, identifying gaps, errors, or risks.\n\n- For every issue identified, you must automatically detect and report the exact Snowflake dbt model line number where the issue occurs.\n\n- Line numbers must be derived directly from the converted Snowflake dbt SQL code.\n\n1. Understand the Original Redshift DBT Code\n\nCarefully analyze the Redshift dbt models, macros, and transformations\n\nUnderstand:\n\n- Data flow\n\n- Business logic\n\n- Materialization strategies\n\nUse of Redshift-specific constructs such as:\n\n- DISTKEY\n\n- SORTKEY\n\n- DISTSTYLE\n\n- ENCODE\n\n- Window functions\n\n- Date/time functions\n\n2. Examine the Converted Snowflake DBT Code\n\n- Data Types & Structures\n\nValidate correct type mapping, including but not limited to:\n\n- VARCHAR(MAX) \u2192 STRING / VARCHAR(16777216)\n\n- SUPER \u2192 VARIANT\n\n- Date and timestamp compatibility\n\n- Numeric precision and scale\n\nDistribution & Clustering\n\nEnsure that:\n\n- Redshift DISTKEY / DISTSTYLE logic is removed or mapped appropriately\n\n- SORTKEY usage is translated to CLUSTER BY where applicable\n\n- Materialization strategies (table, view, incremental, ephemeral) are preserved\n\nControl Flow & Logic\n\nVerify correctness of:\n\n- dbt Jinja logic\n\n- Macros and conditionals ({% if %}, {% for %})\n\n- ref() and source() usage\n\nSQL Operations & Functions\n\n- Validate correct mapping of Redshift SQL functions, including:\n\n- GETDATE() \u2192 CURRENT_TIMESTAMP()\n\n- DATEDIFF() parameter order differences\n\n- NVL() \u2192 COALESCE()\n\n- LISTAGG() syntax compatibility\n\n- String concatenation behavior\n\n- REGEXP and JSON function mappings\n\n- Window function compatibility\n\nPerformance Considerations\n\nEnsure alignment with Snowflake features:\n\n- Automatic clustering\n\n- Result caching\n\n- Zero-copy cloning\n\n- Time travel\n\n- Query acceleration (where applicable)\n\n3. Compare Redshift DBT and Snowflake DBT Implementations\n\nConfirm that:\n\n- All Redshift dbt logic appears in the Snowflake dbt implementation\n\n- Business logic is preserved exactly\n\n- Outputs match for identical inputs\n\n- Data lineage and dependencies are intact\n\n- dbt config blocks are correctly converted\n\n- Pre-hooks and post-hooks are preserved\n\n- Tests and documentation intent is maintained\n\n4. Verify Snowflake DBT Optimizations\n\nReview:\n\n- Micro-partitioning behavior\n\n- Clustering key design (1\u20134 columns recommended)\n\n- Materialization strategies per model\n\n- Transient vs permanent table usage\n\n- Semi-structured data handling (VARIANT, OBJECT, ARRAY)\n\n5. Test the Snowflake DBT Code\n\n- Validate using sample or representative data\n\n- Ensure dbt tests pass\n\n- Confirm output parity with Redshift dbt behavior\n\n- Validate data quality checks and assertions\n\n6. Identify Performance Bottlenecks & Improvements\n\nDetect issues such as:\n\n- Inefficient JOIN patterns\n\n- Missing clustering keys on large tables\n\n- Suboptimal incremental strategies\n\n- Unnecessary type casting\n\n- Non-optimized window functions\n\n- Recommend Snowflake-appropriate optimizations without changing business logic.\n\n7. Document Findings\n\n- Report all discrepancies, risks, and optimization opportunities\n\nEvery issue must include:\n\n- Snowflake dbt line number\n\n- Clear description\n\n- Impact assessment\n\n- Recommendation\n\nExample:\n\nIssue detected at Snowflake DBT Line 27: Incorrect NULL handling does not match Redshift DBT logic.\n\nINPUT:\n\nUse the Redshift DBT code from this file:\n      \n      \n      \n      {{raw_input_false_true}}\n    \n    \n    \n     \n\nAlso take the converted Snowflake DBT output generated by the\u00a0DI Redshift Dbt To Snowflake Dbt Conversion",
          "modelName": "model"
        }
      ],
      "realmId": 79,
      "tags": [
        6,
        12,
        14,
        3,
        4
      ],
      "practiceArea": 6
    }
  },
  "status": "SUCCESS"
}