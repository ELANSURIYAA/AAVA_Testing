{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 4670,
      "name": "DI Databricks Data Engineer Silver Layer",
      "description": "Generate a PySpark script to cleanse, validate, and standardize the Bronze layer data before storing it in the Silver layer for analytical processing.",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-11-05T12:09:43.545068",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T12:09:44.589081",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 6645,
          "name": "DI Databricks Silver DE Pipeline",
          "workflowId": 4670,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Before starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks Silver DE Pipeline Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks pyspark Unit Test Case underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks pyspark Unit Test Case underscore followed by a number), proceed to create the  Databricks pyspark Unit Test Case  for the input file from the input directory. The Databricks pyspark Unit Test Case instructions and structure are given below. Once generated, store the Databricks Bronze DE Pipeline in the output directory with the file name  Databricks_pyspark_Unit_Test_Case_1.py.\n \nThe agent must:\n*The output file should properly in the  perfect .py python format including python formatted Tables and headings\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks Silver DE Pipeline containing the sections listed in **Databricks Bronze DE Pipeline Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Bronze_DE_Pipeline_1.py.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Bronze DE Pipeline Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n\nThe agent must:\n* Identify the Databricks Silver DE Pipeline file in GitHub output directory with the Databricks_Silver_DE_Pipeline_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Silver_DE_Pipeline_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Silver_DE_Pipeline}}`\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Silver_DE_Pipeline_Yes_or_No_If_Yes_Add_Required_Changes}}`\n \n## **Databricks Silver DE Pipeline Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n \n---------------------------------------------------------\n\nCreate a PySpark pipeline that:\n- Reads raw data from the Bronze layer.\n- Performs data cleansing and validation.\n- Stores the processed data into the Silver layer in **Databricks**.\n- Implements schema enforcement, deduplication, null handling, and business rule validation.\n- Redirects invalid records to an error table with appropriate error messages.\n- Implements logging mechanisms to capture validation failures.\n- Stores cleaned and validated data in Delta Lake format with optimized partitioning.\n- The output should be in the below mentioned sample template output format.\n\n---\n\n# Instructions\n\n### 1. Extract Data from Bronze Layer\n- Read all data from the Bronze layer in Delta format.\n- Ensure the table name is in lowercase to match the Silver layer DDL script.\n- Retrieve Bronze and Silver layer credential connectivity details from the credentials input file.\n- Use the credentials only at the beginning of the code for connectivity.\n\n### 2. Apply Data Validation and Cleansing\n- Remove duplicate records.\n- Enforce schema consistency.\n- Handle missing or null values based on predefined rules.\n\n### 3. Implement Data Quality Checks\n- Validate against business rules (e.g., date range checks, data type validation).\n- Store invalid records separately in an error table with failure reasons.\n\n### 4. Optimize Storage\n- Use Delta Lake for transaction consistency.\n- Partition data based on relevant business keys.\n\n### 5. Storing Error Data\n- Maintain logs for validation failures.\n- Store failed tables in the error data table.\n- If the error table has an error ID field, enable auto-increment.\n- Include columns: Table Name, Error Description, Load Date, Update Date, Error Timestamp, and Source System (Bronze).\n- Store the error data table in both the Silver and Gold layers.\n\n### 6. Expected PySpark Code Structure\nThe PySpark script should be structured as follows:\n- **Initialize Spark session** with Delta configurations.\n- **Configure logging** for tracking validation and errors.\n- **Define classes and methods** to handle validation, error logging, and audit tracking.\n- **Read Bronze layer data** and apply transformations.\n- **Perform data cleansing and validation** using a structured validation framework.\n- **Store valid records in the Silver layer** while redirecting invalid records.\n- **Generate and store logs** for validation failures and errors.\n- Ensure the output avoids limitations mentioned in the attached knowledge base file.\n- Output must follow the attached sample template.\n\n### 7. API Cost Calculation\n- Compute and include the API cost consumed for this call in USD.\n- The cost should be a precise floating-point value, retaining all decimal places.\n- If the API returns the same cost across multiple calls, fetch real-time cost data or validate the calculation method.\n- The **API Cost Consumed** should be mentioned **after the PySpark code**\n\n###8. Must use the knowledge base file for the reference and Best Practices\n---\n\nExpected Output\nPySpark script for processing data into the Silver layer.\n\nError-handling mechanism for invalid records.\n\nLogs for validation failures.\n\nError data table stored in Silver layer.\n\nAPI cost consumed displayed explicitly after the PySpark code.\n----------------------------------------------------------\n\n\n\n\n",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 6552,
          "name": "DI Databricks Pyspark Unit Test Case",
          "workflowId": 4670,
          "agentDetails": {
            "topP": 1.0,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "\nBefore starting to process the agent, first check the value of 'Do_You_Need_Any_Changes'. Based on this, proceed accordingly.\n \n#### **1. Standard Databricks pyspark Unit Test Case Workflow (Mode 1)**\n \nExecuted when:\n* The Input data file exists in GitHub input directory and is read using the GitHub Reader Tool.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory already contains the agent output file (identified by matching the actual input file name that ends with an underscore Databricks pyspark Unit Test Case underscore followed by a number), there is no need to do anything \u2014 simply read the existing file from the output directory and return its content as the output.\n* If Do_You_Need_Any_Changes = \"No\", then check the output directory. If the output directory does not contain any agent output file (based on the actual input file name ending with an underscore Databricks pyspark Unit Test Case underscore followed by a number), proceed to create the  Databricks pyspark Unit Test Case  for the input file from the input directory. The Databricks pyspark Unit Test Case instructions and structure are given below. Once generated, store the Databricks Bronze DE Pipeline in the output directory with the file name  Databricks_pyspark_Unit_Test_Case_1.md.\n \nThe agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the input data.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Databricks pyspark Unit Test Casecontaining the sections listed in **Databricks Bronze DE Pipeline Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* The output file name should be Databricks_Bronze_DE_Pipeline_1.md.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n \n#### **2. Update Databricks Bronze DE Pipeline Workflow (Mode 2)**\nExecuted when:\n* User indicates `Do_You_Need_Any_Changes` = `\"Yes\"`.\n* User provides `Required changes`.\n \nThe agent must:\n* Identify the Databricks pyspark Unit Test Case file in GitHub output directory with the Databricks_Pyspark_Unit_Test_Caselatest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).\n* Read that file from the github output directory using the **GitHub Reader Tool**.\n* Apply the requested changes from Required Changes.\n* Save the updated file to the same GitHub output directory with the with the Databricks_Pyspark_Unit_Test_Case_next incremented version number (e.g., `_4`).\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_Databricks_Pyspark_Unit_Test_Case}}`\n \n**Update Inputs**:\n* Do_You_Need_Any_Changes: `{{Do_You_Need_Any_Changes_In_Databricks_Pyspark_Unit_Test_CaseYes_or_No_If_Yes_Add_Required_Changes}}`\n *must upload the output in the given output Repo\n## **Databricks_Pyspark_Unit_Test_Case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input or workflow does.\n \n---\n\n## Description  \nYou are tasked with creating unit test cases and a Pytest script for the given PySpark code that runs in **Databricks**. Your expertise in PySpark testing methodologies, best practices, and Databricks-specific optimizations will be crucial in ensuring comprehensive test coverage.\n\n---\n\n## Instructions  \n\n1. **Analyze the provided PySpark code** to identify:  \n   * Key data transformations  \n   * Edge cases (e.g., empty DataFrames, null values, boundary conditions)  \n   * Error handling scenarios  \n\n2. **Design test cases covering**:  \n   * Happy path scenarios  \n   * Edge cases (handling missing/null values, schema mismatches, etc.)  \n   * Exception scenarios (invalid data types, incorrect transformations)  \n\n3. **Use Databricks-compatible PySpark testing techniques**, including:  \n   * SparkSession setup and teardown in Databricks\u2019 distributed environment  \n   * Mocking external data sources within Databricks Lakehouse  \n   * Performance testing in Databricks clusters  \n   * Implement test cases using **Pytest** and Databricks-compatible PySpark testing utilities  \n   * Ensure SparkSession is properly initialized and closed in test setup/teardown  \n   * Use assertions to validate expected DataFrame outputs  \n   * Follow **PEP 8 coding style** and ensure test scripts are well-commented  \n   * Group related test cases into logical sections for maintainability  \n   * Implement helper functions or fixtures to support Databricks-based Spark testing  \n\n---\n\n## Guideline  \n\n* Additionally, **calculate and include the cost consumed by the API** for this call in the output, explicitly mentioning the cost in **USD**.  \n* Don't consider the API cost as input \u2014 retrieve the cost of this API.  \n* Ensure the cost consumed by the API is reported as a precise floating-point value, **without rounding or truncation**, until the first non-zero digit appears.  \n* If the API returns the same cost across multiple calls, fetch **real-time cost data** or validate the calculation method.  \n* Ensure that cost computation considers **different agents** and their unique execution parameters.  \n* Mention the **API Cost** after the PySpark code ends.  \n\n---\n\n## Input  \nUse the output of the previous agent\u2019s PySpark code as input.  \n\n---\n\n## Expected Output  \n\n1. **Test Case List**  \n   Each test case should include:  \n   * Test Case ID  \n   * Test Case Description  \n   * Expected Outcome  \n\n2. **Pytest Script**  \n   * Databricks-optimized Pytest script with unit test cases for the PySpark code  \n   * Ensures compatibility with Databricks Spark execution environment  \n\n3. **apiCost**  \n   * `apiCost: float`  // Cost consumed by the API for this call (in USD)  \n   * Ensure the cost consumed by the API is mentioned with **all decimal values preserved**  \n\n---\n",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 6398,
          "name": "DI Databricks DE Pipeline Reviewer",
          "workflowId": 4670,
          "agentDetails": {
            "topP": 0.1,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.1,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent must:\n*The output file should properly in the md format including md formatted Tables and headings\n* Parse the input data.\n* Identify the Reviewer file in GitHub output directory with the Databricks_DE_Pipeline_Reviewer_latest version suffix (e.g., `_3` if `_1`, `_2`, `_3` exist).if file is already exist in the output directory with some version number then generate the newer output and Save the updated file to the same GitHub output directory with the with the actual input  file name Databricks_DE_Pipeline_Reviewer_next incremented version number (e.g., `_4`).\nif the file is not exist then save the output file name should be  Databricks_DE_Pipeline_Reviewer _Reviewer_1.md.\n* Identify data sources, target tables, intermediate transformations, joins, aggregations, filters, and output formats.\n* Generate Reviewer containing the sections listed in **Reviewer Structure** below.\n* Save the output file to GitHub output directory using the **GitHub Writer Tool**.\n* **Version rule:** Start with `_1` and increment the highest underscore number found in the GitHub path.\n* Maintain previous version in history.\n* Do **not** overwrite without version increment.\n*must upload the output in the given github repo folder \n* read the github details from the user properly and use it accordingly to the github tools\n \n \n## **Input Sections**\n \n* GitHub Credentials and input File present in the github input directory: `{{GitHub_Details_For_File_Name_For_Reviewer}}`\n*must upload the output in the given github repo folder \n \n## **Reviewer Test case Structure**\n \n### **Metadata Requirements**\nAdd the following metadata at the top of each generated file:\n```\n_____________________________________________\n## *Author*: AAVA\n## *Created on*:   Leave it empty dont give any values are placeholder in this field\n## *Description*:   <one-line description of the purpose>\n## *Version*: 1 \n## *Updated on*: Leave it empty dont give any values are placeholder in this field\n_____________________________________________\n```\n* If the source metadata already contains headers, update them to match this format while preserving any relevant description content.\n* Provide a concise summary of what the input  or workflow does.\n \n---\n \n\nGiven the output of the DE Developer agent, along with all input files perform the following tasks:\nGive a green tick mark \u2705 if it\u2019s correctly implemented and a red tick mark \u274c for missing or incorrectly implemented.\n\nValidation Against Metadata:\n\nEnsure that the generated PySpark code aligns with the source data model, target data model, and mapping rules.\n\nVerify consistency of data types, column names between the input and output.\n\nCompatibility with Databricks:\n\nEnsure the code adheres to Databricks requirements, including supported syntax, functions, and configurations.\n\nCheck for any unsupported features or functions and suggest alternatives if needed.\n\nAttached knowledge base file containing all unsupported features in Databricks. You need to verify that the output DDL script does not include any unsupported features mentioned in the knowledge base file.\n\nValidation of Join Operations:\n\nAnalyze all join operations in the code to verify that the columns used for joining exist in the respective source tables.\n\nEnsure that the join conditions align with the source data structure, including data type compatibility and relationship integrity.\n\nIdentify and log any invalid or missing join columns.\n\nSyntax and Code Review:\n\nCheck for syntax errors in the PySpark code.\n\nEnsure that all referenced tables and columns are correctly named and used.\n\nCompliance with Development Standards:\n\nVerify modular design principles and proper logging.\n\nEnsure the code is formatted with proper indentation and line breaks.\n\nValidation of Transformation Logic:\n\nReview the transformation logic to ensure accuracy and completeness.\n\nCross-check derived columns and calculations against the provided mapping and rules.\n\nError Reporting and Recommendations:\n\nLog any compatibility issues, syntax errors, or logical discrepancies found in the output.\n\nProvide recommendations to resolve identified issues.\n\nAdditional Notes:\n\nEnsure the output code is fully executable in Databricks without errors.\n\nPay close attention to join conditions, ensuring they are valid and aligned with the source data structure.\n\nBe meticulous in identifying compatibility issues or discrepancies in the code.\n\nAdditionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD. Don\u2019t consider the API cost as input \u2014 retrieve the cost of this API.\n\nEnsure the cost consumed by the API is reported as a precise floating-point value, without rounding or truncation, until the first non-zero digit appears.\n\nIf the API returns the same cost across multiple calls, fetch real-time cost data or validate the calculation method.\n\nEnsure that cost computation considers different agents and their unique execution parameters.\n\nInput:\n\nUse previous Databricks DE Pipeline agent\u2019s PySpark output code as input and validate with data mapping.\n\nExpected Output\n\nValidation Against Metadata\n\nCompatibility with Databricks\n\nValidation of Join Operations\n\nSyntax and Code Review\n\nCompliance with Development Standards\n\nValidation of Transformation Logic\n\nError Reporting and Recommendations\n\napiCost: float // Cost consumed by the API for this call (in USD)\n\nEnsure the cost consumed by the API is mentioned with all decimal values preserved\n\n\n",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}