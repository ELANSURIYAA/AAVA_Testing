{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2481,
      "name": "DI Datastage To DBT Doc&Anayze",
      "description": "DESCRIPTION:  \nThe DI_DataStage_to_dbt_Analyzer agent is responsible for parsing DataStage job metadata, reconstructing the ETL workflow, classifying transformations, and mapping each component to its dbt/Snowflake equivalent. The agent produces both a human-readable report (Markdown or text) and a machine-readable JSON summary for downstream processing. The agent must follow the detailed instructions below:\n\nINSTRUCTIONS:\n\nSTEP 1: INPUT DISCOVERY  \n- Check for the presence of DataStage_Job_Graph.txt in the input folder.  \n- If available, read and parse the file to extract all stages, links, and row counts.  \n- Optionally, if DataStage_Job.dsx is present, parse it for deeper metadata (stage properties, expressions, lookup details).  \n- Record the job name (string) and confirm the target platform is \"dbt_Snowflake\".  \n- If the graph file is missing, return error: \"Graph file not found.\"\n\nSTEP 2: DATA FLOW RECONSTRUCTION  \n- Identify and list all stages: input, transformer, join, aggregator, sort, output, etc. ",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-11-05T11:01:18.662382",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:01:19.727655",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4289,
          "name": "DI DataStage Documentation",
          "workflowId": 2481,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "****MASKED****",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 4342,
          "name": "DI DataStage To DBT Analyzer",
          "workflowId": 2481,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "must use this github file reader tool to get the input file from the git\n\nYour task is to process the provided DataStage  file and produce a detailed analysis and metrics report, strictly adhering to the specified markdown format and section structure. The report must be tailored for a DataStage-to-dbt (Snowflake) migration context.\nAll content must be derived from the actual input job file; do not make assumptions or provide summaries without direct evidence from the file.\n\nINSTRUCTIONS:\n\n1. **File Parsing**  \n   - Parse the provided input job file to extract all relevant metadata, including job stages, links, parameters, and transformation logic.\n   - Ensure that all extracted details are accurate and reflect the actual job configuration.\n\n2. **Section Generation**  \n   - Follow the required output format exactly, maintaining section numbering, headings, and markdown layout as shown.\n   - For each section, populate content based on the parsed input file:\n     - **Job Overview:** Summarize the job\u2019s ETL logic, business purpose, and conceptual mapping to dbt layers (staging, intermediate, mart).\n     - **Complexity Metrics:** - Provide a complexity breakdown in the table format:\nCategory  |  Measurement\n* Number of Stages\n* Source/Target Systems\n* Transformation Stages\n* Parameters Used\n* Reusable Components\n* Control Logic\n* External Dependencies\n* Performance Considerations\n* Volume Handling\n* Error Handling\n* Overall Complexity Score.\n*Conversion complexity rating(this is also the part of the table)-Assign a numeric complexity score (0\u2013100) and a conversion complexity rating (Low/Medium/High) with justification. List high-complexity areas with specifics from the job.\n     - **Syntax Differences:** Clearly explain how DataStage constructs map to dbt and Snowflake SQL syntax, using examples relevant to the job.\n     - **Manual Adjustments:** Enumerate all manual interventions required for migration, referencing actual job features (e.g., transformer expressions, parameter usage).\n     - **Optimization Techniques:** Recommend dbt and Snowflake performance improvements, referencing job-specific scenarios. State whether to refactor or rebuild, with reasoning.\n     - **API Cost:** Calculate and display the API cost for this analysis, in full decimal precision (e.g., `apiCost: 0.0182 USD`).\n\n3. **Formatting & Quality Criteria**  \n   - Use markdown headers and tables exactly as specified.\n   - Write in clear, enterprise-style English.\n   - Every section must appear in order, with no omissions or reordering.\n   - Replace all `<placeholders>` with actual parsed values from the input file.\n   - Do not provide summaries or assumptions; only use details present in the input job file.\n\n4. **Output Structure**  \n   - The report must begin with the metadata block (`Author`, `Created on`, `Description`) and follow with sections #1 through #6 as shown.Do not mention any date infront of `Created on` block leave it empty.\n   - All metrics must be presented in table format.\n   - All recommendations and mappings must be specific to dbt and Snowflake.\nAll content must be derived from the actual input file; do not make assumptions or provide summaries without direct evidence from the file.\nInput :\nFor Datastage input git credentials for git file reader tool, use this below input : {{Datastage}}",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 4340,
          "name": "DI DataStage To DBT Plan",
          "workflowId": 2481,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Must use the Git file reader tool to read the input file from the git\n\nYou are tasked with providing a comprehensive development and testing effort estimate for converting DataStage jobs into DBT models. Follow these instructions:\n\nINSTRUCTIONS:\n\nReview the metadata and complexity analysis generated by the DI_DataStage_To_DBT_Analyzer agent.\n\nIdentify DataStage stages, transformations, or flow controls that need manual re-implementation in DBT SQL.\n\nExclude direct 1:1 mapping constructs (e.g., simple column renaming or direct loads). Focus only on logic-heavy transformations like Aggregations, Conditional Derivations, Lookups, Joins, and Business Rules.\n\nEstimate the number of hours required to:\n\nRewrite complex stages and logic-heavy flows into DBT models using SQL, CTEs, and macros\n\nImplement and test DBT models, reusable macros, incremental models, and metadata tracking\n\nValidate data equivalence and business rule consistency between DataStage outputs and DBT model outputs\n\nIf compute cost is required, use warehouse costs (Snowflake) based on data volume, transformation complexity, and runtime.\n\nOUTPUT FORMAT:\n\n\n=============================================\nAuthor:        Ascendion AAVA\nCreated on: (leave it blank)      \nDescription:   Cost and effort estimation for DBT model conversion\n=============================================\n\n1. Cost Estimation\n1.1 Snowflake Runtime Cost\nEstimate runtime cost \n \n \nCost Breakdown:  give the final cost, don't include calculations\n- Compute: $X  \n- Storage: $X  \n \nTotal Estimated Runtime Cost per Job:  \n$X USD per run\n \nJustification:\nstick to theory for justification, do not include calculations. give it in points instead of paragraph \nPercentage of data processed per run\n\nJob complexity \n\nStorage impact\n\nNeed (or lack) of incremental/partitioning optimizations\n\n\n2. Code Fixing and Testing Effort Estimation\n2.1 Manual Model Fixes and  Recon Testing Effort\nDescribe manual tasks required to convert DataStage logic into dbt SQL / Jinja, such as:\nParameter replacement (e.g., $$Audit_Year \u2192 {{ var('audit_year') }})\nStage-to-model conversion (Sequential \u2192 source(), Aggregator \u2192 GROUP BY, Transformer \u2192 SELECT with derived columns)\nSchema and datatype alignment between source and Snowflake targets\nAdding dbt tests for null checks and row-count validation\nEstimate time (hrs) for each activity and subtotal the effort.\n2.2 Output Validation Effort\nOutline tasks for validating dbt outputs against DataStage results:\nCompare row counts and sample records\nValidate aggregations, joins, and derived columns\nExecute dbt tests (unique, not_null, accepted_values)\nDocument and resolve discrepancies\nProvide estimated effort (hours) for validation + documentation.\n2.3 Total Estimated Effort in Hours\nSummarize all efforts in a table:\nTask\tEstimated Hours\tNotes\nManual Code Fixes & Testing\t \t \nOutput Validation\t \t \nTotal Estimated Effort\t \t \nJustify effort based on Analyzer complexity score:\nSimple (0-20) \u2192 \u2264 5 hrs\nModerate (21-50) \u2192 6\u201312 hrs\nComplex (51-100) \u2192 > 12 hrs\n\n\n\n\n\n 3. API Cost Consumption\n\napiCost: X.XXXXX USD\n\n\nInclude the cost consumed by the API for this call in the output\n\nEnsure the cost is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: 0.00431 USD)\n\nINPUT:\n* Use the output from `DI_DataStage_to_DBT_Analyzer` as input  \n* For github credentials for datastage file, use this git file reader tool: {{Datastage}} \n* For github credentials for the environment cost and compute configuration, use this file: {{DBT_Environment_Config}}",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}