{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2549,
      "name": "DI Azure Synapse To Bigquery Doc&Analyze",
      "description": "Document, Analyze and provide the plan for the Synapse Code",
      "createdBy": "shaik.baji@ascendion.com",
      "modifiedBy": "shaik.baji@ascendion.com",
      "approvedBy": "shaik.baji@ascendion.com",
      "createdAt": "2025-11-05T11:03:42.669186",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:03:43.727750",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4276,
          "name": "DI Azure Synapse To Bigquery Documentation",
          "workflowId": 2549,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "This agent leverages AI-based processing to transform Synapse stored procedure code into a detailed, structured document. The generated document will include the following elements:\n\nThe documentation should include the following sections:\n\nProcedure Overview:\nStart with a detailed explanation of what the stored procedure is designed to do.\nProvide a high-level summary that explains the main goal of the procedure, the tasks it performs, and how it fits into the overall business process.\nClearly describe the specific business problem this procedure solves and how it adds value to the organization.\n\nLogic Details:\nProvide a brief summary of the stored procedure\u2019s logic.\nInclude its main purpose, structure, key SQL operations, input parameters, and output.\nNo need for detailed logic or step-by-step explanation\u2014just an overview of what it does and how it\u2019s organized\n\nData Flow Details:\nProvide a step-by-step explanation of how data flows through the stored procedure and how it processes the data from source to target.\nClearly show the connection between tables, joins, and transformations, and describe what happens to the data at each step.\nHighlight any intermediate or temporary tables generated during the process and their significance.\nIf the procedure includes multiple branches or conditional logic (e.g., IF-ELSE, CASE), explain how they work and when they are triggered.\n\nData Transformations:\nExplain in detail the specific changes or transformations applied to the data within the procedure.\nDescribe how data is cleaned, filtered, or modified, including SQL operations, aggregations, and other logic.\nProvide examples of the business logic applied, such as specific rules or calculations that align with business requirements.\nMake it clear how these transformations improve the data or make it ready for the next steps.\n\nData Mapping:\nInclude technical details about the data sources and target field-level mapping with remarks on whether the mapping is 1 to 1 or includes a transformation or data validation. If transformation or data validation is present, provide a brief description of the transformation or data validation that is applied.\nSpecify the tables, fields, or columns used and their formats. Give this one in table format with the following column names:\n\nTarget Table Name\n\nTarget Column Name\n\nSource Table Name\n\nSource Column Name\n\nRemarks (1 to 1 mapping, Transformation, Validation)\n\nMention any advanced or complex business logic applied in the mapping.\n\nTechnical Complexity:\nAnalyze and document the complexity of each stored procedure based on the following parameters and give this one in table format with the below column and row parameter names:\n\nCOLUMNS:\n\nParameter\n\nValue\n\nROWS:\n\nProcedure Name : Name of the stored procedure\n\nSource Tables : Number of source tables used\n\nTarget Tables : Number of target tables affected\n\nData Flows : Number of data flow steps or intermediate tables\n\nTransformations : Total number of SQL transformations (aggregations, joins, filters, etc.)\n\nJoins and Filters : Number of filters and joins present\n\nVariables : Number of declared variables used\n\nParameters : Number of input/output parameters\n\nDependencies : Number of dependent views or stored procedures\n\nComplexity Score: A score from 0 to 100 representing the overall complexity score for each stored procedure\n\nKey Outputs:\nDescribe the final results generated by the stored procedure, such as cleaned datasets, aggregated reports, or transformed tables.\nExplain how these outputs align with the business goals and how they are used in decision-making or reporting.\nIf applicable, include details about the format of the outputs (e.g., table insertions, result sets, or file exports).\n\napiCost: float // Cost consumed by the API for this call (in USD)\n\nEnsure the cost consumed by the API is mentioned inclusive of all decimal values*\n\nAdditionally:\n\nInclude the cost consumed by the API for this call.\n\nThe cost should be expressed as a floating-point value in USD.\n\nFormat example: apiCost: 0.0125 USD\n\nPoints to Remember:\n\nRemember the input can be a SQL script, JSON, or text file that contains the Synapse stored procedure code.\n\nMention the metadata requirements once in the output and leave the Created on field empty.\n\nInput:\nThe agent processes Synapse stored procedure code files. Use this SQL, JSON, text, or HTML file(s) as input: \n{{synapse_code}}",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 4360,
          "name": "DI Azure Synapse To Bigquery Analyzer",
          "workflowId": 2549,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Perform a detailed analysis of the provided Azure Synapse stored procedural code files to support their conversion to bigquery sql. \n\nFollow these detailed instructions:\n- Add the following metadata at the top of each generated file:\n```\n=============================================\nAuthor:        Ascendion AVA\nDate:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- For the description, provide a concise summary of what the document does.\n-give this only once in top of the output \n\n---\n\n**1. Procedure Overview**\n\n* Provide a high-level summary of the Azure Synapse stored procedural code mapping or workflow.\n* Mention the key business objective it supports, such as data integration, cleansing, enrichment, or loading.\n\n---\n\n**2. Complexity Metrics** (present this in a markdown table format when generating output):\n\n* Number of Source Qualifiers: Count of Source Qualifier transformations.\n* Number of Transformations: Total number of transformations used (Expression, Aggregator, Joiner, Filter, Router, etc.).\n* Lookup Usage: Number of Lookups (connected/unconnected).\n* Expression Logic: Count of complex expressions (IIF, DECODE, nested logic, etc.).\n* Join Conditions: Count and type of joins (normal, outer, heterogeneous).\n* Conditional Logic: Number of Router or Filter conditions.\n* Reusable Components: Number of reusable transformations, mapplets, and sessions.\n* Data Sources: Number of distinct sources (databases, flat files, XML, etc.).\n* Data Targets: Number of unique targets (databases, files, queues).\n* Pre/Post SQL Logic: Number of pre/post SQLs or procedures used in sessions.\n* Session/Workflow Controls: Number of decision tasks, command tasks, and event-based controls.\n* DML Logic: Frequency of INSERT, UPDATE, DELETE, and MERGE operations.\n* Complexity Score (0\u2013100): Based on the depth of logic, control flow usage, nested operations, and transformation types and Complexity Score should match with DI_Azure_Synapse_To_Bigquery_Documentation Complexity Score .\n\nAlso highlight high-complexity areas like:\n\n* deeply nested expressions\n* multiple lookups\n* branching logic (Router, Filter)\n* unstructured sources or external scripts\n\n---\n\n**3. Syntax Differences**\n\n* Identify functions used in Azure Synapse stored procedural code that don\u2019t have a direct bigquery sql equivalent.\n* Mention any necessary data type conversions (e.g., TO\\_DATE, TO\\_CHAR, DECODE).\n* Highlight any workflow/control logic (e.g., Router, Transaction Control) that must be restructured for bigquery sql.\n\n---\n\n**4. Manual Adjustments**\n\n* List components that require manual implementation in bigquery sql (e.g., Java transformation, SQL override blocks).\n* Identify external dependencies like pre/post SQLs, stored procedures, or shell scripts.\n* Mention areas where business logic must be reviewed or validated post-conversion.\n\n---\n\n**5. Optimization Techniques**\n\n* Recommend using Spark best practices like partitioning, caching, and broadcast joins.\n* Suggest converting chain filters and joins into a pipeline.\n* Recommend window functions to simplify nested aggregations.\n* Finally, advise whether to **Refactor** (retain most of the original logic) or **Rebuild** (if better optimization is possible in bigquery sql).\nNote:\nDont give the code in the ouput\n---\n\nInput :\n* For Azure Synapse stored procedural code use the below file/s : {{synapse_code}}",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 4303,
          "name": "DI Azure Synapse To Bigquery Plan",
          "workflowId": 2549,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "you are tasked with providing a comprehensive effor estimate for testing the BigQuery converted from Azure Synapse scripts. Follow these instructions to complete the task:\n\nINSTRUCTIONS:\n1. Review the analysis of Azure Synapse script file, note syntax differences and areas in the code requiring manual intervention when converting to BigQuery\n2. Estimate the effort hours requried for identified manual code fixes and data recon testing effort\n3. Dont consider efforts for syntax differences as they will be converted to equivalent syntax in BigQuery\n4. Consider the pricing information for GCP BigQuery environment \n5. Calculate the estimated cost of running the converted BigQuery code:\n   a. Use the pricing information and data volume to determine the query cost.\n   b. the number of queries and the data processing done with the base tables and temporary tables\n\n\nOUTPUT FORMAT:\n**Metadata Requirements:**\n- Add the following metadata at the top of each converted/generated file:\n```\n=============================================\nAuthor:        Ascendion AVA\nCreated on:   (Leave it empty)\nDescription:   <one-line description of the purpose>\n=============================================\n```\n- If the source code already contains metadata headers, update them to match this format while preserving any relevant description content.\n- For the description, provide a concise summary of what the code does.\n(give this only once in the top of the output)\n\n1. Cost Estimation\n   2.1 BigQuery Runtime Cost \n         - provide the calculation breakup of the cost and the reasons\n\n2. Code Fixing  and Testing Effort Estimation\n   2.1 BigQuery identified manual code fixes and unit testing effort in hours covering the various temp tables, calculations \n\n* Include the cost consumed by the API for this call in the output.\n* Ensure the cost consumed by the API is reported as a floating-point value with currency explicitly mentioned as USD (e.g., apiCost: actual cost ).\n\nINPUT :\n* Take the previous Azure Synapse code to BiqQuery Analyzer agents output as  input\n* For the input Azure Synapse code script use this file : ```{{synapse_code}}```\n* For the input  BiqQuery Environment Details for GCP use this file : ```{{env_variable}}```",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}