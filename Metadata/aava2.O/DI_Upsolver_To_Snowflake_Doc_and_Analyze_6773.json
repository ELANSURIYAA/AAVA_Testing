{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 6773,
      "name": "DI Upsolver To Snowflake Doc and Analyze",
      "description": "Upsolver to snowflake",
      "createdBy": "aarthy.jr@ascendion.com",
      "modifiedBy": "aarthy.jr@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2026-01-05T13:38:35.627320",
      "modifiedAt": "2026-01-05T14:25:56.084329",
      "approvedAt": "2026-01-05T14:25:56.074546",
      "status": "APPROVED",
      "comments": {
        "whatWentGood": "n",
        "whatWentWrong": "",
        "improvements": ""
      },
      "isDeleted": false,
      "parentId": 6262,
      "workflowConfigs": {
        "topP": null,
        "maxToken": null,
        "managerLlm": [],
        "temperature": null,
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 20459,
          "name": "DI Upsolver Documentation",
          "workflowId": 6773,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "anthropic.claude-4-sonnet",
          "description": "Please create detailed documentation for the provided Upsolver SQL script / Streaming Job while following a structured format. The documentation must strictly adhere to the formatting and elements described below and must not include any additional explanations, content, or words outside the curly braces. Do not include line breaks except for explicitly specified characters such as \\n, #, and -.\n\n\n\n\nThe documentation must contain the following sections:\n\n\n\n\nOverview of Program:\n\n\n\n\nExplain the purpose of the Upsolver SQL script in detail.\n\n\n\n\nDescribe how this implementation aligns with modern lakehouse architecture and streaming data analytics.\n\n\n\n\nExplain the business problem being addressed and its benefits.\n\n\n\n\nProvide a high-level summary of Upsolver components such as Tables, Jobs, S3 Connections, Merge Operations, Primary Keys, and Glue Catalog integration.\n\n\n\n\nCode Structure and Design:\n\n\n\n\nExplain the structure of the Upsolver SQL code in detail.\n\n\n\n\nDescribe key components like CREATE TABLE, CREATE JOB, MERGE, PRIMARY KEY, S3_OBJECT, and RUN_INTERVAL.\n\n\n\n\nList the primary Upsolver components such as Tables, Jobs, Merge Statements, Transformations, and Streaming Windows.\n\n\n\n\nHighlight dependencies on external storage (S3), AWS Glue Catalog, incremental processing logic, and schema evolution features (ADD_MISSING_COLUMNS).\n\n\n\n\nData Flow and Processing Logic:\n\n\n\n\nExplain how data flows within the Upsolver streaming implementation.\n\n\n\n\nList the source and destination tables, fields, and data types.\n\n\n\n\nExplain the applied transformations, including type casting, case formatting, conditional logic (CASE), filtering based on $commit_time, and upsert logic.\n\n\n\n\nBreak down the code into logical components and represent the control/data flow using a block-style diagram in plain markdown.\n\n\n\n\nFollow these instructions:\n\n\n\n\nAnalyze the input script and identify each logical component, such as:\n\n\n\n\nTable Creation (Raw Table)\n\n\n\n\nTable Creation (Target Table)\n\n\n\n\nJob Definition\n\n\n\n\nMERGE Operation\n\n\n\n\nTransformations (CAST, INITCAP, UPPER, CASE)\n\n\n\n\nIncremental Filtering (run_start_time(), run_end_time())\n\n\n\n\nUpsert Logic (WHEN MATCHED / WHEN NOT MATCHED)\n\n\n\n\nFor each component, create a markdown block using the following format:\n\n\n\n\n+--------------------------------------------------+\n\n| [Block Title] |\n\n| Description: 1\u20132 line summary of the operation |\n\n+--------------------------------------------------+\n\n\n\n\n\n\n\nConnect the blocks using arrows to represent flow:\n\n\n\n\n[Block A]\n\n\u2193\n\n[Block B]\n\n\u2193\n\n[Block C]\n\n\n\n\n\n\n\nUse branching arrows for conditional logic:\n\n\n\n\n[WHEN MATCHED]\n\n\u2193 Yes\n\n[REPLACE TARGET RECORD]\n\n\u2193\n\n[Continue Stream Processing]\n\n\u2191\n\n[WHEN NOT MATCHED] \u2190 [INSERT NEW RECORD]\n\n\n\n\n\n\n\nUse proper indentation and line breaks for sub-flows or nested logic.\n\n\n\n\nDo not include \u201cStart\u201d or \u201cEnd\u201d blocks unless they are explicitly present in the input.\n\n\n\n\nDo not display the original code\u2014only show the logic block diagram in pure markdown format, accurately representing the structure and flow of the script.\n\n\n\n\nData Mapping:\n\n\n\n\nProvide data mapping details, including transformations applied to the data in the below format:\n\n\n\n\nTarget Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks\n\n\n\n\nMapping column will include whether it is 1 to 1 mapping, type casting rule, formatting transformation, conditional rule, or incremental filter condition.\n\n\n\n\nComplexity Analysis:\n\n\n\n\nAnalyze and document the complexity based on the following:\n\n\n\n\nProvide this in table format with two columns:\n\n\n\n\nCategory | Measurement\n\n\n\n\nNumber of Lines: Count of lines in the Upsolver SQL script.\n\n\n\n\nTables Used: Number of tables referenced.\n\n\n\n\nJobs Defined: Number of streaming jobs defined.\n\n\n\n\nMerge Operations: Number of MERGE statements.\n\n\n\n\nTransformations: Number of transformation expressions (CAST, INITCAP, UPPER, CASE).\n\n\n\n\nIncremental Logic: Usage of run_start_time(), run_end_time(), $commit_time.\n\n\n\n\nDML Statements: Number of MERGE, INSERT, REPLACE operations.\n\n\n\n\nConditional Logic: Number of CASE expressions and WHEN MATCHED/NOT MATCHED conditions.\n\n\n\n\nSQL Query Complexity: Number of joins, subqueries, streaming windows.\n\n\n\n\nPerformance Considerations: Streaming interval, incremental processing efficiency, primary key usage.\n\n\n\n\nData Volume Handling: Continuous streaming vs batch ingestion.\n\n\n\n\nDependency Complexity: External dependencies such as S3 connection, Glue Catalog, schema evolution settings.\n\n\n\n\nOverall Complexity Score: Score from 0 to 100.\n\n\n\n\nKey Outputs:\n\n\n\n\nDescribe final outputs such as Curated Lakehouse Tables, Upserted Tables, or Streaming Data Products.\n\n\n\n\nExplain how outputs align with business goals and analytical reporting needs.\n\n\n\n\nSpecify the storage format (e.g., AWS Glue Catalog table, S3-backed table, curated upsert table).\n\n\n\n\nAPI Cost Calculations:\n\n\n\n\nAdditionally, calculate and include the cost consumed by the API for this call in the output, explicitly mentioning the cost in USD.\n\n\n\n\nEnsure the cost consumed by the API is mentioned with inclusive of all decimal value.\n\n\n\n\nPoints to Remember:\n\n\n\n\nGive the metadata requirements at the top of the output only once and leave the created on field in the metadata requirements empty.\n\n\n\n\nDo not provide sample code anywhere and strictly follow the output format; no extra summary or recommendation needed.\n\n\n\n\nDo not repeat metadata above test case code or converted code; give it only once at the top of the output.\n\n\n\n\nInput:\n\n\n\n\nFor Upsolver SQL scripts use below file: {{input1_string_true}} \n\n\n\n\nExpected Output:\n\n\n\n\n{\n\n\"document\": \"# Upsolver SQL Documentation: \\n\\n## 1. Overview of Program\\n\\n### 2. Code Structure and Design\\n\\n### 3. Data Flow and Processing Logic\\n\\n### 4. Data Mapping\\n\\n### 5. Complexity Analysis\\n\\n### 6. Key Outputs\\n\\n### 7. API Cost Calculations\"\n\n}",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 12070,
          "name": "DI Upsolver to snowflake Analyzer",
          "workflowId": 6773,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "INSTRUCTIONS:\n1. **Initial Assessment**:\n   - Analyze the provided Upsolver code file (\n      \n      \n      \n      \n      \n      \n      {{input1_string_true}}\n    \n    \n    \n    \n    \n    \n    ) to extract all relevant metadata, including job stages, links, parameters, and transformation logic.\n   - Validate file format and ensure accurate extraction of job configuration details.\n   - Identify explicit and implicit migration requirements for Snowflake.\n\n2. **Strategic Planning**:\n   - Develop a comprehensive analysis strategy aligned with migration objectives.\n   - Identify dependencies, risks, and areas requiring manual intervention.\n   - Create a detailed implementation roadmap for migration, including quality gates and validation checkpoints.\n\n3. **Systematic Implementation**:\n   - Parse Upsolver code to extract job stages, source/target systems, transformation logic, parameters, reusable components, control logic, external dependencies, performance considerations, volume handling, and error handling.\n   - Generate complexity metrics and assign a numeric complexity score (0\u2013100) with conversion complexity rating (Low/Medium/High), justifying high-complexity areas.\n   - Map Upsolver constructs to Snowflake SQL syntax, providing relevant examples.\n   - Enumerate all manual adjustments required for migration, referencing actual job features.\n   - Recommend Snowflake optimization techniques, stating whether to refactor or rebuild, with reasoning.\n   - Calculate and display API cost for the analysis in full decimal precision.\n\n4. **Quality Assurance**:\n   - Validate all extracted metadata and metrics for accuracy and completeness.\n   - Ensure strict adherence to markdown formatting and section structure.\n   - Perform security, performance, and quality assessments.\n   - Document test results and validation outcomes.\n\n5. **Optimization and Enhancement**:\n   - Identify opportunities for performance improvement and future extensibility.\n   - Apply best practices for ETL migration and Snowflake optimization.\n   - Incorporate feedback and lessons learned.\n\n6. **Comprehensive Documentation**:\n   - Produce a markdown report with the following sections:\n     - Metadata block (Author, Created on, Description)\n     - #1 Job Overview\n     - #2 Complexity Metrics (table)\n     - #3 Syntax Differences\n     - #4 Manual Adjustments(table)\n     - #5 Optimization Techniques\n     - #6 API Cost\n   - Ensure all sections are populated with actual parsed values from the Upsolver code file.\n\n7. **Continuous Monitoring**:\n   - Establish monitoring and feedback mechanisms for future migration analysis.\n   - Track performance metrics and success indicators.\n   - Plan for ongoing maintenance and updates.\n\nINPUT PARAMETERS:\n- \n      \n      \n      \n      \n      \n      \n      {{input1_string_true}}\n    \n    \n    \n    \n    \n    \n    : Upsolver code file to be analyzed\n\nOUTPUT FORMAT:\n- Strict markdown report with all required sections and tables\n- All metrics and recommendations based on actual parsed Upsolver code\n- No summaries or assumptions; only direct evidence from the input file\n- API cost displayed in full decimal precision\n\nSAMPLE:\n\n=============================================\nAuthor: <parsed_author>\nCreated on:\nDescription: <parsed_description>\n\n=============================================\n\n# 1. Job Overview\n<parsed_job_overview>\n\n# 2. Complexity Metrics\n| Category | Measurement |\n|----------|------------|\n| Number of Stages | <parsed_value> |\n| Source/Target Systems | <parsed_value> |\n| Transformation Stages | <parsed_value> |\n| Parameters Used | <parsed_value> |\n| Reusable Components | <parsed_value> |\n| Control Logic | <parsed_value> |\n| External Dependencies | <parsed_value> |\n| Performance Considerations | <parsed_value> |\n| Volume Handling | <parsed_value> |\n| Error Handling | <parsed_value> |\n| Overall Complexity Score | <parsed_value> |\n| Conversion Complexity Rating | <parsed_value> |\n\n# 3. Syntax Differences\n<parsed_syntax_differences>\n\n# 4. Manual Adjustments\n<parsed_manual_adjustments>\n\n# 5. Optimization Techniques\n<parsed_optimization_techniques>\n\n# 6. API Cost\napiCost: <parsed_api_cost> USD\n\nleave created on blank\nfor author give Ascendion AAVA",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 13758,
          "name": "DI upsolver to snowflake plan",
          "workflowId": 6773,
          "agentDetails": {
            "topP": 0.9,
            "maxRpm": 20,
            "preset": "Balanced",
            "maxIter": 10,
            "temperature": 0.5,
            "guardrailIds": [],
            "allowDelegation": false,
            "maxExecutionTime": 150,
            "allowCodeExecution": false,
            "isSafeCodeExecution": false,
            "expectedOutputFormat": "",
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "INSTRUCTIONS:\n1. **Initial Assessment**:\n   - Analyze Upsolver code (\n      \n      {{input1_string_true}}\n    \n    ), Snowflake table details (\n      \n      {{input2_string_true}}\n    \n    ), and DI Upsolver to Snowflake analyzer output (\n      \n      \n    \n    ).\n   - Identify explicit and implicit migration requirements, transformation complexity, and success criteria.\n   - Research relevant ETL migration standards, cost estimation methodologies, and validation frameworks.\n\n2. **Strategic Planning**:\n   - Develop a comprehensive migration strategy aligned with project objectives.\n   - Identify manual re-implementation tasks, dependencies, risks, and mitigation strategies.\n   - Create a detailed implementation roadmap with milestones and quality gates.\n\n3. **Systematic Implementation**:\n   - Exclude direct 1:1 mapping constructs; focus on logic-heavy transformations (aggregations, conditional derivations, lookups, joins, business rules).\n   - Estimate hours for rewriting complex Upsolver logic in Snowflake SQL, implementing/testing models, macros, incremental logic, and metadata tracking.\n   - Align schema and datatypes between Upsolver and Snowflake targets.\n   - Add validation steps for data equivalence and business rule consistency.\n   - Maintain detailed documentation throughout the process.\n\n4. **Quality Assurance**:\n   - Conduct thorough testing and validation of all outputs.\n   - Compare row counts, sample records, aggregations, joins, and derived columns between Upsolver and Snowflake outputs.\n   - Execute Snowflake tests (unique, not_null, accepted_values).\n   - Document and resolve discrepancies.\n\n5. **Optimization and Enhancement**:\n   - Identify optimization opportunities (incremental/partitioning, performance tuning).\n   - Ensure scalability, maintainability, and extensibility.\n   - Incorporate feedback and lessons learned.\n\n6. **Comprehensive Documentation**:\n   - Create detailed documentation covering all aspects of migration, effort estimation, cost breakdown, and validation.\n   - Include troubleshooting guides and maintenance procedures.\n   - Provide recommendations for future improvements.\n\n7. **Continuous Monitoring**:\n   - Establish monitoring and feedback mechanisms.\n   - Track performance metrics and success indicators.\n   - Plan for future updates, maintenance, and improvements.\n\nINPUT PARAMETERS:\n- \n      \n      {{input1_string_true}}\n    \n    : Upsolver code (ETL job definitions, transformation logic)\n- \n      \n      {{input2_string_true}}\n    \n    : Snowflake table details (schema, metadata, partitioning)\n- \n      \n      \n    \n    : DI Upsolver to Snowflake analyzer output (complexity score, metadata analysis)\n\nOUTPUT FORMAT:\n=============================================\nAuthor:        Ascendion AAVA\nCreated on: (leave it blank)\nDescription:   Cost and effort estimation for Upsolver to Snowflake migration\n=============================================\n\n1. Cost Estimation\n1.1 Snowflake Runtime Cost\nEstimate runtime cost\n\nCost Breakdown:\n- Compute: $X\n- Storage: $X\n\nTotal Estimated Runtime Cost per Job:\n$X USD per run\n\nJustification:\n- Percentage of data processed per run\n- Job complexity\n- Storage impact\n- Need (or lack) of incremental/partitioning optimizations\n\n2. Code Fixing and Recontesting Effort Estimation\n2.1 Manual Model Fixes and Recon Testing Effort\n- Parameter replacement (e.g., Upsolver variables \u2192 Snowflake SQL variables)\n- Stage-to-model conversion (Upsolver transformations \u2192 Snowflake SQL/CTEs)\n- Schema and datatype alignment\n- Adding Snowflake tests for null checks and row-count validation\n- Estimate time (hrs) for each activity and subtotal the effort\n\n2.2 Output Validation Effort\n- Compare row counts and sample records\n- Validate aggregations, joins, and derived columns\n- Execute Snowflake tests (unique, not_null, accepted_values)\n- Document and resolve discrepancies\n- Provide estimated effort (hours) for validation + documentation\n\n2.3 Total Estimated Effort in Hours\nSummarize all efforts in a table:\nTask\tEstimated Hours\tNotes\nManual Code Fixes & Recontesting\t\t\nOutput Validation\t\t\nTotal Estimated Effort\t\t\nJustify effort based on Analyzer complexity score:\nSimple (0-20) \u2192 \u2264 5 hrs\nModerate (21-50) \u2192 6\u201312 hrs\nComplex (51-100) \u2192 > 12 hrs\n\n3. API Cost Consumption\napiCost: X.XXXXX USD\n\nchange testing as recontesting",
          "modelName": "model"
        }
      ],
      "realmId": 79,
      "tags": [
        1
      ],
      "practiceArea": 6
    }
  },
  "status": "SUCCESS"
}