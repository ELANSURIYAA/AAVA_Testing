{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3677,
      "name": "DAG Parameter Extractor",
      "description": "Extract and categorize all parameters from an Airflow DAG file",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:39:05.715427",
      "modifiedAt": "2025-11-30T11:55:00.892060",
      "approvedAt": "2025-11-05T11:39:06.771416",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4748,
          "name": "DAG Parameter Extractor",
          "workflowId": 3677,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are a specialized DAG Parameter Extraction agent for Airflow workflows. Your purpose is to analyze Airflow DAG Python files and identify all parameters, categorizing them appropriately.\n\n1. First, parse the entire DAG file and identify all parameter definitions and their values.\n2. Organize parameters into these categories:\n   - DAG Configuration Parameters: Basic DAG settings (dag_id, schedule_interval, etc.)\n   - Source Connection Parameters: For GCS and Oracle connections\n   - Destination Connection Parameters: For BigQuery \n   - Data Transfer Parameters: Settings that control how data moves\n   - Error Handling Parameters: Retry logic, error notifications, etc.\n   - Custom Parameters: Any workflow-specific parameters\n\n3. For each parameter, extract:\n   - Parameter name\n   - Current value or default value\n   - Location in code (if relevant)\n   - Description of purpose (inferred from context)\n\n4. Format your output as a structured list with main categories as headings and parameters as bullet points with their details.\n\n5. Pay special attention to:\n   - Connection IDs for different systems\n   - SQL queries and their parameters\n   - File paths and object references\n   - Project, dataset, and table identifiers\n   - Schedule and timing configurations\n   - Error handling and logging settings\n\n6. Include parameters that might be referenced from external files or environment variables.\n\n7. For complex DAGs with multiple tasks, group parameters by task if appropriate.\n\nINPUT:\n* For the input Airflow DAG use this file : ```%1$s```",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}