{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 3718,
      "name": "Fabric Dim Data Mapping Gold Layer",
      "description": "This workflow is for recommending and creating the gold dimension data mapping",
      "createdBy": "default@ascendion.com",
      "modifiedBy": "default@ascendion.com",
      "approvedBy": "default@ascendion.com",
      "createdAt": "2025-11-05T11:40:20.153499",
      "modifiedAt": "2025-11-30T11:55:00.892060",
      "approvedAt": "2025-11-05T11:40:21.199444",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4649,
          "name": "Fabric Gold Dim Transformation Recommender ",
          "workflowId": 3718,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You will read the Model Conceptual, Data Constraints, Silver Layer Physical DDL script, Gold Layer Physical DDL script and Sample Data and generate transformation rules only for Dimension tables.\n\nINSTRUCTIONS:\n\n1. Parse the Silver Layer DDL script to extract only Dimension tables and their column definitions.\n2. Analyze the Model Conceptual and Data Constraints file to identify key attributes, hierarchies, and necessary transformations for Dimension tables.\n3. Inspect Sample Data to detect patterns, anomalies, and standardization requirements specific to Dimension attributes.\n4. Generate transformation rules for Dimension tables, including:\n* Data Type Conversions: Ensure data types align with reporting and business needs.\n* Column Derivations: Define computed attributes (e.g., concatenations, name standardizations, category hierarchies).\n* Hierarchy Mapping: Define parent-child relationships within dimensions.\n* Normalization and Standardization: Ensure consistent formats (e.g., date formats, uppercase/lowercase standardization, unique key constraints).\n5. Provide SQL transformations for each rule, ensuring alignment with the Silver Layer schema.\n6. Ensure traceability of transformations by linking each rule back to its source from the Model Conceptual, Data Constraints and Silver Layer schema to Gold layer.\n\nOUTPUT FORMAT:\n\n1. Transformation Rules for Dimension Tables:\n* [Rule Name]: [Description]\n    - Rationale: [Explanation]\n    - SQL Example: [Sample SQL transformation]\n\n2. Ensure API cost consumption is included in the output, explicitly reporting the cost as a floating-point value in USD (e.g., apiCost: actual cost).\n\nFor input files:\n* For Model Conceptual, Data Constraints and Sample Data file use the below file as input :\n```%1$s```\n* For Silver Layer Physical DDL script and Gold Layer Physical DDL script file use the below file as input : \n```%2$s```",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 6299,
          "name": "Fabric Gold Dim Transformation Data Mapping",
          "workflowId": 3718,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with creating a detailed data mapping specifically for Dimension tables in the Gold Layer. This mapping will incorporate necessary transformations, validations, and cleansing rules at the attribute level.\nYour work will be based on the silver and gold layer physical model provided and previous Fabric Gold Dim Transformation Recommender agents recommendations\n\nINSTRUCTIONS:\n1. Review the provided silver and gold layer physical model DDL script.\n2. Create a detailed data mapping for Dimension tables from the Silver to Gold Layer, ensuring:\n* Dimension attribute transformations (e.g., category mappings, hierarchical relationships, surrogate key generation etc..).\n* Data validation rules for ensuring consistency (e.g., deduplication, format standardization etc..).\n* Cleansing logic (e.g., handling missing values, removing duplicates, enforcing uniqueness constraints).\n3. Ensure all transformations and rules are compatible with PySpark and Microsoft Fabric.\n4. Include explanations for complex transformations and business rules.\n\nOUTPUT FORMAT:\n1. Overview: Summary of the data mapping approach and key considerations.\n2. Data Mapping for Dimension Tables:\nThe mapping output should be in tabular format with the following fields for each Dimension table and its columns:\n* Target Layer: Gold\n* Target Table: Proper table name as per the Gold Layer DDL script\n* Target Field: Proper field name as per the Gold Layer DDL script\n* Source Layer: Silver\n* Source Table: Proper table name as per the Silver Layer DDL script\n* Source Field: Proper field name as per the Silver Layer DDL script\n* Validation Rule: Required validation rules from the Data Constraints file\n* Transformation Rule: Required transformation rules from the  previous Fabric Gold Dim Transformation Recommender agents output recommendations (e.g., name standardization, hierarchical relationships, normalization etc..).\n3. Ensure API cost consumption is included in the output, explicitly reporting the cost as a floating-point value in USD (e.g., apiCost: actual cost).\n\nInputs:\n* For Silver Layer Physical DDL script and Gold Layer Physical DDL script file use the below file as input : \n```%2$s```\n* Also take input from previous Fabric Gold Dim Transformation Recommender Agent\u2019s output recommendations as input",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 6308,
          "name": "Fabric Gold Data Mapping Reviewer ",
          "workflowId": 3718,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with meticulously reviewing the Gold Layer Data Mapping. Your review should encompass various aspects to guarantee the mapping's quality and alignment with industry standards and mention along with\u2705 for correct implementations and \u274c for wrong implementations.\n\nINSTRUCTIONS:\n1. Review the Detailed Data Mapping from Silver to Gold Layer: \n* Ensure data mapping is correctly performed, and all tables are properly structured. \n* Examine the overall structure of the Gold Layer Data Mapping.\n2. Verify data consistency across all mapped fields : \n* Validate that each column in the Silver Layer is mapped correctly to its corresponding Gold Layer destination. \n3. Verify Dimension Attribute Transformations: Ensure correct category mappings.\n4. Verify Data Validation Rules for Consistency:\n   * Confirm deduplication logic is correctly applied.\n   * Ensure format standardization for fields such as dates, IDs, and codes.\n5. Verify Cleansing Logic:\n   * Validate handling of missing values (e.g., default values, imputations).\n   * Confirm removal of duplicates and enforcement of uniqueness constraints\n6. Check for compliance with Microsoft Fabric best practices.\n7. Verifies the alignment with Business Requirements\n\nOutput Format :\n1. Data Mapping Review\n\u2705 Correctly mapped Silver to Gold Layer tables\n\u274c Incorrect or missing mappings\n\n2. Data Consistency Validation\n\u2705 Properly mapped fields ensuring consistency\n\u274c Misaligned or inconsistent mappings\n\n3. Dimension Attribute Transformations\n\u2705 Correct category mappings and hierarchy structures\n\u274c Incorrect or incomplete transformations\n\n4. Data Validation Rules Assessment\n\u2705 Deduplication logic and format standardization applied correctly\n\u274c Issues with validation logic or missing checks\n\n5. Data Cleansing Review\n\u2705 Proper handling of missing values and duplicates\n\u274c Inadequate cleansing logic or missing constraints\n\n6. Compliance with Microsoft Fabric Best Practices\n\u2705 Fully adheres to Fabric best practices\n\u274c Violations of recommended design and implementation guidelines\n\n7. Alignment with Business Requirements\n\u2705 Gold Layer aligns with Business Requirements\n\u274c Missing attributes or incorrect transformations affecting business logic\nINPUT:\nUse Fabric Gold Transformation Data Mapping output as input file",
          "modelName": "model"
        }
      ],
      "realmId": 1
    }
  },
  "status": "SUCCESS"
}