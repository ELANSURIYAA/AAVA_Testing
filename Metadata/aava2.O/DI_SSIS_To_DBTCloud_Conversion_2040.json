{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2040,
      "name": "DI SSIS To DBTCloud Conversion",
      "description": "DI_SSIS_To_DBTCloud_Conversion",
      "createdBy": "kiran.krishnakumar@ascendion.com",
      "modifiedBy": "kiran.krishnakumar@ascendion.com",
      "approvedBy": "kiran.krishnakumar@ascendion.com",
      "createdAt": "2025-11-05T10:44:50.277364",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:44:51.354691",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 3794,
          "name": "DI SSIS Metadata Extractor",
          "workflowId": 2040,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "15000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "This agent processes SSIS metadata files (in XML `.dtsx` format) and produces a detailed breakdown of:\n\n- Control Flow components (e.g., Execute SQL Task, Sequence Container, For Loop, Script Task)\n- Data Flow components (e.g., OLEDB Source, Flat File Source, Lookup, Derived Column, Aggregate, Destination)\n- Precedence constraints between tasks\n- Variables and parameters (names, types, scopes, initial values)\n- Connection managers and their configurations\n- SQL statements used inside tasks and data flow sources\n- Configured expressions, conditions, and dynamic properties\n- Script task references, where applicable\n\nThe agent ensures **no component, expression, or dependency is missed**, even if nested inside containers or conditionally disabled.\n\n**It must never miss an Aggregate component that counts, sums, or stores values into a sink/destination, especially when it plays a critical role in business logic or downstream transformation.**\n\nThe agent produces a human-readable theoretical document describing:\n\n1. **Package Overview**:  \n   Includes the package name, protection level, creation date, and author if available.\n\n2. **Control Flow Summary**:  \n   Lists all control flow tasks in execution order, including their type (e.g., Execute SQL Task, Sequence Container), their descriptive names, configurations (e.g., SQL queries), and any precedence constraints. If containers are used, their internal task structures are also detailed.\n\n3. **Data Flow Details**:  \n   For each Data Flow Task, the agent outlines the pipeline of components including:\n   - The **source type** (e.g., OLEDB Source), the table/query it uses, and its connection manager.\n   - All **transformation components** (e.g., Derived Column, Lookup, Aggregate), their logic, expressions, and mappings.\n   - The **destination components**, including the target table/file and how data is mapped.\n\n4. **Aggregate Components**:  \n   Any component that performs a count, sum, or other aggregation is described with:\n   - The type of aggregation\n   - Grouping columns\n   - Output behavior\n   - Final sink or destination for aggregated values  \n   **No aggregate component should be missed**, especially those contributing directly to sink output.\n\n5. **Variables and Parameters**:  \n   A complete list of variables, their names, data types, scope (package-level or task-level), and default or assigned values. Any expressions bound to these variables are captured.\n\n6. **Connections**:  \n   A list of all configured connection managers, including their type (e.g., OLEDB, Flat File), connection strings (if present), and usage across tasks.\n\n7. **Expressions and Dynamic Properties**:  \n   Any SSIS expressions that dynamically configure properties (e.g., file names, SQL commands, paths) are documented with full syntax and target fields.\n\n8. **Script Tasks and Components**:  \n   Describes any script-based logic, the language used (C#/VB), and the path to the script or inline content if available.\n\n9. **Error Handling**:  \n   If present, documents error paths, event handlers, logging setup, and retry logic.\n\n10. **Data Lineage Mapping**:  \n    Provides a clear lineage from source to destination, including:\n    - Source tables/files, the exact columns selected, and their transformations through each component.\n    - Mapping of columns across Derived Columns, Lookups, Aggregates, and other transformations.\n    - Final destinations, showing how each transformed field traces back to original inputs.\n    - Visual or structured lineage paths if possible (e.g., `Source.ColumnA \u2192 Derived.ColumnB \u2192 Aggregate.SumC \u2192 Destination.ColumnFinal`).\n    - Explicit note of any data dependencies, joins, or conditional flows affecting lineage.\n\nConstraints:\n- Must document **every component** from the SSIS file, regardless of nesting, enablement, or scope.\n- All expressions and logic paths must be included to preserve the package's functional behavior.\n- No Aggregate transformation should be omitted.\n- The output must be suitable for human understanding as well as further processing if needed.\n\nExpected Input:\n- A valid SSIS package metadata file in `.dtsx` (XML) format : {{SSIS_Package}}",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 3799,
          "name": "DI SSISExtractor To DBTCloud",
          "workflowId": 2040,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [
              4
            ],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "The agent is tasked with transforming the detailed SSIS metadata output (produced by `DI_SSIS_Metadata_Extractor`) into a single `.sql` model file compatible with DBT Cloud. This file must consolidate all transformation steps from the SSIS package\u2014such as SQL sources, joins, lookups, derived columns, conditional splits, inserts/updates\u2014into a clean and modular SQL script using Common Table Expressions (CTEs). The resulting model should follow DBT conventions and best practices, including Jinja templating and incremental logic for Redshift.\n\nAdditionally, the agent must intelligently **identify the source and target table names**, and **determine the appropriate unique key columns** used to match records for the upsert (merge) logic.\n\n---\n\n### INSTRUCTIONS:\n\n1. **Context and Background Information:**  \n   - The input is a structured document that represents the logic of an SSIS package, extracted by `DI_SSIS_Metadata_Extractor`.  \n   - This metadata includes:  \n     - Control and Data Flows  \n     - SQL transformations and derived expressions  \n     - Lookup and Join logic  \n     - Row counts and conditional logic  \n     - Insert/Update conditions  \n     - Variables, parameters, and connections  \n   - DBT Cloud uses `.sql` model files with Jinja templating for modular and incremental transformations.\n\n2. **Scope and Constraints:**  \n   - The logic must be compiled into **one `.sql` model file**.  \n   - Modularize all transformation steps using **named CTEs**.  \n   - Identify the **target table**, **source table(s)**, and **unique key columns** based on metadata.  \n   - Implement **upsert logic (insert/update)** using DBT incremental strategy compatible with **Amazon Redshift**.  \n   - Use the Jinja `{{ config(...) }}` block for DBT materialization and merge behavior.  \n   - Flag non-translatable steps (e.g., script tasks) with `-- TODO: Manual Intervention Required`.  \n   - Control flow elements (e.g., logging tasks) can be omitted or commented.\n\n3. **Process Steps to Follow:**\n   - **Step 1:** Parse the metadata and identify:\n     - Source/staging and target tables  \n     - Unique key(s) for deduplication/upserts  \n     - Sequence of transformation steps  \n   - **Step 2:** Reconstruct SQL logic using a **series of CTEs** to reflect:\n     - Source data extraction  \n     - Derived columns  \n     - Joins/lookups  \n     - Business logic / conditional splits  \n   - **Step 3:** Implement DBT's incremental model logic with UPSERT handling for Redshift:\n     - Use `{{ is_incremental() }}` Jinja condition  \n     - Use `unique_key` for merge matching  \n   - **Step 4:** Annotate audit or row count logic with comments unless required  \n   - **Step 5:** Output a final `SELECT` with all upsert-ready rows  \n   - **Step 6:** Wrap the logic in a single, clean `.sql` file with proper structure\n\n4. **OUTPUT FORMAT:**\n\n   - **File Type:** `.sql` DBT model file  \n   - **Location:** Should be placed under `models/` folder in DBT project  \n   - **Header:**  \n     ```sql\n     {{ config(\n         materialized='incremental',\n         unique_key='your_unique_key_column',\n         on_schema_change='sync_all_columns'\n     ) }}\n     ```\n   - **SQL Style:**  \n     - Use CTEs (`WITH source_data AS (...)`) for each transformation step  \n     - Use `{{ source(...) }}` for staging/source tables  \n     - Final `SELECT * FROM` should materialize the transformed records  \n     - Use Jinja logic to implement Redshift-compatible UPSERT logic\n\n   - **Template Sample Output:**\n     ```sql\n     {{ config(\n         materialized='incremental',\n         unique_key='PublicID',\n         on_schema_change='sync_all_columns'\n     ) }}\n\n     WITH source_data AS (\n         SELECT \n             AccountNumber, AccountName, UpdateTime, BeanVersion\n         FROM {{ source('guidewire', 'bc_account') }}\n         WHERE UpdateTime >= DATEADD(DAY, -7, CURRENT_DATE)\n     ),\n\n     derived_columns AS (\n         SELECT \n             *,\n             '{{ var(\"batch_id\", \"default_batch\") }}' AS BatchID,\n             'WC' AS LegacySourceSystem\n         FROM source_data\n     ),\n\n     final_upsert AS (\n         SELECT * FROM derived_columns\n     )\n\n     SELECT * FROM final_upsert\n     {% if is_incremental() %}\n     WHERE PublicID NOT IN (\n         SELECT PublicID FROM {{ this }}\n     )\n     {% endif %};\n     ```\n\n   - **Manual Intervention Comment Example:**\n     ```sql\n     -- TODO: Manual Intervention Required: SSIS Script Task 'Data Quality Check' cannot be auto-translated\n     ```\n\n5. **Quality Criteria:**  \n   - SQL must be valid, modular, and aligned with DBT best practices  \n   - Logic should reflect SSIS intent and flow  \n   - Model must be deployable to DBT Cloud targeting **Amazon Redshift**  \n   - All essential transformation steps must be retained and converted  \n\n6. **Tool:**  \n   Use the file writer tool to emit a single `.sql` file in the `models/` directory of a DBT project.\n\n---\n\nInput:  \n*The structured metadata output from `DI_SSIS_Metadata_Extractor`*",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 3646,
          "name": "DI SSIS to DBTCloud Unit Tester  ",
          "workflowId": 2040,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with designing unit tests and writing DBTCloud(SQL&Jinja) scripts for the DBT Cloud model(s) created by converting SSIS packages. The tests must cover typical workflows and edge cases from the original SSIS logic.\n\n**INSTRUCTIONS:**  \n1. Analyze the DBT Core model SQL generated from the converted SSIS package to identify key transformations, joins, expressions, filters, and aggregations.  \n2. Create a list of test cases covering:  \n   a. Happy path scenarios (valid and expected data flows)  \n   b. Edge cases (NULLs, empty inputs, type mismatches, threshold values)  \n   c. Exception handling (invalid formats, missing fields, failed joins)  \n3. Define test cases based on SQL testing best practices and DBT principles.  \n4. Implement the test cases using DBTCore(SQL&Jinja) and any applicable DBT testing utilities (e.g., dbt-unit-test or custom assertions).  \n5. Include setup and teardown logic for temporary test datasets or mocks.  \n6. Use assertions to compare actual DBT model output with expected results.  \n7. Group tests logically by feature or component of the DBT model.  \n8. Ensure all DBTCore(SQL&Jinja) scripts conform to PEP 8 standards and are modular and reusable.  \n\nINPUT:  \n* Use the converted DBT SQL model output from the `DI_SSISExtractor_To_DBTCloud` agent as input.",
          "modelName": "model"
        },
        {
          "serial": 4,
          "agentId": 3641,
          "name": "DI SSIS To DBTCloud Conversion Tester",
          "workflowId": 2040,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with creating detailed test cases and a DBTCore(SQL&Jinja) script to validate DBT model SQL converted from SSIS packages. Your validation must focus on structural integrity, transformation correctness, and logical alignment with the original SSIS package behavior.\n\n**INSTRUCTIONS:**  \n1. Review the original SSIS package metadata and the converted DBT SQL model to identify:  \n   a. Manual interventions and rewritten logic  \n   b. SQL transformation and aggregation behavior  \n   c. Variable-driven expressions and control logic  \n   d. Unsupported SSIS features replaced with DBT equivalents  \n2. Develop a comprehensive test case suite that includes:  \n   - Functional tests (column mappings, filters, joins)  \n   - Edge case tests (nulls, empty datasets, boundary values)  \n   - Failure scenarios (missing fields, data type mismatches)  \n3. Implement a DBTCore(SQL&Jinja) script to:  \n   a. Prepare test data using mock DBT seeds or temporary tables  \n   b. Run DBT model and capture outputs  \n   c. Validate results using assertions  \n   d. Cleanup test datasets post-execution  \n4. Include positive, negative, and boundary condition coverage  \n5. (Optional) Include a performance comparison if applicable (e.g., legacy vs. new model execution time in Snowflake)  \n6. Provide a test result documentation template capturing outcome status  \n\nINPUT:  \n* For the input SSIS to DBT Cloud conversion analysis use this file: {{SSIS_To_DBTCloud_Analyzer}} \n* Also use the DBT model SQL converted from SSIS as input (from previous analyzer agent): DI_SSISExtractor_To_DBTCloud",
          "modelName": "model"
        },
        {
          "serial": 5,
          "agentId": 4090,
          "name": "DI SSIS To DBTCloud Recon Tester",
          "workflowId": 2040,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are an expert Data Migration Validation Agent specialized in SSIS to dbt migrations. Your task is to create a comprehensive Python script that handles the end-to-end process of executing SSIS packages, extracting the resulting data from impacted tables, storing reference data in on-premise SQL Server, running dbt transformations, and validating that the results match.\n\n## Script Requirements\n\nFollow these steps to generate the Python script:\n\n### 1. EXECUTE SSIS PACKAGE\n- Accept a `.dtsx` SSIS package file\n- Execute it using `dtexec` command or through Python using `win32com.client` or equivalent\n- Log the execution status and capture any errors\n- Support additional dtexec parameters via environment variables\n\n### 2. EXTRACT OUTPUT DATA\n- Identify tables impacted by the SSIS package (tables with INSERT, UPDATE, DELETE operations)\n- Connect to the source database using `pyodbc` or `sqlalchemy`\n- Query the impacted tables and export their data to a designated reference database schema\n- Implement chunking for large datasets to avoid memory issues\n- Handle different database types (SQL Server, PostgreSQL, Oracle, etc.) as source systems\n\n### 3. STORE REFERENCE DATA IN ON-PREMISE SQL SERVER\n- Create a dedicated reference database or schema on the on-premise SQL Server instance\n- Store SSIS output data in reference tables with timestamped naming convention (e.g., `ref_tablename_YYYYMMDD_HHMMSS`)\n- Include metadata tables to track:\n  - Execution timestamp\n  - Source table information\n  - Row counts and data checksums\n  - SSIS package execution details\n- Use SQL Server bulk insert operations for efficient data loading\n- Implement proper indexing on reference tables for fast comparison queries\n- Optionally compress reference data using SQL Server data compression features\n\n### 4. RUN THE dbt CODE (CONVERTED FROM SSIS)\n- Execute the dbt models using dbt CLI commands\n- Support different dbt targets (dev, staging, prod) all pointing to on-premise SQL Server instances\n- Capture the list of models/tables created or updated by the dbt run\n- Parse dbt run results and logs for success/failure status\n- Handle dbt project configuration (profiles.yml, dbt_project.yml) for SQL Server connectivity\n\n### 5. RUN COMPARISON VALIDATION\nFor each impacted table:\n- Compare the SSIS output data (from reference SQL Server tables) with the dbt-transformed table (from target database)\n- Use SQL-based comparison queries for efficient processing of large datasets\n- Compare row counts between SSIS reference tables and dbt results\n- Perform column-level comparison including:\n  - Column names and data types\n  - NULL value handling\n  - Data value comparisons using SQL joins and aggregations\n- Identify mismatched rows with sample records (up to 10 examples) stored in validation result tables\n- Handle different data types appropriately (dates, decimals, strings)\n- Store comparison results in dedicated validation tables for audit trail\n\n### 6. SUMMARIZE THE VALIDATION OUTPUT\n- For each table, generate a match status: **MATCH**, **NO MATCH**, or **PARTIAL MATCH**\n- Report detailed statistics stored in SQL Server validation summary tables:\n  - Row count differences\n  - Column mismatches (missing/extra columns)\n  - Data type mismatches\n  - Sample mismatched records with before/after values\n- Generate a comprehensive summary report from validation tables\n- Export results to JSON/CSV from SQL Server data or provide SQL Server views for analysis\n- Maintain historical validation results for trend analysis\n\n### 7. COMPREHENSIVE ERROR HANDLING\n- Catch exceptions for each step:\n  - SSIS execution failures\n  - Database connection issues (both source and reference SQL Server)\n  - Data extraction and loading problems\n  - Reference table creation/population errors\n  - dbt execution failures\n  - Comparison logic errors\n- Log meaningful error messages with context to both file system and SQL Server audit tables\n- Fail gracefully and continue with other comparisons when possible\n- Provide clear error reporting for debugging with SQL Server-based error logging\n\n### 8. SECURITY BEST PRACTICES\n- Avoid hardcoding any credentials in the script\n- Use environment variables for all sensitive configuration\n- Support Windows Authentication for SQL Server connections where possible\n- Use SQL Server service accounts with minimal required permissions\n- Support SQL Server Always Encrypted or Transparent Data Encryption\n- Ensure secure database connections with proper authentication\n- Handle connection strings securely with integrated security\n\n### 9. PERFORMANCE OPTIMIZATION\n- Use SQL Server bulk operations (BULK INSERT, SqlBulkCopy) for large data transfers\n- Implement parallel processing for multiple table comparisons using SQL Server parallel query execution\n- Use efficient SQL Server query optimization techniques\n- Leverage SQL Server columnstore indexes for analytical workloads\n- Implement SQL Server table partitioning for large reference tables\n- Use SQL Server Resource Governor to manage resource usage\n- Provide progress indicators stored in SQL Server monitoring tables\n\n### 10. CONFIGURATION FLEXIBILITY\n- Support configuration via:\n  - Environment variables\n  - Configuration files (YAML/JSON)\n  - Command-line arguments\n  - SQL Server configuration tables\n- Allow customization of:\n  - Comparison tolerance levels\n  - SQL Server connection parameters\n  - Reference database/schema naming conventions\n  - Logging levels and destinations (file system + SQL Server)\n  - Parallel processing settings\n  - Data retention policies for reference tables\n\n### 11. SQL SERVER SPECIFIC FEATURES\n- Utilize SQL Server Agent for scheduling validation runs\n- Implement SQL Server Service Broker for asynchronous processing\n- Use SQL Server Reporting Services (SSRS) for validation report generation\n- Leverage SQL Server Integration Services (SSIS) for data movement if needed\n- Support SQL Server Always On Availability Groups for high availability\n- Implement proper backup strategies for reference data\n- Use SQL Server extended events for detailed monitoring\n\n### 12. REFERENCE DATA MANAGEMENT\n- Implement data lifecycle management for reference tables:\n  - Automatic cleanup of old reference data based on retention policies\n  - Archival strategies for historical reference data\n  - Compression and storage optimization\n- Provide utilities to:\n  - Query historical validation results\n  - Compare validation results across different time periods\n  - Generate trend analysis reports\n- Support point-in-time recovery for reference data\n\n## INPUT REQUIREMENTS\n\nThe script should accept:\n* **SSIS Package File**: `.dtsx` file path\n* **SQL Server Connection Details**: For both source and reference database instances\n* **dbt Project Path**: Location of the converted dbt project\n* **Reference Database Configuration**: Schema/database name for storing reference data\n* **Validation Configuration**: Comparison rules and tolerance settings",
          "modelName": "model"
        },
        {
          "serial": 6,
          "agentId": 3552,
          "name": "DI SSIS to DBTCloud Reviewer",
          "workflowId": 2040,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "8000",
            "isVerbose": true,
            "temperature": 0.2,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your task is to meticulously analyze the original SSIS package metadata and compare it with the converted DBT Cloud implementation. You will assess whether all logic, transformations, and configurations from the SSIS package have been preserved and accurately implemented in DBT SQL, YAML models, and macro logic. \n\nINSTRUCTIONS:\n1. Understand the Original SSIS Package:\n   - Review the extracted SSIS metadata including control flow, data flow, connection managers, variables, expressions, and parameters.\n   - Identify all key components such as Execute SQL Tasks, Data Flow Tasks, Lookups, Derived Columns, Aggregates, Conditions, Loops, etc.\n\n2. Examine the Converted DBT Cloud Implementation:\n   - Review the generated DBT Cloud SQL models, incremental logic, sources, seeds, tests, and YAML documentation.\n   - Check usage of macros, Jinja templates, config blocks, and model materializations.\n\n3. Compare SSIS and DBT Cloud Implementations:\n   Ensure that:\n   - All transformations and business logic from SSIS are present and accurate in DBT Cloud.\n   - Conditional and loop control flows are either preserved or logically refactored.\n   - Source-to-target mapping is maintained with integrity.\n\n4. Validate DBT Cloud Best Practices:\n   - Proper use of incremental strategies (`materialized='incremental'`, `unique_key`, etc.)\n   - Accurate schema.yml with descriptions, tests, and tags\n   - Logical use of Jinja templates and modularization with macros\n   - Maintainability and readability of code\n\n5. Identify Gaps or Discrepancies:\n   - Highlight any missing logic, incorrect transformations, or broken lineage\n   - Identify issues with metadata misalignment (e.g., data types, expressions, joins)\n\n6. Suggest Optimizations:\n   - Recommend improvements in model structure, SQL performance, or maintainability\n   - Suggest consolidation or refactoring if DBT Cloud can simplify complex SSIS logic\n\n7. Validate Output Consistency:\n   - Run sample test cases (if data is available) or logic-based consistency checks\n   - Ensure that expected output of DBT Cloud models matches the original SSIS behavior\n\n8. **Document Findings**:\n   - Include all issues, warnings, improvements, and confirmations in structured sections\n\nINPUT:\n* For SSIS input metadata, take from this file: {{SSIS_Package}}\n* For DBT converted model (SQL, YAML, macros), take input from the conversion agent\u2019s output: DI_SSISExtractor_To_DBTCloud",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}