{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 2495,
      "name": "DI TSQL DQReviewAgent",
      "description": "To thoroughly review and validate the outputs generated by the DI_OptimiseTSQLScript Agent (optimized T-SQL scripts) and the DI_DetermineDQRulesFromInstructions Agent (Data Quality rules with descriptions and rationale), ensuring functional alignment, adherence to coding standards, maintainability, and performance efficiency.\n",
      "createdBy": "dipali.nale@ascendion.com",
      "modifiedBy": "dipali.nale@ascendion.com",
      "approvedBy": "dipali.nale@ascendion.com",
      "createdAt": "2025-11-05T11:01:45.691508",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T11:01:46.750654",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 4302,
          "name": "DI DetermineDQRulesFromInstructions",
          "workflowId": 2495,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "24000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your task is to perform a comprehensive, row-by-row analysis of the provided Excel sheet containing survey data on compensation models. You MUST process every single data category, entity, and element\u2014no summaries, no assumptions, no shortcuts.\n\n\u2022\tCRITICAL EXECUTION RULES:\n\t\u2022\tNEVER provide summary tables without actual processing of each row in the Excel sheet.\n\t\u2022\tNEVER assume element definitions or DQ rules\u2014derive them from the sheet or industry best practices.\n\t\u2022\tALWAYS show the actual data category, entity, and element name as listed in the input Excel.\n\t\u2022\tALWAYS confirm that each DQ rule is relevant, precise, and justified.\n\t\u2022\tPROCESS ONE ROW AT A TIME with full visibility.\n\n\u2022\tMANDATORY EXECUTION STEPS:\n\n\tSTEP 1: FILE INGESTION\n\t\t-  Ingesthe Excel sheet \n\t\t- For each row, extract Data Category, Entity, and Element Name exactly as present in the sheet.\n\n\tSTEP 2: GUIDELINES CHECK\n\t\t- For each element, check if a DQ rule is specified in the 'Guidelines for Rules' (provided as a reference or separate sheet).\n\t\t- If a rule exists, use it verbatim and cite the guideline source in Remarks.\n\t\t- If no rule exists, define a DQ rule based on industry best practices for survey data (e.g., completeness, validity, consistency, uniqueness, referential integrity).\n\n\tSTEP 3: DQ RULE DEFINITION\n\t\t- For each element, specify:\n\t\t\t\u2022\tData Quality Rule Name (e.g., \"Completeness Check\", \"Value Range Validation\", \"Referential Integrity\")\n\t\t\t\u2022\tData Quality Rule Description (detailed, actionable, and specific to the element)\n\t\t\t\u2022\tRemarks (explain how the rule was determined: guideline reference or industry best practice, with rationale)\n\n\tSTEP 4: OUTPUT GENERATION\n\t\t- For each processed row, generate an output row with the following columns:\n\t\t\t\u2022\tData Category\n\t\t\t\u2022\tEntity\n\t\t\t\u2022\tElement Name\n\t\t\t\u2022\tData Quality Rule Name\n\t\t\t\u2022\tData Quality Rule Description\n\t\t\t\u2022\tRemarks\n\n\tSTEP 5: QUALITY ASSURANCE\n\t\t- Ensure every rule is precise, relevant, and aligned with best practices for survey-based datasets.\n\t\t- Validate output for completeness and clarity.\n\n\u2022\tERROR HANDLING:\n\t\u2022\tIf a row is missing required fields, note this in the Remarks and propose a DQ rule to address missingness.\n\t\u2022\tIf an element is ambiguous, document the ambiguity and propose a conservative DQ rule.\n\t\u2022\tNEVER skip rows due to errors; always attempt to define a DQ rule.\n\n\n\u2022\tREQUIRED VISIBILITY:\n\tFor each row, you MUST show:\n\t1.\tThe actual Data Category, Entity, and Element Name as listed in the Excel.\n\t2.\tThe DQ Rule Name and Description.\n\t3.\tRemarks explaining the rule determination.\n\n\u2022\tOUTPUT FORMAT:\n\tOutput MUST be in Markdown table format with the following columns:\n| Data Category | Entity | Element Name | Data Quality Rule Name | Data Quality Rule Description | Remarks |\n- Each cell should be clearly formatted.\n- Rule descriptions must be actionable and specific.\n- Remarks must reference either the guideline or the industry best practice used.\n\n\u2022\tQUALITY CRITERIA:\n\t- No missing fields in output.\n\t- Rules are precise, relevant, and justified.\n\t- Output is readable, well-formatted, and suitable for technical and business stakeholders.\n\nINSTRUCTION FOR GITHUB TOOLS:\n1.Use GITHUB file reader tool to read the input file from gihub \n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_ Create_T-SQLDQRules\"\nInput\n{{github_credintials_op}} -for the user github credentials use this input from user",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 4183,
          "name": "DI OptimizeTSQLScript",
          "workflowId": 2495,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "24000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your task is to take the outputs of the previous agent\u2014comprising all T-SQL quality checks\u2014and optimize, deduplicate, and consolidate them into a single, cohesive set of production-ready T-SQL scripts. You must ensure these scripts are efficient, reliable, and adhere to best practices for production environments.\n\nINSTRUCTIONS:\n\n**STEP 1: INPUT ANALYSIS**\n- Review the previous agent's output containing all T-SQL quality checks.\n- Identify all unique checks, including but not limited to: data validation, referential integrity, null checks, data type enforcement, business rule validation, and performance-related checks.\n- Note any redundant, overlapping, or conflicting checks.\n\n**STEP 2: CONSOLIDATION & OPTIMIZATION**\n- Remove duplicate or overlapping checks, ensuring each quality rule is represented only once.\n- Refactor checks for efficiency:\n  - Use set-based operations instead of cursors or row-by-row processing where possible.\n  - Combine related checks into single queries or procedures for maintainability.\n  - Ensure all scripts are idempotent and can be safely re-run.\n- Standardize naming conventions, error handling, and logging mechanisms.\n- Parameterize scripts where applicable for reusability and flexibility.\n\n**STEP 3: PRODUCTION READINESS ENHANCEMENTS**\n- Add comprehensive error handling and transaction management to prevent partial updates or data corruption.\n- Implement logging for failed checks, including timestamp, error details, and affected records.\n- Ensure scripts are compatible with the target SQL Server version and follow organizational security and performance best practices.\n- Include comments and documentation within the scripts for clarity and maintainability.\n\n**STEP 4: OUTPUT STRUCTURING**\n- Organize the final scripts into logical sections:\n  1. **Pre-Check Setup**: Variable declarations, temp tables, configuration.\n  2. **Quality Checks**: Each check clearly labeled and documented.\n  3. **Error Handling & Logging**: Centralized error capture and reporting.\n  4. **Summary Reporting**: Output summary of all checks and their results.\n- Provide a summary table (in markdown) listing each quality check, its purpose, and its status (optimized/merged/new/removed).\n\n**STEP 5: VALIDATION**\n- Review the consolidated scripts for completeness and correctness.\n- Ensure all original quality rules are represented and optimized.\n- Validate scripts for syntax, performance, and logical correctness.\n\n**OUTPUT FORMAT:**\n\n- **T-SQL Scripts**:  \n  - Format: Plain text, with clear section headers and inline documentation.\n  - Structure: As per Step 4 above.\n  - Quality: Must be production-ready, efficient, and maintainable.\n\n**QUALITY CRITERIA:**\n- No redundant or duplicate checks.\n- All scripts are idempotent and safe for production use.\n- Clear, maintainable, and well-documented code.\n- Comprehensive error handling and logging.\n- All original quality rules are accounted for.\n\n**KNOWLEDGE BASE**\n-Added knowledge base for better understanding on TSQL Optimization \n\n**INSTRUCTION FOR GITHUB TOOLS:**\n1.Read input from previous agent \"DI_ Create_T-SQLDQRules\"\n2.Use the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_OptimiseTSQLScript\"\nInput\n{{github_credintials}} -for the user github credentials use this input from user\n",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 4286,
          "name": "DI TSQL DQReviewAgent",
          "workflowId": 2495,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4500",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Your task is to perform a comprehensive, step-by-step review and comparison of the outputs from the DI_OptimiseTSQLScript Agent output and the DI_DetermineDQRulesFromInstructions Agent output. The review must confirm that the T-SQL scripts faithfully implement all DQ rules, adhere to best practices, and are optimized for maintainability and performance.\n\nINSTRUCTIONS:\n\nSTEP 1: INPUT COLLECTION  \n- Obtain the latest outputs from both agents:  \n  - The optimized T-SQL script(s) from DI_OptimiseTSQLScript Agent.  \n  - The DQ rules (with descriptions and rationale) from DI_DetermineDQRulesFromInstructions Agent.  \n- Clearly display both outputs in your review for full transparency.\n\nSTEP 2: FUNCTIONAL ALIGNMENT VALIDATION  \n- For each DQ rule:  \n  - Identify the corresponding logic in the T-SQL script.  \n  - Confirm that the script implements the rule precisely as described, including edge cases and exceptions.  \n  - Note any mismatches, missing implementations, or ambiguities.\n- For each section of the T-SQL script:  \n  - Ensure every validation, transformation, or filter aligns with a documented DQ rule.\n- Document all findings with references to specific lines or sections.\n\nSTEP 3: CODING STANDARDS & MAINTAINABILITY REVIEW  \n- Assess the T-SQL script for adherence to industry coding standards:  \n  - Naming conventions  \n  - Use of comments and documentation  \n  - Modularity and readability  \n  - Avoidance of deprecated or non-standard constructs  \n- Evaluate maintainability:  \n  - Are logic blocks clearly separated and documented?  \n  - Is the script structured for easy updates and debugging?  \n  - Are magic numbers or hard-coded values avoided?\n- Highlight any violations or improvement opportunities.\n\nSTEP 4: PERFORMANCE EFFICIENCY ANALYSIS  \n- Review the script for performance best practices:  \n  - Efficient use of indexes  \n  - Avoidance of unnecessary subqueries or cursors  \n  - Proper use of set-based operations vs. row-by-row processing  \n  - Query plan considerations (e.g., joins, filters, aggregations)\n- Suggest optimizations if bottlenecks or inefficiencies are detected.\n\nSTEP 5: OUTPUT STRUCTURE & FINAL REPORT  \n- Summarize your findings in a structured report as per the OUTPUT FORMAT below.\n- Clearly indicate:  \n  - Alignment status for each DQ rule  \n  - Coding standards and maintainability assessment  \n  - Performance review and recommendations  \n  - Any critical issues or blockers\n\nOUTPUT FORMAT:  \n- The report must be delivered in Markdown format.\n- Structure your report as follows:\n\n```\n# Data Quality & T-SQL Script Review Report\n\n## 1. Inputs Reviewed\n### 1.1 Optimized T-SQL Script\n[Paste or reference the full script here]\n\n### 1.2 Data Quality Rules\n| Rule ID | Description | Rationale |\n|---------|-------------|-----------|\n| ...     | ...         | ...       |\n\n## 2. Functional Alignment Matrix\n| DQ Rule ID | Implemented in Script? | Script Reference (line/section) | Notes/Discrepancies |\n|------------|-----------------------|---------------------------------|---------------------|\n| ...        | Yes/No                | ...                             | ...                 |\n\n## 3. Coding Standards & Maintainability Review\n- [ ] Naming conventions: (Pass/Fail/Comments)\n- [ ] Comments & documentation: (Pass/Fail/Comments)\n- [ ] Modularity: (Pass/Fail/Comments)\n- [ ] Readability: (Pass/Fail/Comments)\n- [ ] Use of best practices: (Pass/Fail/Comments)\n- [ ] Maintainability summary: (Detailed notes)\n\n## 4. Performance Efficiency Review\n- [ ] Index usage: (Pass/Fail/Comments)\n- [ ] Query structure: (Pass/Fail/Comments)\n- [ ] Set-based operations: (Pass/Fail/Comments)\n- [ ] Potential bottlenecks: (List and recommendations)\n- [ ] Performance summary: (Detailed notes)\n\n## 5. Issues & Recommendations\n- List all critical issues, gaps, or improvement areas.\n- Provide actionable recommendations for remediation.\n\n## 6. Final Assessment\n- Overall alignment: (Aligned/Partial/Misaligned)\n- Readiness for deployment: (Ready/Needs revision)\n```\n\nQUALITY CRITERIA:  \n- All DQ rules and script logic must be explicitly cross-referenced.\n- The report must be detailed, actionable, and free from ambiguity.\n- Use clear, professional language and formatting.\n- Do not omit or summarize steps; provide full transparency.\n\n**INSTRUCTION FOR GITHUB TOOLS:**\nUse the Github file write tool to upload the output file in github Output Folder \nOutput_File_Name=Output_\"DI_TSQL_DQReviewAgent\"\nInput\n{{github_credintials_review}} -for the user github credentials use this input from user",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}