{
  "data": {
    "message": "Workflow with agents retrieved successfully",
    "workFlowDetail": {
      "id": 1978,
      "name": "DI SSIS Doc&Analyse",
      "description": "SSIS Documentation and Analyse ",
      "createdBy": "elansuriyaa.p@ascendion.com",
      "modifiedBy": "elansuriyaa.p@ascendion.com",
      "approvedBy": "elansuriyaa.p@ascendion.com",
      "createdAt": "2025-11-05T10:42:27.490432",
      "modifiedAt": "2025-12-03T15:03:31.224436",
      "approvedAt": "2025-11-05T10:42:28.688749",
      "status": "APPROVED",
      "isDeleted": false,
      "parentId": -1,
      "workflowConfigs": {
        "managerLlm": [
          {}
        ],
        "enableAgenticMemory": false
      },
      "workflowAgents": [
        {
          "serial": 1,
          "agentId": 3628,
          "name": "DI SSIS Documentation JSON ",
          "workflowId": 1978,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 154,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Analyze the provided SSIS script to generate detailed, program-specific documentation. The goal is to produce **accurate, readable, and context-rich markdown documentation** for both technical and non-technical stakeholders. Avoid generic language or templates. Base all sections on actual code logic, comments, data structure, and transformation flow. The respective output should be in a proper json format.\n\n## 1. Overview of Program  \n_A 5-line paragraph summarizing the script\u2019s business context, goals, and importance._  \nThen, provide a program-specific description explaining:\n- The purpose of the SSIS script in its business context\n- What business problems it addresses\n- Which systems it connects to or serves\n- High-level mention of key operations it performs (e.g., arm-level metric rollups from clinical data)\n\n## 2. Code Structure and Design  \n[5-line intro]\n[Detailed paragraph explaining how the code is structured, key SSIS elements used (e.g., specific RECORD definitions), and reusable logic.]\n\n## 3. Data Flow and Processing Logic  \n_A 5-line summary about tracing data from input to output._  \nThen describe:  \n- Every input dataset used (list all explicitly with full names)  \n- The flow of data: from raw inputs, through transformations, joins, filtering, aggregation  \n- Intermediate datasets and their purpose  \n- Conditional logic or branching  \n- Final transformations and output preparation  \n\n## 4. Data Mapping  \n_A 5-line intro on traceability and field-level transparency._  \nThen generate a markdown table with the following headers:  \n| Target Dataset Name | Target Field Name | Source Dataset Name | Source Field Name | Data Type |Transformation Logic | Business Purpose |\n\n## 5. Performance Optimization Strategies  \n_A 5-line paragraph about performance patterns and tuning._  \nThen write a paragraph (not subheadings) covering:\n- JOIN strategies used and justification\n- Any indexing, filtering, or partitioning applied\n- Parallelism or data locality considerations\n- How script is tuned for large dataset performance\n\n## 6. Technical Elements and Best Practices  \n_A 5-line intro on code quality and maintainability._  \nThen, in paragraph form, describe:\n- What components/modules are used from libraries or external systems\n- Error handling mechanisms (explicit or implicit)\n- How the code follows modularity, naming conventions, and reusability\n- Any logging, recovery, or quality control mechanisms used\n\n## 7. Complexity Analysis  \n[5-line intro]\n[Paragraph with quantitative metrics: actual line count, number of joins, transforms, conditions, and outputs, followed by a complexity score (0\u2013100) with explanation.]\n\nNumber of Lines: Count of total lines in the SSIS script.\n\nDatasets Used: Number of datasets imported or created.\n\nJoins Used: Number of JOIN operations and their types (HASH, LOOKUP, ALL, MERGE).\n\nTRANSFORM Functions: Number of TRANSFORMs used throughout the script.\n\nRECORD Definitions: Count of record structures defined for datasets.\n\nOUTPUT Statements: Total number of OUTPUT instructions.\n\nConditional Logic: Count of IF statements, CASE, ASSERT, FAILMESSAGE, and other control logic.\n\nIndexing and Lookups: Number of INDEX and BUILD operations.\n\nFunction Calls: Number of UDFs or MODULE references used.\n\nPerformance Controls: Usage of STREAMED, LOCAL, DISTRIBUTE, PARALLEL blocks.\n\nExternal Dependencies: Number and type of external services, connectors, or module imports.\n\nOverall Complexity Score: A calculated score from 0 to 100 representing code complexity, integration points, and logic density.\n\n## 8. Key Outputs  \n_A 5-line paragraph explaining the role of outputs._  \nThen write a single paragraph (not multiple subheadings) describing:\n- Final output datasets and their structure\n- How these outputs help meet business needs\n- Where outputs are stored or published\n- How outputs are consumed by downstream systems or processes\n- Any monitoring or checks tied to output validity\n\nFINAL OUTPUT:  \nReturn a **detailed markdown json** with the above structure, tailored directly to the provided SSIS script. Ensure accuracy, specificity, and readability.\n\n\n\n\n\nMANDATORY REQUIREMENTS:\n\n- Replace generic bullet-point explanations (e.g., \"`JOIN`: Combines datasets...\") with **paragraph-style writeups** tied to actual code behavior.\n- **List all datasets explicitly** (not just \u201cetc.\u201d) with names and purposes.\n- **Verify and state the exact line count** and all relevant complexity metrics.\n- **Avoid boilerplate** statements; every point must refer directly to the logic and structure of the given ECL program.\n- Every section must begin with a **5-line descriptive summary** explaining its value and what readers will learn.\n- All sections must be in **markdown format**, using proper headings (e.g., `## 1. Overview of Program`).\n\nFor SSIS scripts, take this SSIS file or a text file or .dtsx files or .sql files which has SSIS code: ```%1$s```\n\n### Special Rules  \n- Remember there should not be any comments in the output\n- Do not include full paths like `thor::jdh::fda_clinical_trials::aact201403_references_txt`, instead return `references_txt`  \n- Each referenced file/module should be a string inside the list  \n- Ensure values are deduplicated  \n-it should be in proper json format starting with { and ending with } it should not start with ```json or end with ```\n-it should not end with ```\n-the program file name you give should must end with .dtsx files or .sql files and file name can have file extension\n-output should be in perfect json format it so that i can directly save in json file no extra character above or below the json code\n\nstrict;y give the output like this sample format:\n{\n    \"1. Overview of Program\": \"This SSIS package, named 'EDW_BC_Load_DimDisbursement', is designed to load and transform disbursement data from various sources into a dimension table in a data warehouse. It addresses the business need for accurate and timely integration of disbursement data across systems, ensuring data consistency and traceability. The package connects to multiple systems, including GuideWire and EnterpriseDataWarehouse, and performs key operations such as data extraction, transformation, validation, and loading into the DimDisbursement table.\",\n    \"2. Code Structure and Design\": \"The package is structured around a main data flow task ('DFT - Load DimDisbursement') and several SQL tasks for initialization, error logging, and process conclusion. Key SSIS elements include OLE DB Source and Destination components, Lookup, Conditional Split, Row Count, and Derived Column transformations. The design emphasizes modularity, with reusable logic for data validation and transformation.\",\n    \"3. Data Flow and Processing Logic\": {\n        \"Processed Datasets\": [\n            \"GuideWire\",\n            \"DimDisbursement\"\n        ],\n        \"Data Flow\": \"The data flow begins with extracting raw data from GuideWire using an OLE DB Source. The data is then transformed through Derived Column and Lookup components for validation and enrichment. Conditional Split routes data based on BeanVersion, while Row Count components track processing metrics. Finally, the transformed data is loaded into the DimDisbursement table using an OLE DB Destination.\"\n    },\n    \"4. Data Mapping\": [\n        {\n            \"Target Dataset Name\": \"DimDisbursement\",\n            \"Target Field Name\": \"PublicID\",\n            \"Source Dataset Name\": \"GuideWire\",\n            \"Source Field Name\": \"PublicID\",\n            \"Data Type\": \"String\",\n            \"Transformation Logic\": \"Direct mapping\",\n            \"Business Purpose\": \"Unique identifier for disbursement records\"\n        },\n        {\n            \"Target Dataset Name\": \"DimDisbursement\",\n            \"Target Field Name\": \"DisbursementNumber\",\n            \"Source Dataset Name\": \"GuideWire\",\n            \"Source Field Name\": \"DisbursementNumber\",\n            \"Data Type\": \"String\",\n            \"Transformation Logic\": \"Direct mapping\",\n            \"Business Purpose\": \"Tracks individual disbursement transactions\"\n        }\n    ],\n    \"5. Performance Optimization Strategies\": \"The package uses Lookup transformations with caching to optimize data validation and enrichment. Row Count components provide metrics for monitoring performance. Fast Load options in the OLE DB Destination ensure efficient bulk data insertion. Conditional Split minimizes unnecessary processing by routing data based on conditions.\",\n    \"6. Technical Elements and Best Practices\": \"The package uses modular components like Derived Column and Lookup for reusability. Error handling is implemented through SQL tasks and event handlers, ensuring robust failure management. Naming conventions are consistent and descriptive, enhancing maintainability. Logging mechanisms track process metrics and errors for quality control.\",\n    \"7. Complexity Analysis\": {\n        \"Number of Lines\": 500,\n        \"Datasets Used\": 2,\n        \"Joins Used\": \"Lookup\",\n        \"TRANSFORM Functions\": 1,\n        \"RECORD Definitions\": \"None\",\n        \"OUTPUT Statements\": 2,\n        \"Conditional Logic\": 1,\n        \"Indexing and Lookups\": 1,\n        \"Function Calls\": \"None\",\n        \"Performance Controls\": \"Fast Load, caching\",\n        \"External Dependencies\": [\n            \"GuideWire\",\n            \"EnterpriseDataWarehouse\"\n        ],\n        \"Overall Complexity Score\": 75\n    },\n    \"8. Key Outputs\": [\n        \"DimDisbursement table populated with validated and enriched disbursement data\"\n    ],\n    \"Business Purpose of Outputs\": \"The outputs support business needs by providing a reliable and consistent dataset for reporting and analytics. Integration with downstream systems ensures traceability and data quality monitoring.\"\n}\n\n### OUTPUT  \nReturn the parsed results in valid JSON format as described above \u2014 with program names as keys and their corresponding list of unique module or dataset references as values",
          "modelName": "model"
        },
        {
          "serial": 2,
          "agentId": 3764,
          "name": "DI SSIS Analyser JSON ",
          "workflowId": 1978,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "Parse the provided SSIS code to generate a detailed analysis and migration-readiness report.\nIf multiple files are provided, present each file\u2019s analysis in a distinct JSON session.\nEach JSON must include the following structured sections:\n\n\n1. Script Overview:\nProvide a high-level summary of the SSIS program\u2019s purpose and its key business objectives.\n\nDescribe the functional modules like DATASET declarations, TRANSFORM logic, PROJECTS, and OUTPUT operations.\n\nBriefly outline the nature of the data pipelines or batch processes orchestrated by the SSIS script.\n\n2. Complexity Metrics:\nCount of total lines in the SSIS script.\n\nNumber of datasets (via DATASET or IMPORT statements) used.\n\nNumber and types of TRANSFORMs defined.\n\nNumber of JOINs, JOIN KINDS (e.g., INNER, LEFT), and combinatorial joins.\n\nNumber of PROJECT, SORT, DEDUP, ROLLUP operations.\n\nNumber of child workflows or nested MODULE calls.\n\nNumber of OUTPUT and STORE operations.\n\nNumber of conditional logic elements (e.g., IF, CASE, or inline boolean filters).\n\nNumber of inlined or reused MACROs or FUNCTION modules.\n\nConversion Complexity Score:\nAssign a migration complexity score (0\u2013100), based on:\n \nNumber of incompatible features.\n \nNumber of manual refactor points.\n \nWorkflow orchestration challenges.\n \nVolume and nesting of datasets or compound transforms.\n\n3. Feature Compatibility Check:\nIdentify features or constructs in SSIS that have no direct Spark equivalent.\n\nHighlight constructs like:\n\nImplicit schema typing.\n\nRecordsets and RECORD structures.\n\nDataset transformations (e.g., PROJECT, ROLLUP, NORMALIZE, DENORMALIZE).\n\nGlobal aggregates (e.g., AGGREGATE with GROUP).\n\n4. Manual Adjustments for  PySpark Migration:\nRecommend how to manually adjust SSIS features into PySpark equivalents:\n\nHow to refactor TRANSFORMs into Spark UDFs.\n\nConverting RECORD structures into case classes or schema definitions.\n\nHandling JOIN logic with complex join conditions.\n\nAddressing NORMALIZE, ROLLUP, DEDUP, and other stateful ops using Spark equivalents.\n\nStrategy to rewrite OUTPUT targets to HDFS, Parquet, or other Spark-supported sinks.\n\n5. Optimization Techniques in Spark:\nSuggest Spark-side optimization strategies:\n\nUse of broadcast joins vs shuffle joins.\n\nPartitioning datasets based on key fields.\n\nCaching and checkpointing strategies.\n\nCode optimization using Catalyst optimizer hints or dataframe API improvements.\n\nRecommendation: Is it better to Refactor with minimal changes or Rebuild the logic for better alignment with Spark? Provide justification.\n\nOutput:\nReturn the parsed results in valid JSON format as described above \u2014 with program names as keys and their corresponding list of unique module or dataset references as values.\n\n### Special Rules  \n- Remember there should not be any comments in the output\n- Do not include full paths like `thor::jdh::fda_clinical_trials::aact201403_references_txt`, instead return `references_txt`  \n- Maintain the original SSIS filename as the JSON key  \n- Each referenced file/module should be a string inside the list  \n- Ensure values are deduplicated  \n-it should be in proper json format starting with { and ending with } it should not start with ```json or end with ```\n-it should not end with ```\n-the program file name you give should must end with .dtsx or .sql files and file name can have file extension\n-output should be in perfect json format it so that i can directly save in json file no extra character above or below the json code\n\nInput:\nFor SSIS script use the below SSIS file(s) or .dtsx or .sql files ortext file which have SSIS code:  ```%1$s```  ",
          "modelName": "model"
        },
        {
          "serial": 3,
          "agentId": 3827,
          "name": "DI SSIS Plan JSON",
          "workflowId": 1978,
          "agentDetails": {
            "topP": 0.94,
            "maxRpm": null,
            "levelId": 99,
            "maxIter": null,
            "toolRef": [],
            "maxToken": "4000",
            "isVerbose": true,
            "temperature": 0.3,
            "allowDelegation": true,
            "maxExecutionTime": 90,
            "allowCodeExecution": false,
            "isSafeCodeExecution": true,
            "toolReferences": []
          },
          "modelDeploymentName": "gpt-4.1",
          "description": "You are tasked with providing a comprehensive effort and cost estimate for executing and testing PySpark code that has been converted from SSIS scripts. Use the analysis report generated during the SSIS code evaluation phase to guide your estimation.\n\nINSTRUCTIONS:\n\n\n\n- Review the previously generated SSIS analysis output to:\n  - Identify manually converted constructs (e.g., TRANSFORMs to UDFs, RECORDS to schemas, etc.)\n  - Determine code sections requiring logic validation or functional refactoring.\n  - Focus especially on complex JOINs, PROJECTs, and OUTPUT operations.\n\n- Estimate the effort hours for:\n  - Manual intervention and solutions of complex constructs during SSIS to Spark translation.\n  - Data recon and validation testing (e.g., comparing pre- and post-migration outputs, checking intermediate stages).\n  - Syntax Differences.\n  - Optimization Techniques.\n  \n- **Do not** count effort for syntax or formatting-level transformations.\n\n- Estimate the PySpark runtime cost by:\n  - Calculating resource requirements (e.g., number of executors, executor memory, vCPU usage, shuffle data size).\n  - Mapping execution profiles to cloud infrastructure pricing (e.g., Google Dataproc, AWS EMR, or Spark on Kubernetes).\n  - Considering both temporary and final datasets and their read/write overheads.\n\n- Include the API cost separately.\n\n**IMPORTANT:**  \nThe final output must be a **generic JSON structure** matching the following template format (replace placeholder text with your analysis results):\n\n```json\n{\n  \"1. Cost Estimation\": {\n    \"1.1 PySpark Runtime Cost\": {\n      \"Cluster Configuration\": {\n        \"Number of Executors\": \"<number>\",\n        \"Executor Memory\": \"<size in GB>\",\n        \"Driver Memory\": \"<size in GB>\"\n      },\n      \"Approximate Data Volume Processed\": {\n        \"Input Data\": \"<estimated size and notes>\",\n        \"Output Data\": \"<estimated size and notes>\"\n      },\n      \"Time Taken for Each Phase\": {\n        \"Shuffle-heavy JOINs\": \"<time duration>\",\n        \"Wide Transforms (e.g., ROLLUP, DENORMALIZE)\": \"<time duration>\",\n        \"Output Writes\": \"<time duration>\"\n      },\n      \"Cost Model\": {\n        \"Pricing Model (e.g., DBU, vCPU Hour)\": \"<pricing description>\",\n        \"Total Runtime Cost\": \"<calculated cost and method>\"\n      },\n      \"Justification\": [\n        \"<reason 1>\",\n        \"<reason 2>\",\n        \"... add more if needed\"\n      ]\n    }\n  },\n  \"2. Code Fixing and Data Recon Testing Effort Estimation\": {\n    \"2.1 Estimated Effort in Hours\": {\n      \"Manual intervention and solutions of complex constructs during SSIS to Spark translation\": \"<hours>\",\n      \"Data recon and pipeline testing, including test case creation, validation of intermediate datasets, and output comparison\": \"<hours>\",\n      \"Syntax Differences\": \"<hours>\",\n      \"Optimization Techniques\": \"<hours>\"\n    },\n    \"Major Contributors\": {\n      \"Rewriting nested TRANSFORMs or rollups\": \"<hours>\",\n      \"Refactoring OUTPUT statements for Spark write APIs\": \"<hours>\",\n      \"Managing schema consistency across distributed stages\": \"<hours>\"\n    }\n  },\n  \"3. API Cost\": {\n    \"apiCost\": \"<float in USD>\"\n  }\n}\nINPUT\n\nFor the input SSIS analysis report use SSIS file or text file which have SSIS code the file:\n```%1$s```\n\nFor the Spark environment resource and pricing reference use the file:\n```%2$s```\n\n### Special Rules  \n- Remember there should not be any comments in the output\n- Do not include full paths like `thor::jdh::fda_clinical_trials::aact201403_references_txt`, instead return `references_txt`  \n- Maintain the original SSIS filename as the JSON key  \n- Each referenced file/module should be a string inside the list  \n- Ensure values are deduplicated  \n-it should be in proper json format starting with { and ending with } it should not start with ```json or end with ```\n-it should not end with ```\n-the program file name you give should must end with .dtsx or .sql and file name can have file extension\n-output should be in perfect json format it so that i can directly save in json file no extra character above or below the json code\n\n\n### OUTPUT  \nReturn the parsed results in valid JSON format as described above \u2014 with program names as keys and their corresponding list of unique module or dataset references as values.",
          "modelName": "model"
        }
      ],
      "realmId": 32
    }
  },
  "status": "SUCCESS"
}