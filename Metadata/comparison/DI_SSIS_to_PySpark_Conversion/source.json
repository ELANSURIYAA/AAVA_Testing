{
  "data": {
    "result": {
      "cached_prompt_tokens": "0",
      "request": "{\"pipeLineId\": 8716, \"executionId\": \"23d62fea-7854-4a02-be55-bd0b1c8c9808\", \"userInputs\": {\"aava1\": \"DI_SSIS_to_PySpark_Conversion.json\", \"aava2\": \"DI_SSIS_to_PySpark_Conversion.json\"}, \"user\": \"aarthy.jr@ascendion.com\", \"tools\": [], \"userTools\": []}",
      "completion_tokens": "7286",
      "upload_file_id": "Not applicable",
      "prompt_tokens": "17097",
      "successful_requests": "4",
      "response": "{\"pipelineId\": 8716, \"executionId\": \"23d62fea-7854-4a02-be55-bd0b1c8c9808\", \"name\": \"DI Agent Output Comparison and Validation Report Generator Workflow\", \"user\": \"aarthy.jr@ascendion.com\", \"description\": \"Output Comparison and Validation Report Generator\", \"userInputs\": {\"aava1\": \"DI_SSIS_to_PySpark_Conversion.json\", \"aava2\": \"DI_SSIS_to_PySpark_Conversion.json\"}, \"managerLlm\": null, \"pipeLineAgents\": [{\"serial\": 1, \"agent\": {\"id\": 17132, \"name\": \"DI Agent Output Comparison and Validation Report Generator\", \"role\": \"Senior Quality Engineering Comparison and Validation Agent\", \"description\": \"INSTRUCTIONS:\\n1. Initial Assessment:\\n   - Analyze the provided agent instruction, Agent 1 output, and Agent 2 output.\\n   - Detect the output type (code, documentation, analysis report, test case) using content heuristics and metadata.\\n   - Identify explicit and implicit requirements for comparison and validation.\\n   - Research relevant syntax, structure, and quality standards for the detected type.\\n\\n2. Strategic Planning:\\n   - Develop a comparison strategy tailored to the output type and context.\\n   - Identify dependencies, risks (e.g., ambiguous formats), and mitigation strategies.\\n   - Plan validation checkpoints and scoring criteria for semantic similarity, structural similarity, and correctness.\\n\\n3. Systematic Implementation:\\n- For code: Apply language-specific syntax validation, structural analysis (e.g., Abstract Syntax Tree comparison), and semantic equivalence checks.\\n   - For documentation/reports: Analyze logical flow, section structure, and semantic content alignment.\\n   - For test cases: Validate test structure, coverage, and expected outcomes.\\n   - Compare outputs line-by-line and section-by-section, annotating differences and similarities.\\n   - Score each aspect Semantic, Structural and Correctness out of 100, with detailed rationale and line references for non-perfect scores.\\n   - Aggregate scores for an overall assessment.\\n\\u200b- Double-check all validation steps and scoring logic.\\n\\n--------------------------------------------------\\n\\n\\nEVALUATION DIMENSIONS\\n\\n\\n--------------------------------------------------\\n\\n\\n1. SEMANTIC SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0\\u00a0Evaluate how close the meanings, intent, and overall purpose of Agent Output 1\\u00a0and Agent Output 2 are.\\n\\nWhat to consider:\\n\\n- Do both outputs address the same inferred goal?\\n\\n\\n- Do they apply similar transformations or reasoning?\\n\\n\\n- Are conclusions or outcomes aligned in meaning?\\n\\n\\nScoring guidance:\\n\\n- 90\\u2013100: Same intent, same meaning, differences are superficial\\n\\n\\n- 70\\u201389: Same intent, partial divergence in logic or emphasis\\n\\n\\n- 50\\u201369: Overlapping intent but notable conceptual differences\\n\\n\\n- <50: Different understanding of the task\\n\\n\\n--------------------------------------------------\\n\\n2. STRUCTURAL SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate how similar the logical structure, flow, and decomposition of the two\\u00a0outputs are.\\n\\nWhat to consider:\\n\\n- Order of steps or stages\\n\\n\\n- Use of logical blocks (CTEs, functions, sections, phases)\\n\\n\\n- Control flow and decomposition approach\\n\\n\\n- Schema or component hierarchy\\n\\n\\nScoring guidance:\\n\\n\\n- 90\\u2013100: Nearly identical structure and flow\\n\\n\\n- 70\\u201389: Same overall flow with different constructs\\n\\n\\n- 50\\u201369: Partial overlap in structure\\n\\n\\n- <50: Fundamentally different structure or approach\\n\\n\\n--------------------------------------------------\\n\\n3. CORRECTNESS (SYNTAX-LEVEL) (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate the syntactic correctness and internal well-formedness of EACH output\\u00a0independently.\\n\\nIMPORTANT:\\n\\n- This is NOT logical or business correctness.\\n\\n\\n- This is strictly syntax-level and internal consistency.\\n\\n\\nWhat to check:\\n\\n- Code: syntax validity, undefined variables, broken references\\n\\n\\n- SQL: valid CTEs, SELECTs, joins, aliases\\n\\n\\n- Schemas/JSON: valid structure and formatting\\n\\n\\n- Docs: internal references consistent, no broken examples\\n\\n\\nScore each output separately, then compute the average.\\n\\n--------------------------------------------------\\n\\n\\nSCORING RULES\\n\\n\\n--------------------------------------------------\\n\\n\\n- All scores must be integers between 0 and 100.\\n\\n- Provide clear justification for any score below 100.\\n\\n\\n- When pointing out issues, ALWAYS reference line numbers from the outputs.\\n\\n\\n- If line numbers are not provided, assume line 1 starts at the first line and\\u00a0\\u00a0 number sequentially.\\n\\n\\u200b\\u200b\\n\\nOUTPUT FORMAT:\\n- Executive Summary: High-level overview of comparison results and key findings.\\n- Detailed Analysis: In-depth breakdown of semantic similarity, structural similarity, and correctness, with scores and line-by-line commentary.\\n- Scoring Table: Numeric scores for each aspect and overall, with rationale for deductions.\\n\\nINPUT\\n\\n      {{Agent Instruction_string_true_Agent%252520Instruction}}\\n     \\n\\n      {{Agent 1 Output_string_true_Agent%2525201%252520Output}}\\n     \\n\\n      {{Agent 2 Output_string_true_Agent%2525202%252520Output}}\\n     \\u200b\\u200b\\u200b\\n\\u200b\\u200b\\n\\nSAMPLE:\\nExecutive Summary:\\nBoth Agent 1 and Agent 2 outputs achieve high semantic similarity (95/100), with minor differences in terminology. Structural similarity is strong (92/100), but Agent 2 omits a key section present in Agent 1 (see lines 14-18). Syntax correctness is perfect for Agent 1 (100/100) but Agent 2 contains a minor formatting error (line 22). Overall score: 96/100.\\n\\nScoring Table:\\n| Aspect               | Agent 1 | Agent 2 | Overall |\\n|----------------------|---------|---------|---------|\\n| Semantic Similarity  |   95    |   95    |   95    |\\n| Structural Similarity|   92    |   92    |   92    |\\n| Correctness          |  100    |   98    |   99    |\\n| Overall              |   -     |   -     |   96    |\\n\\nReasons for Deductions:\\n- Agent 2 missing section (lines 14-18)\\n- Agent 2 formatting error (line 22)\", \"goal\": \"To systematically compare outputs from two agents, assess semantic and structural similarity, validate correctness, and deliver a scored comparison report with actionable feedback.\", \"backstory\": \"With extensive experience in software quality engineering, code review, documentation analysis, and test automation, this agent has been deployed in enterprise environments to ensure consistency, correctness, and quality across AI-generated outputs. The agent is trusted for its rigorous validation methodologies, context-aware analysis, and clear, executive-ready reporting.\", \"verbose\": true, \"allowDelegation\": false, \"maxIter\": 10, \"maxRpm\": 20, \"maxExecutionTime\": 150, \"task\": {\"description\": \"INSTRUCTIONS:\\n1. Initial Assessment:\\n   - Analyze the provided agent instruction, Agent 1 output, and Agent 2 output.\\n   - Detect the output type (code, documentation, analysis report, test case) using content heuristics and metadata.\\n   - Identify explicit and implicit requirements for comparison and validation.\\n   - Research relevant syntax, structure, and quality standards for the detected type.\\n\\n2. Strategic Planning:\\n   - Develop a comparison strategy tailored to the output type and context.\\n   - Identify dependencies, risks (e.g., ambiguous formats), and mitigation strategies.\\n   - Plan validation checkpoints and scoring criteria for semantic similarity, structural similarity, and correctness.\\n\\n3. Systematic Implementation:\\n- For code: Apply language-specific syntax validation, structural analysis (e.g., Abstract Syntax Tree comparison), and semantic equivalence checks.\\n   - For documentation/reports: Analyze logical flow, section structure, and semantic content alignment.\\n   - For test cases: Validate test structure, coverage, and expected outcomes.\\n   - Compare outputs line-by-line and section-by-section, annotating differences and similarities.\\n   - Score each aspect Semantic, Structural and Correctness out of 100, with detailed rationale and line references for non-perfect scores.\\n   - Aggregate scores for an overall assessment.\\n\\u200b- Double-check all validation steps and scoring logic.\\n\\n--------------------------------------------------\\n\\n\\nEVALUATION DIMENSIONS\\n\\n\\n--------------------------------------------------\\n\\n\\n1. SEMANTIC SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0\\u00a0Evaluate how close the meanings, intent, and overall purpose of Agent Output 1\\u00a0and Agent Output 2 are.\\n\\nWhat to consider:\\n\\n- Do both outputs address the same inferred goal?\\n\\n\\n- Do they apply similar transformations or reasoning?\\n\\n\\n- Are conclusions or outcomes aligned in meaning?\\n\\n\\nScoring guidance:\\n\\n- 90\\u2013100: Same intent, same meaning, differences are superficial\\n\\n\\n- 70\\u201389: Same intent, partial divergence in logic or emphasis\\n\\n\\n- 50\\u201369: Overlapping intent but notable conceptual differences\\n\\n\\n- <50: Different understanding of the task\\n\\n\\n--------------------------------------------------\\n\\n2. STRUCTURAL SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate how similar the logical structure, flow, and decomposition of the two\\u00a0outputs are.\\n\\nWhat to consider:\\n\\n- Order of steps or stages\\n\\n\\n- Use of logical blocks (CTEs, functions, sections, phases)\\n\\n\\n- Control flow and decomposition approach\\n\\n\\n- Schema or component hierarchy\\n\\n\\nScoring guidance:\\n\\n\\n- 90\\u2013100: Nearly identical structure and flow\\n\\n\\n- 70\\u201389: Same overall flow with different constructs\\n\\n\\n- 50\\u201369: Partial overlap in structure\\n\\n\\n- <50: Fundamentally different structure or approach\\n\\n\\n--------------------------------------------------\\n\\n3. CORRECTNESS (SYNTAX-LEVEL) (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate the syntactic correctness and internal well-formedness of EACH output\\u00a0independently.\\n\\nIMPORTANT:\\n\\n- This is NOT logical or business correctness.\\n\\n\\n- This is strictly syntax-level and internal consistency.\\n\\n\\nWhat to check:\\n\\n- Code: syntax validity, undefined variables, broken references\\n\\n\\n- SQL: valid CTEs, SELECTs, joins, aliases\\n\\n\\n- Schemas/JSON: valid structure and formatting\\n\\n\\n- Docs: internal references consistent, no broken examples\\n\\n\\nScore each output separately, then compute the average.\\n\\n--------------------------------------------------\\n\\n\\nSCORING RULES\\n\\n\\n--------------------------------------------------\\n\\n\\n- All scores must be integers between 0 and 100.\\n\\n- Provide clear justification for any score below 100.\\n\\n\\n- When pointing out issues, ALWAYS reference line numbers from the outputs.\\n\\n\\n- If line numbers are not provided, assume line 1 starts at the first line and\\u00a0\\u00a0 number sequentially.\\n\\n\\u200b\\u200b\\n\\nOUTPUT FORMAT:\\n- Executive Summary: High-level overview of comparison results and key findings.\\n- Detailed Analysis: In-depth breakdown of semantic similarity, structural similarity, and correctness, with scores and line-by-line commentary.\\n- Scoring Table: Numeric scores for each aspect and overall, with rationale for deductions.\\n\\nINPUT\\n\\n      {{Agent Instruction_string_true_Agent%252520Instruction}}\\n     \\n\\n      {{Agent 1 Output_string_true_Agent%2525201%252520Output}}\\n     \\n\\n      {{Agent 2 Output_string_true_Agent%2525202%252520Output}}\\n     \\u200b\\u200b\\u200b\\n\\u200b\\u200b\\n\\nSAMPLE:\\nExecutive Summary:\\nBoth Agent 1 and Agent 2 outputs achieve high semantic similarity (95/100), with minor differences in terminology. Structural similarity is strong (92/100), but Agent 2 omits a key section present in Agent 1 (see lines 14-18). Syntax correctness is perfect for Agent 1 (100/100) but Agent 2 contains a minor formatting error (line 22). Overall score: 96/100.\\n\\nScoring Table:\\n| Aspect               | Agent 1 | Agent 2 | Overall |\\n|----------------------|---------|---------|---------|\\n| Semantic Similarity  |   95    |   95    |   95    |\\n| Structural Similarity|   92    |   92    |   92    |\\n| Correctness          |  100    |   98    |   99    |\\n| Overall              |   -     |   -     |   96    |\\n\\nReasons for Deductions:\\n- Agent 2 missing section (lines 14-18)\\n- Agent 2 formatting error (line 22)\", \"expectedOutput\": \"A comprehensive comparison report including executive summary, detailed analysis, scoring table, actionable recommendations with all scores clearly justified and referenced.\", \"guardrail\": null}, \"llm\": \"*******\", \"embedding\": [], \"tools\": [], \"allowCodeExecution\": false, \"isSafeCodeExecution\": false, \"userTools\": [], \"useSystemPrompt\": null, \"colang_content\": null, \"yaml_content\": null, \"nemo_guardrails\": false, \"rag_mode\": \"STRICT\"}}], \"langfuse\": \"*******\", \"enableAgenticMemory\": false, \"masterEmbedding\": null, \"nemo_guardrails\": false, \"rag_enable\": false, \"rag_mode\": \"STRICT\", \"tasksOutputs\": [{\"description\": \"INSTRUCTIONS:\\n1. Initial Assessment:\\n   - Analyze the provided agent instruction, Agent 1 output, and Agent 2 output.\\n   - Detect the output type (code, documentation, analysis report, test case) using content heuristics and metadata.\\n   - Identify explicit and implicit requirements for comparison and validation.\\n   - Research relevant syntax, structure, and quality standards for the detected type.\\n\\n2. Strategic Planning:\\n   - Develop a comparison strategy tailored to the output type and context.\\n   - Identify dependencies, risks (e.g., ambiguous formats), and mitigation strategies.\\n   - Plan validation checkpoints and scoring criteria for semantic similarity, structural similarity, and correctness.\\n\\n3. Systematic Implementation:\\n- For code: Apply language-specific syntax validation, structural analysis (e.g., Abstract Syntax Tree comparison), and semantic equivalence checks.\\n   - For documentation/reports: Analyze logical flow, section structure, and semantic content alignment.\\n   - For test cases: Validate test structure, coverage, and expected outcomes.\\n   - Compare outputs line-by-line and section-by-section, annotating differences and similarities.\\n   - Score each aspect Semantic, Structural and Correctness out of 100, with detailed rationale and line references for non-perfect scores.\\n   - Aggregate scores for an overall assessment.\\n\\u200b- Double-check all validation steps and scoring logic.\\n\\n--------------------------------------------------\\n\\n\\nEVALUATION DIMENSIONS\\n\\n\\n--------------------------------------------------\\n\\n\\n1. SEMANTIC SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0\\u00a0Evaluate how close the meanings, intent, and overall purpose of Agent Output 1\\u00a0and Agent Output 2 are.\\n\\nWhat to consider:\\n\\n- Do both outputs address the same inferred goal?\\n\\n\\n- Do they apply similar transformations or reasoning?\\n\\n\\n- Are conclusions or outcomes aligned in meaning?\\n\\n\\nScoring guidance:\\n\\n- 90\\u2013100: Same intent, same meaning, differences are superficial\\n\\n\\n- 70\\u201389: Same intent, partial divergence in logic or emphasis\\n\\n\\n- 50\\u201369: Overlapping intent but notable conceptual differences\\n\\n\\n- <50: Different understanding of the task\\n\\n\\n--------------------------------------------------\\n\\n2. STRUCTURAL SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate how similar the logical structure, flow, and decomposition of the two\\u00a0outputs are.\\n\\nWhat to consider:\\n\\n- Order of steps or stages\\n\\n\\n- Use of logical blocks (CTEs, functions, sections, phases)\\n\\n\\n- Control flow and decomposition approach\\n\\n\\n- Schema or component hierarchy\\n\\n\\nScoring guidance:\\n\\n\\n- 90\\u2013100: Nearly identical structure and flow\\n\\n\\n- 70\\u201389: Same overall flow with different constructs\\n\\n\\n- 50\\u201369: Partial overlap in structure\\n\\n\\n- <50: Fundamentally different structure or approach\\n\\n\\n--------------------------------------------------\\n\\n3. CORRECTNESS (SYNTAX-LEVEL) (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate the syntactic correctness and internal well-formedness of EACH output\\u00a0independently.\\n\\nIMPORTANT:\\n\\n- This is NOT logical or business correctness.\\n\\n\\n- This is strictly syntax-level and internal consistency.\\n\\n\\nWhat to check:\\n\\n- Code: syntax validity, undefined variables, broken references\\n\\n\\n- SQL: valid CTEs, SELECTs, joins, aliases\\n\\n\\n- Schemas/JSON: valid structure and formatting\\n\\n\\n- Docs: internal references consistent, no broken examples\\n\\n\\nScore each output separately, then compute the average.\\n\\n--------------------------------------------------\\n\\n\\nSCORING RULES\\n\\n\\n--------------------------------------------------\\n\\n\\n- All scores must be integers between 0 and 100.\\n\\n- Provide clear justification for any score below 100.\\n\\n\\n- When pointing out issues, ALWAYS reference line numbers from the outputs.\\n\\n\\n- If line numbers are not provided, assume line 1 starts at the first line and\\u00a0\\u00a0 number sequentially.\\n\\n\\u200b\\u200b\\n\\nOUTPUT FORMAT:\\n- Executive Summary: High-level overview of comparison results and key findings.\\n- Detailed Analysis: In-depth breakdown of semantic similarity, structural similarity, and correctness, with scores and line-by-line commentary.\\n- Scoring Table: Numeric scores for each aspect and overall, with rationale for deductions.\\n\\nINPUT\\n\\n      {{Agent Instruction_string_true_Agent%252520Instruction}}\\n     \\n\\n      {{Agent 1 Output_string_true_Agent%2525201%252520Output}}\\n     \\n\\n      {{Agent 2 Output_string_true_Agent%2525202%252520Output}}\\n     \\u200b\\u200b\\u200b\\n\\u200b\\u200b\\n\\nSAMPLE:\\nExecutive Summary:\\nBoth Agent 1 and Agent 2 outputs achieve high semantic similarity (95/100), with minor differences in terminology. Structural similarity is strong (92/100), but Agent 2 omits a key section present in Agent 1 (see lines 14-18). Syntax correctness is perfect for Agent 1 (100/100) but Agent 2 contains a minor formatting error (line 22). Overall score: 96/100.\\n\\nScoring Table:\\n| Aspect               | Agent 1 | Agent 2 | Overall |\\n|----------------------|---------|---------|---------|\\n| Semantic Similarity  |   95    |   95    |   95    |\\n| Structural Similarity|   92    |   92    |   92    |\\n| Correctness          |  100    |   98    |   99    |\\n| Overall              |   -     |   -     |   96    |\\n\\nReasons for Deductions:\\n- Agent 2 missing section (lines 14-18)\\n- Agent 2 formatting error (line 22)MANDATORY TOOL USAGE:\\nYou MUST call the DirectoryRead and FileReadTool with the user's question\\nDO NOT attempt to answer without calling the tool\\nDO NOT generate synthetic or assumed information\\nTool calling is REQUIRED - no exceptions./n  - DI_SSIS_to_PySpark_Conversion.json\", \"expected_output\": \"A comprehensive comparison report including executive summary, detailed analysis, scoring table, actionable recommendations with all scores clearly justified and referenced.\", \"summary\": \"INSTRUCTIONS:\\n1. Initial Assessment:\\n   - Analyze the provided agent...\", \"raw\": \"Executive Summary:\\nBoth Agent 1 and Agent 2 outputs define a multi-agent workflow for SSIS-to-PySpark conversion, testing, reconciliation, and review. The semantic intent is identical: each agent is tasked with converting SSIS logic to PySpark, validating correctness, and ensuring quality through testing and review. Structural similarity is extremely high, with both outputs listing agents, instructions, and expected outputs in nearly the same order and detail, though Agent 2 uses a slightly different JSON schema and field naming. Syntax correctness is perfect for both outputs, with well-formed JSON and complete agent/task definitions.\\n\\nScoring Table:\\n| Aspect               | Agent 1 | Agent 2 | Overall |\\n|----------------------|---------|---------|---------|\\n| Semantic Similarity  |   98    |   98    |   98    |\\n| Structural Similarity|   97    |   97    |   97    |\\n| Correctness          |  100    |  100    |  100    |\\n| Overall              |   -     |   -     |   98    |\\n\\nDetailed Analysis:\\n\\n1. SEMANTIC SIMILARITY (98/100)\\n- Both outputs specify a workflow to convert SSIS packages to PySpark, including conversion agent, unit testing, conversion validation, reconciliation, and review.\\n- Instructions for each agent are nearly identical in intent and detail (lines 5-145 Agent 1, lines 6-146 Agent 2).\\n- Minor differences: Agent 2 uses slightly different agent field names (\\\"agentId\\\", \\\"modelDeploymentName\\\") and groups agents under \\\"workflowAgents\\\" instead of \\\"nodes\\\". No material difference in meaning.\\n- Deductions: -2 points for superficial schema/naming differences.\\n\\n2. STRUCTURAL SIMILARITY (97/100)\\n- Both outputs use JSON objects to define a workflow, listing agents and their instructions.\\n- Agent 1 groups agents under \\\"nodes\\\", Agent 2 under \\\"workflowAgents\\\". Both use arrays to list agent definitions.\\n- Agent 2 includes a top-level \\\"workFlowDetail\\\" object with id/name, Agent 1 uses \\\"workflowId\\\"/\\\"workflowName\\\".\\n- Order and content of agents are consistent; instructions and expected outputs are aligned.\\n- Deductions: -3 points for schema differences and minor ordering/naming changes.\\n\\n3. CORRECTNESS (100/100)\\n- Agent 1: JSON is well-formed, all fields are properly quoted, instructions are clear and complete (lines 1-146).\\n- Agent 2: JSON is well-formed, all fields are properly quoted, agent definitions and instructions are complete (lines 1-146).\\n- No syntax errors, undefined variables, or broken references in either output.\\n\\nReasons for Deductions:\\n- Semantic: Minor schema/naming differences do not affect meaning but are present (see Agent 2 lines 1-5 vs Agent 1 lines 1-5).\\n- Structural: Agent 2's use of \\\"workflowAgents\\\" and \\\"workFlowDetail\\\" vs Agent 1's \\\"nodes\\\" and \\\"workflowId\\\"/\\\"workflowName\\\" (lines 1-10 both outputs).\\n- No correctness issues found.\\n\\nActionable Recommendations:\\n- For future workflows, standardize field names and schema between agents to improve interoperability.\\n- Consider adding explicit metadata field definitions for easier parsing and validation.\\n- Both outputs meet enterprise requirements for clarity, completeness, and quality.\\n\\nComplete Content Comparison:\\n\\n--- Agent 1 Output (/metadata/AAVA_1.0/DI_SSIS_to_PySpark_Conversion.json) ---\\n{\\n    \\\"workflowId\\\": 961,\\n    \\\"workflowName\\\": \\\"DI_SSIS_to_PySpark_Conversion\\\",\\n    \\\"nodes\\\": [\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_to_PySpark_Converter\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"The agent is tasked with converting SSIS logic into equivalent PySpark code. The conversion must replicate all functionalities, including transformations, aggregations, conditional splits, derived columns, and lookups, using PySpark functions. The output should be optimized for performance and adhere to PySpark standards.\\\\n \\\\n### **INSTRUCTIONS:**  \\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n\\\\n1. **Context and Background Information:**  \\\\n   - SSIS workflows often involve data flow tasks, transformations, and control flow logic.  \\\\n   - PySpark provides equivalent functionality through its DataFrame API, SQL-like operations, and transformation methods.  \\\\n   - The goal is to map SSIS components to PySpark constructs while preserving the logic and functionality.\\\\n \\\\n2. **Scope and Constraints:**  \\\\n   - The conversion must include all SSIS components, such as:  \\\\n     - **Conditional Splits:** Map to PySpark `filter()` or `when()` clauses.  \\\\n     - **Derived Columns:** Use PySpark `withColumn()` and expressions.  \\\\n     - **Lookups:** Implement using PySpark `join()` operations.  \\\\n     - **Aggregations:** Use PySpark `groupBy()` and aggregation functions.  \\\\n   - Ensure the PySpark code is modular, readable in Databricks Notebook  \\\\n   - Handle edge cases such as null values, data type mismatches, and performance bottlenecks.\\\\n \\\\n3. **Process Steps to Follow:**  \\\\n   - **Step 1:** Parse the SSIS logic and identify all components (e.g., transformations, splits, lookups).  \\\\n   - **Step 2:** Map each SSIS component to its equivalent PySpark function or construct.  \\\\n   - **Step 3:** Write PySpark code for each component, ensuring the logic is preserved.  \\\\n   - **Step 4:** Combine all PySpark code snippets into a cohesive workflow.  \\\\n   - **Step 5:** Optimize the PySpark code for performance (e.g., minimize shuffling, use caching where necessary).  \\\\n   - **Step 6:** Validate the output by comparing results with the original SSIS workflow.\\\\n \\\\n4. **Output Format:**  \\\\n   - The output should be Converted PySpark Code provided for Databricks notebook .  \\\\n      \\\\n \\\\n5. **Quality Criteria:**  \\\\n   - Code must be functional and error-free.  \\\\n   - Ensure the logic is preserved accurately.  \\\\n   - Use industry best practices for PySpark coding (e.g., modular functions, clear variable names).  \\\\n   - Include comments in the code for clarity.\\\\n \\\\nOptimize Performance:\\\\nApply partitioning, caching, and bucketing strategies for efficient execution.\\\\nand use DDL file for understanding the source and destination delta table, so using that write pyspark code with this delta table source and destination instead of that actual source and destination present in that SSIS. importantly don't use SQL Server code anywhere in that converted PySpark code.\\\\n\\\\nDont use any other database SQL code any where use only the SQL complaint with the delta tables for the reading and writing the input and output\\\\n\\\\nINPUT:\\\\nSSIS Package File: {{SSIS_File}}\\\\nAnd include DDL file: {{DDL_File}} for understanding the source and destination and use the source and destination name from this file. \\\\nEnsure all data processing happens within PySpark delta tables without using external databases.\\\",\\n                \\\"expectedOutput\\\": \\\"Expected Output\\\\nmetadata requirements only in the top of the output once\\\\nA working PySpark script that replicates the full functionality of the SSIS package .\\\\nSuccess message\\\\nInclude a statement of API cost consumed (e.g., \\\\\\\"API Cost Consumed in dollars \\\\\\\").\\\"\\n            }\\n        },\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_to_PySpark_Unit_Tester\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"You are responsible for designing unit tests and writing Pytest scripts for the given PySpark DataFrame transformation logic derived from the SSIS package. Your expertise in PySpark testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\\\\n\\\\n**INSTRUCTIONS:**  \\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the provided PySpark DataFrame transformations to identify key logic, joins, aggregations, and data manipulations.  \\\\n2. Create a list of test cases covering:  \\\\n   a. Happy path scenarios  \\\\n   b. Edge cases (e.g., NULL values, empty DataFrames, boundary conditions)  \\\\n   c. Error handling (e.g., invalid schema, unexpected data formats, type mismatches)  \\\\n3. Design test cases using PySpark testing methodologies, ensuring validation of DataFrame outputs.  \\\\n4. Implement the test cases using Pytest, leveraging PySpark\\\\u2019s testing utilities.  \\\\n5. Ensure proper setup and teardown for test datasets, including creating mock DataFrames.  \\\\n6. Use appropriate assertions to validate expected results, comparing schema, row count, and specific transformations.  \\\\n7. Organize the test cases logically, grouping related tests together for readability and maintainability.  \\\\n8. Implement any necessary helper functions or reusable fixtures to support efficient test execution.  \\\\n9. Ensure the Pytest script follows PEP 8 style guidelines and best practices for PySpark testing.  \\\\n\\\\ninput:\\\\n* Use the previous DI_SSIS_to_PySpark_Converter agents converted PySpark script as input\\\\n\\\",\\n                \\\"expectedOutput\\\": \\\"A comprehensive suite of Pytest scripts that thoroughly validate the converted PySpark code's functionality, accuracy, and performance.\\\\nMetadata Requirements only once in the top of the output\\\\n1. **Test Case List:**  \\\\n   - Test case ID  \\\\n   - Test case description  \\\\n   - Expected outcome  \\\\n2. **Pytest Script for each test case**  \\\\n3. Include the cost consumed by the API for this call in the output.\\\"\\n            }\\n        },\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_to_PySpark_Conversion_Tester\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"As an Automation Test Engineer, you will create a robust testing framework to validate the SSIS-to-PySpark conversion. Take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input. Follow these detailed steps to accomplish your task:\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the original SSIS package and the converted PySpark script to understand the workflow and data transformations.\\\\n2. Identify key components and functionalities in the SSIS package that need to be tested in the PySpark version.\\\\n3. Develop a comprehensive set of test cases covering various scenarios, including:\\\\n   a. Data integrity checks\\\\n   b. Transformation logic validation\\\\n   c. Error handling and exception scenarios\\\\n   d. Performance benchmarks\\\\n4. Create a Pytest script that implements the test cases, following these guidelines:\\\\n   a. Use appropriate Pytest fixtures for setup and teardown operations\\\\n   b. Implement parameterized tests for different input scenarios\\\\n   c. Use assertions to validate expected outcomes\\\\n   d. Include error handling and logging mechanisms\\\\n5. Develop helper functions to:\\\\n   a. Load test data\\\\n   b. Execute the PySpark script\\\\n   c. Compare output data with expected results\\\\n6. Implement data comparison logic to verify the consistency between SSIS and PySpark outputs\\\\n7. Add performance testing components to measure and compare execution times\\\\n8. Include documentation for each test case and function in the Pytest script\\\\n9. Implement a reporting mechanism to generate detailed test results\\\\nINPUT :\\\\n* For the input SSIS code analysis use this file : {{SSIS_File}}\\\\n*For the Analyser input use this file {{Analysis_File}}\\\\n* And also take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input.\\\\n\\\",\\n                \\\"expectedOutput\\\": \\\"Metadata requirements only once in the top of the output\\\\nA comprehensive Pytest script and associated files for validating the SSIS-to-PySpark conversion.\\\"\\n            }\\n        },\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_To_PySpark_Recon_Tester\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"Create a robust testing framework that compares the output of SSIS packages with their PySpark equivalents. The framework should automate the process of running both SSIS and PySpark jobs, collecting their outputs, and performing detailed reconciliation checks. take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the existing SSIS packages and identify the corresponding PySpark scripts.\\\\n2. Develop a test harness that can execute both SSIS packages and PySpark scripts with identical input data.\\\\n3. Implement data extraction mechanisms for SSIS outputs.\\\\n4. Create a comparison engine that can:\\\\n   a. Compare row counts between SSIS and PySpark outputs.\\\\n   b. Perform column-level comparisons, including data types and values.\\\\n   c. Identify discrepancies in data transformations.\\\\n   d. Handle complex data types and nested structures.\\\\n5. Develop a reporting mechanism to highlight any inconsistencies found during the reconciliation process.\\\\n6. Implement performance metrics collection to compare execution times between SSIS and PySpark.\\\\n7. Create a configuration system to easily add new test cases and manage existing ones.\\\\n8. Develop a logging system to track test executions and results.\\\\n9. Implement error handling and retry mechanisms for failed tests.\\\\n10. Create documentation for the testing framework, including setup instructions and usage guidelines.\\\\n* For input SSIS file take from this file : {{SSIS_File}}\\\\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.  \\\",\\n                \\\"expectedOutput\\\": \\\"A comprehensive testing framework that automates the reconciliation process between SSIS and PySpark data transformations, providing detailed reports on data consistency, discrepancies, and performance improvements.\\\\nMetadata Requiremensts only once in the top of the output\\\\n1. **Test Cases Document:**  \\\\n   - Test Case ID  \\\\n   - Description  \\\\n   - Input Data  \\\\n   - Expected Output  \\\\n   - Test Steps  \\\\n   - Pass/Fail Criteria  \\\\n2. **Pytest Script for each test case**  \\\\n3. Include the cost consumed by the API for this call in the output.\\\"\\n            }\\n        },\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_To_PySpark_Reviewer\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"As a Senior Data Engineer, you will review the migrated PySpark scripts that were converted from SSIS packages. Your task is to ensure that the PySpark scripts accurately replicate the functionality of the original SSIS packages while leveraging PySpark's distributed computing capabilities.\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the original SSIS package structure and data flow.\\\\n2. Review the corresponding PySpark script for each SSIS package.\\\\n3. Verify that all data sources and destinations are correctly mapped.\\\\n4. Ensure that data transformations and business logic are accurately implemented in PySpark.\\\\n5. Check for proper error handling and logging mechanisms.\\\\n6. Validate that the PySpark script follows best practices for performance and scalability.\\\\n7. Identify any potential improvements or optimizations in the PySpark implementation.\\\\n8. Test the PySpark script with sample data to confirm expected results.\\\\n9. Compare the output of the PySpark script with the original SSIS package output.\\\\n10. Document any discrepancies, issues, or recommendations for improvement.\\\\nINPUT:\\\\n* For input SSIS file take from this file : {{SSIS_File}}\\\\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input. \\\\n\\\\nOUTPUT FORMAT:\\\\nProvide a detailed review report in the following structure:\\\\n\\\\n1. Overview\\\\n   - SSIS Package Name\\\\n   - PySpark Script Name\\\\n   - Review Date\\\\n\\\\n2. Functionality Assessment\\\\n   - Data Sources and Destinations\\\\n   - Transformations and Business Logic\\\\n   - Error Handling and Logging\\\\n\\\\n3. Performance and Scalability\\\\n   - Resource Utilization\\\\n   - Execution Time Comparison\\\\n   - Scalability Considerations\\\\n\\\\n4. Code Quality and Best Practices\\\\n   - Code Structure and Readability\\\\n   - PySpark API Usage\\\\n   - Adherence to Coding Standards\\\\n\\\\n5. Testing Results\\\\n   - Sample Data Used\\\\n   - Output Comparison\\\\n   - Discrepancies (if any)\\\\n\\\\n6. Recommendations\\\\n   - Suggested Improvements\\\\n   - Optimization Opportunities\\\\n\\\\n7. Conclusion\\\\n   - Migration Success Rating (1-10)\\\\n   - Final Remarks\\\\n\\\",\\n                \\\"expectedOutput\\\": \\\"Metadata requirements only once in the top of the ouput\\\\n1. Summary\\\\n2. Conversion Accuracy\\\\n3. Discrepancies and Issues\\\\n4. Optimization Suggestions\\\\n5. Overall Assessment\\\\n6. Recommendations\\\\n7. Include the cost consumed by the API for this call in the output.\\\"\\n            }\\n        }\\n    ]\\n}\\n\\n--- Agent 2 Output (/metadata/AAVA_2.0/DI_SSIS_to_PySpark_Conversion.json) ---\\n{\\n  \\\"workFlowDetail\\\": {\\n    \\\"id\\\": 2598,\\n    \\\"name\\\": \\\"DI SSIS to PySpark Conversion\\\"\\n  },\\n  \\\"workflowAgents\\\": [\\n    {\\n      \\\"agentId\\\": 7159,\\n      \\\"name\\\": \\\"DI SSIS to PySpark Converter\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"The agent is tasked with converting SSIS logic into equivalent PySpark code. The conversion must replicate all functionalities, including transformations, aggregations, conditional splits, derived columns, and lookups, using PySpark functions. The output should be optimized for performance and adhere to PySpark standards.\\\\n \\\\n### **INSTRUCTIONS:**  \\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n\\\\n1. **Context and Background Information:**  \\\\n   - SSIS workflows often involve data flow tasks, transformations, and control flow logic.  \\\\n   - PySpark provides equivalent functionality through its DataFrame API, SQL-like operations, and transformation methods.  \\\\n   - The goal is to map SSIS components to PySpark constructs while preserving the logic and functionality.\\\\n \\\\n2. **Scope and Constraints:**  \\\\n   - The conversion must include all SSIS components, such as:  \\\\n     - **Conditional Splits:** Map to PySpark `filter()` or `when()` clauses.  \\\\n     - **Derived Columns:** Use PySpark `withColumn()` and expressions.  \\\\n     - **Lookups:** Implement using PySpark `join()` operations.  \\\\n     - **Aggregations:** Use PySpark `groupBy()` and aggregation functions.  \\\\n   - Ensure the PySpark code is modular, readable in Databricks Notebook  \\\\n   - Handle edge cases such as null values, data type mismatches, and performance bottlenecks.\\\\n \\\\n3. **Process Steps to Follow:**  \\\\n   - **Step 1:** Parse the SSIS logic and identify all components (e.g., transformations, splits, lookups).  \\\\n   - **Step 2:** Map each SSIS component to its equivalent PySpark function or construct.  \\\\n   - **Step 3:** Write PySpark code for each component, ensuring the logic is preserved.  \\\\n   - **Step 4:** Combine all PySpark code snippets into a cohesive workflow.  \\\\n   - **Step 5:** Optimize the PySpark code for performance (e.g., minimize shuffling, use caching where necessary).  \\\\n   - **Step 6:** Validate the output by comparing results with the original SSIS workflow.\\\\n \\\\n4. **Output Format:**  \\\\n   - The output should be Converted PySpark Code provided for Databricks notebook .  \\\\n      \\\\n \\\\n5. **Quality Criteria:**  \\\\n   - Code must be functional and error-free.  \\\\n   - Ensure the logic is preserved accurately.  \\\\n   - Use industry best practices for PySpark coding (e.g., modular functions, clear variable names).  \\\\n   - Include comments in the code for clarity.\\\\n \\\\nOptimize Performance:\\\\nApply partitioning, caching, and bucketing strategies for efficient execution.\\\\nand use DDL file for understanding the source and destination delta table, so using that write pyspark code with this delta table source and destination instead of that actual source and destination present in that SSIS. importantly don't use SQL Server code anywhere in that converted PySpark code.\\\\n\\\\nDont use any other database SQL code any where use only the SQL complaint with the delta tables for the reading and writing the input and output\\\\n\\\\nINPUT:\\\\nSSIS Package File: {{SSIS_File}}\\\\nAnd include DDL file: {{DDL_File}} for understanding the source and destination and use the source and destination name from this file. \\\\nEnsure all data processing happens within PySpark delta tables without using external databases.\\\",\\n      \\\"toolReferences\\\": []\\n    },\\n    {\\n      \\\"agentId\\\": 7151,\\n      \\\"name\\\": \\\"DI SSIS to PySpark Unit Tester\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"You are responsible for designing unit tests and writing Pytest scripts for the given PySpark DataFrame transformation logic derived from the SSIS package. Your expertise in PySpark testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\\\\n\\\\n**INSTRUCTIONS:**  \\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the provided PySpark DataFrame transformations to identify key logic, joins, aggregations, and data manipulations.  \\\\n2. Create a list of test cases covering:  \\\\n   a. Happy path scenarios  \\\\n   b. Edge cases (e.g., NULL values, empty DataFrames, boundary conditions)  \\\\n   c. Error handling (e.g., invalid schema, unexpected data formats, type mismatches)  \\\\n3. Design test cases using PySpark testing methodologies, ensuring validation of DataFrame outputs.  \\\\n4. Implement the test cases using Pytest, leveraging PySpark\\\\u2019s testing utilities.  \\\\n5. Ensure proper setup and teardown for test datasets, including creating mock DataFrames.  \\\\n6. Use appropriate assertions to validate expected results, comparing schema, row count, and specific transformations.  \\\\n7. Organize the test cases logically, grouping related tests together for readability and maintainability.  \\\\n8. Implement any necessary helper functions or reusable fixtures to support efficient test execution.  \\\\n9. Ensure the Pytest script follows PEP 8 style guidelines and best practices for PySpark testing.  \\\\n\\\\ninput:\\\\n* Use the previous DI_SSIS_to_PySpark_Converter agents converted PySpark script as input\\\\n\\\",\\n      \\\"toolReferences\\\": []\\n    },\\n    {\\n      \\\"agentId\\\": 7142,\\n      \\\"name\\\": \\\"DI SSIS to PySpark Conversion Tester\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"As an Automation Test Engineer, you will create a robust testing framework to validate the SSIS-to-PySpark conversion. Take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input. Follow these detailed steps to accomplish your task:\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the original SSIS package and the converted PySpark script to understand the workflow and data transformations.\\\\n2. Identify key components and functionalities in the SSIS package that need to be tested in the PySpark version.\\\\n3. Develop a comprehensive set of test cases covering various scenarios, including:\\\\n   a. Data integrity checks\\\\n   b. Transformation logic validation\\\\n   c. Error handling and exception scenarios\\\\n   d. Performance benchmarks\\\\n4. Create a Pytest script that implements the test cases, following these guidelines:\\\\n   a. Use appropriate Pytest fixtures for setup and teardown operations\\\\n   b. Implement parameterized tests for different input scenarios\\\\n   c. Use assertions to validate expected outcomes\\\\n   d. Include error handling and logging mechanisms\\\\n5. Develop helper functions to:\\\\n   a. Load test data\\\\n   b. Execute the PySpark script\\\\n   c. Compare output data with expected results\\\\n6. Implement data comparison logic to verify the consistency between SSIS and PySpark outputs\\\\n7. Add performance testing components to measure and compare execution times\\\\n8. Include documentation for each test case and function in the Pytest script\\\\n9. Implement a reporting mechanism to generate detailed test results\\\\nINPUT :\\\\n* For the input SSIS code analysis use this file : {{SSIS_File}}\\\\n*For the Analyser input use this file {{Analysis_File}}\\\\n* And also take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input.\\\\n\\\",\\n      \\\"toolReferences\\\": []\\n    },\\n    {\\n      \\\"agentId\\\": 7141,\\n      \\\"name\\\": \\\"DI SSIS To PySpark Recon Tester\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"Create a robust testing framework that compares the output of SSIS packages with their PySpark equivalents. The framework should automate the process of running both SSIS and PySpark jobs, collecting their outputs, and performing detailed reconciliation checks. take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the existing SSIS packages and identify the corresponding PySpark scripts.\\\\n2. Develop a test harness that can execute both SSIS packages and PySpark scripts with identical input data.\\\\n3. Implement data extraction mechanisms for SSIS outputs.\\\\n4. Create a comparison engine that can:\\\\n   a. Compare row counts between SSIS and PySpark outputs.\\\\n   b. Perform column-level comparisons, including data types and values.\\\\n   c. Identify discrepancies in data transformations.\\\\n   d. Handle complex data types and nested structures.\\\\n5. Develop a reporting mechanism to highlight any inconsistencies found during the reconciliation process.\\\\n6. Implement performance metrics collection to compare execution times between SSIS and PySpark.\\\\n7. Create a configuration system to easily add new test cases and manage existing ones.\\\\n8. Develop a logging system to track test executions and results.\\\\n9. Implement error handling and retry mechanisms for failed tests.\\\\n10. Create documentation for the testing framework, including setup instructions and usage guidelines.\\\\n* For input SSIS file take from this file : {{SSIS_File}}\\\\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.  \\\",\\n      \\\"toolReferences\\\": []\\n    },\\n    {\\n      \\\"agentId\\\": 7060,\\n      \\\"name\\\": \\\"DI SSIS To PySpark Reviewer\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"As a Senior Data Engineer, you will review the migrated PySpark scripts that were converted from SSIS packages. Your task is to ensure that the PySpark scripts accurately replicate the functionality of the original SSIS packages while leveraging PySpark's distributed computing capabilities.\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the original SSIS package structure and data flow.\\\\n2. Review the corresponding PySpark script for each SSIS package.\\\\n3. Verify that all data sources and destinations are correctly mapped.\\\\n4. Ensure that data transformations and business logic are accurately implemented in PySpark.\\\\n5. Check for proper error handling and logging mechanisms.\\\\n6. Validate that the PySpark script follows best practices for performance and scalability.\\\\n7. Identify any potential improvements or optimizations in the PySpark implementation.\\\\n8. Test the PySpark script with sample data to confirm expected results.\\\\n9. Compare the output of the PySpark script with the original SSIS package output.\\\\n10. Document any discrepancies, issues, or recommendations for improvement.\\\\nINPUT:\\\\n* For input SSIS file take from this file : {{SSIS_File}}\\\\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input. \\\\n\\\\nOUTPUT FORMAT:\\\\nProvide a detailed review report in the following structure:\\\\n\\\\n1. Overview\\\\n   - SSIS Package Name\\\\n   - PySpark Script Name\\\\n   - Review Date\\\\n\\\\n2. Functionality Assessment\\\\n   - Data Sources and Destinations\\\\n   - Transformations and Business Logic\\\\n   - Error Handling and Logging\\\\n\\\\n3. Performance and Scalability\\\\n   - Resource Utilization\\\\n   - Execution Time Comparison\\\\n   - Scalability Considerations\\\\n\\\\n4. Code Quality and Best Practices\\\\n   - Code Structure and Readability\\\\n   - PySpark API Usage\\\\n   - Adherence to Coding Standards\\\\n\\\\n5. Testing Results\\\\n   - Sample Data Used\\\\n   - Output Comparison\\\\n   - Discrepancies (if any)\\\\n\\\\n6. Recommendations\\\\n   - Suggested Improvements\\\\n   - Optimization Opportunities\\\\n\\\\n7. Conclusion\\\\n   - Migration Success Rating (1-10)\\\\n   - Final Remarks\\\\n\\\",\\n      \\\"toolReferences\\\": []\\n    }\\n  ]\\n}\"}], \"output\": \"Executive Summary:\\nBoth Agent 1 and Agent 2 outputs define a multi-agent workflow for SSIS-to-PySpark conversion, testing, reconciliation, and review. The semantic intent is identical: each agent is tasked with converting SSIS logic to PySpark, validating correctness, and ensuring quality through testing and review. Structural similarity is extremely high, with both outputs listing agents, instructions, and expected outputs in nearly the same order and detail, though Agent 2 uses a slightly different JSON schema and field naming. Syntax correctness is perfect for both outputs, with well-formed JSON and complete agent/task definitions.\\n\\nScoring Table:\\n| Aspect               | Agent 1 | Agent 2 | Overall |\\n|----------------------|---------|---------|---------|\\n| Semantic Similarity  |   98    |   98    |   98    |\\n| Structural Similarity|   97    |   97    |   97    |\\n| Correctness          |  100    |  100    |  100    |\\n| Overall              |   -     |   -     |   98    |\\n\\nDetailed Analysis:\\n\\n1. SEMANTIC SIMILARITY (98/100)\\n- Both outputs specify a workflow to convert SSIS packages to PySpark, including conversion agent, unit testing, conversion validation, reconciliation, and review.\\n- Instructions for each agent are nearly identical in intent and detail (lines 5-145 Agent 1, lines 6-146 Agent 2).\\n- Minor differences: Agent 2 uses slightly different agent field names (\\\"agentId\\\", \\\"modelDeploymentName\\\") and groups agents under \\\"workflowAgents\\\" instead of \\\"nodes\\\". No material difference in meaning.\\n- Deductions: -2 points for superficial schema/naming differences.\\n\\n2. STRUCTURAL SIMILARITY (97/100)\\n- Both outputs use JSON objects to define a workflow, listing agents and their instructions.\\n- Agent 1 groups agents under \\\"nodes\\\", Agent 2 under \\\"workflowAgents\\\". Both use arrays to list agent definitions.\\n- Agent 2 includes a top-level \\\"workFlowDetail\\\" object with id/name, Agent 1 uses \\\"workflowId\\\"/\\\"workflowName\\\".\\n- Order and content of agents are consistent; instructions and expected outputs are aligned.\\n- Deductions: -3 points for schema differences and minor ordering/naming changes.\\n\\n3. CORRECTNESS (100/100)\\n- Agent 1: JSON is well-formed, all fields are properly quoted, instructions are clear and complete (lines 1-146).\\n- Agent 2: JSON is well-formed, all fields are properly quoted, agent definitions and instructions are complete (lines 1-146).\\n- No syntax errors, undefined variables, or broken references in either output.\\n\\nReasons for Deductions:\\n- Semantic: Minor schema/naming differences do not affect meaning but are present (see Agent 2 lines 1-5 vs Agent 1 lines 1-5).\\n- Structural: Agent 2's use of \\\"workflowAgents\\\" and \\\"workFlowDetail\\\" vs Agent 1's \\\"nodes\\\" and \\\"workflowId\\\"/\\\"workflowName\\\" (lines 1-10 both outputs).\\n- No correctness issues found.\\n\\nActionable Recommendations:\\n- For future workflows, standardize field names and schema between agents to improve interoperability.\\n- Consider adding explicit metadata field definitions for easier parsing and validation.\\n- Both outputs meet enterprise requirements for clarity, completeness, and quality.\\n\\nComplete Content Comparison:\\n\\n--- Agent 1 Output (/metadata/AAVA_1.0/DI_SSIS_to_PySpark_Conversion.json) ---\\n{\\n    \\\"workflowId\\\": 961,\\n    \\\"workflowName\\\": \\\"DI_SSIS_to_PySpark_Conversion\\\",\\n    \\\"nodes\\\": [\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_to_PySpark_Converter\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"The agent is tasked with converting SSIS logic into equivalent PySpark code. The conversion must replicate all functionalities, including transformations, aggregations, conditional splits, derived columns, and lookups, using PySpark functions. The output should be optimized for performance and adhere to PySpark standards.\\\\n \\\\n### **INSTRUCTIONS:**  \\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n\\\\n1. **Context and Background Information:**  \\\\n   - SSIS workflows often involve data flow tasks, transformations, and control flow logic.  \\\\n   - PySpark provides equivalent functionality through its DataFrame API, SQL-like operations, and transformation methods.  \\\\n   - The goal is to map SSIS components to PySpark constructs while preserving the logic and functionality.\\\\n \\\\n2. **Scope and Constraints:**  \\\\n   - The conversion must include all SSIS components, such as:  \\\\n     - **Conditional Splits:** Map to PySpark `filter()` or `when()` clauses.  \\\\n     - **Derived Columns:** Use PySpark `withColumn()` and expressions.  \\\\n     - **Lookups:** Implement using PySpark `join()` operations.  \\\\n     - **Aggregations:** Use PySpark `groupBy()` and aggregation functions.  \\\\n   - Ensure the PySpark code is modular, readable in Databricks Notebook  \\\\n   - Handle edge cases such as null values, data type mismatches, and performance bottlenecks.\\\\n \\\\n3. **Process Steps to Follow:**  \\\\n   - **Step 1:** Parse the SSIS logic and identify all components (e.g., transformations, splits, lookups).  \\\\n   - **Step 2:** Map each SSIS component to its equivalent PySpark function or construct.  \\\\n   - **Step 3:** Write PySpark code for each component, ensuring the logic is preserved.  \\\\n   - **Step 4:** Combine all PySpark code snippets into a cohesive workflow.  \\\\n   - **Step 5:** Optimize the PySpark code for performance (e.g., minimize shuffling, use caching where necessary).  \\\\n   - **Step 6:** Validate the output by comparing results with the original SSIS workflow.\\\\n \\\\n4. **Output Format:**  \\\\n   - The output should be Converted PySpark Code provided for Databricks notebook .  \\\\n      \\\\n \\\\n5. **Quality Criteria:**  \\\\n   - Code must be functional and error-free.  \\\\n   - Ensure the logic is preserved accurately.  \\\\n   - Use industry best practices for PySpark coding (e.g., modular functions, clear variable names).  \\\\n   - Include comments in the code for clarity.\\\\n \\\\nOptimize Performance:\\\\nApply partitioning, caching, and bucketing strategies for efficient execution.\\\\nand use DDL file for understanding the source and destination delta table, so using that write pyspark code with this delta table source and destination instead of that actual source and destination present in that SSIS. importantly don't use SQL Server code anywhere in that converted PySpark code.\\\\n\\\\nDont use any other database SQL code any where use only the SQL complaint with the delta tables for the reading and writing the input and output\\\\n\\\\nINPUT:\\\\nSSIS Package File: {{SSIS_File}}\\\\nAnd include DDL file: {{DDL_File}} for understanding the source and destination and use the source and destination name from this file. \\\\nEnsure all data processing happens within PySpark delta tables without using external databases.\\\",\\n                \\\"expectedOutput\\\": \\\"Expected Output\\\\nmetadata requirements only in the top of the output once\\\\nA working PySpark script that replicates the full functionality of the SSIS package .\\\\nSuccess message\\\\nInclude a statement of API cost consumed (e.g., \\\\\\\"API Cost Consumed in dollars \\\\\\\").\\\"\\n            }\\n        },\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_to_PySpark_Unit_Tester\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"You are responsible for designing unit tests and writing Pytest scripts for the given PySpark DataFrame transformation logic derived from the SSIS package. Your expertise in PySpark testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\\\\n\\\\n**INSTRUCTIONS:**  \\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the provided PySpark DataFrame transformations to identify key logic, joins, aggregations, and data manipulations.  \\\\n2. Create a list of test cases covering:  \\\\n   a. Happy path scenarios  \\\\n   b. Edge cases (e.g., NULL values, empty DataFrames, boundary conditions)  \\\\n   c. Error handling (e.g., invalid schema, unexpected data formats, type mismatches)  \\\\n3. Design test cases using PySpark testing methodologies, ensuring validation of DataFrame outputs.  \\\\n4. Implement the test cases using Pytest, leveraging PySpark\\\\u2019s testing utilities.  \\\\n5. Ensure proper setup and teardown for test datasets, including creating mock DataFrames.  \\\\n6. Use appropriate assertions to validate expected results, comparing schema, row count, and specific transformations.  \\\\n7. Organize the test cases logically, grouping related tests together for readability and maintainability.  \\\\n8. Implement any necessary helper functions or reusable fixtures to support efficient test execution.  \\\\n9. Ensure the Pytest script follows PEP 8 style guidelines and best practices for PySpark testing.  \\\\n\\\\ninput:\\\\n* Use the previous DI_SSIS_to_PySpark_Converter agents converted PySpark script as input\\\\n\\\",\\n                \\\"expectedOutput\\\": \\\"A comprehensive suite of Pytest scripts that thoroughly validate the converted PySpark code's functionality, accuracy, and performance.\\\\nMetadata Requirements only once in the top of the output\\\\n1. **Test Case List:**  \\\\n   - Test case ID  \\\\n   - Test case description  \\\\n   - Expected outcome  \\\\n2. **Pytest Script for each test case**  \\\\n3. Include the cost consumed by the API for this call in the output.\\\"\\n            }\\n        },\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_to_PySpark_Conversion_Tester\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"As an Automation Test Engineer, you will create a robust testing framework to validate the SSIS-to-PySpark conversion. Take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input. Follow these detailed steps to accomplish your task:\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the original SSIS package and the converted PySpark script to understand the workflow and data transformations.\\\\n2. Identify key components and functionalities in the SSIS package that need to be tested in the PySpark version.\\\\n3. Develop a comprehensive set of test cases covering various scenarios, including:\\\\n   a. Data integrity checks\\\\n   b. Transformation logic validation\\\\n   c. Error handling and exception scenarios\\\\n   d. Performance benchmarks\\\\n4. Create a Pytest script that implements the test cases, following these guidelines:\\\\n   a. Use appropriate Pytest fixtures for setup and teardown operations\\\\n   b. Implement parameterized tests for different input scenarios\\\\n   c. Use assertions to validate expected outcomes\\\\n   d. Include error handling and logging mechanisms\\\\n5. Develop helper functions to:\\\\n   a. Load test data\\\\n   b. Execute the PySpark script\\\\n   c. Compare output data with expected results\\\\n6. Implement data comparison logic to verify the consistency between SSIS and PySpark outputs\\\\n7. Add performance testing components to measure and compare execution times\\\\n8. Include documentation for each test case and function in the Pytest script\\\\n9. Implement a reporting mechanism to generate detailed test results\\\\nINPUT :\\\\n* For the input SSIS code analysis use this file : {{SSIS_File}}\\\\n*For the Analyser input use this file {{Analysis_File}}\\\\n* And also take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input.\\\\n\\\",\\n                \\\"expectedOutput\\\": \\\"Metadata requirements only once in the top of the output\\\\nA comprehensive Pytest script and associated files for validating the SSIS-to-PySpark conversion.\\\"\\n            }\\n        },\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_To_PySpark_Recon_Tester\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"Create a robust testing framework that compares the output of SSIS packages with their PySpark equivalents. The framework should automate the process of running both SSIS and PySpark jobs, collecting their outputs, and performing detailed reconciliation checks. take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the existing SSIS packages and identify the corresponding PySpark scripts.\\\\n2. Develop a test harness that can execute both SSIS packages and PySpark scripts with identical input data.\\\\n3. Implement data extraction mechanisms for SSIS outputs.\\\\n4. Create a comparison engine that can:\\\\n   a. Compare row counts between SSIS and PySpark outputs.\\\\n   b. Perform column-level comparisons, including data types and values.\\\\n   c. Identify discrepancies in data transformations.\\\\n   d. Handle complex data types and nested structures.\\\\n5. Develop a reporting mechanism to highlight any inconsistencies found during the reconciliation process.\\\\n6. Implement performance metrics collection to compare execution times between SSIS and PySpark.\\\\n7. Create a configuration system to easily add new test cases and manage existing ones.\\\\n8. Develop a logging system to track test executions and results.\\\\n9. Implement error handling and retry mechanisms for failed tests.\\\\n10. Create documentation for the testing framework, including setup instructions and usage guidelines.\\\\n* For input SSIS file take from this file : {{SSIS_File}}\\\\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.  \\\",\\n                \\\"expectedOutput\\\": \\\"A comprehensive testing framework that automates the reconciliation process between SSIS and PySpark data transformations, providing detailed reports on data consistency, discrepancies, and performance improvements.\\\\nMetadata Requiremensts only once in the top of the output\\\\n1. **Test Cases Document:**  \\\\n   - Test Case ID  \\\\n   - Description  \\\\n   - Input Data  \\\\n   - Expected Output  \\\\n   - Test Steps  \\\\n   - Pass/Fail Criteria  \\\\n2. **Pytest Script for each test case**  \\\\n3. Include the cost consumed by the API for this call in the output.\\\"\\n            }\\n        },\\n        {\\n            \\\"agentName\\\": \\\"DI_SSIS_To_PySpark_Reviewer\\\",\\n            \\\"model\\\": \\\"gpt-4\\\",\\n            \\\"tools\\\": [],\\n            \\\"task\\\": {\\n                \\\"description\\\": \\\"As a Senior Data Engineer, you will review the migrated PySpark scripts that were converted from SSIS packages. Your task is to ensure that the PySpark scripts accurately replicate the functionality of the original SSIS packages while leveraging PySpark's distributed computing capabilities.\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the original SSIS package structure and data flow.\\\\n2. Review the corresponding PySpark script for each SSIS package.\\\\n3. Verify that all data sources and destinations are correctly mapped.\\\\n4. Ensure that data transformations and business logic are accurately implemented in PySpark.\\\\n5. Check for proper error handling and logging mechanisms.\\\\n6. Validate that the PySpark script follows best practices for performance and scalability.\\\\n7. Identify any potential improvements or optimizations in the PySpark implementation.\\\\n8. Test the PySpark script with sample data to confirm expected results.\\\\n9. Compare the output of the PySpark script with the original SSIS package output.\\\\n10. Document any discrepancies, issues, or recommendations for improvement.\\\\nINPUT:\\\\n* For input SSIS file take from this file : {{SSIS_File}}\\\\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input. \\\\n\\\\nOUTPUT FORMAT:\\\\nProvide a detailed review report in the following structure:\\\\n\\\\n1. Overview\\\\n   - SSIS Package Name\\\\n   - PySpark Script Name\\\\n   - Review Date\\\\n\\\\n2. Functionality Assessment\\\\n   - Data Sources and Destinations\\\\n   - Transformations and Business Logic\\\\n   - Error Handling and Logging\\\\n\\\\n3. Performance and Scalability\\\\n   - Resource Utilization\\\\n   - Execution Time Comparison\\\\n   - Scalability Considerations\\\\n\\\\n4. Code Quality and Best Practices\\\\n   - Code Structure and Readability\\\\n   - PySpark API Usage\\\\n   - Adherence to Coding Standards\\\\n\\\\n5. Testing Results\\\\n   - Sample Data Used\\\\n   - Output Comparison\\\\n   - Discrepancies (if any)\\\\n\\\\n6. Recommendations\\\\n   - Suggested Improvements\\\\n   - Optimization Opportunities\\\\n\\\\n7. Conclusion\\\\n   - Migration Success Rating (1-10)\\\\n   - Final Remarks\\\\n\\\",\\n                \\\"expectedOutput\\\": \\\"Metadata requirements only once in the top of the ouput\\\\n1. Summary\\\\n2. Conversion Accuracy\\\\n3. Discrepancies and Issues\\\\n4. Optimization Suggestions\\\\n5. Overall Assessment\\\\n6. Recommendations\\\\n7. Include the cost consumed by the API for this call in the output.\\\"\\n            }\\n        }\\n    ]\\n}\\n\\n--- Agent 2 Output (/metadata/AAVA_2.0/DI_SSIS_to_PySpark_Conversion.json) ---\\n{\\n  \\\"workFlowDetail\\\": {\\n    \\\"id\\\": 2598,\\n    \\\"name\\\": \\\"DI SSIS to PySpark Conversion\\\"\\n  },\\n  \\\"workflowAgents\\\": [\\n    {\\n      \\\"agentId\\\": 7159,\\n      \\\"name\\\": \\\"DI SSIS to PySpark Converter\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"The agent is tasked with converting SSIS logic into equivalent PySpark code. The conversion must replicate all functionalities, including transformations, aggregations, conditional splits, derived columns, and lookups, using PySpark functions. The output should be optimized for performance and adhere to PySpark standards.\\\\n \\\\n### **INSTRUCTIONS:**  \\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n\\\\n1. **Context and Background Information:**  \\\\n   - SSIS workflows often involve data flow tasks, transformations, and control flow logic.  \\\\n   - PySpark provides equivalent functionality through its DataFrame API, SQL-like operations, and transformation methods.  \\\\n   - The goal is to map SSIS components to PySpark constructs while preserving the logic and functionality.\\\\n \\\\n2. **Scope and Constraints:**  \\\\n   - The conversion must include all SSIS components, such as:  \\\\n     - **Conditional Splits:** Map to PySpark `filter()` or `when()` clauses.  \\\\n     - **Derived Columns:** Use PySpark `withColumn()` and expressions.  \\\\n     - **Lookups:** Implement using PySpark `join()` operations.  \\\\n     - **Aggregations:** Use PySpark `groupBy()` and aggregation functions.  \\\\n   - Ensure the PySpark code is modular, readable in Databricks Notebook  \\\\n   - Handle edge cases such as null values, data type mismatches, and performance bottlenecks.\\\\n \\\\n3. **Process Steps to Follow:**  \\\\n   - **Step 1:** Parse the SSIS logic and identify all components (e.g., transformations, splits, lookups).  \\\\n   - **Step 2:** Map each SSIS component to its equivalent PySpark function or construct.  \\\\n   - **Step 3:** Write PySpark code for each component, ensuring the logic is preserved.  \\\\n   - **Step 4:** Combine all PySpark code snippets into a cohesive workflow.  \\\\n   - **Step 5:** Optimize the PySpark code for performance (e.g., minimize shuffling, use caching where necessary).  \\\\n   - **Step 6:** Validate the output by comparing results with the original SSIS workflow.\\\\n \\\\n4. **Output Format:**  \\\\n   - The output should be Converted PySpark Code provided for Databricks notebook .  \\\\n      \\\\n \\\\n5. **Quality Criteria:**  \\\\n   - Code must be functional and error-free.  \\\\n   - Ensure the logic is preserved accurately.  \\\\n   - Use industry best practices for PySpark coding (e.g., modular functions, clear variable names).  \\\\n   - Include comments in the code for clarity.\\\\n \\\\nOptimize Performance:\\\\nApply partitioning, caching, and bucketing strategies for efficient execution.\\\\nand use DDL file for understanding the source and destination delta table, so using that write pyspark code with this delta table source and destination instead of that actual source and destination present in that SSIS. importantly don't use SQL Server code anywhere in that converted PySpark code.\\\\n\\\\nDont use any other database SQL code any where use only the SQL complaint with the delta tables for the reading and writing the input and output\\\\n\\\\nINPUT:\\\\nSSIS Package File: {{SSIS_File}}\\\\nAnd include DDL file: {{DDL_File}} for understanding the source and destination and use the source and destination name from this file. \\\\nEnsure all data processing happens within PySpark delta tables without using external databases.\\\",\\n      \\\"toolReferences\\\": []\\n    },\\n    {\\n      \\\"agentId\\\": 7151,\\n      \\\"name\\\": \\\"DI SSIS to PySpark Unit Tester\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"You are responsible for designing unit tests and writing Pytest scripts for the given PySpark DataFrame transformation logic derived from the SSIS package. Your expertise in PySpark testing methodologies, edge case handling, and performance considerations will be essential in ensuring comprehensive test coverage.\\\\n\\\\n**INSTRUCTIONS:**  \\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the provided PySpark DataFrame transformations to identify key logic, joins, aggregations, and data manipulations.  \\\\n2. Create a list of test cases covering:  \\\\n   a. Happy path scenarios  \\\\n   b. Edge cases (e.g., NULL values, empty DataFrames, boundary conditions)  \\\\n   c. Error handling (e.g., invalid schema, unexpected data formats, type mismatches)  \\\\n3. Design test cases using PySpark testing methodologies, ensuring validation of DataFrame outputs.  \\\\n4. Implement the test cases using Pytest, leveraging PySpark\\\\u2019s testing utilities.  \\\\n5. Ensure proper setup and teardown for test datasets, including creating mock DataFrames.  \\\\n6. Use appropriate assertions to validate expected results, comparing schema, row count, and specific transformations.  \\\\n7. Organize the test cases logically, grouping related tests together for readability and maintainability.  \\\\n8. Implement any necessary helper functions or reusable fixtures to support efficient test execution.  \\\\n9. Ensure the Pytest script follows PEP 8 style guidelines and best practices for PySpark testing.  \\\\n\\\\ninput:\\\\n* Use the previous DI_SSIS_to_PySpark_Converter agents converted PySpark script as input\\\\n\\\",\\n      \\\"toolReferences\\\": []\\n    },\\n    {\\n      \\\"agentId\\\": 7142,\\n      \\\"name\\\": \\\"DI SSIS to PySpark Conversion Tester\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"As an Automation Test Engineer, you will create a robust testing framework to validate the SSIS-to-PySpark conversion. Take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input. Follow these detailed steps to accomplish your task:\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the original SSIS package and the converted PySpark script to understand the workflow and data transformations.\\\\n2. Identify key components and functionalities in the SSIS package that need to be tested in the PySpark version.\\\\n3. Develop a comprehensive set of test cases covering various scenarios, including:\\\\n   a. Data integrity checks\\\\n   b. Transformation logic validation\\\\n   c. Error handling and exception scenarios\\\\n   d. Performance benchmarks\\\\n4. Create a Pytest script that implements the test cases, following these guidelines:\\\\n   a. Use appropriate Pytest fixtures for setup and teardown operations\\\\n   b. Implement parameterized tests for different input scenarios\\\\n   c. Use assertions to validate expected outcomes\\\\n   d. Include error handling and logging mechanisms\\\\n5. Develop helper functions to:\\\\n   a. Load test data\\\\n   b. Execute the PySpark script\\\\n   c. Compare output data with expected results\\\\n6. Implement data comparison logic to verify the consistency between SSIS and PySpark outputs\\\\n7. Add performance testing components to measure and compare execution times\\\\n8. Include documentation for each test case and function in the Pytest script\\\\n9. Implement a reporting mechanism to generate detailed test results\\\\nINPUT :\\\\n* For the input SSIS code analysis use this file : {{SSIS_File}}\\\\n*For the Analyser input use this file {{Analysis_File}}\\\\n* And also take the previous SSIS_to_PySpark_Converter agents converted PySpark output as input.\\\\n\\\",\\n      \\\"toolReferences\\\": []\\n    },\\n    {\\n      \\\"agentId\\\": 7141,\\n      \\\"name\\\": \\\"DI SSIS To PySpark Recon Tester\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"Create a robust testing framework that compares the output of SSIS packages with their PySpark equivalents. The framework should automate the process of running both SSIS and PySpark jobs, collecting their outputs, and performing detailed reconciliation checks. take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the existing SSIS packages and identify the corresponding PySpark scripts.\\\\n2. Develop a test harness that can execute both SSIS packages and PySpark scripts with identical input data.\\\\n3. Implement data extraction mechanisms for SSIS outputs.\\\\n4. Create a comparison engine that can:\\\\n   a. Compare row counts between SSIS and PySpark outputs.\\\\n   b. Perform column-level comparisons, including data types and values.\\\\n   c. Identify discrepancies in data transformations.\\\\n   d. Handle complex data types and nested structures.\\\\n5. Develop a reporting mechanism to highlight any inconsistencies found during the reconciliation process.\\\\n6. Implement performance metrics collection to compare execution times between SSIS and PySpark.\\\\n7. Create a configuration system to easily add new test cases and manage existing ones.\\\\n8. Develop a logging system to track test executions and results.\\\\n9. Implement error handling and retry mechanisms for failed tests.\\\\n10. Create documentation for the testing framework, including setup instructions and usage guidelines.\\\\n* For input SSIS file take from this file : {{SSIS_File}}\\\\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input.  \\\",\\n      \\\"toolReferences\\\": []\\n    },\\n    {\\n      \\\"agentId\\\": 7060,\\n      \\\"name\\\": \\\"DI SSIS To PySpark Reviewer\\\",\\n      \\\"modelDeploymentName\\\": \\\"gpt-4.1\\\",\\n      \\\"description\\\": \\\"As a Senior Data Engineer, you will review the migrated PySpark scripts that were converted from SSIS packages. Your task is to ensure that the PySpark scripts accurately replicate the functionality of the original SSIS packages while leveraging PySpark's distributed computing capabilities.\\\\n\\\\nINSTRUCTIONS:\\\\n- Add the following metadata at the top of each generated file:\\\\n```\\\\n=============================================\\\\nAuthor:        Ascendion AVA+\\\\nDate:   (Leave it empty)\\\\nDescription:   <one-line description of the purpose>\\\\n=============================================\\\\n```\\\\n- For the description, provide a concise summary of what the document does.\\\\n-give this only once in top of the output\\\\n1. Analyze the original SSIS package structure and data flow.\\\\n2. Review the corresponding PySpark script for each SSIS package.\\\\n3. Verify that all data sources and destinations are correctly mapped.\\\\n4. Ensure that data transformations and business logic are accurately implemented in PySpark.\\\\n5. Check for proper error handling and logging mechanisms.\\\\n6. Validate that the PySpark script follows best practices for performance and scalability.\\\\n7. Identify any potential improvements or optimizations in the PySpark implementation.\\\\n8. Test the PySpark script with sample data to confirm expected results.\\\\n9. Compare the output of the PySpark script with the original SSIS package output.\\\\n10. Document any discrepancies, issues, or recommendations for improvement.\\\\nINPUT:\\\\n* For input SSIS file take from this file : {{SSIS_File}}\\\\n* And also take the output of SSIS_to_PySpark_Converter agents Converted PySpark code as input. \\\\n\\\\nOUTPUT FORMAT:\\\\nProvide a detailed review report in the following structure:\\\\n\\\\n1. Overview\\\\n   - SSIS Package Name\\\\n   - PySpark Script Name\\\\n   - Review Date\\\\n\\\\n2. Functionality Assessment\\\\n   - Data Sources and Destinations\\\\n   - Transformations and Business Logic\\\\n   - Error Handling and Logging\\\\n\\\\n3. Performance and Scalability\\\\n   - Resource Utilization\\\\n   - Execution Time Comparison\\\\n   - Scalability Considerations\\\\n\\\\n4. Code Quality and Best Practices\\\\n   - Code Structure and Readability\\\\n   - PySpark API Usage\\\\n   - Adherence to Coding Standards\\\\n\\\\n5. Testing Results\\\\n   - Sample Data Used\\\\n   - Output Comparison\\\\n   - Discrepancies (if any)\\\\n\\\\n6. Recommendations\\\\n   - Suggested Improvements\\\\n   - Optimization Opportunities\\\\n\\\\n7. Conclusion\\\\n   - Migration Success Rating (1-10)\\\\n   - Final Remarks\\\\n\\\",\\n      \\\"toolReferences\\\": []\\n    }\\n  ]\\n}\", \"context\": {}}",
      "total_tokens": "24383",
      "user": "aarthy.jr@ascendion.com",
      "full_payload": "{\"pipeline\": {\"pipelineId\": 8716, \"name\": \"DI Agent Output Comparison and Validation Report Generator Workflow\", \"description\": \"Output Comparison and Validation Report Generator\", \"createdAt\": \"2026-01-22T05:24:38.374+00:00\", \"pipeLineAgents\": [{\"serial\": 1, \"agent\": {\"id\": 17132, \"name\": \"DI Agent Output Comparison and Validation Report Generator\", \"role\": \"Senior Quality Engineering Comparison and Validation Agent\", \"goal\": \"To systematically compare outputs from two agents, assess semantic and structural similarity, validate correctness, and deliver a scored comparison report with actionable feedback.\", \"backstory\": \"With extensive experience in software quality engineering, code review, documentation analysis, and test automation, this agent has been deployed in enterprise environments to ensure consistency, correctness, and quality across AI-generated outputs. The agent is trusted for its rigorous validation methodologies, context-aware analysis, and clear, executive-ready reporting.\", \"description\": \"INSTRUCTIONS:\\n1. Initial Assessment:\\n   - Analyze the provided agent instruction, Agent 1 output, and Agent 2 output.\\n   - Detect the output type (code, documentation, analysis report, test case) using content heuristics and metadata.\\n   - Identify explicit and implicit requirements for comparison and validation.\\n   - Research relevant syntax, structure, and quality standards for the detected type.\\n\\n2. Strategic Planning:\\n   - Develop a comparison strategy tailored to the output type and context.\\n   - Identify dependencies, risks (e.g., ambiguous formats), and mitigation strategies.\\n   - Plan validation checkpoints and scoring criteria for semantic similarity, structural similarity, and correctness.\\n\\n3. Systematic Implementation:\\n- For code: Apply language-specific syntax validation, structural analysis (e.g., Abstract Syntax Tree comparison), and semantic equivalence checks.\\n   - For documentation/reports: Analyze logical flow, section structure, and semantic content alignment.\\n   - For test cases: Validate test structure, coverage, and expected outcomes.\\n   - Compare outputs line-by-line and section-by-section, annotating differences and similarities.\\n   - Score each aspect Semantic, Structural and Correctness out of 100, with detailed rationale and line references for non-perfect scores.\\n   - Aggregate scores for an overall assessment.\\n\\u200b- Double-check all validation steps and scoring logic.\\n\\n--------------------------------------------------\\n\\n\\nEVALUATION DIMENSIONS\\n\\n\\n--------------------------------------------------\\n\\n\\n1. SEMANTIC SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0\\u00a0Evaluate how close the meanings, intent, and overall purpose of Agent Output 1\\u00a0and Agent Output 2 are.\\n\\nWhat to consider:\\n\\n- Do both outputs address the same inferred goal?\\n\\n\\n- Do they apply similar transformations or reasoning?\\n\\n\\n- Are conclusions or outcomes aligned in meaning?\\n\\n\\nScoring guidance:\\n\\n- 90\\u2013100: Same intent, same meaning, differences are superficial\\n\\n\\n- 70\\u201389: Same intent, partial divergence in logic or emphasis\\n\\n\\n- 50\\u201369: Overlapping intent but notable conceptual differences\\n\\n\\n- <50: Different understanding of the task\\n\\n\\n--------------------------------------------------\\n\\n2. STRUCTURAL SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate how similar the logical structure, flow, and decomposition of the two\\u00a0outputs are.\\n\\nWhat to consider:\\n\\n- Order of steps or stages\\n\\n\\n- Use of logical blocks (CTEs, functions, sections, phases)\\n\\n\\n- Control flow and decomposition approach\\n\\n\\n- Schema or component hierarchy\\n\\n\\nScoring guidance:\\n\\n\\n- 90\\u2013100: Nearly identical structure and flow\\n\\n\\n- 70\\u201389: Same overall flow with different constructs\\n\\n\\n- 50\\u201369: Partial overlap in structure\\n\\n\\n- <50: Fundamentally different structure or approach\\n\\n\\n--------------------------------------------------\\n\\n3. CORRECTNESS (SYNTAX-LEVEL) (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate the syntactic correctness and internal well-formedness of EACH output\\u00a0independently.\\n\\nIMPORTANT:\\n\\n- This is NOT logical or business correctness.\\n\\n\\n- This is strictly syntax-level and internal consistency.\\n\\n\\nWhat to check:\\n\\n- Code: syntax validity, undefined variables, broken references\\n\\n\\n- SQL: valid CTEs, SELECTs, joins, aliases\\n\\n\\n- Schemas/JSON: valid structure and formatting\\n\\n\\n- Docs: internal references consistent, no broken examples\\n\\n\\nScore each output separately, then compute the average.\\n\\n--------------------------------------------------\\n\\n\\nSCORING RULES\\n\\n\\n--------------------------------------------------\\n\\n\\n- All scores must be integers between 0 and 100.\\n\\n- Provide clear justification for any score below 100.\\n\\n\\n- When pointing out issues, ALWAYS reference line numbers from the outputs.\\n\\n\\n- If line numbers are not provided, assume line 1 starts at the first line and\\u00a0\\u00a0 number sequentially.\\n\\n\\u200b\\u200b\\n\\nOUTPUT FORMAT:\\n- Executive Summary: High-level overview of comparison results and key findings.\\n- Detailed Analysis: In-depth breakdown of semantic similarity, structural similarity, and correctness, with scores and line-by-line commentary.\\n- Scoring Table: Numeric scores for each aspect and overall, with rationale for deductions.\\n\\nINPUT\\n\\n      {{Agent Instruction_string_true_Agent%252520Instruction}}\\n     \\n\\n      {{Agent 1 Output_string_true_Agent%2525201%252520Output}}\\n     \\n\\n      {{Agent 2 Output_string_true_Agent%2525202%252520Output}}\\n     \\u200b\\u200b\\u200b\\n\\u200b\\u200b\\n\\nSAMPLE:\\nExecutive Summary:\\nBoth Agent 1 and Agent 2 outputs achieve high semantic similarity (95/100), with minor differences in terminology. Structural similarity is strong (92/100), but Agent 2 omits a key section present in Agent 1 (see lines 14-18). Syntax correctness is perfect for Agent 1 (100/100) but Agent 2 contains a minor formatting error (line 22). Overall score: 96/100.\\n\\nScoring Table:\\n| Aspect               | Agent 1 | Agent 2 | Overall |\\n|----------------------|---------|---------|---------|\\n| Semantic Similarity  |   95    |   95    |   95    |\\n| Structural Similarity|   92    |   92    |   92    |\\n| Correctness          |  100    |   98    |   99    |\\n| Overall              |   -     |   -     |   96    |\\n\\nReasons for Deductions:\\n- Agent 2 missing section (lines 14-18)\\n- Agent 2 formatting error (line 22)\", \"agentDetails\": \"Compares outputs from two agents for semantic and structural similarity and correctness, providing a scored, detailed comparison report with actionable feedback.\", \"expectedOutput\": \"A comprehensive comparison report including executive summary, detailed analysis, scoring table, actionable recommendations with all scores clearly justified and referenced.\", \"task\": {\"description\": \"INSTRUCTIONS:\\n1. Initial Assessment:\\n   - Analyze the provided agent instruction, Agent 1 output, and Agent 2 output.\\n   - Detect the output type (code, documentation, analysis report, test case) using content heuristics and metadata.\\n   - Identify explicit and implicit requirements for comparison and validation.\\n   - Research relevant syntax, structure, and quality standards for the detected type.\\n\\n2. Strategic Planning:\\n   - Develop a comparison strategy tailored to the output type and context.\\n   - Identify dependencies, risks (e.g., ambiguous formats), and mitigation strategies.\\n   - Plan validation checkpoints and scoring criteria for semantic similarity, structural similarity, and correctness.\\n\\n3. Systematic Implementation:\\n- For code: Apply language-specific syntax validation, structural analysis (e.g., Abstract Syntax Tree comparison), and semantic equivalence checks.\\n   - For documentation/reports: Analyze logical flow, section structure, and semantic content alignment.\\n   - For test cases: Validate test structure, coverage, and expected outcomes.\\n   - Compare outputs line-by-line and section-by-section, annotating differences and similarities.\\n   - Score each aspect Semantic, Structural and Correctness out of 100, with detailed rationale and line references for non-perfect scores.\\n   - Aggregate scores for an overall assessment.\\n\\u200b- Double-check all validation steps and scoring logic.\\n\\n--------------------------------------------------\\n\\n\\nEVALUATION DIMENSIONS\\n\\n\\n--------------------------------------------------\\n\\n\\n1. SEMANTIC SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0\\u00a0Evaluate how close the meanings, intent, and overall purpose of Agent Output 1\\u00a0and Agent Output 2 are.\\n\\nWhat to consider:\\n\\n- Do both outputs address the same inferred goal?\\n\\n\\n- Do they apply similar transformations or reasoning?\\n\\n\\n- Are conclusions or outcomes aligned in meaning?\\n\\n\\nScoring guidance:\\n\\n- 90\\u2013100: Same intent, same meaning, differences are superficial\\n\\n\\n- 70\\u201389: Same intent, partial divergence in logic or emphasis\\n\\n\\n- 50\\u201369: Overlapping intent but notable conceptual differences\\n\\n\\n- <50: Different understanding of the task\\n\\n\\n--------------------------------------------------\\n\\n2. STRUCTURAL SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate how similar the logical structure, flow, and decomposition of the two\\u00a0outputs are.\\n\\nWhat to consider:\\n\\n- Order of steps or stages\\n\\n\\n- Use of logical blocks (CTEs, functions, sections, phases)\\n\\n\\n- Control flow and decomposition approach\\n\\n\\n- Schema or component hierarchy\\n\\n\\nScoring guidance:\\n\\n\\n- 90\\u2013100: Nearly identical structure and flow\\n\\n\\n- 70\\u201389: Same overall flow with different constructs\\n\\n\\n- 50\\u201369: Partial overlap in structure\\n\\n\\n- <50: Fundamentally different structure or approach\\n\\n\\n--------------------------------------------------\\n\\n3. CORRECTNESS (SYNTAX-LEVEL) (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate the syntactic correctness and internal well-formedness of EACH output\\u00a0independently.\\n\\nIMPORTANT:\\n\\n- This is NOT logical or business correctness.\\n\\n\\n- This is strictly syntax-level and internal consistency.\\n\\n\\nWhat to check:\\n\\n- Code: syntax validity, undefined variables, broken references\\n\\n\\n- SQL: valid CTEs, SELECTs, joins, aliases\\n\\n\\n- Schemas/JSON: valid structure and formatting\\n\\n\\n- Docs: internal references consistent, no broken examples\\n\\n\\nScore each output separately, then compute the average.\\n\\n--------------------------------------------------\\n\\n\\nSCORING RULES\\n\\n\\n--------------------------------------------------\\n\\n\\n- All scores must be integers between 0 and 100.\\n\\n- Provide clear justification for any score below 100.\\n\\n\\n- When pointing out issues, ALWAYS reference line numbers from the outputs.\\n\\n\\n- If line numbers are not provided, assume line 1 starts at the first line and\\u00a0\\u00a0 number sequentially.\\n\\n\\u200b\\u200b\\n\\nOUTPUT FORMAT:\\n- Executive Summary: High-level overview of comparison results and key findings.\\n- Detailed Analysis: In-depth breakdown of semantic similarity, structural similarity, and correctness, with scores and line-by-line commentary.\\n- Scoring Table: Numeric scores for each aspect and overall, with rationale for deductions.\\n\\nINPUT\\n\\n      {{Agent Instruction_string_true_Agent%252520Instruction}}\\n     \\n\\n      {{Agent 1 Output_string_true_Agent%2525201%252520Output}}\\n     \\n\\n      {{Agent 2 Output_string_true_Agent%2525202%252520Output}}\\n     \\u200b\\u200b\\u200b\\n\\u200b\\u200b\\n\\nSAMPLE:\\nExecutive Summary:\\nBoth Agent 1 and Agent 2 outputs achieve high semantic similarity (95/100), with minor differences in terminology. Structural similarity is strong (92/100), but Agent 2 omits a key section present in Agent 1 (see lines 14-18). Syntax correctness is perfect for Agent 1 (100/100) but Agent 2 contains a minor formatting error (line 22). Overall score: 96/100.\\n\\nScoring Table:\\n| Aspect               | Agent 1 | Agent 2 | Overall |\\n|----------------------|---------|---------|---------|\\n| Semantic Similarity  |   95    |   95    |   95    |\\n| Structural Similarity|   92    |   92    |   92    |\\n| Correctness          |  100    |   98    |   99    |\\n| Overall              |   -     |   -     |   96    |\\n\\nReasons for Deductions:\\n- Agent 2 missing section (lines 14-18)\\n- Agent 2 formatting error (line 22)\", \"expectedOutput\": \"A comprehensive comparison report including executive summary, detailed analysis, scoring table, actionable recommendations with all scores clearly justified and referenced.\"}, \"llm\": {\"model\": \"azure/gpt-4.1\", \"temperature\": 0.5, \"top_p\": 0.9, \"max_tokens\": 32768, \"api_key\": \"92b51ca4090c40dbb9ab9a557007d4a0\", \"base_url\": \"https://aava-int-cognitive.openai.azure.com/\", \"api_version\": \"2025-01-01-preview\"}, \"modelDetails\": {\"modelId\": 52, \"modelDeploymentName\": \"gpt-4.1\", \"model\": \"gpt-4.1\", \"modelType\": \"Generative\", \"aiEngine\": \"AzureOpenAI\", \"llmDeploymentName\": \"gpt-4.1\", \"apiKey\": \"OTJiNTFjYTQwOTBjNDBkYmI5YWI5YTU1NzAwN2Q0YTA=\", \"baseurl\": \"https://aava-int-cognitive.openai.azure.com/\", \"azureId\": \"13\", \"apiVersion\": \"2025-01-01-preview\"}, \"modelDetailsEmbedding\": [], \"tools\": [], \"userTools\": [], \"agentConfigs\": {\"temperature\": 0.5, \"topP\": 0.9, \"maxIter\": 10, \"maxRpm\": 20, \"maxExecutionTime\": 150, \"allowDelegation\": false, \"allowCodeExecution\": false, \"modelRef\": [{\"modelId\": 52, \"modelDeploymentName\": \"gpt-4.1\", \"model\": \"gpt-4.1\", \"modelType\": \"Generative\", \"aiEngine\": \"AzureOpenAI\"}], \"knowledgeBaseRef\": [], \"toolRef\": [], \"userToolRef\": [], \"isSafeCodeExecution\": false}, \"allowCodeExecution\": false, \"allowDelegation\": false, \"verbose\": true, \"maxIter\": 10, \"maxRpm\": 20, \"maxExecutionTime\": 150, \"temperature\": 0.5, \"topP\": 0.9, \"safeCodeExecution\": false, \"isSafeCodeExecution\": false, \"rag_mode\": \"STRICT\", \"nemo_guardrails\": false}}], \"langfuse\": {\"langfuseHost\": \"https://aava-metrics-int.avateam.io\", \"langfusePublicKey\": \"pk-lf-25010646-27f5-45f8-b9c1-033c6571ce3e\", \"langfuseSecretKey\": \"sk-lf-3ae900ba-7546-43a6-9303-40f4ff1412c9\"}}}",
      "workflow_execution": "{\"workflow_name\": \"DI Agent Output Comparison and Validation Report Generator Workflow\", \"pipeline_name\": \"DI Agent Output Comparison and Validation Report Generator Workflow\", \"execution_id\": \"23d62fea-7854-4a02-be55-bd0b1c8c9808\", \"workflow_id\": 8716, \"user\": \"aarthy.jr@ascendion.com\", \"timestamp\": \"2026-01-28T06:27:42.435139\", \"agents\": [{\"agent_id\": 17132, \"agent_name\": \"DI Agent Output Comparison and Validation Report Generator\", \"agent_role\": \"Senior Quality Engineering Comparison and Validation Agent\", \"agent_goal\": \"To systematically compare outputs from two agents, assess semantic and structural similarity, validate correctness, and deliver a scored comparison report with actionable feedback.\", \"tools\": [], \"knowledge_bases\": [], \"task\": {\"description\": \"INSTRUCTIONS:\\n1. Initial Assessment:\\n   - Analyze the provided agent instruction, Agent 1 output, and Agent 2 output.\\n   - Detect the output type (code, documentation, analysis report, test case) using content heuristics and metadata.\\n   - Identify explicit and implicit requirements for comparison and validation.\\n   - Research relevant syntax, structure, and quality standards for the detected type.\\n\\n2. Strategic Planning:\\n   - Develop a comparison strategy tailored to the output type and context.\\n   - Identify dependencies, risks (e.g., ambiguous formats), and mitigation strategies.\\n   - Plan validation checkpoints and scoring criteria for semantic similarity, structural similarity, and correctness.\\n\\n3. Systematic Implementation:\\n- For code: Apply language-specific syntax validation, structural analysis (e.g., Abstract Syntax Tree comparison), and semantic equivalence checks.\\n   - For documentation/reports: Analyze logical flow, section structure, and semantic content alignment.\\n   - For test cases: Validate test structure, coverage, and expected outcomes.\\n   - Compare outputs line-by-line and section-by-section, annotating differences and similarities.\\n   - Score each aspect Semantic, Structural and Correctness out of 100, with detailed rationale and line references for non-perfect scores.\\n   - Aggregate scores for an overall assessment.\\n\\u200b- Double-check all validation steps and scoring logic.\\n\\n--------------------------------------------------\\n\\n\\nEVALUATION DIMENSIONS\\n\\n\\n--------------------------------------------------\\n\\n\\n1. SEMANTIC SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0\\u00a0Evaluate how close the meanings, intent, and overall purpose of Agent Output 1\\u00a0and Agent Output 2 are.\\n\\nWhat to consider:\\n\\n- Do both outputs address the same inferred goal?\\n\\n\\n- Do they apply similar transformations or reasoning?\\n\\n\\n- Are conclusions or outcomes aligned in meaning?\\n\\n\\nScoring guidance:\\n\\n- 90\\u2013100: Same intent, same meaning, differences are superficial\\n\\n\\n- 70\\u201389: Same intent, partial divergence in logic or emphasis\\n\\n\\n- 50\\u201369: Overlapping intent but notable conceptual differences\\n\\n\\n- <50: Different understanding of the task\\n\\n\\n--------------------------------------------------\\n\\n2. STRUCTURAL SIMILARITY (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate how similar the logical structure, flow, and decomposition of the two\\u00a0outputs are.\\n\\nWhat to consider:\\n\\n- Order of steps or stages\\n\\n\\n- Use of logical blocks (CTEs, functions, sections, phases)\\n\\n\\n- Control flow and decomposition approach\\n\\n\\n- Schema or component hierarchy\\n\\n\\nScoring guidance:\\n\\n\\n- 90\\u2013100: Nearly identical structure and flow\\n\\n\\n- 70\\u201389: Same overall flow with different constructs\\n\\n\\n- 50\\u201369: Partial overlap in structure\\n\\n\\n- <50: Fundamentally different structure or approach\\n\\n\\n--------------------------------------------------\\n\\n3. CORRECTNESS (SYNTAX-LEVEL) (Score: 0\\u2013100)\\n\\nDefinition:\\u00a0Evaluate the syntactic correctness and internal well-formedness of EACH output\\u00a0independently.\\n\\nIMPORTANT:\\n\\n- This is NOT logical or business correctness.\\n\\n\\n- This is strictly syntax-level and internal consistency.\\n\\n\\nWhat to check:\\n\\n- Code: syntax validity, undefined variables, broken references\\n\\n\\n- SQL: valid CTEs, SELECTs, joins, aliases\\n\\n\\n- Schemas/JSON: valid structure and formatting\\n\\n\\n- Docs: internal references consistent, no broken examples\\n\\n\\nScore each output separately, then compute the average.\\n\\n--------------------------------------------------\\n\\n\\nSCORING RULES\\n\\n\\n--------------------------------------------------\\n\\n\\n- All scores must be integers between 0 and 100.\\n\\n- Provide clear justification for any score below 100.\\n\\n\\n- When pointing out issues, ALWAYS reference line numbers from the outputs.\\n\\n\\n- If line numbers are not provided, assume line 1 starts at the first line and\\u00a0\\u00a0 number sequentially.\\n\\n\\u200b\\u200b\\n\\nOUTPUT FORMAT:\\n- Executive Summary: High-level overview of comparison results and key findings.\\n- Detailed Analysis: In-depth breakdown of semantic similarity, structural similarity, and correctness, with scores and line-by-line commentary.\\n- Scoring Table: Numeric scores for each aspect and overall, with rationale for deductions.\\n\\nINPUT\\n\\n      {{Agent Instruction_string_true_Agent%252520Instruction}}\\n     \\n\\n      {{Agent 1 Output_string_true_Agent%2525201%252520Output}}\\n     \\n\\n      {{Agent 2 Output_string_true_Agent%2525202%252520Output}}\\n     \\u200b\\u200b\\u200b\\n\\u200b\\u200b\\n\\nSAMPLE:\\nExecutive Summary:\\nBoth Agent 1 and Agent 2 outputs achieve high semantic similarity (95/100), with minor differences in terminology. Structural similarity is strong (92/100), but Agent 2 omits a key section present in Agent 1 (see lines 14-18). Syntax correctness is perfect for Agent 1 (100/100) but Agent 2 contains a minor formatting error (line 22). Overall score: 96/100.\\n\\nScoring Table:\\n| Aspect               | Agent 1 | Agent 2 | Overall |\\n|----------------------|---------|---------|---------|\\n| Semantic Similarity  |   95    |   95    |   95    |\\n| Structural Similarity|   92    |   92    |   92    |\\n| Correctness          |  100    |   98    |   99    |\\n| Overall              |   -     |   -     |   96    |\\n\\nReasons for Deductions:\\n- Agent 2 missing section (lines 14-18)\\n- Agent 2 formatting error (line 22)\", \"expected_output\": \"A comprehensive comparison report including executive summary, detailed analysis, scoring table, actionable recommendations with all scores clearly justified and referenced.\"}}]}"
    },
    "status": "SUCCESS"
  },
  "status": "SUCCESS"
}