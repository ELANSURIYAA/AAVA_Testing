================================
Author: AAVA
Created on: 
Description: Unit test cases and Pytest script for validating Databricks implementation of LOAD_FACT_EXECUTIVE_SUMMARY procedure, which loads FACT_EXECUTIVE_SUMMARY from STG_HOLDING_METRICS with data quality and referential integrity checks.
================================

Test Case List:

| Test Case ID | Description                                                                 | Expected Outcome                                                                                      |
|--------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All dimension and staging data present and valid                | Records inserted into FACT_EXECUTIVE_SUMMARY with correct transformations and joins                   |
| TC02         | Edge: STG_HOLDING_METRICS contains NULL income_amount                       | income_amount set to 0 in FACT_EXECUTIVE_SUMMARY                                                      |
| TC03         | Edge: STG_HOLDING_METRICS contains negative income_amount                   | income_amount set to 0 in FACT_EXECUTIVE_SUMMARY                                                      |
| TC04         | Edge: STG_HOLDING_METRICS is empty                                          | No records inserted into FACT_EXECUTIVE_SUMMARY                                                       |
| TC05         | Edge: Missing dimension key (e.g., unmatched product_id)                    | No record inserted for unmatched product_id                                                           |
| TC06         | Error: STG_HOLDING_METRICS missing required columns                         | Procedure fails with descriptive error                                                                |
| TC07         | Error: Invalid data types in STG_HOLDING_METRICS (e.g., string in amount)   | Procedure fails or skips invalid rows                                                                 |
| TC08         | Edge: Duplicate keys in dimension tables                                    | Only valid joins are performed, duplicates handled as per business logic                              |
| TC09         | Edge: Large dataset performance                                             | Procedure completes within reasonable time and inserts all valid records                              |
| TC10         | Edge: NULLs in dimension tables                                             | No record inserted for unmatched dimension keys                                                       |

Pytest Script:

```python
================================
Author: AAVA
Created on: 
Description: Pytest script for validating Databricks SQL logic for LOAD_FACT_EXECUTIVE_SUMMARY procedure.
================================

import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
import pandas as pd

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder \
        .appName("UnitTest_LOAD_FACT_EXECUTIVE_SUMMARY") \
        .master("local[2]") \
        .getOrCreate()
    yield spark
    spark.stop()

def create_staging_df(spark, data):
    return spark.createDataFrame([Row(**row) for row in data])

def create_dim_df(spark, data):
    return spark.createDataFrame([Row(**row) for row in data])

def run_procedure(spark, stg_df, dim_date_df, dim_inst_df, dim_corp_df, dim_prod_df):
    stg_df.createOrReplaceTempView("STG_HOLDING_METRICS")
    dim_date_df.createOrReplaceTempView("DIM_DATE")
    dim_inst_df.createOrReplaceTempView("DIM_INSTITUTION")
    dim_corp_df.createOrReplaceTempView("DIM_CORPORATION")
    dim_prod_df.createOrReplaceTempView("DIM_PRODUCT")
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW staging_metrics AS
        SELECT * FROM STG_HOLDING_METRICS
    """)
    spark.sql("""
        CREATE OR REPLACE TABLE FACT_EXECUTIVE_SUMMARY AS
        SELECT 
            dt.date_key,
            inst.institution_id,
            corp.corporation_id,
            prod.product_id,
            stg.a120_amount,
            stg.a120_count,
            stg.a30_to_59_amount,
            stg.a30_to_59_count,
            stg.a60_to_89_amount,
            stg.a60_to_89_count,
            stg.a90_to_119_amount,
            stg.a90_to_119_count,
            stg.charge_off_amount,
            stg.charge_off_count,
            stg.fraud_amount,
            stg.fraud_count,
            CASE 
                WHEN stg.income_amount IS NULL OR stg.income_amount < 0 THEN 0
                ELSE stg.income_amount
            END AS income_amount,
            stg.number_of_accounts,
            stg.purchases_amount,
            stg.purchases_count
        FROM staging_metrics stg
        INNER JOIN DIM_DATE dt ON dt.date_key = stg.date_value
        INNER JOIN DIM_INSTITUTION inst ON inst.institution_id = stg.institution_id
        INNER JOIN DIM_CORPORATION corp ON corp.corporation_id = stg.corporation_id
        INNER JOIN DIM_PRODUCT prod ON prod.product_id = stg.product_id
    """)
    return spark.table("FACT_EXECUTIVE_SUMMARY")

def teardown_tables(spark):
    for tbl in ["FACT_EXECUTIVE_SUMMARY", "staging_metrics", "STG_HOLDING_METRICS", "DIM_DATE", "DIM_INSTITUTION", "DIM_CORPORATION", "DIM_PRODUCT"]:
        spark.sql(f"DROP TABLE IF EXISTS {tbl}")
        spark.sql(f"DROP VIEW IF EXISTS {tbl}")

@pytest.mark.parametrize("stg_data, expected_income", [
    # TC01: Happy path
    ([{
        "date_value": 20230101, "institution_id": 1, "corporation_id": 1, "product_id": 1,
        "a120_amount": 100, "a120_count": 1, "a30_to_59_amount": 50, "a30_to_59_count": 1,
        "a60_to_89_amount": 30, "a60_to_89_count": 1, "a90_to_119_amount": 20, "a90_to_119_count": 1,
        "charge_off_amount": 10, "charge_off_count": 1, "fraud_amount": 5, "fraud_count": 1,
        "income_amount": 200, "number_of_accounts": 2, "purchases_amount": 150, "purchases_count": 3
    }], [200]),
    # TC02: NULL income_amount
    ([{
        "date_value": 20230101, "institution_id": 1, "corporation_id": 1, "product_id": 1,
        "a120_amount": 100, "a120_count": 1, "a30_to_59_amount": 50, "a30_to_59_count": 1,
        "a60_to_89_amount": 30, "a60_to_89_count": 1, "a90_to_119_amount": 20, "a90_to_119_count": 1,
        "charge_off_amount": 10, "charge_off_count": 1, "fraud_amount": 5, "fraud_count": 1,
        "income_amount": None, "number_of_accounts": 2, "purchases_amount": 150, "purchases_count": 3
    }], [0]),
    # TC03: Negative income_amount
    ([{
        "date_value": 20230101, "institution_id": 1, "corporation_id": 1, "product_id": 1,
        "a120_amount": 100, "a120_count": 1, "a30_to_59_amount": 50, "a30_to_59_count": 1,
        "a60_to_89_amount": 30, "a60_to_89_count": 1, "a90_to_119_amount": 20, "a90_to_119_count": 1,
        "charge_off_amount": 10, "charge_off_count": 1, "fraud_amount": 5, "fraud_count": 1,
        "income_amount": -100, "number_of_accounts": 2, "purchases_amount": 150, "purchases_count": 3
    }], [0]),
])
def test_income_amount_cases(spark, stg_data, expected_income):
    teardown_tables(spark)
    dim_date = [{"date_key": 20230101}]
    dim_inst = [{"institution_id": 1}]
    dim_corp = [{"corporation_id": 1}]
    dim_prod = [{"product_id": 1}]
    stg_df = create_staging_df(spark, stg_data)
    dim_date_df = create_dim_df(spark, dim_date)
    dim_inst_df = create_dim_df(spark, dim_inst)
    dim_corp_df = create_dim_df(spark, dim_corp)
    dim_prod_df = create_dim_df(spark, dim_prod)
    result_df = run_procedure(spark, stg_df, dim_date_df, dim_inst_df, dim_corp_df, dim_prod_df)
    result = [row.income_amount for row in result_df.collect()]
    assert result == expected_income

def test_empty_staging(spark):
    # TC04: Edge case - empty staging
    teardown_tables(spark)
    stg_df = create_staging_df(spark, [])
    dim_date_df = create_dim_df(spark, [{"date_key": 20230101}])
    dim_inst_df = create_dim_df(spark, [{"institution_id": 1}])
    dim_corp_df = create_dim_df(spark, [{"corporation_id": 1}])
    dim_prod_df = create_dim_df(spark, [{"product_id": 1}])
    result_df = run_procedure(spark, stg_df, dim_date_df, dim_inst_df, dim_corp_df, dim_prod_df)
    assert result_df.count() == 0

def test_missing_dimension_key(spark):
    # TC05: Edge case - unmatched product_id
    teardown_tables(spark)
    stg_data = [{
        "date_value": 20230101, "institution_id": 1, "corporation_id": 1, "product_id": 999,
        "a120_amount": 100, "a120_count": 1, "a30_to_59_amount": 50, "a30_to_59_count": 1,
        "a60_to_89_amount": 30, "a60_to_89_count": 1, "a90_to_119_amount": 20, "a90_to_119_count": 1,
        "charge_off_amount": 10, "charge_off_count": 1, "fraud_amount": 5, "fraud_count": 1,
        "income_amount": 200, "number_of_accounts": 2, "purchases_amount": 150, "purchases_count": 3
    }]
    stg_df = create_staging_df(spark, stg_data)
    dim_date_df = create_dim_df(spark, [{"date_key": 20230101}])
    dim_inst_df = create_dim_df(spark, [{"institution_id": 1}])
    dim_corp_df = create_dim_df(spark, [{"corporation_id": 1}])
    dim_prod_df = create_dim_df(spark, [{"product_id": 1}])  # product_id 999 missing
    result_df = run_procedure(spark, stg_df, dim_date_df, dim_inst_df, dim_corp_df, dim_prod_df)
    assert result_df.count() == 0

def test_missing_columns(spark):
    # TC06: Error case - missing required columns
    teardown_tables(spark)
    stg_data = [{
        "date_value": 20230101, "institution_id": 1, "corporation_id": 1, "product_id": 1
        # Missing amounts and counts
    }]
    stg_df = spark.createDataFrame(pd.DataFrame(stg_data))
    dim_date_df = create_dim_df(spark, [{"date_key": 20230101}])
    dim_inst_df = create_dim_df(spark, [{"institution_id": 1}])
    dim_corp_df = create_dim_df(spark, [{"corporation_id": 1}])
    dim_prod_df = create_dim_df(spark, [{"product_id": 1}])
    with pytest.raises(Exception):
        run_procedure(spark, stg_df, dim_date_df, dim_inst_df, dim_corp_df, dim_prod_df)

def test_invalid_data_types(spark):
    # TC07: Error case - invalid data types
    teardown_tables(spark)
    stg_data = [{
        "date_value": 20230101, "institution_id": 1, "corporation_id": 1, "product_id": 1,
        "a120_amount": "invalid", "a120_count": 1, "a30_to_59_amount": 50, "a30_to_59_count": 1,
        "a60_to_89_amount": 30, "a60_to_89_count": 1, "a90_to_119_amount": 20, "a90_to_119_count": 1,
        "charge_off_amount": 10, "charge_off_count": 1, "fraud_amount": 5, "fraud_count": 1,
        "income_amount": 200, "number_of_accounts": 2, "purchases_amount": 150, "purchases_count": 3
    }]
    stg_df = create_staging_df(spark, stg_data)
    dim_date_df = create_dim_df(spark, [{"date_key": 20230101}])
    dim_inst_df = create_dim_df(spark, [{"institution_id": 1}])
    dim_corp_df = create_dim_df(spark, [{"corporation_id": 1}])
    dim_prod_df = create_dim_df(spark, [{"product_id": 1}])
    with pytest.raises(Exception):
        run_procedure(spark, stg_df, dim_date_df, dim_inst_df, dim_corp_df, dim_prod_df)

def test_duplicate_keys_in_dimension(spark):
    # TC08: Edge case - duplicate keys in dimension tables
    teardown_tables(spark)
    stg_data = [{
        "date_value": 20230101, "institution_id": 1, "corporation_id": 1, "product_id": 1,
        "a120_amount": 100, "a120_count": 1, "a30_to_59_amount": 50, "a30_to_59_count": 1,
        "a60_to_89_amount": 30, "a60_to_89_count": 1, "a90_to_119_amount": 20, "a90_to_119_count": 1,
        "charge_off_amount": 10, "charge_off_count": 1, "fraud_amount": 5, "fraud_count": 1,
        "income_amount": 200, "number_of_accounts": 2, "purchases_amount": 150, "purchases_count": 3
    }]
    stg_df = create_staging_df(spark, stg_data)
    dim_date_df = create_dim_df(spark, [{"date_key": 20230101}, {"date_key": 20230101}])
    dim_inst_df = create_dim_df(spark, [{"institution_id": 1}, {"institution_id": 1}])
    dim_corp_df = create_dim_df(spark, [{"corporation_id": 1}, {"corporation_id": 1}])
    dim_prod_df = create_dim_df(spark, [{"product_id": 1}, {"product_id": 1}])
    result_df = run_procedure(spark, stg_df, dim_date_df, dim_inst_df, dim_corp_df, dim_prod_df)
    # Should insert as many records as the cartesian join allows; business logic may require deduplication
    assert result_df.count() >= 1

def test_large_dataset_performance(spark):
    # TC09: Edge case - large dataset
    teardown_tables(spark)
    stg_data = [{
        "date_value": 20230101, "institution_id": 1, "corporation_id": 1, "product_id": 1,
        "a120_amount": i, "a120_count": i, "a30_to_59_amount": i, "a30_to_59_count": i,
        "a60_to_89_amount": i, "a60_to_89_count": i, "a90_to_119_amount": i, "a90_to_119_count": i,
        "charge_off_amount": i, "charge_off_count": i, "fraud_amount": i, "fraud_count": i,
        "income_amount": i, "number_of_accounts": i, "purchases_amount": i, "purchases_count": i
    } for i in range(1000)]
    stg_df = create_staging_df(spark, stg_data)
    dim_date_df = create_dim_df(spark, [{"date_key": 20230101}])
    dim_inst_df = create_dim_df(spark, [{"institution_id": 1}])
    dim_corp_df = create_dim_df(spark, [{"corporation_id": 1}])
    dim_prod_df = create_dim_df(spark, [{"product_id": 1}])
    result_df = run_procedure(spark, stg_df, dim_date_df, dim_inst_df, dim_corp_df, dim_prod_df)
    assert result_df.count() == 1000

def test_nulls_in_dimension_tables(spark):
    # TC10: Edge case - NULLs in dimension tables
    teardown_tables(spark)
    stg_data = [{
        "date_value": 20230101, "institution_id": 1, "corporation_id": 1, "product_id": 1,
        "a120_amount": 100, "a120_count": 1, "a30_to_59_amount": 50, "a30_to_59_count": 1,
        "a60_to_89_amount": 30, "a60_to_89_count": 1, "a90_to_119_amount": 20, "a90_to_119_count": 1,
        "charge_off_amount": 10, "charge_off_count": 1, "fraud_amount": 5, "fraud_count": 1,
        "income_amount": 200, "number_of_accounts": 2, "purchases_amount": 150, "purchases_count": 3
    }]
    stg_df = create_staging_df(spark, stg_data)
    dim_date_df = create_dim_df(spark, [{"date_key": None}])
    dim_inst_df = create_dim_df(spark, [{"institution_id": None}])
    dim_corp_df = create_dim_df(spark, [{"corporation_id": None}])
    dim_prod_df = create_dim_df(spark, [{"product_id": None}])
    result_df = run_procedure(spark, stg_df, dim_date_df, dim_inst_df, dim_corp_df, dim_prod_df)
    assert result_df.count() == 0

```

apiCost: 0.0040 USD