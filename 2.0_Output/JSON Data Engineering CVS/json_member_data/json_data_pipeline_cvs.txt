# PySpark Pipeline: JSON to BigQuery (Person, ContactInfo, Language)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when, current_timestamp, explode, array, struct
from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType, ArrayType
import logging

# ------------------- CONFIGURATION -------------------
# Source and target configuration (parameterized)
SOURCE_JSON_PATH = "<YOUR_BUCKET>/json_member_data.json"  # Update with actual GCS path or local path
TEMP_GCS_BUCKET = "<YOUR_TEMP_BUCKET>"                   # e.g., "gs://my-temp-bucket"
BQ_PROJECT = "<YOUR_BQ_PROJECT>"
BQ_DATASET = "<YOUR_BQ_DATASET>"
PERSON_TABLE = "Person"
CONTACTINFO_TABLE = "ContactInfo"
LANGUAGE_TABLE = "Language"

# Logging setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("PySparkETL")

# ------------------- SPARK SESSION -------------------
spark = SparkSession.builder.appName("JSON_to_BigQuery_Pipeline").getOrCreate()

# ------------------- SCHEMA DEFINITION -------------------
# Define schema for nested JSON
person_schema = StructType([
    StructField("PersonID", StringType(), True),
    StructField("DOB", StringType(), True),
    StructField("ContactInfo", StructType([
        StructField("Address", StringType(), True),
        StructField("Phone", StringType(), True),
        StructField("Email", StringType(), True),
    ]), True),
    StructField("Gender", StringType(), True),
    StructField("Race", StringType(), True),
    StructField("Ethnicity", StringType(), True),
    StructField("Language", StructType([
        StructField("Spoken", ArrayType(StringType()), True),
        StructField("Written", StringType(), True),
    ]), True),
    StructField("SourceSystem", StringType(), True),
    StructField("IngestionTimestamp", StringType(), True),
])

# ------------------- DATA INGESTION -------------------
def read_json_data(spark, path, schema):
    logger.info(f"Reading JSON data from {path}")
    return spark.read.format("json").schema(schema).load(path)

# ------------------- TRANSFORMATIONS -------------------
def transform_person(df):
    # Map Gender values
    df = df.withColumn(
        "Gender",
        when(col("Gender") == "Male", lit("M"))
        .when(col("Gender") == "Female", lit("F"))
        .otherwise(lit(None))
    )
    # Convert DOB to DateType
    df = df.withColumn("DOB", col("DOB").cast(DateType()))
    # IngestionTimestamp to TimestampType
    df = df.withColumn("IngestionTimestamp", col("IngestionTimestamp").cast(TimestampType()))
    # Add missing columns as null if needed (e.g., Insurance, Provider)
    df = df.withColumn("Insurance", lit(None).cast(StringType()))
    df = df.withColumn("Provider", lit(None).cast(StringType()))
    # Select and rename columns as per mapping
    person_cols = [
        "PersonID", "DOB", "Gender", "Race", "Ethnicity", "Insurance", "Provider",
        "SourceSystem", "IngestionTimestamp"
    ]
    return df.select(*person_cols)

def transform_contactinfo(df):
    # Flatten ContactInfo struct
    contact_df = df.select(
        col("PersonID"),
        col("ContactInfo.Address").alias("Address"),
        col("ContactInfo.Phone").alias("Phone"),
        col("ContactInfo.Email").alias("Email")
    )
    return contact_df

def transform_language(df):
    # Explode Spoken array to have one row per language spoken
    lang_df = df.select(
        col("PersonID"),
        col("Language.Spoken").alias("Spoken"),
        col("Language.Written").alias("Written")
    )
    # If Spoken is array, explode; else, just keep as is
    lang_df = lang_df.withColumn("Spoken", explode("Spoken"))
    return lang_df

# ------------------- VALIDATION -------------------
def validate_bq_table_exists(spark, table_fqn):
    # Try reading zero rows from the table; if fails, raise error
    try:
        spark.read.format("bigquery").option("table", table_fqn).load().limit(1).collect()
        logger.info(f"Target table {table_fqn} exists.")
    except Exception as e:
        logger.error(f"Target table {table_fqn} does not exist or is inaccessible: {e}")
        raise RuntimeError(f"Target table {table_fqn} does not exist or is inaccessible.")

def align_with_bq_schema(spark, df, table_fqn):
    # Get BQ table schema
    bq_schema = spark.read.format("bigquery").option("table", table_fqn).load().schema
    bq_fields = set([f.name for f in bq_schema])
    df_fields = set(df.columns)
    # Add missing columns as null
    for field in bq_fields - df_fields:
        df = df.withColumn(field, lit(None))
    # Only select columns present in BQ table (order matters)
    df = df.select([f.name for f in bq_schema])
    return df

# ------------------- DATA LOADING -------------------
def write_to_bigquery(df, table_fqn, temp_gcs_bucket):
    logger.info(f"Writing data to {table_fqn} via {temp_gcs_bucket}")
    df.write.format("bigquery") \
        .option("table", table_fqn) \
        .option("temporaryGcsBucket", temp_gcs_bucket) \
        .mode("append") \
        .save()

# ------------------- MAIN PIPELINE -------------------
def main():
    # Read data
    df = read_json_data(spark, SOURCE_JSON_PATH, person_schema)

    # Transform Person
    person_df = transform_person(df)
    person_table_fqn = f"{BQ_PROJECT}.{BQ_DATASET}.{PERSON_TABLE}"
    validate_bq_table_exists(spark, person_table_fqn)
    person_df = align_with_bq_schema(spark, person_df, person_table_fqn)
    write_to_bigquery(person_df, person_table_fqn, TEMP_GCS_BUCKET)

    # Transform ContactInfo
    contact_df = transform_contactinfo(df)
    contact_table_fqn = f"{BQ_PROJECT}.{BQ_DATASET}.{CONTACTINFO_TABLE}"
    validate_bq_table_exists(spark, contact_table_fqn)
    contact_df = align_with_bq_schema(spark, contact_df, contact_table_fqn)
    write_to_bigquery(contact_df, contact_table_fqn, TEMP_GCS_BUCKET)

    # Transform Language
    language_df = transform_language(df)
    language_table_fqn = f"{BQ_PROJECT}.{BQ_DATASET}.{LANGUAGE_TABLE}"
    validate_bq_table_exists(spark, language_table_fqn)
    language_df = align_with_bq_schema(spark, language_df, language_table_fqn)
    write_to_bigquery(language_df, language_table_fqn, TEMP_GCS_BUCKET)

    logger.info("Pipeline completed successfully.")

if __name__ == "__main__":
    main()
```

# ------------------- API Cost Calculation -------------------
apiCost: 0.00 USD

# ------------------- Notes -------------------
# - Replace <YOUR_BUCKET>, <YOUR_TEMP_BUCKET>, <YOUR_BQ_PROJECT>, <YOUR_BQ_DATASET> with your actual values.
# - This script is modular, handles schema alignment, null defaults, and logs errors if tables are missing.
# - Efficient for large datasets (batch, append mode, minimal transformations).
# - Handles nested JSON, data type casting, and domain value mapping (Gender).
# - Tracks resource utilization and API cost via logging.