=============================================
Author:        Ascendion AVA+
Date:   
Description:   PySpark workflow to load and transform Billing Account data into the DimBillingAccount Delta table, replicating the full logic of the SSIS EDW_BC_Load_DimBillingAccount package using only PySpark DataFrame and Delta Lake operations.
=============================================

# Databricks PySpark Notebook: Load DimBillingAccount

from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import *
from delta.tables import DeltaTable

# ------------------------------------------
# Parameters (replace with widgets/configs as needed)
# ------------------------------------------
# Example: days_back = -1 for yesterday, etc.
days_back = -1
batch_id = 12345  # Replace with actual batch id logic

# ------------------------------------------
# Helper Functions
# ------------------------------------------

def get_current_timestamp():
    return F.current_timestamp()

# ------------------------------------------
# Source Table Names (from DDL)
# ------------------------------------------
# Replace with your actual Delta table paths if needed
SRC_BC_ACCOUNT = "bc_account"
SRC_BCTL_ACCOUNTTYPE = "bctl_accounttype"
SRC_BCTL_BILLINGLEVEL = "bctl_billinglevel"
SRC_BCTL_CUSTOMERSERVICETIER = "bctl_customerservicetier"
SRC_BC_SECURITYZONE = "bc_securityzone"
SRC_BCTL_DELINQUENCYSTATUS = "bctl_delinquencystatus"
SRC_BCTL_ACCOUNTSEGMENT = "bctl_accountsegment"
SRC_BC_PARENTACCT = "bc_ParentAcct"
SRC_BC_ACCOUNTCONTACT = "bc_accountcontact"
SRC_BC_ACCOUNTCONTACTROLE = "bc_accountcontactrole"
SRC_BCTL_ACCOUNTROLE = "bctl_accountrole"
SRC_BC_CONTACT = "bc_contact"
SRC_BC_ADDRESS = "bc_address"
SRC_BCTL_STATE = "bctl_state"

TGT_DIM_BILLING_ACCOUNT = "DimBillingAccount"

# ------------------------------------------
# Read Source Delta Tables
# ------------------------------------------

bc_account = spark.read.format("delta").table(SRC_BC_ACCOUNT)
bctl_accounttype = spark.read.format("delta").table(SRC_BCTL_ACCOUNTTYPE)
bctl_billinglevel = spark.read.format("delta").table(SRC_BCTL_BILLINGLEVEL)
bctl_customerservicetier = spark.read.format("delta").table(SRC_BCTL_CUSTOMERSERVICETIER)
bc_securityzone = spark.read.format("delta").table(SRC_BC_SECURITYZONE)
bctl_delinquencystatus = spark.read.format("delta").table(SRC_BCTL_DELINQUENCYSTATUS)
bctl_accountsegment = spark.read.format("delta").table(SRC_BCTL_ACCOUNTSEGMENT)
bc_parentacct = spark.read.format("delta").table(SRC_BC_PARENTACCT)
bc_accountcontact = spark.read.format("delta").table(SRC_BC_ACCOUNTCONTACT)
bc_accountcontactrole = spark.read.format("delta").table(SRC_BC_ACCOUNTCONTACTROLE)
bctl_accountrole = spark.read.format("delta").table(SRC_BCTL_ACCOUNTROLE)
bc_contact = spark.read.format("delta").table(SRC_BC_CONTACT)
bc_address = spark.read.format("delta").table(SRC_BC_ADDRESS)
bctl_state = spark.read.format("delta").table(SRC_BCTL_STATE)

# ------------------------------------------
# Build ParentAcct CTE
# ------------------------------------------
parent_acct = (
    bc_parentacct.alias("pa")
    .join(
        bc_account.alias("act"),
        F.col("act.ID") == F.col("pa.ForeignEntityID"),
        "inner"
    )
    .select(
        F.col("pa.OwnerID"),
        F.col("act.AccountNumber").cast("int").alias("ParentAccountNumber"),
        F.concat(F.col("pa.BeanVersion"), F.col("act.BeanVersion")).alias("BeanVersion"),
        F.col("act.UpdateTime").alias("UpdateTime")
    )
)

# ------------------------------------------
# Build InsuredInfo CTE
# ------------------------------------------
insured_info = (
    bc_accountcontact.alias("ac")
    .join(
        bc_accountcontactrole.alias("acr"),
        F.col("acr.AccountContactID") == F.col("ac.ID"),
        "inner"
    )
    .join(
        bctl_accountrole.alias("tlar"),
        F.col("tlar.ID") == F.col("acr.Role"),
        "inner"
    )
    .join(
        bc_contact.alias("c"),
        F.col("c.ID") == F.col("ac.ContactID"),
        "left"
    )
    .join(
        bc_address.alias("a"),
        F.col("a.ID") == F.col("c.PrimaryAddressID"),
        "left"
    )
    .join(
        bctl_state.alias("tls"),
        F.col("tls.ID") == F.col("a.State"),
        "left"
    )
    .filter(F.col("tlar.TYPECODE") == "insured")
    .select(
        F.col("ac.InsuredAccountID").alias("AccountID"),
        F.col("c.FirstName"),
        F.col("c.LastName"),
        F.col("a.AddressLine1"),
        F.col("a.AddressLine2"),
        F.col("a.AddressLine3"),
        F.col("a.City"),
        F.col("a.PostalCode"),
        F.col("tls.NAME").alias("State"),
        F.concat(
            F.col("ac.BeanVersion"),
            F.col("acr.BeanVersion"),
            F.col("c.BeanVersion"),
            F.col("a.BeanVersion")
        ).alias("BeanVersion"),
        F.col("ac.UpdateTime").alias("ac_UpdateTime"),
        F.col("acr.UpdateTime").alias("acr_UpdateTime"),
        F.col("c.UpdateTime").alias("c_UpdateTime"),
        F.col("a.UpdateTime").alias("a_UpdateTime")
    )
)

# ------------------------------------------
# Join All for Main Source DataFrame
# ------------------------------------------

src_df = (
    bc_account.alias("dt")
    .join(bctl_accounttype.alias("at"), F.col("at.ID") == F.col("dt.AccountType"), "left")
    .join(parent_acct.alias("ParentAcct"), F.col("ParentAcct.OwnerID") == F.col("dt.ID"), "left")
    .join(bctl_billinglevel.alias("bl"), F.col("bl.ID") == F.col("dt.BillingLevel"), "left")
    .join(bctl_customerservicetier.alias("cst"), F.col("cst.ID") == F.col("dt.ServiceTier"), "left")
    .join(bc_securityzone.alias("sz"), F.col("sz.ID") == F.col("dt.SecurityZoneID"), "left")
    .join(insured_info.alias("InsuredInfo"), F.col("InsuredInfo.AccountID") == F.col("dt.ID"), "left")
    .join(bctl_delinquencystatus.alias("tlds"), F.col("tlds.ID") == F.col("dt.DelinquencyStatus"), "left")
    .join(bctl_accountsegment.alias("bas"), F.col("bas.ID") == F.col("dt.Segment"), "left")
    .select(
        F.col("dt.AccountNumber"),
        F.col("dt.AccountName"),
        F.col("at.NAME").cast("string").alias("AccountTypeName"),
        F.col("ParentAcct.ParentAccountNumber"),
        F.col("bl.NAME").cast("string").alias("BillingLevelName"),
        F.col("bas.NAME").cast("string").alias("Segment"),
        F.col("cst.NAME").cast("string").alias("ServiceTierName"),
        F.col("sz.Name").alias("SecurityZone"),
        F.col("InsuredInfo.FirstName"),
        F.col("InsuredInfo.LastName"),
        F.col("InsuredInfo.AddressLine1"),
        F.col("InsuredInfo.AddressLine2"),
        F.col("InsuredInfo.AddressLine3"),
        F.col("InsuredInfo.City").cast("string").alias("City"),
        F.col("InsuredInfo.State").cast("string").alias("State"),
        F.col("InsuredInfo.PostalCode").cast("string").alias("PostalCode"),
        F.col("dt.CloseDate").alias("AccountCloseDate"),
        F.col("dt.CreateTime").alias("AccountCreationDate"),
        F.col("tlds.NAME").cast("string").alias("DeliquencyStatusName"),
        F.col("dt.FirstTwicePerMthInvoiceDOM").alias("FirstTwicePerMonthInvoiceDayOfMonth"),
        F.col("dt.SecondTwicePerMthInvoiceDOM").alias("SecondTwicePerMonthInvoiceDayOfMonth"),
        F.col("dt.PublicID"),
        F.col("dt.ID").alias("GWRowNumber"),
        F.concat(
            F.col("dt.BeanVersion"),
            F.coalesce(F.col("ParentAcct.BeanVersion"), F.lit("")),
            F.coalesce(F.col("sz.BeanVersion"), F.lit(""))
        ).cast("string").alias("BeanVersion"),
        F.when(F.col("dt.Retired") == 0, F.lit(1)).otherwise(F.lit(0)).alias("IsActive"),
        F.lit("WC").alias("LegacySourceSystem"),
        # For incremental load filtering
        F.col("dt.UpdateTime"),
        F.col("ParentAcct.UpdateTime").alias("ParentAcct_UpdateTime"),
        F.col("sz.UpdateTime").alias("sz_UpdateTime"),
        F.col("InsuredInfo.ac_UpdateTime"),
        F.col("InsuredInfo.acr_UpdateTime"),
        F.col("InsuredInfo.c_UpdateTime"),
        F.col("InsuredInfo.a_UpdateTime")
    )
)

# ------------------------------------------
# Incremental Load Filter (DATEADD(D,?,GETDATE()))
# ------------------------------------------
from datetime import datetime, timedelta

date_threshold = datetime.now() + timedelta(days=days_back)

src_df_filtered = src_df.filter(
    (F.col("UpdateTime") >= F.lit(date_threshold)) |
    (F.col("ParentAcct_UpdateTime") >= F.lit(date_threshold)) |
    (F.col("sz_UpdateTime") >= F.lit(date_threshold)) |
    (F.col("ac_UpdateTime") >= F.lit(date_threshold)) |
    (F.col("acr_UpdateTime") >= F.lit(date_threshold)) |
    (F.col("c_UpdateTime") >= F.lit(date_threshold)) |
    (F.col("a_UpdateTime") >= F.lit(date_threshold))
)

# ------------------------------------------
# Select Only Required Columns for Target Table
# ------------------------------------------
final_cols = [
    "PublicID", "AccountNumber", "AccountName", "AccountTypeName", "ParentAccountNumber",
    "BillingLevelName", "Segment", "ServiceTierName", "SecurityZone", "FirstName", "LastName",
    "AddressLine1", "AddressLine2", "AddressLine3", "City", "State", "PostalCode",
    "AccountCloseDate", "AccountCreationDate", "DeliquencyStatusName",
    "FirstTwicePerMonthInvoiceDayOfMonth", "SecondTwicePerMonthInvoiceDayOfMonth",
    "GWRowNumber", "BeanVersion", "IsActive", "LegacySourceSystem"
]

src_final_df = src_df_filtered.select(*final_cols)

# ------------------------------------------
# Lookup Existing DimBillingAccount for SCD2/Upsert Logic
# ------------------------------------------
tgt_df = spark.read.format("delta").table(TGT_DIM_BILLING_ACCOUNT)

# Join on PublicID and LegacySourceSystem to find matches
lookup_df = tgt_df.select("PublicID", "BeanVersion", "LegacySourceSystem")

src_with_lookup = src_final_df.join(
    lookup_df,
    (src_final_df.PublicID == lookup_df.PublicID) & 
    (src_final_df.LegacySourceSystem == lookup_df.LegacySourceSystem),
    "left"
).withColumn(
    "is_match", F.when(src_final_df.BeanVersion == lookup_df.BeanVersion, F.lit(True)).otherwise(F.lit(False))
).withColumn(
    "is_existing", F.when(lookup_df.PublicID.isNotNull(), F.lit(True)).otherwise(F.lit(False))
)

# ------------------------------------------
# Split for Insert, Update, Unchanged
# ------------------------------------------
insert_df = src_with_lookup.filter(~F.col("is_existing")).drop("is_match", "is_existing", "BeanVersion_1")
update_df = src_with_lookup.filter((F.col("is_existing")) & (~F.col("is_match"))).drop("is_match", "is_existing", "BeanVersion_1")
unchange_df = src_with_lookup.filter((F.col("is_existing")) & (F.col("is_match"))).drop("is_match", "is_existing", "BeanVersion_1")

# ------------------------------------------
# Write to DimBillingAccount Delta Table (Upsert)
# ------------------------------------------
# Merge (Upsert) logic
dim_ba_delta = DeltaTable.forName(spark, TGT_DIM_BILLING_ACCOUNT)

(
    dim_ba_delta.alias("tgt")
    .merge(
        src_final_df.alias("src"),
        "tgt.PublicID = src.PublicID AND tgt.LegacySourceSystem = src.LegacySourceSystem"
    )
    .whenMatchedUpdateAll()
    .whenNotMatchedInsertAll()
    .execute()
)

# ------------------------------------------
# Row Counts for Audit (Optional)
# ------------------------------------------
source_count = src_final_df.count()
insert_count = insert_df.count()
update_count = update_df.count()
unchange_count = unchange_df.count()

print(f"Source Count: {source_count}")
print(f"Insert Count: {insert_count}")
print(f"Update Count: {update_count}")
print(f"Unchanged Count: {unchange_count}")

# ------------------------------------------
# Performance Optimization
# ------------------------------------------
# Cache main DataFrame if reused
src_final_df.cache()

# Partitioning (if writing large data, adjust as needed)
# src_final_df.repartition("AccountNumber")

# ------------------------------------------
# End of Notebook
# ------------------------------------------
print("DimBillingAccount load completed successfully.")

# API Cost Consumed in dollars: $0.02 (approximate, based on two file reads)