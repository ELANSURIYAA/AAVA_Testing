=============================================
Author:        Ascendion AVA+
Date:   
Description:   PySpark workflow to load and transform DimBillingAccount data, replicating SSIS logic, using Delta tables for source and destination.
=============================================

# Databricks PySpark Notebook: EDW_BC_Load_DimBillingAccount Conversion

# Imports
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, concat, lit, current_timestamp, expr
from pyspark.sql.types import *
from pyspark.sql import functions as F

# Start Spark session (Databricks automatically provides this)
spark = SparkSession.builder.getOrCreate()

# Set batch_id and other parameters (simulate SSIS variables)
batch_id = 12345  # Replace with dynamic batch id logic if needed
legacy_source_system = "WC"
days_delta = -1  # Replace with dynamic parameter if needed

# Read Delta tables as sources
bc_account = spark.read.format("delta").table("bc_account")
bctl_accounttype = spark.read.format("delta").table("bctl_accounttype")
bc_ParentAcct = spark.read.format("delta").table("bc_ParentAcct")
bctl_billinglevel = spark.read.format("delta").table("bctl_billinglevel")
bctl_customerservicetier = spark.read.format("delta").table("bctl_customerservicetier")
bc_securityzone = spark.read.format("delta").table("bc_securityzone")
bc_accountcontact = spark.read.format("delta").table("bc_accountcontact")
bc_accountcontactrole = spark.read.format("delta").table("bc_accountcontactrole")
bctl_accountrole = spark.read.format("delta").table("bctl_accountrole")
bc_contact = spark.read.format("delta").table("bc_contact")
bc_address = spark.read.format("delta").table("bc_address")
bctl_state = spark.read.format("delta").table("bctl_state")
bctl_delinquencystatus = spark.read.format("delta").table("bctl_delinquencystatus")
bctl_accountsegment = spark.read.format("delta").table("bctl_accountsegment")

# Read destination Delta table
dim_billing_account = spark.read.format("delta").table("DimBillingAccount")

# =========================
# Step 1: Build ParentAcct CTE
parent_acct_df = (
    bc_ParentAcct.alias("pa")
    .join(bc_account.alias("act"), col("act.ID") == col("pa.ForeignEntityID"), "inner")
    .select(
        col("pa.OwnerID"),
        col("act.AccountNumber").cast("int").alias("ParentAccountNumber"),
        concat(col("pa.BeanVersion"), col("act.BeanVersion")).alias("BeanVersion"),
        col("act.UpdateTime")
    )
)

# Step 2: Build InsuredInfo CTE
insured_info_df = (
    bc_accountcontact.alias("ac")
    .join(bc_accountcontactrole.alias("acr"), col("acr.AccountContactID") == col("ac.ID"), "inner")
    .join(bctl_accountrole.alias("tlar"), col("tlar.ID") == col("acr.Role"), "inner")
    .join(bc_contact.alias("c"), col("c.ID") == col("ac.ContactID"), "left")
    .join(bc_address.alias("a"), col("a.ID") == col("c.PrimaryAddressID"), "left")
    .join(bctl_state.alias("tls"), col("tls.ID") == col("a.State"), "left")
    .filter(col("tlar.TYPECODE") == "insured")
    .select(
        col("ac.InsuredAccountID").alias("AccountID"),
        col("c.FirstName"),
        col("c.LastName"),
        col("a.AddressLine1"),
        col("a.AddressLine2"),
        col("a.AddressLine3"),
        col("a.City"),
        col("a.PostalCode"),
        col("tls.NAME").alias("State"),
        concat(col("ac.BeanVersion"), col("acr.BeanVersion"), col("c.BeanVersion"), col("a.BeanVersion")).alias("BeanVersion"),
        col("ac.UpdateTime").alias("ac_UpdateTime"),
        col("acr.UpdateTime").alias("acr_UpdateTime"),
        col("c.UpdateTime").alias("c_UpdateTime"),
        col("a.UpdateTime").alias("a_UpdateTime")
    )
)

# Step 3: Build main transformation DataFrame
main_df = (
    bc_account.alias("dt")
    .join(bctl_accounttype.alias("at"), col("at.ID") == col("dt.AccountType"), "left")
    .join(parent_acct_df.alias("ParentAcct"), col("ParentAcct.OwnerID") == col("dt.ID"), "left")
    .join(bctl_billinglevel.alias("bl"), col("bl.ID") == col("dt.BillingLevel"), "left")
    .join(bctl_customerservicetier.alias("cst"), col("cst.ID") == col("dt.ServiceTier"), "left")
    .join(bc_securityzone.alias("sz"), col("sz.ID") == col("dt.SecurityZoneID"), "left")
    .join(insured_info_df.alias("InsuredInfo"), col("InsuredInfo.AccountID") == col("dt.ID"), "left")
    .join(bctl_delinquencystatus.alias("tlds"), col("tlds.ID") == col("dt.DelinquencyStatus"), "left")
    .join(bctl_accountsegment.alias("bas"), col("bas.ID") == col("dt.Segment"), "left")
    .select(
        col("dt.AccountNumber"),
        col("dt.AccountName"),
        col("at.NAME").cast("string").alias("AccountTypeName"),
        col("ParentAcct.ParentAccountNumber"),
        col("bl.NAME").cast("string").alias("BillingLevelName"),
        col("bas.NAME").cast("string").alias("Segment"),
        col("cst.NAME").cast("string").alias("ServiceTierName"),
        col("sz.Name").alias("SecurityZone"),
        col("InsuredInfo.FirstName"),
        col("InsuredInfo.LastName"),
        col("InsuredInfo.AddressLine1"),
        col("InsuredInfo.AddressLine2"),
        col("InsuredInfo.AddressLine3"),
        col("InsuredInfo.City").cast("string").alias("City"),
        col("InsuredInfo.State").cast("string").alias("State"),
        col("InsuredInfo.PostalCode").cast("string").alias("PostalCode"),
        col("dt.CloseDate").alias("AccountCloseDate"),
        col("dt.CreateTime").alias("AccountCreationDate"),
        col("tlds.NAME").cast("string").alias("DeliquencyStatusName"),
        col("dt.FirstTwicePerMthInvoiceDOM").alias("FirstTwicePerMonthInvoiceDayOfMonth"),
        col("dt.SecondTwicePerMthInvoiceDOM").alias("SecondTwicePerMonthInvoiceDayOfMonth"),
        col("dt.PublicID"),
        col("dt.ID").alias("GWRowNumber"),
        concat(col("dt.BeanVersion"), col("ParentAcct.BeanVersion"), col("sz.BeanVersion")).cast("string").alias("BeanVersion"),
        when(col("dt.Retired") == 0, lit(1)).otherwise(lit(0)).alias("IsActive"),
        lit(legacy_source_system).alias("LegacySourceSystem")
    )
)

# Step 4: Filter rows based on update time logic (simulate SSIS WHERE clause)
filter_expr = (
    (col("dt.UpdateTime") >= expr(f"date_add(current_date(), {days_delta})")) |
    (col("ParentAcct.UpdateTime") >= expr(f"date_add(current_date(), {days_delta})")) |
    (col("sz.UpdateTime") >= expr(f"date_add(current_date(), {days_delta})")) |
    (col("InsuredInfo.ac_UpdateTime") >= expr(f"date_add(current_date(), {days_delta})")) |
    (col("InsuredInfo.acr_UpdateTime") >= expr(f"date_add(current_date(), {days_delta})")) |
    (col("InsuredInfo.c_UpdateTime") >= expr(f"date_add(current_date(), {days_delta})")) |
    (col("InsuredInfo.a_UpdateTime") >= expr(f"date_add(current_date(), {days_delta})"))
)

main_df_filtered = main_df.filter(filter_expr)

# Step 5: Lookup logic for BeanVersion (simulate SSIS lookup)
lookup_df = dim_billing_account.select("PublicID", "BeanVersion").alias("lookup")
main_with_lookup = (
    main_df_filtered.alias("src")
    .join(lookup_df, col("src.PublicID") == col("lookup.PublicID"), "left")
    .withColumn("BeanVersionMatch", col("src.BeanVersion") == col("lookup.BeanVersion"))
)

# Step 6: Conditional Split for Insert/Update/Unchanged
insert_df = main_with_lookup.filter(col("lookup.PublicID").isNull())
update_df = main_with_lookup.filter((col("lookup.PublicID").isNotNull()) & (~col("BeanVersionMatch")))
unchange_df = main_with_lookup.filter((col("lookup.PublicID").isNotNull()) & (col("BeanVersionMatch")))

# Step 7: Write to DimBillingAccount Delta table
# Insert new records
insert_df_to_write = insert_df.select([col for col in main_df.columns])
insert_df_to_write.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable("DimBillingAccount")

# Update existing records (simulate SSIS update)
# Use Delta Lake merge for upsert
from delta.tables import DeltaTable

delta_dim_ba = DeltaTable.forName(spark, "DimBillingAccount")
update_df_to_write = update_df.select([col for col in main_df.columns])

delta_dim_ba.alias("tgt").merge(
    update_df_to_write.alias("src"),
    "tgt.PublicID = src.PublicID AND tgt.LegacySourceSystem = src.LegacySourceSystem"
).whenMatchedUpdateAll().execute()

# Step 8: Audit counts (simulate SSIS row count outputs)
source_count = main_df_filtered.count()
insert_count = insert_df.count()
update_count = update_df.count()
unchange_count = unchange_df.count()

# Step 9: Write audit info to Audit Delta table
audit_record = spark.createDataFrame([
    {
        "ControlID": batch_id,
        "BatchID": batch_id,
        "Status": "Completed",
        "InitiateDtm": current_timestamp(),
        "CompletedInd": 1,
        "ConcludeDtm": current_timestamp(),
        "SourceCount": source_count,
        "InsertCount": insert_count,
        "UpdateCount": update_count,
        "UnChangeCount": unchange_count
    }
], schema="""
    ControlID INT,
    BatchID INT,
    Status STRING,
    InitiateDtm TIMESTAMP,
    CompletedInd INT,
    ConcludeDtm TIMESTAMP,
    SourceCount INT,
    InsertCount INT,
    UpdateCount INT,
    UnChangeCount INT
""")

audit_record.write.format("delta").mode("append").saveAsTable("Audit")

# Step 10: Performance Optimization
# Cache frequently used DataFrames
main_df_filtered.cache()
insert_df.cache()
update_df.cache()
unchange_df.cache()

# Partitioning and bucketing strategies (example)
# If AccountNumber is high cardinality, consider partitioning
# main_df_filtered.write.format("delta").mode("overwrite").partitionBy("AccountNumber").saveAsTable("DimBillingAccount")

# Success message
print("EDW_BC_Load_DimBillingAccount SSIS logic successfully converted to PySpark Delta workflow.")

# API Cost Consumed in dollars: $0.02