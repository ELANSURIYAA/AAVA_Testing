=============================================
Author:        Ascendion AVA+
Date:   
Description:   Pytest-based test suite to validate the SSIS-to-PySpark conversion for the EDW_BC_Load_DimBillingAccount workflow, covering data integrity, transformation logic, error handling, and performance.
=============================================

# test_edw_bc_load_dimbillingaccount.py

import pytest
import logging
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from delta.tables import DeltaTable

# -------------------------------
# Logging Configuration
# -------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# -------------------------------
# Pytest Fixtures
# -------------------------------

@pytest.fixture(scope="session")
def spark():
    """Fixture to initialize a Spark session for all tests."""
    spark = SparkSession.builder \
        .appName("Test_EDW_BC_Load_DimBillingAccount") \
        .master("local[*]") \
        .getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture(scope="session")
def test_data(spark):
    """
    Loads minimal, representative test data for all required source tables.
    Returns a dict of DataFrames.
    """
    # Example: Replace with actual test data loading logic (CSV, Parquet, etc.)
    # For illustration, we create small DataFrames in-memory.
    data = {
        "bc_account": spark.createDataFrame([
            {
                "ID": 1, "AccountNumber": 1001, "AccountName": "Acme Corp",
                "AccountType": 10, "BillingLevel": 20, "ServiceTier": 30,
                "SecurityZoneID": 40, "DelinquencyStatus": 50, "Segment": 60,
                "CloseDate": None, "CreateTime": "2023-01-01 10:00:00",
                "FirstTwicePerMthInvoiceDOM": 5, "SecondTwicePerMthInvoiceDOM": 20,
                "PublicID": "PUB1", "BeanVersion": "A", "Retired": 0, "UpdateTime": "2023-06-01 12:00:00"
            }
        ]),
        "bctl_accounttype": spark.createDataFrame([
            {"ID": 10, "NAME": "Corporate"}
        ]),
        "bc_parentacct": spark.createDataFrame([
            {"OwnerID": 1, "ForeignEntityID": 2, "BeanVersion": "B"}
        ]),
        "bctl_billinglevel": spark.createDataFrame([
            {"ID": 20, "NAME": "Level1"}
        ]),
        "bctl_customerservicetier": spark.createDataFrame([
            {"ID": 30, "NAME": "Tier1"}
        ]),
        "bc_securityzone": spark.createDataFrame([
            {"ID": 40, "Name": "Zone1", "BeanVersion": "C", "UpdateTime": "2023-06-01 12:00:00"}
        ]),
        "bc_accountcontact": spark.createDataFrame([
            {"ID": 100, "InsuredAccountID": 1, "ContactID": 200, "BeanVersion": "D", "UpdateTime": "2023-06-01 12:00:00"}
        ]),
        "bc_accountcontactrole": spark.createDataFrame([
            {"ID": 110, "AccountContactID": 100, "Role": 120, "BeanVersion": "E", "UpdateTime": "2023-06-01 12:00:00"}
        ]),
        "bctl_accountrole": spark.createDataFrame([
            {"ID": 120, "TYPECODE": "insured"}
        ]),
        "bc_contact": spark.createDataFrame([
            {"ID": 200, "FirstName": "John", "LastName": "Doe", "PrimaryAddressID": 300, "BeanVersion": "F", "UpdateTime": "2023-06-01 12:00:00"}
        ]),
        "bc_address": spark.createDataFrame([
            {"ID": 300, "AddressLine1": "123 Main St", "AddressLine2": "", "AddressLine3": "", "City": "Metropolis", "PostalCode": "12345", "State": 400, "BeanVersion": "G", "UpdateTime": "2023-06-01 12:00:00"}
        ]),
        "bctl_state": spark.createDataFrame([
            {"ID": 400, "NAME": "NY"}
        ]),
        "bctl_delinquencystatus": spark.createDataFrame([
            {"ID": 50, "NAME": "Current"}
        ]),
        "bctl_accountsegment": spark.createDataFrame([
            {"ID": 60, "NAME": "Segment1"}
        ]),
    }
    return data

@pytest.fixture(scope="function")
def setup_delta_tables(spark, test_data, tmp_path_factory):
    """
    Sets up in-memory Delta tables for all required sources and the target.
    Returns a dict of table names and DeltaTable objects.
    """
    base_path = str(tmp_path_factory.mktemp("delta"))
    table_objs = {}
    for tbl, df in test_data.items():
        path = f"{base_path}/{tbl}"
        df.write.format("delta").mode("overwrite").save(path)
        spark.sql(f"CREATE TABLE IF NOT EXISTS {tbl} USING DELTA LOCATION '{path}'")
        table_objs[tbl] = DeltaTable.forPath(spark, path)
    # Create empty target table
    target_path = f"{base_path}/DimBillingAccount"
    schema = """
        PublicID STRING, AccountNumber INT, AccountName STRING, AccountTypeName STRING,
        ParentAccountNumber INT, BillingLevelName STRING, Segment STRING, ServiceTierName STRING,
        SecurityZone STRING, FirstName STRING, LastName STRING, AddressLine1 STRING,
        AddressLine2 STRING, AddressLine3 STRING, City STRING, State STRING, PostalCode STRING,
        AccountCloseDate STRING, AccountCreationDate STRING, DeliquencyStatusName STRING,
        FirstTwicePerMonthInvoiceDayOfMonth INT, SecondTwicePerMonthInvoiceDayOfMonth INT,
        GWRowNumber INT, BeanVersion STRING, IsActive INT, LegacySourceSystem STRING
    """
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS DimBillingAccount (
            {schema}
        ) USING DELTA LOCATION '{target_path}'
    """)
    table_objs["DimBillingAccount"] = DeltaTable.forPath(spark, target_path)
    return table_objs

# -------------------------------
# Helper Functions
# -------------------------------

def run_pyspark_workflow(spark):
    """
    Import and execute the PySpark workflow for EDW_BC_Load_DimBillingAccount.
    Assumes the workflow is implemented as a function named `main()` in module `edw_bc_load_dimbillingaccount`.
    """
    # Import the workflow module (should be in PYTHONPATH)
    import edw_bc_load_dimbillingaccount
    edw_bc_load_dimbillingaccount.main(spark)
    # The workflow should write to the Delta table "DimBillingAccount"

def fetch_output_df(spark):
    """
    Fetches the output DataFrame from the target Delta table.
    """
    return spark.read.format("delta").table("DimBillingAccount")

def compare_dataframes(df_actual, df_expected, ignore_cols=None):
    """
    Compares two DataFrames for equality, ignoring specified columns.
    Returns True if equal, False otherwise.
    """
    if ignore_cols:
        df_actual = df_actual.drop(*ignore_cols)
        df_expected = df_expected.drop(*ignore_cols)
    # Sort by all columns for deterministic comparison
    actual_sorted = df_actual.sort(df_actual.columns)
    expected_sorted = df_expected.sort(df_expected.columns)
    return actual_sorted.collect() == expected_sorted.collect()

# -------------------------------
# Test Cases
# -------------------------------

def test_data_integrity(spark, setup_delta_tables):
    """
    Test that all records from the source are loaded and transformed correctly.
    """
    logger.info("Running data integrity test...")
    run_pyspark_workflow(spark)
    output_df = fetch_output_df(spark)
    # For this test, we expect one output row with specific values
    expected = {
        "PublicID": "PUB1",
        "AccountNumber": 1001,
        "AccountName": "Acme Corp",
        "AccountTypeName": "Corporate",
        "ParentAccountNumber": None,  # No parent in test data
        "BillingLevelName": "Level1",
        "Segment": "Segment1",
        "ServiceTierName": "Tier1",
        "SecurityZone": "Zone1",
        "FirstName": "John",
        "LastName": "Doe",
        "AddressLine1": "123 Main St",
        "AddressLine2": "",
        "AddressLine3": "",
        "City": "Metropolis",
        "State": "NY",
        "PostalCode": "12345",
        "AccountCloseDate": None,
        "AccountCreationDate": "2023-01-01 10:00:00",
        "DeliquencyStatusName": "Current",
        "FirstTwicePerMonthInvoiceDayOfMonth": 5,
        "SecondTwicePerMonthInvoiceDayOfMonth": 20,
        "GWRowNumber": 1,
        "BeanVersion": pytest.approx(str),  # Accept any string
        "IsActive": 1,
        "LegacySourceSystem": "WC"
    }
    actual = output_df.collect()[0].asDict()
    for k, v in expected.items():
        if v is pytest.approx(str):
            assert isinstance(actual[k], str)
        else:
            assert actual[k] == v, f"Column {k}: expected {v}, got {actual[k]}"

def test_transformation_logic(spark, setup_delta_tables):
    """
    Test transformation logic for IsActive, BeanVersion, and hardcoded columns.
    """
    logger.info("Running transformation logic test...")
    run_pyspark_workflow(spark)
    output_df = fetch_output_df(spark)
    row = output_df.collect()[0].asDict()
    # IsActive logic
    assert row["IsActive"] == 1, "IsActive should be 1 when Retired=0"
    # BeanVersion is a string concatenation
    assert isinstance(row["BeanVersion"], str) and len(row["BeanVersion"]) > 0
    # LegacySourceSystem
    assert row["LegacySourceSystem"] == "WC"

def test_error_handling(spark, setup_delta_tables):
    """
    Test error handling by injecting bad data (e.g., missing required join).
    Should not raise unhandled exceptions.
    """
    logger.info("Running error handling test...")
    # Remove required lookup table to trigger join failure
    spark.sql("DROP TABLE IF EXISTS bctl_accounttype")
    try:
        run_pyspark_workflow(spark)
        output_df = fetch_output_df(spark)
        # Should produce row with AccountTypeName as None due to left join
        assert output_df.filter(col("AccountTypeName").isNull()).count() == 1
    except Exception as e:
        logger.error(f"Workflow failed with error: {e}")
        pytest.fail("Workflow should handle missing lookup gracefully.")

def test_performance(spark, setup_delta_tables):
    """
    Test that the workflow completes within a reasonable time for small data.
    """
    logger.info("Running performance test...")
    start = time.time()
    run_pyspark_workflow(spark)
    duration = time.time() - start
    logger.info(f"Workflow completed in {duration:.2f} seconds")
    assert duration < 30, "Workflow took too long for small test data"

@pytest.mark.parametrize("retired,is_active", [
    (0, 1),
    (1, 0),
    (None, 0)
])
def test_isactive_logic(spark, setup_delta_tables, retired, is_active):
    """
    Parameterized test for IsActive calculation.
    """
    df = setup_delta_tables["bc_account"].toDF()
    df = df.withColumn("Retired", col("Retired") if retired is not None else None)
    df.write.format("delta").mode("overwrite").saveAsTable("bc_account")
    run_pyspark_workflow(spark)
    output_df = fetch_output_df(spark)
    assert output_df.collect()[0]["IsActive"] == is_active

def test_data_comparison_helper(spark):
    """
    Test the DataFrame comparison helper.
    """
    df1 = spark.createDataFrame([{"A": 1, "B": 2}])
    df2 = spark.createDataFrame([{"A": 1, "B": 2}])
    assert compare_dataframes(df1, df2)
    df3 = spark.createDataFrame([{"A": 1, "B": 3}])
    assert not compare_dataframes(df1, df3)

# -------------------------------
# Reporting
# -------------------------------

def pytest_terminal_summary(terminalreporter, exitstatus, config):
    """
    Custom test summary reporting.
    """
    total = terminalreporter._numcollected
    passed = len(terminalreporter.stats.get("passed", []))
    failed = len(terminalreporter.stats.get("failed", []))
    logger.info(f"Total tests: {total}, Passed: {passed}, Failed: {failed}")

# -------------------------------
# End of test_edw_bc_load_dimbillingaccount.py
# -------------------------------

"""
Test Suite Documentation:

- test_data_integrity: Validates that all expected columns and values are loaded as per the SSIS logic.
- test_transformation_logic: Checks transformation rules for IsActive, BeanVersion, and hardcoded columns.
- test_error_handling: Ensures the workflow handles missing lookup tables or join failures gracefully.
- test_performance: Measures execution time for small data and ensures it is within acceptable limits.
- test_isactive_logic: Parameterized test for IsActive calculation (Retired=0â†’1, else 0).
- test_data_comparison_helper: Validates the DataFrame comparison utility.
- pytest_terminal_summary: Custom reporting for test results.

Helper functions are provided for loading test data, running the PySpark workflow, fetching output, and comparing DataFrames.

To use:
- Place this script alongside your PySpark workflow module (as edw_bc_load_dimbillingaccount.py with a main(spark) function).
- Run with: pytest test_edw_bc_load_dimbillingaccount.py

API Cost Consumed in dollars: $0.05
"""