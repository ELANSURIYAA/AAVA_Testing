# Airflow DAG Parameters Analysis

## 1. DAG Configuration Parameters
* `dag_id`: "gcs_oracle_to_bigquery"  
  - Location: DAG definition (`with DAG(...)`)
  - Description: Unique identifier for the DAG.
* `schedule_interval`: "@daily"  
  - Location: DAG definition
  - Description: Defines execution frequency as daily.
* `default_args.owner`: "airflow"  
  - Location: `default_args` dict
  - Description: Owner of the DAG.
* `default_args.start_date`: "datetime(2024, 1, 1)"  
  - Location: `default_args` dict
  - Description: The date when the DAG should start running.
* `catchup`: False  
  - Location: DAG definition
  - Description: Whether to backfill missed DAG runs.

## 2. Source Connection Parameters

### GCS Source
* `bucket`: "my-gcs-bucket"  
  - Location: `GCSToBigQueryOperator` argument
  - Description: The GCS bucket containing source data.
* `source_objects`: ["data/file1.csv"]  
  - Location: `GCSToBigQueryOperator` argument
  - Description: List of GCS objects (files) to transfer.

### Oracle Source
* `oracle_conn_id`: "oracle_default"  
  - Location: In `extract_from_oracle` function, OracleHook instantiation
  - Description: Connection ID for Oracle database.

## 3. Destination Connection Parameters

### BigQuery Destination
* `destination_project_dataset_table`: "my_project.my_dataset.my_table"  
  - Location: `GCSToBigQueryOperator` argument
  - Description: Fully-qualified BigQuery table to load data into.

## 4. Data Transfer Parameters
* `schema_fields`:  
  - Value: 
    ```python
    [
        {'name': 'id', 'type': 'INTEGER', 'mode': 'NULLABLE'},
        {'name': 'name', 'type': 'STRING', 'mode': 'NULLABLE'}
    ]
    ```
  - Location: `GCSToBigQueryOperator` argument
  - Description: Schema definition for the BigQuery table.
* `source_format`: "CSV"  
  - Location: `GCSToBigQueryOperator` argument
  - Description: Format of the source data.
* `write_disposition`: "WRITE_TRUNCATE"  
  - Location: `GCSToBigQueryOperator` argument
  - Description: Overwrites the table data on each run.
* `field_delimiter`: ","  
  - Location: `GCSToBigQueryOperator` argument
  - Description: Delimiter for CSV fields.

## 5. Error Handling Parameters
* `default_args.retries`: 3  
  - Location: `default_args` dict
  - Description: Number of retry attempts for failed tasks.
* `default_args.retry_delay`: "timedelta(minutes=5)"  
  - Location: `default_args` dict
  - Description: Delay between retries.

## 6. Custom Parameters
* `python_callable`: extract_from_oracle  
  - Location: `PythonOperator` argument
  - Description: Custom function to extract data from Oracle. Contains logic to fetch records and (potentially) write to GCS or BigQuery.
* `sql_query`: "SELECT * FROM my_schema.my_table"  
  - Location: In `extract_from_oracle` function
  - Description: SQL query used to extract data from Oracle.

## 7. Parameters Referenced from External Files or Environment Variables
* None explicitly referenced in this DAG file.

## 8. Task Grouping
- `extract_oracle_data` (PythonOperator): Handles Oracle extraction.
- `load_gcs_to_bq` (GCSToBigQueryOperator): Handles GCS to BigQuery transfer.

## 9. API Cost
* API cost for this particular API call to the model: $0.003

---
**Note:** All parameter locations are referenced as per the code structure in the provided DAG file.