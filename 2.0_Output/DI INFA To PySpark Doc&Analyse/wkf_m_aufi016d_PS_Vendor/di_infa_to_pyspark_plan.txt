=============================================
Author:        Ascendion AVA+
Date:          
Description:   Cost and effort estimation for PySpark conversion
=============================================

---

## 1. Cost Estimation

### 1.1 PySpark Runtime Cost

**Summary:**  
The Informatica mapping/workflow (`m_aufi016d_PS_Vendor` / `wkf_aufi016d_PS_Vendor`) is a simple data ingestion and enrichment pipeline. It reads a 500 MB delimited flat file (`PS_VENDOR_S4`), enriches the data with audit year and load date, and writes to an Oracle table (`PS_VENDOR`). There are no joins, lookups, or aggregations—just direct mapping and two simple transformations.

**Azure Databricks Environment Details:**  
- Standard job cluster: ~$0.22 per DBU/hour
- Estimated compute cost: ~$0.60 per hour per node (as per environment file)
- Data processed: ~500 MB read, ~500 MB write
- Job duration: For a simple pipeline of this size, expected runtime is 10–15 minutes on a small (2–4 node) cluster

**Cost Breakdown:**
- **Compute:**  
  - 2 nodes × 0.25 hours × $0.60 = $0.30 (rounded up for overhead)
- **Storage:**  
  - 1 GB processed (input + output), negligible for a single run
- **Total Estimated Runtime Cost:**  
  - **$0.30 USD** per run (conservative estimate, actual may be lower with optimizations)

**Justification:**  
- Minimal transformations, no shuffles or heavy computation
- No joins, lookups, or aggregations
- Direct mapping and JDBC write
- Cost may further reduce with Delta Lake optimizations or spot instances

---

## 2. Code Fixing and Testing Effort Estimation

### 2.1 PySpark Code Manual Fixes and Unit Testing Effort

**Manual Code Fixes Required:**
- Replace Informatica parameter `$$Audit_Year` with a PySpark config or runtime argument
- Replace `SESSSTARTTIME` with PySpark's `current_timestamp()` at job start
- Read flat file with custom delimiter (`||`), handle nulls (`*`), US-ASCII encoding
- Implement JDBC write to Oracle with pre-load truncation
- Implement error/reject handling (write bad records to a file)
- Logging for session/workflow equivalent

**Effort Estimate:**
- Parameterization and timestamp logic: 0.5 hour
- Flat file read (delimiter, nulls, encoding): 0.5 hour
- JDBC write with truncate: 0.5 hour
- Error/reject handling: 0.5 hour
- Logging setup: 0.5 hour
- Unit testing (basic): 0.5 hour

**Total Manual Code Fixes & Unit Testing:**  
**2.5 hours**

### 2.2 Output Validation Effort

**Tasks:**
- Run both Informatica and PySpark pipelines on the same input
- Compare outputs (row counts, field-level values, audit/load date correctness)
- Validate error/reject file creation and logging

**Effort Estimate:**  
- Data comparison script and validation: 1 hour
- Manual review and sign-off: 0.5 hour

**Total Output Validation Effort:**  
**1.5 hours**

### 2.3 Total Estimated Effort in Hours

**Summary Table:**

| Task                                 | Effort (hours) |
|-------------------------------------- |:--------------:|
| Manual code fixes & unit testing      | 2.5            |
| Output validation                    | 1.5            |
| **Total**                            | **4.0**        |

**Justification:**  
- The mapping is simple, with no joins, lookups, or aggregations.
- Most effort is in adapting parameterization, file read/write, and error handling.
- Testing effort is low due to the straightforward logic and 1:1 field mapping.

---

## 3. API Cost Consumption

```
apiCost: 0.0520 USD
```

---

**End of output for: wkf_m_aufi016d_PS_Vendor.txt__ai85ctd_**

---