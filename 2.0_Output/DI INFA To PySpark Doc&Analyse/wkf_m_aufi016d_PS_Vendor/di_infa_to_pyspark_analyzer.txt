```
=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Informatica workflow and mapping analysis for PS_VENDOR vendor data ingestion and Oracle loading, to support conversion to PySpark.
=============================================

---

## 1. Workflow Overview

This Informatica workflow (`wkf_aufi016d_PS_Vendor`) and mapping (`m_aufi016d_PS_Vendor`) orchestrate the ingestion of vendor master data from a delimited flat file (`profrec_vendor.txt`), enrich the data with audit and load date metadata, and load it into the `PS_VENDOR` table in an Oracle database. The business objective is to ensure accurate and auditable transfer of vendor data from PeopleSoft to Profit Recovery, supporting downstream reporting and analytics.

- **Number of mappings per workflow/session:** 1 mapping (`m_aufi016d_PS_Vendor`) per session (`s_m_aufi016d_PS_Vendor`)
- **Key business objective:** Data integration, audit enrichment, and Oracle table loading for vendor master data.

---

## 2. Complexity Metrics

| Metric                        | Value / Type                                             |
|-------------------------------|---------------------------------------------------------|
| Number of Source Qualifiers   | 1 (`SQ_PS_VENDOR_S4`)                                   |
| Number of Transformations     | 2 (Source Qualifier, Expression)                        |
| Lookup Usage                  | 0                                                       |
| Expression Logic              | 2 (Simple assignment, no nested logic)                  |
| Join Conditions               | 0                                                       |
| Conditional Logic             | 0                                                       |
| Reusable Components           | 0 (all transformations are non-reusable)                |
| Data Sources                  | 1 (Flat File, delimited, US-ASCII)                      |
| Data Source Type              | Flat File                                               |
| Data Targets                  | 1 (Oracle table)                                        |
| Data Target Type              | Oracle                                                  |
| Pre/Post SQL Logic            | 0                                                       |
| Session/Workflow Controls     | 1 Start task, 1 Session task, workflow variables        |
| DML Logic                     | INSERT, TRUNCATE (no UPDATE/DELETE/MERGE)               |
| Complexity Score (0â€“100)      | 15 (Low; direct mapping, minimal transformation)        |

### High-Complexity Areas

- **None detected:** No nested expressions, lookups, branching logic, or unstructured sources.

---

## 3. Syntax Differences

- **Functions with no direct PySpark equivalent:** 
    - `SESSSTARTTIME` (Informatica system variable for session start time) must be replaced with a timestamp generated at runtime in PySpark.
    - Mapping parameter `$$Audit_Year` (Informatica parameter variable) should be handled as a runtime parameter or config variable in PySpark.
- **Data type conversions:**
    - Flat file fields (string, number) mapped to Oracle types (varchar2, number, date).
    - `AUD_YR_NBR` (number) and `LOAD_DT` (date) require explicit type handling in PySpark.
- **Workflow/control logic:**
    - No Router, Filter, or Transaction Control transformations present.
    - Session-level reject file and error handling must be implemented with PySpark exception handling and logging.

---

## 4. Manual Adjustments

- **Components requiring manual implementation:**
    - Audit year parameter (`$$Audit_Year`) must be passed as a PySpark config or runtime argument.
    - Session start time (`SESSSTARTTIME`) must be set using PySpark's timestamp functions.
    - Flat file reading: PySpark must handle custom delimiter (`||`), null character (`*`), and US-ASCII encoding.
    - Target table loading: PySpark must connect to Oracle using JDBC, handle truncation before insert, and manage error/reject files.
- **External dependencies:**
    - Source file location: `$PMSourceFileDir/corp/SrcFiles/profrec_vendor.txt`
    - Target Oracle connection: `BAT_AUFI_RWH`
    - Reject file directory: `$PMBadFileDir/corp/BadFiles/ps_vendor1.bad`
    - Session/workflow log directories: `$PMSessionLogDir/corp/SessLogs/`, `$PMWorkflowLogDir/corp/WorkflowLogs/`
- **Business logic validation:**
    - Confirm audit year and load date enrichment logic post-conversion.
    - Validate error handling and reject file creation in PySpark.

---

## 5. Optimization Techniques

- **Spark best practices:**
    - Use partitioning when reading large flat files for parallel processing.
    - Cache intermediate DataFrames if reused.
    - Use broadcast joins if joining with small lookup tables (not applicable here).
- **Pipeline conversion:**
    - Chain flat file read, enrichment, and JDBC write in a single PySpark pipeline.
    - Use DataFrame API for direct column mapping and enrichment.
- **Window functions:**
    - Not applicable (no aggregation or windowing logic).
- **Refactor vs. Rebuild:**
    - **Refactor:** Retain original logic, as the workflow is simple and direct.
    - **Rebuild:** Only if additional audit, error handling, or performance improvements are needed.

---

**End of Analysis for: wkf_m_aufi016d_PS_Vendor.txt__ai85ctd_**
```