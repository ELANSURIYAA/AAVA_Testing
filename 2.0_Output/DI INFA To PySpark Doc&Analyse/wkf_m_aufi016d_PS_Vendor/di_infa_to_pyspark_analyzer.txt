=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Informatica workflow analysis for PS_VENDOR data ingestion and transformation from flat file to Oracle, supporting conversion to PySpark.
=============================================

---

## 1. Workflow Overview

**Workflow Name:** `wkf_aufi016d_PS_Vendor`  
**Mapping Name:** `m_aufi016d_PS_Vendor` (1 mapping in workflow/session)

This Informatica workflow ingests vendor data from a flat file (`PS_VENDOR_S4`), applies audit year and load timestamp enrichment, and loads the data into an Oracle target table (`PS_VENDOR`). The business objective is to ensure consistent, auditable vendor master data integration from source files into the enterprise data warehouse for reporting and analytics.

---

## 2. Complexity Metrics

| Metric                        | Value/Type                                                                                 |
|-------------------------------|-------------------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (`SQ_PS_VENDOR_S4`)                                                                     |
| Number of Transformations     | 2 (1 Source Qualifier, 1 Expression)                                                      |
| Lookup Usage                  | 0 (No lookups used)                                                                       |
| Expression Logic              | 2 (Simple assignments: mapping variable and session start time)                           |
| Join Conditions               | 0 (No joins)                                                                              |
| Conditional Logic             | 0 (No Router or Filter transformations)                                                   |
| Reusable Components           | 0 (No reusable transformations, mapplets, or sessions)                                    |
| Data Sources                  | 1 (Flat File, US-ASCII, delimited by `||`)                                               |
| Data Targets                  | 1 (Oracle table: `PS_VENDOR`)                                                             |
| Pre/Post SQL Logic            | 0 (No pre/post SQL or procedures in session)                                              |
| Session/Workflow Controls     | 1 (Basic Start task, single session, workflow variables for status/error/row counts)      |
| DML Logic                     | INSERT (primary), DELETE (truncate option enabled), no UPDATE or MERGE                    |
| Complexity Score (0â€“100)      | 15 (Simple, direct mapping, minimal transformation)                                       |
| Data Source Type              | Flat File (Delimited, US-ASCII)                                                           |
| Data Target Type              | Oracle (Relational)                                                                       |

**High-Complexity Areas:**  
- None. No nested expressions, no lookups, no branching logic, no unstructured sources, no external scripts.

---

## 3. Syntax Differences

- **Functions/Logic:**
  - `$$Audit_Year` (mapping variable): In PySpark, this would be a parameter or broadcast variable.
  - `SESSSTARTTIME` (session variable): In PySpark, use current timestamp or pass as job parameter.
- **Data Type Conversions:**
  - Informatica's `number`/`decimal` and `string` types map to PySpark's `IntegerType`, `StringType`, and `DateType`.
  - Oracle `date` type (`LOAD_DT`) should be mapped to PySpark `TimestampType`.
- **No Informatica-specific functions (e.g., DECODE, IIF, TO_DATE) are used.**
- **No workflow/control logic (Router, Transaction Control) to restructure.**

---

## 4. Manual Adjustments

- **Components Requiring Manual Implementation:**
  - Mapping variables (`$$Audit_Year`) should be passed as parameters or environment variables in PySpark.
  - Session start time (`SESSSTARTTIME`) should be replaced with PySpark's `current_timestamp()` or job start time.
- **External Dependencies:**
  - Source file path (`profrec_vendor.txt`) and reject/log directories must be parameterized or configured in PySpark.
  - Oracle connection (`BAT_AUFI_RWH`) must be set up using Spark JDBC.
  - Reject file handling: PySpark does not natively create reject files; custom logic is needed for error records.
- **Business Logic Review:**
  - Ensure the audit and load date logic is preserved.
  - Validate data type mappings (especially date/time fields) post-conversion.

---

## 5. Optimization Techniques

- **Spark Best Practices:**
  - Use `spark.read.csv` with appropriate delimiter (`||`), schema, and encoding for flat file ingestion.
  - Use partitioning on large files if applicable, though this mapping is simple and may not require it.
  - Use `current_timestamp()` for load date.
  - Use parameterization for audit year and file paths.
- **Pipeline Optimization:**
  - Chain the read, transformation (audit year/load date), and write operations in a single pipeline.
  - No need for caching or broadcast joins due to mapping simplicity.
- **Window Functions:**
  - Not required, as there are no aggregations or windowed calculations.
- **Refactor vs. Rebuild:**
  - **Refactor**: Retain the original logic, as the mapping is already optimal and simple. No need for a full rebuild.

---

**End of Analysis for: `wkf_m_aufi016d_PS_Vendor.txt__171jk8nr`**

*No additional Informatica XML files detected for analysis in the provided archive.*