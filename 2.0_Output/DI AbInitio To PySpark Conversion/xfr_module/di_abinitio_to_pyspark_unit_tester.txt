```
==================================================================
Author:        Ascendion AVA+
Created on:    
Description:   Unit Test Suite for Ab Initio to PySpark Conversion
==================================================================
```

#### 1. Test Case Inventory:
| Test Case ID | Description | Scenario Type | Expected Outcome |
|--------------|-------------|---------------|------------------|
| TC001 | Validate successful transformation with valid input | Happy Path | Transformed DataFrame matches expected output |
| TC002 | Test behavior with NULL values in critical columns | Edge Case | NULLs handled correctly without failure |
| TC003 | Missing column in input | Negative Test | Raise appropriate error |
| TC004 | Deduplication on key columns | Happy Path | Output DataFrame has no duplicate key rows |
| TC005 | Empty input dataset | Edge Case | Output DataFrame is empty but no crash |
| TC006 | Data type mismatch in input | Negative Test | Raise appropriate error or handle gracefully |
| TC007 | Unexpected schema/field order | Negative Test | Transformation works regardless of field order |
| TC008 | Reject handling (invalid values) | Edge Case | Rows with invalid values are handled per spec |
| TC009 | Lookup/join miss scenario | Edge Case | Rows with no match handled per business rule |
| TC010 | Boundary values for numeric/date fields | Edge Case | Transformation handles boundary values correctly |

#### 2. Pytest Script Template (with coverage for all major business rules):

```python
import pytest
from pyspark.sql import SparkSession
from chispa.dataframe_comparer import assert_df_equality
from pyspark.sql.types import StructType, StructField, StringType, LongType, DecimalType, DateType, TimestampType

# Import transformation functions and schemas
from tmpi40szl14 import (
    transform_table_adaptor_first,
    transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2,
    transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p3,
    transform_table_adaptor
)
from tmpox7hlhgj import (
    iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema,
    iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_schema,
    stg_cons_csv_dental_clm_dtl_hx_schema
)

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local").appName("unit-test").getOrCreate()

def make_df(spark, data, schema):
    return spark.createDataFrame(data, schema)

# TC001: Happy Path - Valid Input Transformation
def test_TC001_valid_transformation(spark):
    input_data = [
        ("ID1", "PFX", 1, 1, "CLM1", None, 1, "FMT", "METH", "PROD", "QLFR", "TYPE", 100.0, "CONTRACT", 0.5, "TYP", "VER", "D1", "D2", "D3", "D4", "F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9", "F10", 200.0, "CTL1", "OCD1", "OCD2", "OCD3", "OCD4", "OCD5", "OP1", "OP2", "OP3", "OP4", "OP5", "PLC", "POS", "PB1", "PB2", "PB3", "PB4", "PB5", "POP1", "POP2", "POP3", "POP4", "POP5", "PROC", "DESC", "MOD1", "MOD2", "MOD3", "MOD4", 2.0, "QLFR", "PCI", "AUTH1", "AUTH2", "AUTH3", "AUTH4", "AUTH5", "PAOP1", "PAOP2", "PAOP3", "PAOP4", "PAOP5", "PAOPQ1", "PAOPQ2", "PAOPQ3", "PAOPQ4", "PAOPQ5", None, "QCD", "REF1", "REF2", "REF3", "REF4", "REF5", "ROPIQ1", "ROPIQ2", "ROPIQ3", "ROPIQ4", "ROPIQ5", "REJ", None, 300.0, "HCPCS", "UNT", "CLM2", "ORG", 400.0, 500.0, 600.0, 123, None, 0.1, "T01", "T01S01", "T01S02", "T01S03", "T01S04", "T01S05", "T01S06", "T01S07", "T01S08", "T01S09", "T01S10", "T01S11", "T01S12", "T01S13", "T01S14", "T01S15", "T01S16", "T01S17", "T01S18", "T01S19", "T01S20", "T01S21", "T01S22", "T01S23", "T01S24", "T01S25", "T01S26", "T01S27", "T01S28", "T01S29", "T01S30", "T01S31", "T01S32", "T02", None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, N