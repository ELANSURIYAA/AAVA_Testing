```
=============================================
Author: Ascendion AVA+
Date: 
Description: Pytest scripts and test case documentation for validating PySpark conversion of TAMBR_RINGS.txt, including syntax changes, manual interventions, and functional equivalence.
=============================================

# 1. Syntactical Changes Made

- PROC SQL replaced with PySpark DataFrame operations and Spark SQL.
- DATA steps replaced with DataFrame transformations.
- Macro variables and macro functions converted to Python variables and functions.
- Conditional logic in DATA steps (e.g., IF/CASE) replaced with PySpark `when` and `otherwise`.
- Custom geodist function implemented as PySpark UDF.
- Windowing logic for deduplication and ranking implemented via PySpark Window functions.
- Aggregation and percentile calculations (e.g., PROC UNIVARIATE) replaced with `percentile_approx` and groupBy in PySpark.
- Frequency tables (PROC FREQ) replaced with DataFrame `groupBy(...).count()` or `crosstab`.
- Table drops and temp table management handled by Spark session and explicit DataFrame deletes.
- Error handling and logging added in Python.
- BigQuery integration for data read/write.

# 2. Manual Intervention Required

- Frequency tables: PROC FREQ not natively supported; must use DataFrame groupBy/count or crosstab.
- Inline SAS macros (mdrop_mac, masofdt) rewritten as Python functions.
- Macro variable expansion and dynamic table names handled with Python string formatting.
- Some ambiguous logic (e.g., max_dist assignment) flagged for confirmation.
- Data mocking required for test validation.
- Some SAS-specific options (e.g., BULKLOAD, BL_METHOD) omitted or handled via PySpark/BigQuery defaults.

# 3. Test Case Document

| Test Case ID | Description | Preconditions | Test Steps | Expected Result | Actual Result | Pass/Fail Status |
|--------------|-------------|---------------|------------|----------------|--------------|------------------|
| TC01 | Validate happy path: correct transformation and join outputs for valid customer and branch data | Sample customer and branch DataFrames with valid data | Run join and transformation logic | Output DataFrame matches expected schema and values | As observed | TBD |
| TC02 | Validate UDF geodist: correct calculation of distance between lat/long pairs | Provide two valid lat/long pairs | Call geodist UDF | Calculated distance matches Haversine output | As observed | TBD |
| TC03 | Validate join logic: customers with matching branches are joined correctly | Matching keys in customer and branch DataFrames | Perform join | Only valid pairs present | As observed | TBD |
| TC04 | Validate filter: only open branches and customers with valid account counts are included | DataFrames with mixed open/closed flags and account counts | Apply filter logic | Output excludes closed branches, zero accounts | As observed | TBD |
| TC05 | Edge case: NULL latitude/longitude in customer/branch data | DataFrames with NULLs in lat/long | Run geodist and downstream logic | Output handles NULLs (returns None or excludes) | As observed | TBD |
| TC06 | Edge case: Zero rows in input DataFrames | Empty DataFrames | Run full pipeline | Output DataFrames are empty, no errors | As observed | TBD |
| TC07 | Edge case: Duplicate LP_IDs in customer data | DataFrame with duplicate LP_IDs | Apply Window and deduplication logic | Only first occurrence per LP_ID | As observed | TBD |
| TC08 | Error handling: Type mismatch in latitude/longitude columns | DataFrame with wrong types | Run geodist logic | Exception is raised or handled | As observed | TBD |
| TC09 | Schema validation: Output DataFrame columns and types match expected | Output DataFrame from pipeline | Check schema | Schema matches expected | As observed | TBD |
| TC10 | Aggregation: 80th percentile ring calculation per branch | Sufficient sample data | Run percentile_approx logic | Percentile values as expected | As observed | TBD |
| TC11 | Performance: DataFrame caching and partitioning improve runtime | Large DataFrame | Time with/without caching | Runtime is less with caching | As observed | TBD |
| TC12 | Frequency tables: groupBy/count logic matches SAS output | Sample DataFrame for frequencies | Run groupBy/count | Counts match expected | As observed | TBD |

# 4. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession, functions as F, types as T, Window
from math import radians, cos, sin, asin, sqrt

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("tambr_rings_test").getOrCreate()

@pytest.fixture
def sample_branch_df(spark):
    data = [
        ("10001", "R", "Y", 40.7128, -74.0060, "A", "GEN1", None, "Branch1", "NYC", "NY", "10001"),
        ("10002", "U", "Y", 34.0522, -118.2437, "B", "GEN2", None, "Branch2", "LA", "CA", "90001"),
    ]
    schema = T.StructType([
        T.StructField("HGN_BR_ID", T.StringType()),
        T.StructField("BR_TYP", T.StringType()),
        T.StructField("BR_OPN_FLG", T.StringType()),
        T.StructField("branchlat", T.DoubleType()),
        T.StructField("branchlong", T.DoubleType()),
        T.StructField("METRO_COMMUNITY_CDE", T.StringType()),
        T.StructField("GEN_CDE", T.StringType()),
        T.StructField("TBA_CLS_DVSTD_DT", T.StringType()),
        T.StructField("BRICK_AND_MORTOR_NM", T.StringType()),
        T.StructField("CITY", T.StringType()),
        T.StructField("ST", T.StringType()),
        T.StructField("ZIP_CDE", T.StringType()),
    ])
    return spark.createDataFrame(data, schema)

@pytest.fixture
def sample_customer_df(spark):
    data = [
        ("CUST1", "10001", "Y", 12, 40.7130, -74.0059, "R", "10001", "NYC", "NY", "10001"),
        ("CUST2", "10002", "Y", 24, 34.0520, -118.2435, "U", "10002", "LA", "CA", "90001"),
    ]
    schema = T.StructType([
        T.StructField("LP_ID", T.StringType()),
        T.StructField("PRTY_BR", T.StringType()),
        T.StructField("OPN_ACCT_FLG", T.StringType()),
        T.StructField("NBR_OF_MOS_OPN", T.IntegerType()),
        T.StructField("custlat", T.DoubleType()),
        T.StructField("custlong", T.DoubleType()),
        T.StructField("BR_TYP", T.StringType()),
        T.StructField("HGN_BR_ID", T.StringType()),
        T.StructField("CITY", T.StringType()),
        T.StructField("ST", T.StringType()),
        T.StructField("ZIP_CDE", T.StringType()),
    ])
    return spark.createDataFrame(data, schema)

def geodist_udf(lat1, lon1, lat2, lon2):
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    r = 3956
    return c * r

geodist = F.udf(geodist_udf, T.DoubleType())

# TC01: Happy path transformation and join
def test_happy_path_transformation(spark, sample_branch_df, sample_customer_df):
    """
    Test join and transformation logic for valid customer and branch data.
    Corresponds to main transformation and join logic in TAMBR_RINGS.txt.
    """
    joined = sample_customer_df.join(
        sample_branch_df,
        sample_customer_df.PRTY_BR == sample_branch_df.HGN_BR_ID,
        "inner"
    )
    assert joined.count() == 2
    assert set(joined.columns) >= {"LP_ID", "PRTY_BR", "branchlat", "branchlong"}

# TC02: UDF geodist calculation
def test_geodist_udf():
    """
    Test geodist UDF for correct distance calculation.
    Corresponds to geodist logic in SAS DATA step.
    """
    dist = geodist_udf(40.7128, -74.0060, 40.7130, -74.0059)
    assert round(dist, 3) == 0.014  # Approximate value in miles

# TC03: Join logic validation
def test_join_logic(spark, sample_branch_df, sample_customer_df):
    """
    Test that only valid customer-branch pairs are present after join.
    """
    joined = sample_customer_df.join(
        sample_branch_df,
        sample_customer_df.PRTY_BR == sample_branch_df.HGN_BR_ID,
        "inner"
    )
    lp_ids = [row.LP_ID for row in joined.collect()]
    assert "CUST1" in lp_ids and "CUST2" in lp_ids

# TC04: Filter logic validation
def test_filter_logic(spark, sample_branch_df):
    """
    Test that only open branches with valid lat/long are included.
    Corresponds to SAS filter on BR_OPN_FLG, lat/long.
    """
    filtered = sample_branch_df.filter((F.col("BR_OPN_FLG") == "Y") & (F.col("branchlat") > 1) & (F.col("branchlong") < -1))
    assert filtered.count() == 2

# TC05: Edge case NULL latitude/longitude
def test_null_latlong_handling(spark, sample_branch_df):
    """
    Test handling of NULL lat/long in branch data.
    Should not error; geodist should return None.
    """
    df = sample_branch_df.withColumn("branchlat", F.lit(None).cast(T.DoubleType()))
    df = df.withColumn("branchlong", F.lit(None).cast(T.DoubleType()))
    result = df.withColumn("dist", geodist(F.col("branchlat"), F.col("branchlong"), F.lit(40.7130), F.lit(-74.0059)))
    assert result.filter(F.col("dist").isNull()).count() == df.count()

# TC06: Zero rows in input DataFrames
def test_zero_rows(spark):
    """
    Test pipeline with empty DataFrame.
    Should produce empty output, no errors.
    """
    schema = T.StructType([T.StructField("A", T.StringType())])
    empty_df = spark.createDataFrame([], schema)
    assert empty_df.count() == 0

# TC07: Duplicate LP_IDs in customer data
def test_duplicate_lp_ids(spark, sample_customer_df):
    """
    Test deduplication logic for duplicate LP_IDs.
    Corresponds to DATA step with BY LP_ID; IF FIRST.LP_ID.
    """
    dup_df = sample_customer_df.union(sample_customer_df)
    window_lp = Window.partitionBy("LP_ID").orderBy("LP_ID")
    deduped = dup_df.withColumn("rn", F.row_number().over(window_lp)).filter(F.col("rn") == 1).drop("rn")
    assert deduped.count() == 2

# TC08: Error handling for type mismatch
def test_type_mismatch(spark, sample_branch_df):
    """
    Test error handling for type mismatch in lat/long columns.
    Should raise exception.
    """
    df = sample_branch_df.withColumn("branchlat", F.lit("not_a_float"))
    with pytest.raises(Exception):
        df.withColumn("dist", geodist(F.col("branchlat"), F.col("branchlong"), F.lit(40.7130), F.lit(-74.0059))).collect()

# TC09: Schema validation
def test_schema_validation(sample_branch_df):
    """
    Test that schema matches expected output.
    """
    expected_fields = {"HGN_BR_ID", "BR_TYP", "BR_OPN_FLG", "branchlat", "branchlong", "METRO_COMMUNITY_CDE", "GEN_CDE", "BRICK_AND_MORTOR_NM", "CITY", "ST", "ZIP_CDE"}
    actual_fields = set([f.name for f in sample_branch_df.schema.fields])
    assert expected_fields.issubset(actual_fields)

# TC10: Aggregation 80th percentile ring calculation
def test_percentile_aggregation(spark, sample_branch_df, sample_customer_df):
    """
    Test percentile_approx logic for ring calculation.
    Corresponds to PROC UNIVARIATE in SAS.
    """
    joined = sample_customer_df.join(
        sample_branch_df,
        sample_customer_df.PRTY_BR == sample_branch_df.HGN_BR_ID,
        "inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist(
            F.col("branchlat"), F.col("branchlong"),
            F.col("custlat"), F.col("custlong")
        )
    )
    percentile_window = Window.partitionBy("PRTY_BR")
    result = joined.withColumn(
        "prtybr80",
        F.expr("percentile_approx(dist_to_prty_br, 0.8) over (partition by PRTY_BR)")
    )
    assert "prtybr80" in result.columns

# TC11: Performance improvement (mocked)
def test_performance_improvement(spark, sample_branch_df):
    """
    Test that caching improves performance (mocked for small data).
    """
    import time
    df = sample_branch_df
    start = time.time()
    df.cache()
    df.count()
    end = time.time()
    assert (end - start) < 2  # Should be fast for small data

# TC12: Frequency tables groupBy/count
def test_frequency_tables(spark, sample_branch_df):
    """
    Test groupBy/count as replacement for PROC FREQ.
    """
    freq = sample_branch_df.groupBy("BR_TYP", "METRO_COMMUNITY_CDE").count()
    assert freq.count() > 0

```

# 5. Test Execution Report Template

| Test Case ID | Description | Expected Result | Actual Result | Pass/Fail | Notes |
|--------------|-------------|----------------|--------------|-----------|-------|
| TC01         | ...         | ...            | ...          | ...       | ...   |
| TC02         | ...         | ...            | ...          | ...       | ...   |
| ...          | ...         | ...            | ...          | ...       | ...   |

# 6. API Cost

apiCost: 0.0023 USD
```