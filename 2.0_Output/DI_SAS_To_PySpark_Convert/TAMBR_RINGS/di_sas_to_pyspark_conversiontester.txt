```
=============================================
Author: Ascendion AVA+
Date: 
Description: Pytest-based validation suite for TAMBR_RINGS SAS-to-PySpark conversion, covering syntax changes, manual interventions, and functional equivalence.
=============================================

# 1. Syntactical Changes Made

- PROC SQL CONNECT TO DB2 replaced with Spark SQL/DataFrame API.
- DATA steps and conditional logic replaced with DataFrame operations and UDFs.
- Macro variable usage replaced with Python variables and functions.
- SAS macros (mdrop_mac, masofdt) replaced by Python functions.
- SAS BY-group logic replaced with Window functions.
- SAS PROC FREQ replaced by DataFrame groupBy/count.
- SAS conditional logic (IF/CASE) replaced with DataFrame .filter(), .when(), .otherwise().
- SAS proc univariate percentile replaced by percentile_approx in PySpark.
- Table creation and dropping handled via BigQuery read/write functions.
- Error handling and logging added in Python.
- All joins (INNER, LEFT, FULL) mapped to DataFrame .join() with appropriate join type.

# 2. Manual Intervention Required

- Macro definitions (mdrop_mac, masofdt) required manual Python function implementation.
- SAS proc freq reporting replaced with DataFrame groupBy/count or export to BI tool.
- SAS-specific reporting logic not directly convertible; manual review required.
- SAS macro variable expansion and dynamic table naming replaced by Python string formatting.
- SAS date arithmetic (intnx, put) replaced by Python datetime logic.
- SAS conditional logic for missing values (IN (' '), IS NOT NULL) mapped to Python/SQL equivalents.
- Handling of bad lat/long and geomatchcode required manual DataFrame filtering.
- Performance optimization (caching, partitioning, broadcast joins) suggested for PySpark.

# 3. Test Case Document

| Test Case ID | Description                                                                                  | Preconditions                              | Test Steps                                                                                         | Expected Result                                                                                      | Actual Result | Pass/Fail Status |
|--------------|---------------------------------------------------------------------------------------------|--------------------------------------------|----------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|---------------|------------------|
| TC01         | Happy path: All input tables have valid data, all joins succeed, no NULLs                    | Valid sample DataFrames                    | Run pipeline end-to-end with valid data                                                            | Output DataFrames match expected schema and data, correct row counts, correct aggregations            |               |                  |
| TC02         | Edge: Input DataFrames are empty                                                             | Empty DataFrames                           | Run pipeline with empty DataFrames                                                                 | Output DataFrames are empty, no errors                                                               |               |                  |
| TC03         | Edge: Input DataFrames contain NULLs in join keys                                            | DataFrames with NULL join keys             | Run joins and check output                                                                         | Rows with NULL join keys handled per join type                                                        |               |                  |
| TC04         | Edge: Input DataFrames contain duplicate LP_IDs or BR_IDs                                    | DataFrames with duplicates                 | Run window logic for most used branch/customer selection                                            | Only first (or correct) row per LP_ID/BR_ID selected                                                 |               |                  |
| TC05         | Edge: Branches with invalid lat/long (e.g., 0, None)                                         | DataFrames with invalid lat/long           | Filter branches, check output                                                                      | Invalid branches filtered to bad_latlong_branch, not in rings_branch_data                            |               |                  |
| TC06         | Edge: Customers with invalid geomatchcode                                                    | DataFrames with invalid geomatchcode       | Filter customers, check output                                                                     | Invalid customers filtered to bad_latlong_cust, not in rings_cust_data                               |               |                  |
| TC07         | UDF: geodist returns correct distance, including NULL handling                               | Data for geodist                           | Run geodist UDF on sample data                                                                     | geodist returns None if any input is None, correct miles otherwise                                   |               |                  |
| TC08         | Aggregation: 80th percentile calculation per branch                                          | Data for percentile                        | Run percentile_approx aggregation                                                                  | percentile_approx returns correct value, schema matches, no errors                                   |               |                  |
| TC09         | Error: Type mismatch in lat/long columns                                                     | DataFrames with wrong types                | Run filter on lat/long                                                                             | Raises AnalysisException or handles gracefully                                                       |               |                  |
| TC10         | Error: Missing required columns in input DataFrames                                          | DataFrames missing columns                 | Select missing column                                                                              | Raises AnalysisException                                                                             |               |                  |
| TC11         | Schema: Output DataFrames have correct columns and types (matches SAS output)                | Valid sample DataFrames                    | Check schema of output                                                                             | Schema matches expected definition                                                                   |               |                  |
| TC12         | Filtering: Only open, valid branches (BR_OPN_FLG='Y', BR_TYP in allowed list, etc.) included | Valid sample DataFrames                    | Filter branches, check output                                                                      | Output only contains valid branches                                                                  |               |                  |
| TC13         | Filtering: Only customers with OPN_ACCT_CNT > 0 included                                    | Valid sample DataFrames                    | Filter customers, check output                                                                     | Output only contains valid customers                                                                 |               |                  |
| TC14         | Performance: Large input DataFrames processed efficiently (smoke test)                       | Large sample DataFrames                    | Run pipeline on large data                                                                         | Test completes within reasonable time, no memory errors                                              |               |                  |

# 4. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.utils import AnalysisException
import math

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("tambr_rings_test").getOrCreate()

@pytest.fixture
def geodist_udf():
    def geodist(lat1, lon1, lat2, lon2):
        if None in (lat1, lon1, lat2, lon2):
            return None
        R = 3958.8
        dlat = math.radians(lat2 - lat1)
        dlon = math.radians(lon2 - lon1)
        a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2
        c = 2 * math.asin(math.sqrt(a))
        return R * c
    return F.udf(geodist, T.DoubleType())

@pytest.fixture
def sample_branch_df(spark):
    data = [
        ("10001", "R", "Y", 40.0, -75.0, "A", "G", None, "Main Branch", "Philly", "PA", "19104"),
        ("10002", "U", "Y", 0.0, 0.0, "B", "G", None, "Uni Branch", "Boston", "MA", "02115"),
        ("00001", "R", "Y", 40.0, -75.0, "A", "G", None, "Fake Branch", "Nowhere", "ZZ", "00000"),
        ("10003", "I", "Y", None, -75.0, "C", "G", None, "NullLat Branch", "NYC", "NY", "10001"),
    ]
    schema = "HGN_BR_ID string, BR_TYP string, BR_OPN_FLG string, LATITUDE_UPDT double, LONGITUDE_UPDT double, METRO_COMMUNITY_CDE string, GEN_CDE string, TBA_CLS_DVSTD_DT string, BRICK_AND_MORTOR_NM string, CITY string, ST string, ZIP_CDE string"
    return spark.createDataFrame(data, schema=schema)

@pytest.fixture
def sample_cust_df(spark):
    data = [
        ("C1", "10001", "A1", "Y", 12, "10001", "10001", "TYP1", 1, "1", 40.0, -75.0),
        ("C2", "10002", "A2", "Y", 6, "10002", "10002", "TYP2", 1, "1", 41.0, -76.0),
        ("C3", "10003", "A3", "N", 24, None, None, "TYP3", 0, "0", None, None),
    ]
    schema = "LP_ID string, PRTY_BR string, CUST_PRTY_ACCT_ID string, OPN_ACCT_FLG string, NBR_OF_MOS_OPN int, MOST_USED_BR string, MOST_USED_OLD string, HGN_CUST_TYP_CDE string, OPN_ACCT_CNT int, geomatchcode string, custlat double, custlong double"
    return spark.createDataFrame(data, schema=schema)

def test_happy_path_branch_filtering(sample_branch_df, spark):
    df = sample_branch_df.filter(
        (F.col("BR_TYP").isin(['R', 'U', 'I', 'T', 'C'])) &
        (F.col("BR_OPN_FLG") == 'Y') &
        (F.col("HGN_BR_ID") != '00001') &
        (F.col("LATITUDE_UPDT").isNotNull())
    )
    good = df.filter((F.col("LATITUDE_UPDT") > 1) & (F.col("LONGITUDE_UPDT") < -1))
    bad = df.filter(~((F.col("LATITUDE_UPDT") > 1) & (F.col("LONGITUDE_UPDT") < -1)))
    assert good.count() == 1
    assert bad.count() == 2

def test_empty_input(spark):
    empty = spark.createDataFrame([], "HGN_BR_ID string, BR_TYP string, BR_OPN_FLG string, LATITUDE_UPDT double, LONGITUDE_UPDT double, METRO_COMMUNITY_CDE string, GEN_CDE string, TBA_CLS_DVSTD_DT string, BRICK_AND_MORTOR_NM string, CITY string, ST string, ZIP_CDE string")
    assert empty.count() == 0

def test_null_join_keys(sample_cust_df, sample_branch_df, spark):
    null_row = [("C4", None, "A4", "Y", 10, "10001", "10001", "TYP4", 1, "1", 42.0, -77.0)]
    null_df = spark.createDataFrame(null_row, sample_cust_df.schema)
    custs = sample_cust_df.union(null_df)
    joined = custs.join(sample_branch_df, custs.PRTY_BR == sample_branch_df.HGN_BR_ID, "inner")
    assert "C4" not in [r.LP_ID for r in joined.collect()]

def test_duplicate_lp_id_branch_selection(spark):
    data = [
        ("C1", "B1", 10, 5, 100.0, 1),
        ("C1", "B2", 8, 4, 90.0, 2),
        ("C2", "B3", 12, 6, 110.0, 1),
    ]
    schema = "LP_ID string, BR_ID string, branch_used_days_3mo int, branch_used_days_prev int, branch_trans_amount_3mo double, dummy int"
    df = spark.createDataFrame(data, schema=schema)
    from pyspark.sql.window import Window
    w = Window.partitionBy("LP_ID").orderBy(F.desc("branch_used_days_3mo"), F.desc("branch_used_days_prev"), F.desc("branch_trans_amount_3mo"))
    df2 = df.withColumn("rn", F.row_number().over(w)).filter(F.col("rn") == 1)
    assert df2.count() == 2
    assert set(df2.select("BR_ID").rdd.flatMap(lambda x: x).collect()) == {"B1", "B3"}

def test_branch_bad_latlong(sample_branch_df, spark):
    bad = sample_branch_df.filter(~((F.col("LATITUDE_UPDT") > 1) & (F.col("LONGITUDE_UPDT") < -1)))
    assert bad.count() == 3

def test_customer_bad_geomatchcode(sample_cust_df, spark):
    bad = sample_cust_df.filter(F.col("geomatchcode").isin(['0', ' ']))
    assert bad.count() == 1

def test_geodist_udf(geodist_udf, spark):
    data = [(40.0, -75.0, 41.0, -76.0), (None, -75.0, 41.0, -76.0)]
    df = spark.createDataFrame(data, "lat1 double, lon1 double, lat2 double, lon2 double")
    df2 = df.withColumn("dist", geodist_udf("lat1", "lon1", "lat2", "lon2"))
    results = df2.select("dist").rdd.flatMap(lambda x: x).collect()
    assert results[0] == pytest.approx(86.901, 0.1)
    assert results[1] is None

def test_percentile_approx(spark):
    data = [("B1", 10.0), ("B1", 20.0), ("B1", 30.0), ("B2", 40.0), ("B2", 50.0)]
    df = spark.createDataFrame(data, "BR_ID string, dist double")
    result = df.groupBy("BR_ID").agg(F.expr("percentile_approx(dist, 0.8)").alias("p80"))
    out = {row["BR_ID"]: row["p80"] for row in result.collect()}
    assert out["B1"] == 26.0
    assert out["B2"] == 50.0

def test_type_mismatch_latlong(spark):
    data = [("10001", "R", "Y", "not_a_float", -75.0)]
    schema = "HGN_BR_ID string, BR_TYP string, BR_OPN_FLG string, LATITUDE_UPDT string, LONGITUDE_UPDT double"
    df = spark.createDataFrame(data, schema=schema)
    with pytest.raises(AnalysisException):
        df.filter(F.col("LATITUDE_UPDT") > 1).count()

def test_missing_column(spark):
    data = [("10001", "R", "Y", 40.0)]
    schema = "HGN_BR_ID string, BR_TYP string, BR_OPN_FLG string, LATITUDE_UPDT double"
    df = spark.createDataFrame(data, schema=schema)
    with pytest.raises(AnalysisException):
        df.select("NONEXISTENT_COLUMN").show()

def test_schema_output(sample_branch_df):
    expected_fields = set(['HGN_BR_ID', 'BR_TYP', 'BR_OPN_FLG', 'LATITUDE_UPDT', 'LONGITUDE_UPDT', 'METRO_COMMUNITY_CDE', 'GEN_CDE', 'TBA_CLS_DVSTD_DT', 'BRICK_AND_MORTOR_NM', 'CITY', 'ST', 'ZIP_CDE'])
    actual_fields = set(f.name for f in sample_branch_df.schema.fields)
    assert expected_fields == actual_fields

def test_filtering_open_branches(sample_branch_df):
    df = sample_branch_df.filter((F.col("BR_OPN_FLG") == 'Y') & (F.col("BR_TYP").isin(['R', 'U', 'I', 'T', 'C'])))
    assert all([row.BR_OPN_FLG == 'Y' for row in df.collect()])

def test_filtering_valid_customers(sample_cust_df):
    df = sample_cust_df.filter(F.col("OPN_ACCT_CNT") > 0)
    assert all([row.OPN_ACCT_CNT > 0 for row in df.collect()])

@pytest.mark.skip(reason="Performance test - enable for large datasets only")
def test_performance_large_input(spark):
    rows = [("C" + str(i), "B" + str(i%10), float(i), float(i%10)) for i in range(100000)]
    schema = "LP_ID string, BR_ID string, lat double, lon double"
    df = spark.createDataFrame(rows, schema=schema)
    result = df.groupBy("BR_ID").agg(F.count("*").alias("cnt"))
    assert result.count() == 10
```

# 5. Test Execution Report Template

| Test Case ID | Description | Expected Result | Actual Result | Pass/Fail Status | Comments |
|--------------|-------------|----------------|--------------|------------------|----------|
| TC01         |             |                |              |                  |          |
| TC02         |             |                |              |                  |          |
| ...          |             |                |              |                  |          |

# 6. API Cost

apiCost: 0.0646 USD

```
apiCost: 0.0646 USD
```

All required test cases, Pytest script, and documentation are provided above.