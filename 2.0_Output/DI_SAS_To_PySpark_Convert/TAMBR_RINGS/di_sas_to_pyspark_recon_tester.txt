```
=============================================
Author: Ascendion AVA+
Date: 
Description: Comprehensive Pytest suite for validating PySpark TAMBR_RINGS transformation logic, including joins, aggregations, UDFs, and edge cases.
=============================================

# 1. Syntactical Changes Made

- PROC SQL replaced with Spark SQL/DataFrame operations.
- DATA steps replaced with PySpark DataFrame transformations.
- Macro variable usage replaced with Python variables and helper functions.
- Conditional logic in DATA steps (e.g., IF statements) mapped to PySpark's `when` and `otherwise`.
- SAS geodist function replaced with a Python haversine UDF.
- SAS PROC UNIVARIATE/class/pctlpts replaced with PySpark `percentile_approx` for percentiles.
- Table creation and dropping handled by BigQuery overwrite mode and Spark temp views.
- SAS macros (`mdrop_mac`, `masofdt`) replaced with Python helper functions.
- SAS proc freq replaced with PySpark groupBy/count/show.

# 2. Manual Intervention Required

- Custom geodist logic: Implemented haversine UDF in PySpark.
- Macro variable substitution: All SAS macros for date and table names replaced with Python functions.
- PROC UNIVARIATE/class/pctlpts: Rewritten using PySpark `percentile_approx`.
- Table naming conventions and dataset/project setup: Manual update required for BigQuery references.
- Handling of missing SAS-specific functions: Manual mapping or replacement in PySpark.
- Error handling and logging: Added Python logging and try/except blocks.

# 3. Test Case Document

| Test Case ID | Description                                                                 | Preconditions                                   | Test Steps                                                                 | Expected Result                                                                                             | Actual Result | Pass/Fail Status |
|--------------|-----------------------------------------------------------------------------|-------------------------------------------------|---------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|--------------|------------------|
| TC01         | Happy path: Valid data, all joins succeed, all columns present              | All input DataFrames have valid schema and data | Run transformation pipeline end-to-end                                    | Output DataFrame matches expected schema and values, correct distance calculations, correct percentiles      |              |                  |
| TC02         | Edge: NULL latitude/longitude in customer or branch                         | Customer or branch has NULL lat/long            | Run distance calculation                                                  | Distance columns are NULL or handled gracefully, no crash                                                    |              |                  |
| TC03         | Edge: Zero rows in input DataFrames                                         | Input DataFrames are empty                      | Run transformation pipeline                                               | Output DataFrame is empty, schema is correct                                                                 |              |                  |
| TC04         | Edge: Duplicate LP_IDs in input                                            | Input DataFrame has duplicate LP_IDs            | Run deduplication logic                                                   | Only one row per LP_ID in deduplicated output                                                                |              |                  |
| TC05         | Edge: Customer with no matching branch                                      | Customer has PRTY_BR not present in branches    | Run join operation                                                        | Customer is excluded from final output as per join logic                                                     |              |                  |
| TC06         | Edge: Branch with missing priority/most used ring                           | Branch not present in ring DataFrames           | Merge ring DataFrames with branches                                       | Corresponding ring columns set to 0                                                                          |              |                  |
| TC07         | Error: Type mismatch in latitude/longitude columns                          | Latitude/longitude columns contain wrong type   | Run distance calculation                                                  | Exception is raised or handled, test fails gracefully                                                        |              |                  |
| TC08         | Error: Missing required columns in input                                    | Input DataFrame missing columns                 | Run transformation pipeline                                               | Exception is raised or handled, test fails gracefully                                                        |              |                  |
| TC09         | Happy path: All customers have good lat/long, all branches valid            | All customers and branches have valid lat/long  | Run transformation pipeline                                               | All customers are processed, distances and percentiles are correct                                           |              |                  |
| TC10         | Edge: Customer with invalid geomatchcode                                    | Customer has geomatchcode in ('0', ' ')         | Run geomatchcode filter                                                   | Customer routed to bad_latlong_cust, not included in main output                                             |              |                  |

# 4. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import *
from pyspark.sql import functions as F
from math import radians, cos, sin, asin, sqrt

# Helper UDF for haversine distance (as in main code)
def haversine(lat1, lon1, lat2, lon2):
    if None in (lat1, lon1, lat2, lon2):
        return None
    lat1, lon1, lat2, lon2 = map(float, [lat1, lon1, lat2, lon2])
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    r = 3956
    return c * r

@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("pytest-TAMBR_RINGS").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def haversine_udf():
    return F.udf(haversine, DoubleType())

@pytest.fixture
def sample_customers(spark):
    schema = StructType([
        StructField("LP_ID", StringType(), True),
        StructField("PRTY_BR", StringType(), True),
        StructField("OPN_ACCT_FLG", StringType(), True),
        StructField("NBR_OF_MOS_OPN", IntegerType(), True),
        StructField("CUSTLAT", DoubleType(), True),
        StructField("CUSTLONG", DoubleType(), True),
        StructField("MOST_USED_BR", StringType(), True),
        StructField("geomatchcode", StringType(), True)
    ])
    data = [
        ("1", "B1", "Y", 5, 40.0, -75.0, "B1", "1"),
        ("2", "B2", "Y", 10, 41.0, -74.0, "B2", "1"),
        ("3", "B3", "N", 15, 42.0, -73.0, "B3", "1"),
        ("4", "B4", "Y", 25, 43.0, -72.0, "B4", "0"),  # bad lat/long
        ("5", "B5", "Y", 12, None, None, "B5", "1"),    # NULL lat/long
    ]
    return spark.createDataFrame(data, schema)

@pytest.fixture
def sample_branches(spark):
    schema = StructType([
        StructField("HGN_BR_ID", StringType(), True),
        StructField("BR_TYP", StringType(), True),
        StructField("BR_OPN_FLG", StringType(), True),
        StructField("branchlat", DoubleType(), True),
        StructField("branchlong", DoubleType(), True),
        StructField("METRO_COMMUNITY_CDE", StringType(), True),
        StructField("BRICK_AND_MORTOR_NM", StringType(), True),
        StructField("CITY", StringType(), True),
        StructField("ST", StringType(), True)
    ])
    data = [
        ("B1", "R", "Y", 40.1, -75.1, "M", "Branch 1", "City1", "ST1"),
        ("B2", "U", "Y", 41.1, -74.1, "C", "Branch 2", "City2", "ST2"),
        ("B3", "I", "Y", 42.1, -73.1, "S", "Branch 3", "City3", "ST3"),
        ("B4", "T", "Y", 43.1, -72.1, "M", "Branch 4", "City4", "ST4"),
        ("B5", "C", "Y", None, None, "C", "Branch 5", "City5", "ST5"),  # NULL lat/long
    ]
    return spark.createDataFrame(data, schema)

@pytest.fixture
def sample_most_used(spark):
    schema = StructType([
        StructField("LP_ID", StringType(), True),
        StructField("BR_ID", StringType(), True)
    ])
    data = [
        ("1", "B1"),
        ("2", "B2"),
        ("3", "B3"),
        ("5", "B5"),
    ]
    return spark.createDataFrame(data, schema)

# Utility: Compare DataFrames (schema and content)
def assert_df_equality(df1, df2, ignore_nullable=True):
    assert df1.schema == df2.schema
    data1 = set(tuple(row) for row in df1.collect())
    data2 = set(tuple(row) for row in df2.collect())
    assert data1 == data2

# TC01: Happy path
def test_happy_path(spark, sample_customers, sample_branches, haversine_udf):
    # Only customers with OPN_ACCT_FLG == 'Y' and 0 <= NBR_OF_MOS_OPN <= 24 and good geomatchcode
    cust = sample_customers.filter(
        (F.col("OPN_ACCT_FLG") == "Y") &
        (F.col("NBR_OF_MOS_OPN") <= 24) &
        (F.col("NBR_OF_MOS_OPN") >= 0) &
        (~F.col("geomatchcode").isin("0", " "))
    )
    branches = sample_branches.filter((F.col("branchlat") > 1) & (F.col("branchlong") < -1))
    joined = cust.join(branches, cust.PRTY_BR == branches.HGN_BR_ID, "inner")
    joined = joined.withColumn(
        "dist_to_prty_br",
        haversine_udf("branchlat", "branchlong", "CUSTLAT", "CUSTLONG")
    )
    # Check schema and a sample value
    assert "dist_to_prty_br" in joined.columns
    assert joined.count() > 0

# TC02: NULL lat/long
def test_null_latlong(spark, sample_customers, sample_branches, haversine_udf):
    cust = sample_customers.filter(F.col("LP_ID") == "5")
    branches = sample_branches.filter(F.col("HGN_BR_ID") == "B5")
    joined = cust.join(branches, cust.PRTY_BR == branches.HGN_BR_ID, "inner")
    joined = joined.withColumn(
        "dist_to_prty_br",
        haversine_udf("branchlat", "branchlong", "CUSTLAT", "CUSTLONG")
    )
    row = joined.collect()[0]
    assert row["dist_to_prty_br"] is None

# TC03: Zero rows
def test_zero_rows(spark, sample_customers, sample_branches):
    empty_cust = sample_customers.filter(F.lit(False))
    empty_branches = sample_branches.filter(F.lit(False))
    joined = empty_cust.join(empty_branches, empty_cust.PRTY_BR == empty_branches.HGN_BR_ID, "inner")
    assert joined.count() == 0
    assert joined.schema is not None

# TC04: Duplicate LP_IDs
def test_deduplication(spark, sample_customers):
    # Duplicate LP_ID
    schema = sample_customers.schema
    data = [("1", "B1", "Y", 5, 40.0, -75.0, "B1", "1"), ("1", "B1", "Y", 5, 40.0, -75.0, "B1", "1")]
    df = spark.createDataFrame(data, schema)
    w = Window.partitionBy("LP_ID").orderBy(F.asc("LP_ID"))
    deduped = df.withColumn("rn", F.row_number().over(w)).filter(F.col("rn") == 1).drop("rn")
    assert deduped.count() == 1

# TC05: Customer with no matching branch
def test_customer_no_matching_branch(spark, sample_customers, sample_branches):
    cust = sample_customers.filter(F.col("LP_ID") == "1")
    branches = sample_branches.filter(F.col("HGN_BR_ID") == "B999")  # No match
    joined = cust.join(branches, cust.PRTY_BR == branches.HGN_BR_ID, "inner")
    assert joined.count() == 0

# TC06: Branch with missing priority/most used ring
def test_missing_ring_columns(spark, sample_branches):
    # Simulate missing percentiles
    branch_data2 = sample_branches.select(
        F.col("HGN_BR_ID").alias("TAMBR"),
        F.col("BR_TYP").alias("branch_typ"),
        F.col("BRICK_AND_MORTOR_NM").alias("branch_name"),
        F.col("CITY").alias("branch_city"),
        F.col("ST").alias("branch_state"),
        "branchlat", "branchlong",
        F.col("METRO_COMMUNITY_CDE").alias("metcomm_cde")
    )
    priority_ring_df = spark.createDataFrame([], StructType([StructField("TAMBR", StringType()), StructField("priority_ring", DoubleType())]))
    most_used_ring_df = spark.createDataFrame([], StructType([StructField("TAMBR", StringType()), StructField("most_used_ring", DoubleType())]))
    tambr_rings = branch_data2.join(priority_ring_df, "TAMBR", "left") \
        .join(most_used_ring_df, "TAMBR", "left") \
        .withColumn("priority_ring", F.coalesce(F.col("priority_ring"), F.lit(0))) \
        .withColumn("most_used_ring", F.coalesce(F.col("most_used_ring"), F.lit(0)))
    assert tambr_rings.filter(F.col("priority_ring") == 0).count() == tambr_rings.count()
    assert tambr_rings.filter(F.col("most_used_ring") == 0).count() == tambr_rings.count()

# TC07: Type mismatch in latitude/longitude
def test_type_mismatch_latlong(spark, sample_customers, sample_branches, haversine_udf):
    schema = sample_customers.schema
    data = [("1", "B1", "Y", 5, "not_a_float", -75.0, "B1", "1")]
    df = spark.createDataFrame(data, schema)
    branches = sample_branches.filter(F.col("HGN_BR_ID") == "B1")
    joined = df.join(branches, df.PRTY_BR == branches.HGN_BR_ID, "inner")
    with pytest.raises(Exception):
        joined.withColumn("dist_to_prty_br", haversine_udf("branchlat", "branchlong", "CUSTLAT", "CUSTLONG")).collect()

# TC08: Missing required columns
def test_missing_columns(spark, sample_customers):
    schema = StructType([StructField("LP_ID", StringType(), True)])
    df = spark.createDataFrame([("1",)], schema)
    with pytest.raises(Exception):
        df.select("PRTY_BR").collect()

# TC09: All customers have good lat/long, all branches valid
def test_all_good_latlong(spark, sample_customers, sample_branches, haversine_udf):
    cust = sample_customers.filter(F.col("LP_ID").isin("1", "2"))
    branches = sample_branches.filter(F.col("HGN_BR_ID").isin("B1", "B2"))
    joined = cust.join(branches, cust.PRTY_BR == branches.HGN_BR_ID, "inner")
    joined = joined.withColumn(
        "dist_to_prty_br",
        haversine_udf("branchlat", "branchlong", "CUSTLAT", "CUSTLONG")
    )
    assert joined.filter(F.col("dist_to_prty_br").isNotNull()).count() == joined.count()

# TC10: Customer with invalid geomatchcode
def test_invalid_geomatchcode(spark, sample_customers):
    bad = sample_customers.filter(F.col("geomatchcode").isin("0", " "))
    assert bad.count() == 1

```

# 5. Test Execution Report Template

| Test Case ID | Description | Expected Result | Actual Result | Pass/Fail Status | Comments |
|--------------|-------------|----------------|--------------|------------------|----------|
| TC01         |             |                |              |                  |          |
| TC02         |             |                |              |                  |          |
| TC03         |             |                |              |                  |          |
| TC04         |             |                |              |                  |          |
| TC05         |             |                |              |                  |          |
| TC06         |             |                |              |                  |          |
| TC07         |             |                |              |                  |          |
| TC08         |             |                |              |                  |          |
| TC09         |             |                |              |                  |          |
| TC10         |             |                |              |                  |          |

# 6. API Cost

apiCost: 0.0023 USD
```