```
=============================================
Author: Ascendion AVA+
Date: 
Description: Pytest script and test case documentation for validating the TAMBR_RINGS SAS-to-PySpark conversion, covering syntax changes, manual interventions, and logic equivalence.
=============================================

1. Syntactical changes made
---------------------------
- PROC SQL CONNECT TO DB2 replaced with PySpark DataFrame read/write using BigQuery connector.
- SAS DATA steps replaced with PySpark DataFrame transformations.
- Macro variables (e.g., &campid, &SYSUSERID) replaced with Python variables and config.
- SAS conditional logic (IF/CASE) replaced with PySpark DataFrame .when/.otherwise expressions.
- PROC UNIVARIATE percentile calculation replaced with PySpark percentile_approx.
- SAS macro functions (mdrop_mac, masofdt) replaced with Python helper functions.
- SAS PROC FREQ replaced with PySpark groupBy/count/show for reporting.
- Table drops and existence checks replaced with BigQuery Python client logic.
- SAS date formatting replaced with Python datetime formatting.
- SAS geodist function replaced with Python Haversine UDF.
- SAS output logic for missing values replaced with PySpark coalesce and default values.
- SAS join logic replaced with PySpark DataFrame .join.

2. Manual intervention required
-------------------------------
- Dynamic user name and macro variable expansion: Replace <user> and macro variables with Python config/environment variables.
- Confirm and set GCP project/dataset names in Python variables.
- SAS-specific error handling/reporting: Expand Python logging if needed.
- PROC FREQ output formatting: Customize reporting as per requirements.
- Macro logic for table dropping and date formatting: Ensure Python helpers match SAS intent.
- Any additional SAS macros referenced elsewhere should be ported to Python as needed.
- Audit/logging requirements: Expand logging section if required.

3. Test Case Document
---------------------
| Test Case ID | Description | Preconditions | Test Steps | Expected Result | Actual Result | Pass/Fail Status |
|--------------|-------------|---------------|------------|----------------|--------------|------------------|
| TC01 | Validate output DataFrame schema matches expected columns and types | PySpark session, sample DataFrames | Compare DataFrame schema to expected | Output schema matches TAMBR_RINGS.txt specification | | |
| TC02 | Happy path: Valid data transformation for customer and branch join | Valid sample customer/branch DataFrames | Perform join, check output | Output contains correct rows and values | | |
| TC03 | Edge case: Input DataFrames with NULL latitude/longitude | DataFrames with NULL lat/long | Filter for bad lat/long | Rows with bad lat/long separated correctly | | |
| TC04 | Edge case: Input DataFrames with zero rows | Empty DataFrames | Run through pipeline | Output DataFrames are empty, no errors | | |
| TC05 | Edge case: Duplicate LP_IDs in most used logic | DataFrame with duplicate LP_IDs | Window function, select first | Only first record per LP_ID retained | | |
| TC06 | Error handling: Type mismatch in latitude/longitude columns | DataFrame with type mismatch | Run geodist UDF | Exception raised or handled gracefully | | |
| TC07 | Aggregation: 80th percentile ring calculation | DataFrame with known distances | Calculate percentile | Percentile matches expected value | | |
| TC08 | Filtering: Only open branches of allowed types included | Sample branch DataFrame | Filter by type/status | Output excludes closed/fictitious branches | | |
| TC09 | UDF: geodist calculation correctness | DataFrame with known coordinates | Calculate distance | Distance matches known value | | |
| TC10 | Performance: PySpark faster than SAS baseline (mocked) | Mocked timing | Measure runtime | Runtime below threshold | | |
| TC11 | Edge case: Missing fields in input DataFrames | DataFrame missing columns | Select missing column | Exception raised or handled gracefully | | |
| TC12 | Final output: Merged rings DataFrame correctness | Sample DataFrames | Merge, fill missing rings | Output matches expected structure | | |

4. Pytest Script for each of the test cases
-------------------------------------------
```python
import pytest
from pyspark.sql import SparkSession, Row, functions as F, types as T, Window
import math

@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("pytest-tambr-rings").getOrCreate()
    yield spark
    spark.stop()

def geodist(lat1, lon1, lat2, lon2):
    if None in [lat1, lon1, lat2, lon2]:
        return None
    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2
    c = 2 * math.asin(math.sqrt(a))
    r = 3956
    return c * r

geodist_udf = F.udf(geodist, T.DoubleType())

@pytest.fixture
def sample_customers(spark):
    data = [
        Row(LP_ID="1", PRTY_BR="101", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat=40.0, custlong=-75.0, MOST_USED_BR="101", geomatchcode="A", OPN_ACCT_CNT=1),
        Row(LP_ID="2", PRTY_BR="102", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=24, custlat=41.0, custlong=-76.0, MOST_USED_BR="102", geomatchcode="A", OPN_ACCT_CNT=1),
        Row(LP_ID="3", PRTY_BR="103", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=6, custlat=None, custlong=None, MOST_USED_BR="103", geomatchcode="0", OPN_ACCT_CNT=1),
    ]
    return spark.createDataFrame(data)

@pytest.fixture
def sample_branches(spark):
    data = [
        Row(HGN_BR_ID="101", BR_TYP="T", BR_OPN_FLG="Y", branchlat=40.1, branchlong=-75.1, METRO_COMMUNITY_CDE="A", BRICK_AND_MORTOR_NM="Branch 101", CITY="CityA", ST="PA", ZIP_CDE="19000"),
        Row(HGN_BR_ID="102", BR_TYP="R", BR_OPN_FLG="Y", branchlat=41.1, branchlong=-76.1, METRO_COMMUNITY_CDE="B", BRICK_AND_MORTOR_NM="Branch 102", CITY="CityB", ST="NY", ZIP_CDE="10000"),
        Row(HGN_BR_ID="103", BR_TYP="U", BR_OPN_FLG="Y", branchlat=None, branchlong=None, METRO_COMMUNITY_CDE="C", BRICK_AND_MORTOR_NM="Branch 103", CITY="CityC", ST="NJ", ZIP_CDE="08000"),
        Row(HGN_BR_ID="00001", BR_TYP="C", BR_OPN_FLG="Y", branchlat=0.0, branchlong=0.0, METRO_COMMUNITY_CDE="X", BRICK_AND_MORTOR_NM="Fictitious", CITY="CityX", ST="XX", ZIP_CDE="00000"),
    ]
    return spark.createDataFrame(data)

def test_schema(sample_customers, sample_branches):
    # TC01: Validate schema
    expected_customer_fields = {"LP_ID", "PRTY_BR", "OPN_ACCT_FLG", "NBR_OF_MOS_OPN", "custlat", "custlong", "MOST_USED_BR", "geomatchcode", "OPN_ACCT_CNT"}
    assert set(sample_customers.columns) == expected_customer_fields

def test_happy_path_join(sample_customers, sample_branches, spark):
    # TC02: Happy path join
    joined = sample_customers.join(sample_branches, sample_customers.PRTY_BR == sample_branches.HGN_BR_ID, "inner")
    assert joined.count() == 2
    assert set(joined.select("LP_ID").rdd.flatMap(lambda x: x).collect()) == {"1", "2"}

def test_bad_latlong_separation(sample_customers, spark):
    # TC03: Edge case for bad lat/long
    rings_cust_data = sample_customers.filter(~F.col("geomatchcode").isin(['0', ' ']))
    bad_latlong_cust = sample_customers.filter(F.col("geomatchcode").isin(['0', ' ']))
    assert rings_cust_data.count() == 2
    assert bad_latlong_cust.count() == 1

def test_zero_rows(spark):
    # TC04: Edge case zero rows
    empty_df = spark.createDataFrame([], T.StructType([
        T.StructField("LP_ID", T.StringType(), True),
        T.StructField("PRTY_BR", T.StringType(), True),
    ]))
    assert empty_df.count() == 0

def test_duplicate_lp_id_handling(spark):
    # TC05: Duplicate LP_IDs
    data = [
        Row(LP_ID="1", NBR_OF_MOS_OPN=12),
        Row(LP_ID="1", NBR_OF_MOS_OPN=10),
        Row(LP_ID="2", NBR_OF_MOS_OPN=24),
    ]
    df = spark.createDataFrame(data)
    w = Window.partitionBy("LP_ID").orderBy("NBR_OF_MOS_OPN")
    df_first = df.withColumn("rn", F.row_number().over(w)).filter(F.col("rn") == 1).drop("rn")
    assert df_first.count() == 2
    assert set(df_first.select("LP_ID").rdd.flatMap(lambda x: x).collect()) == {"1", "2"}

def test_type_mismatch_latlong(spark):
    # TC06: Type mismatch
    data = [
        Row(branchlat="not_a_float", branchlong=-75.1),
    ]
    df = spark.createDataFrame(data)
    with pytest.raises(Exception):
        df.withColumn("dist", geodist_udf("branchlat", "branchlong", F.lit(40.0), F.lit(-75.0))).collect()

def test_percentile_aggregation(spark):
    # TC07: 80th percentile calculation
    data = [
        Row(PRTY_BR="101", dist_to_prty_br=10.0),
        Row(PRTY_BR="101", dist_to_prty_br=20.0),
        Row(PRTY_BR="101", dist_to_prty_br=30.0),
        Row(PRTY_BR="101", dist_to_prty_br=40.0),
        Row(PRTY_BR="101", dist_to_prty_br=50.0),
    ]
    df = spark.createDataFrame(data)
    ring_priority = df.groupBy("PRTY_BR").agg(F.expr('percentile_approx(dist_to_prty_br, 0.8)').alias("prtybr80"))
    val = ring_priority.select("prtybr80").collect()[0][0]
    assert math.isclose(val, 42.0, rel_tol=0.1)

def test_branch_filtering(sample_branches):
    # TC08: Filtering for open branches
    filtered = sample_branches.filter(
        (F.col("BR_TYP").isin(['R','U','I','T','C'])) &
        (F.col("BR_OPN_FLG") == 'Y') &
        (F.col("HGN_BR_ID") != '00001') &
        (F.col("branchlat").isNotNull())
    )
    assert filtered.count() == 2

def test_geodist_udf(spark):
    # TC09: geodist calculation
    df = spark.createDataFrame([Row(branchlat=40.0, branchlong=-75.0, custlat=41.0, custlong=-76.0)])
    df = df.withColumn("dist", geodist_udf("branchlat", "branchlong", "custlat", "custlong"))
    val = df.select("dist").collect()[0][0]
    assert math.isclose(val, 86.7, rel_tol=0.1)

def test_performance_mocked():
    # TC10: Performance (mocked)
    import time
    start = time.time()
    time.sleep(0.1)
    end = time.time()
    assert (end - start) < 1.0

def test_missing_fields(spark):
    # TC11: Missing fields
    data = [Row(LP_ID="1")]
    df = spark.createDataFrame(data)
    with pytest.raises(Exception):
        df.select("PRTY_BR").collect()

def test_final_output_merge(spark):
    # TC12: Final output merge, missing rings set to 0
    branch_data2 = spark.createDataFrame([
        Row(TAMBR="101", branch_typ="T", branch_name="Branch 101", branch_city="CityA", branch_state="PA", branchlat=40.1, branchlong=-75.1, metcomm_cde="A"),
        Row(TAMBR="102", branch_typ="R", branch_name="Branch 102", branch_city="CityB", branch_state="NY", branchlat=41.1, branchlong=-76.1, metcomm_cde="B"),
    ])
    ring_priority2 = spark.createDataFrame([
        Row(TAMBR="101", priority_ring=42.0),
    ])
    ring_most_used2 = spark.createDataFrame([
        Row(TAMBR="101", most_used_ring=35.0),
    ])
    final_rings = branch_data2.join(ring_priority2, "TAMBR", "left") \
        .join(ring_most_used2, "TAMBR", "left") \
        .withColumn("priority_ring", F.coalesce(F.col("priority_ring"), F.lit(0))) \
        .withColumn("most_used_ring", F.coalesce(F.col("most_used_ring"), F.lit(0))) \
        .withColumn("max_dist", F.lit(40))
    vals = final_rings.select("priority_ring", "most_used_ring", "max_dist").collect()
    assert vals[0][0] == 42.0 and vals[0][1] == 35.0 and vals[0][2] == 40
    assert vals[1][0] == 0 and vals[1][1] == 0 and vals[1][2] == 40
```

5. Api Cost
-----------
apiCost: 0.0823 USD

(0.08 from PySpark conversion + 0.0023 from analysis read; all decimal values included; currency: USD)
```