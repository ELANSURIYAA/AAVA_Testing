=============================================
Author: Ascendion AVA+
Date: 
Description: Pytest-based unit tests for TAMBR_RINGS PySpark data transformation pipeline, validating joins, aggregations, UDFs, schema, and edge cases.
=============================================

# 1. Test Case List

| Test Case ID | Description                                                                                  | Expected Outcome                                                                                      |
|--------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All input tables have valid data, all joins succeed, no NULLs                    | Output DataFrames match expected schema and data, correct row counts, correct aggregations            |
| TC02         | Edge: Input DataFrames are empty                                                             | Output DataFrames are empty, no errors                                                               |
| TC03         | Edge: Input DataFrames contain NULLs in join keys                                            | Rows with NULL join keys are handled per join type (e.g., left join keeps left rows, inner drops)    |
| TC04         | Edge: Input DataFrames contain duplicate LP_IDs or BR_IDs                                    | Only first (or correct) row per LP_ID/BR_ID is selected as per window logic                          |
| TC05         | Edge: Branches with invalid lat/long (e.g., 0, None)                                         | Such branches are filtered to bad_latlong_branch, not in rings_branch_data                           |
| TC06         | Edge: Customers with invalid geomatchcode                                                    | Such customers are filtered to bad_latlong_cust, not in rings_cust_data                              |
| TC07         | UDF: geodist returns correct distance, including NULL handling                               | geodist returns None if any input is None, correct miles otherwise                                   |
| TC08         | Aggregation: 80th percentile calculation per branch                                          | percentile_approx returns correct value, schema matches, no errors                                   |
| TC09         | Error: Type mismatch in lat/long columns                                                     | Raises AnalysisException or handles gracefully                                                       |
| TC10         | Error: Missing required columns in input DataFrames                                          | Raises AnalysisException                                                                             |
| TC11         | Schema: Output DataFrames have correct columns and types (matches SAS output)                | Schema matches expected definition                                                                   |
| TC12         | Filtering: Only open, valid branches (BR_OPN_FLG='Y', BR_TYP in allowed list, etc.) included | Output only contains valid branches                                                                  |
| TC13         | Filtering: Only customers with OPN_ACCT_CNT > 0 included                                    | Output only contains valid customers                                                                 |
| TC14         | Performance: Large input DataFrames processed efficiently (smoke test)                       | Test completes within reasonable time, no memory errors                                              |

# 2. Pytest Script for each test case

```python
import pytest
from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.utils import AnalysisException
from chispa.dataframe_comparer import assert_df_equality
import math

# ---- Fixtures ----

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("tambr_rings_test").getOrCreate()

@pytest.fixture
def geodist_udf():
    def geodist(lat1, lon1, lat2, lon2):
        if None in (lat1, lon1, lat2, lon2):
            return None
        R = 3958.8
        dlat = math.radians(lat2 - lat1)
        dlon = math.radians(lon2 - lon1)
        a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2
        c = 2 * math.asin(math.sqrt(a))
        return R * c
    return F.udf(geodist, T.DoubleType())

# ---- Helper Data ----

@pytest.fixture
def sample_branch_df(spark):
    data = [
        ("10001", "R", "Y", 40.0, -75.0, "A", "G", None, "Main Branch", "Philly", "PA", "19104"),
        ("10002", "U", "Y", 0.0, 0.0, "B", "G", None, "Uni Branch", "Boston", "MA", "02115"),
        ("00001", "R", "Y", 40.0, -75.0, "A", "G", None, "Fake Branch", "Nowhere", "ZZ", "00000"),
        ("10003", "I", "Y", None, -75.0, "C", "G", None, "NullLat Branch", "NYC", "NY", "10001"),
    ]
    schema = "HGN_BR_ID string, BR_TYP string, BR_OPN_FLG string, LATITUDE_UPDT double, LONGITUDE_UPDT double, METRO_COMMUNITY_CDE string, GEN_CDE string, TBA_CLS_DVSTD_DT string, BRICK_AND_MORTOR_NM string, CITY string, ST string, ZIP_CDE string"
    return spark.createDataFrame(data, schema=schema)

@pytest.fixture
def sample_cust_df(spark):
    data = [
        ("C1", "10001", "A1", "Y", 12, "10001", "10001", "TYP1", 1, "1", 40.0, -75.0),
        ("C2", "10002", "A2", "Y", 6, "10002", "10002", "TYP2", 1, "1", 41.0, -76.0),
        ("C3", "10003", "A3", "N", 24, None, None, "TYP3", 0, "0", None, None),
    ]
    schema = "LP_ID string, PRTY_BR string, CUST_PRTY_ACCT_ID string, OPN_ACCT_FLG string, NBR_OF_MOS_OPN int, MOST_USED_BR string, MOST_USED_OLD string, HGN_CUST_TYP_CDE string, OPN_ACCT_CNT int, geomatchcode string, custlat double, custlong double"
    return spark.createDataFrame(data, schema=schema)

@pytest.fixture
def sample_geo_df(spark):
    data = [
        ("C1", "1", 40.0, -75.0, "PA"),
        ("C2", "1", 41.0, -76.0, "MA"),
    ]
    schema = "LP_ID string, GEO_MTCH_CDE string, GEO_LATITUDE double, GEO_LONGITUDE double, GEO_FIPS_ST_CDE string"
    return spark.createDataFrame(data, schema=schema)

# ---- Test Cases ----

def test_happy_path_branch_filtering(sample_branch_df, spark):
    # Only open, valid branches, not '00001', lat > 1, long < -1
    df = sample_branch_df.filter(
        (F.col("BR_TYP").isin(['R', 'U', 'I', 'T', 'C'])) &
        (F.col("BR_OPN_FLG") == 'Y') &
        (F.col("HGN_BR_ID") != '00001') &
        (F.col("LATITUDE_UPDT").isNotNull())
    )
    good = df.filter((F.col("LATITUDE_UPDT") > 1) & (F.col("LONGITUDE_UPDT") < -1))
    bad = df.filter(~((F.col("LATITUDE_UPDT") > 1) & (F.col("LONGITUDE_UPDT") < -1)))
    assert good.count() == 1
    assert bad.count() == 2

def test_empty_input(spark):
    empty = spark.createDataFrame([], "HGN_BR_ID string, BR_TYP string, BR_OPN_FLG string, LATITUDE_UPDT double, LONGITUDE_UPDT double, METRO_COMMUNITY_CDE string, GEN_CDE string, TBA_CLS_DVSTD_DT string, BRICK_AND_MORTOR_NM string, CITY string, ST string, ZIP_CDE string")
    assert empty.count() == 0

def test_null_join_keys(sample_cust_df, sample_branch_df, spark):
    # Add a row with NULL PRTY_BR to customer
    null_row = [("C4", None, "A4", "Y", 10, "10001", "10001", "TYP4", 1, "1", 42.0, -77.0)]
    null_df = spark.createDataFrame(null_row, sample_cust_df.schema)
    custs = sample_cust_df.union(null_df)
    joined = custs.join(sample_branch_df, custs.PRTY_BR == sample_branch_df.HGN_BR_ID, "inner")
    # Should not include C4
    assert "C4" not in [r.LP_ID for r in joined.collect()]

def test_duplicate_lp_id_branch_selection(spark):
    # Simulate window logic for most_used selection
    data = [
        ("C1", "B1", 10, 5, 100.0, 1),
        ("C1", "B2", 8, 4, 90.0, 2),
        ("C2", "B3", 12, 6, 110.0, 1),
    ]
    schema = "LP_ID string, BR_ID string, branch_used_days_3mo int, branch_used_days_prev int, branch_trans_amount_3mo double, dummy int"
    df = spark.createDataFrame(data, schema=schema)
    from pyspark.sql.window import Window
    w = Window.partitionBy("LP_ID").orderBy(F.desc("branch_used_days_3mo"), F.desc("branch_used_days_prev"), F.desc("branch_trans_amount_3mo"))
    df2 = df.withColumn("rn", F.row_number().over(w)).filter(F.col("rn") == 1)
    assert df2.count() == 2
    assert set(df2.select("BR_ID").rdd.flatMap(lambda x: x).collect()) == {"B1", "B3"}

def test_branch_bad_latlong(sample_branch_df, spark):
    bad = sample_branch_df.filter(~((F.col("LATITUDE_UPDT") > 1) & (F.col("LONGITUDE_UPDT") < -1)))
    assert bad.count() == 3

def test_customer_bad_geomatchcode(sample_cust_df, spark):
    bad = sample_cust_df.filter(F.col("geomatchcode").isin(['0', ' ']))
    assert bad.count() == 1

def test_geodist_udf(geodist_udf, spark):
    data = [(40.0, -75.0, 41.0, -76.0), (None, -75.0, 41.0, -76.0)]
    df = spark.createDataFrame(data, "lat1 double, lon1 double, lat2 double, lon2 double")
    df2 = df.withColumn("dist", geodist_udf("lat1", "lon1", "lat2", "lon2"))
    results = df2.select("dist").rdd.flatMap(lambda x: x).collect()
    assert results[0] == pytest.approx(86.901, 0.1)
    assert results[1] is None

def test_percentile_approx(spark):
    data = [("B1", 10.0), ("B1", 20.0), ("B1", 30.0), ("B2", 40.0), ("B2", 50.0)]
    df = spark.createDataFrame(data, "BR_ID string, dist double")
    result = df.groupBy("BR_ID").agg(F.expr("percentile_approx(dist, 0.8)").alias("p80"))
    out = {row["BR_ID"]: row["p80"] for row in result.collect()}
    assert out["B1"] == 26.0  # 80th percentile of [10,20,30]
    assert out["B2"] == 50.0  # 80th percentile of [40,50]

def test_type_mismatch_latlong(spark):
    data = [("10001", "R", "Y", "not_a_float", -75.0)]
    schema = "HGN_BR_ID string, BR_TYP string, BR_OPN_FLG string, LATITUDE_UPDT string, LONGITUDE_UPDT double"
    df = spark.createDataFrame(data, schema=schema)
    with pytest.raises(AnalysisException):
        df.filter(F.col("LATITUDE_UPDT") > 1).count()

def test_missing_column(spark):
    data = [("10001", "R", "Y", 40.0)]
    schema = "HGN_BR_ID string, BR_TYP string, BR_OPN_FLG string, LATITUDE_UPDT double"
    df = spark.createDataFrame(data, schema=schema)
    with pytest.raises(AnalysisException):
        df.select("NONEXISTENT_COLUMN").show()

def test_schema_output(sample_branch_df):
    expected_fields = set(['HGN_BR_ID', 'BR_TYP', 'BR_OPN_FLG', 'LATITUDE_UPDT', 'LONGITUDE_UPDT', 'METRO_COMMUNITY_CDE', 'GEN_CDE', 'TBA_CLS_DVSTD_DT', 'BRICK_AND_MORTOR_NM', 'CITY', 'ST', 'ZIP_CDE'])
    actual_fields = set(f.name for f in sample_branch_df.schema.fields)
    assert expected_fields == actual_fields

def test_filtering_open_branches(sample_branch_df):
    df = sample_branch_df.filter((F.col("BR_OPN_FLG") == 'Y') & (F.col("BR_TYP").isin(['R', 'U', 'I', 'T', 'C'])))
    assert all([row.BR_OPN_FLG == 'Y' for row in df.collect()])

def test_filtering_valid_customers(sample_cust_df):
    df = sample_cust_df.filter(F.col("OPN_ACCT_CNT") > 0)
    assert all([row.OPN_ACCT_CNT > 0 for row in df.collect()])

@pytest.mark.skip(reason="Performance test - enable for large datasets only")
def test_performance_large_input(spark):
    # Generate large DataFrame
    rows = [("C" + str(i), "B" + str(i%10), float(i), float(i%10)) for i in range(100000)]
    schema = "LP_ID string, BR_ID string, lat double, lon double"
    df = spark.createDataFrame(rows, schema=schema)
    # Simple group by
    result = df.groupBy("BR_ID").agg(F.count("*").alias("cnt"))
    assert result.count() == 10

```

# 3. API Cost Calculation

apiCost: 0.0623 USD

# Notes

- These tests use Pytest and are compatible with chispa for DataFrame equality checks.
- The tests cover happy path, edge cases (empty, NULLs, duplicates), error handling (type mismatch, missing columns), UDF correctness, aggregations, and schema validation.
- Fixtures provide reusable sample DataFrames.
- For real-world use, parameterize paths and add more integration tests with actual data if available.
- The performance test is marked as skipped by default; enable as needed.

# 4. Output

All test cases and scripts are provided above.  
apiCost: 0.0623 USD