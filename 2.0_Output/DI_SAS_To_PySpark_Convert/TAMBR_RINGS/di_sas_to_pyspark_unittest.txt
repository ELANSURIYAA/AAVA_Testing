```
=============================================
Author: Ascendion AVA+
Date: 
Description: Pytest scripts for validating PySpark transformation logic, joins, aggregations, and edge cases for TAMBR_RINGS customer/branch ring analysis.
=============================================

# 1. Test Case List

| Test Case ID | Description | Expected Outcome |
|--------------|-------------|-----------------|
| TC01 | Validate happy path: correct transformation and join outputs for valid customer and branch data | Output DataFrame matches expected schema and data values |
| TC02 | Validate UDF geodist: correct calculation of distance between lat/long pairs | Calculated distance matches Haversine formula output |
| TC03 | Validate join logic: customers with matching branches are joined correctly | Only valid customer-branch pairs present in output |
| TC04 | Validate filter: only open branches and customers with valid account counts are included | Output excludes closed branches and customers with zero accounts |
| TC05 | Edge case: NULL latitude/longitude in customer/branch data | Output handles NULLs according to logic (uses fallback or excludes as appropriate) |
| TC06 | Edge case: Zero rows in input DataFrames | Output DataFrames are empty, no errors thrown |
| TC07 | Edge case: Duplicate LP_IDs in customer data | Window logic selects only first occurrence per LP_ID |
| TC08 | Error handling: Type mismatch in latitude/longitude columns | PySpark throws appropriate error or handles gracefully |
| TC09 | Schema validation: Output DataFrame columns and types match expected TAMBR_RINGS.txt output | Schema matches expected definition |
| TC10 | Aggregation: 80th percentile ring calculation per branch | Percentile values match expected output for sample data |
| TC11 | Performance: DataFrame caching and partitioning improve runtime | Runtime is less than baseline for large datasets |
| TC12 | Frequency tables: groupBy/count logic matches manual SAS output | Counts per branch_type/metcomm_cde match expected |

# 2. Pytest Script for each test case

```python
import pytest
from pyspark.sql import SparkSession, functions as F, types as T, Window
from math import radians, cos, sin, asin, sqrt

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("tambr_rings_test").getOrCreate()

@pytest.fixture
def sample_branch_df(spark):
    data = [
        ("10001", "R", "Y", 40.7128, -74.0060, "A", "GEN1", None, "Branch1", "NYC", "NY", "10001"),
        ("10002", "U", "Y", 34.0522, -118.2437, "B", "GEN2", None, "Branch2", "LA", "CA", "90001"),
    ]
    schema = T.StructType([
        T.StructField("HGN_BR_ID", T.StringType()),
        T.StructField("BR_TYP", T.StringType()),
        T.StructField("BR_OPN_FLG", T.StringType()),
        T.StructField("branchlat", T.DoubleType()),
        T.StructField("branchlong", T.DoubleType()),
        T.StructField("METRO_COMMUNITY_CDE", T.StringType()),
        T.StructField("GEN_CDE", T.StringType()),
        T.StructField("TBA_CLS_DVSTD_DT", T.StringType()),
        T.StructField("BRICK_AND_MORTOR_NM", T.StringType()),
        T.StructField("CITY", T.StringType()),
        T.StructField("ST", T.StringType()),
        T.StructField("ZIP_CDE", T.StringType()),
    ])
    return spark.createDataFrame(data, schema)

@pytest.fixture
def sample_customer_df(spark):
    data = [
        ("CUST1", "10001", "Y", 12, 40.7130, -74.0059, "R", "10001", "NYC", "NY", "10001"),
        ("CUST2", "10002", "Y", 24, 34.0520, -118.2435, "U", "10002", "LA", "CA", "90001"),
    ]
    schema = T.StructType([
        T.StructField("LP_ID", T.StringType()),
        T.StructField("PRTY_BR", T.StringType()),
        T.StructField("OPN_ACCT_FLG", T.StringType()),
        T.StructField("NBR_OF_MOS_OPN", T.IntegerType()),
        T.StructField("custlat", T.DoubleType()),
        T.StructField("custlong", T.DoubleType()),
        T.StructField("BR_TYP", T.StringType()),
        T.StructField("HGN_BR_ID", T.StringType()),
        T.StructField("CITY", T.StringType()),
        T.StructField("ST", T.StringType()),
        T.StructField("ZIP_CDE", T.StringType()),
    ])
    return spark.createDataFrame(data, schema)

def geodist_udf(lat1, lon1, lat2, lon2):
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    r = 3956
    return c * r

geodist = F.udf(geodist_udf, T.DoubleType())

# TC01: Happy path transformation and join
def test_happy_path_transformation(spark, sample_branch_df, sample_customer_df):
    joined = sample_customer_df.join(
        sample_branch_df,
        sample_customer_df.PRTY_BR == sample_branch_df.HGN_BR_ID,
        "inner"
    )
    assert joined.count() == 2
    assert set(joined.columns) >= {"LP_ID", "PRTY_BR", "branchlat", "branchlong"}

# TC02: UDF geodist calculation
def test_geodist_udf():
    dist = geodist_udf(40.7128, -74.0060, 40.7130, -74.0059)
    assert round(dist, 3) == 0.014  # Approximate value in miles

# TC03: Join logic validation
def test_join_logic(spark, sample_branch_df, sample_customer_df):
    joined = sample_customer_df.join(
        sample_branch_df,
        sample_customer_df.PRTY_BR == sample_branch_df.HGN_BR_ID,
        "inner"
    )
    lp_ids = [row.LP_ID for row in joined.collect()]
    assert "CUST1" in lp_ids and "CUST2" in lp_ids

# TC04: Filter logic validation
def test_filter_logic(spark, sample_branch_df):
    filtered = sample_branch_df.filter((F.col("BR_OPN_FLG") == "Y") & (F.col("branchlat") > 1) & (F.col("branchlong") < -1))
    assert filtered.count() == 2

# TC05: Edge case NULL latitude/longitude
def test_null_latlong_handling(spark, sample_branch_df):
    df = sample_branch_df.withColumn("branchlat", F.lit(None).cast(T.DoubleType()))
    df = df.withColumn("branchlong", F.lit(None).cast(T.DoubleType()))
    # Should not error, but geodist should return None
    result = df.withColumn("dist", geodist(F.col("branchlat"), F.col("branchlong"), F.lit(40.7130), F.lit(-74.0059)))
    assert result.filter(F.col("dist").isNull()).count() == df.count()

# TC06: Zero rows in input DataFrames
def test_zero_rows(spark):
    schema = T.StructType([T.StructField("A", T.StringType())])
    empty_df = spark.createDataFrame([], schema)
    assert empty_df.count() == 0

# TC07: Duplicate LP_IDs in customer data
def test_duplicate_lp_ids(spark, sample_customer_df):
    dup_df = sample_customer_df.union(sample_customer_df)
    window_lp = Window.partitionBy("LP_ID").orderBy("LP_ID")
    deduped = dup_df.withColumn("rn", F.row_number().over(window_lp)).filter(F.col("rn") == 1).drop("rn")
    assert deduped.count() == 2

# TC08: Error handling for type mismatch
def test_type_mismatch(spark, sample_branch_df):
    df = sample_branch_df.withColumn("branchlat", F.lit("not_a_float"))
    with pytest.raises(Exception):
        df.withColumn("dist", geodist(F.col("branchlat"), F.col("branchlong"), F.lit(40.7130), F.lit(-74.0059))).collect()

# TC09: Schema validation
def test_schema_validation(sample_branch_df):
    expected_fields = {"HGN_BR_ID", "BR_TYP", "BR_OPN_FLG", "branchlat", "branchlong", "METRO_COMMUNITY_CDE", "GEN_CDE", "BRICK_AND_MORTOR_NM", "CITY", "ST", "ZIP_CDE"}
    actual_fields = set([f.name for f in sample_branch_df.schema.fields])
    assert expected_fields.issubset(actual_fields)

# TC10: Aggregation 80th percentile ring calculation
def test_percentile_aggregation(spark, sample_branch_df, sample_customer_df):
    joined = sample_customer_df.join(
        sample_branch_df,
        sample_customer_df.PRTY_BR == sample_branch_df.HGN_BR_ID,
        "inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist(
            F.col("branchlat"), F.col("branchlong"),
            F.col("custlat"), F.col("custlong")
        )
    )
    percentile_window = Window.partitionBy("PRTY_BR")
    result = joined.withColumn(
        "prtybr80",
        F.expr("percentile_approx(dist_to_prty_br, 0.8) over (partition by PRTY_BR)")
    )
    assert "prtybr80" in result.columns

# TC11: Performance improvement (mocked)
def test_performance_improvement(spark, sample_branch_df):
    import time
    df = sample_branch_df
    start = time.time()
    df.cache()
    df.count()
    end = time.time()
    assert (end - start) < 2  # Should be fast for small data

# TC12: Frequency tables groupBy/count
def test_frequency_tables(spark, sample_branch_df):
    freq = sample_branch_df.groupBy("BR_TYP", "METRO_COMMUNITY_CDE").count()
    assert freq.count() > 0

```

# 3. API Cost Consumed

apiCost: 0.0023 USD

```
=============================================