# PySpark Conversion of Talend Job: data_transform_pipeline.java
# This script is an automated translation of the provided Talend job into PySpark, preserving the ETL logic and structure.
# Please provide your GitHub repo name ({{repo}}) and API token ({{token}}) to upload this file using the DI_Github_File_Writer tool.

import sys
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StringType

# Initialize Spark session
spark = SparkSession.builder \
    .appName("AI_POC_Postgre_PySpark") \
    .getOrCreate()

# Database connection parameters (replace with your actual values or use a config)
db_host = "<HOST>"         # e.g., "localhost"
db_port = "<PORT>"         # e.g., "5432"
db_name = "<DATABASE>"     # e.g., "mydb"
db_user = "<USER>"         # e.g., "myuser"
db_password = "<PASSWORD>" # e.g., "mypassword"
db_url = f"jdbc:postgresql://{db_host}:{db_port}/{db_name}"

# File output path (replace with your actual value)
output_file_path = "<FILEPATH>"  # e.g., "/tmp/output.csv"

# =========================
# tDBInput_1 mapped to df_employee
# =========================
df_employee = spark.read \
    .format("jdbc") \
    .option("url", db_url) \
    .option("dbtable", "(select id, name, emp as employee, manager_id from employee) as emp") \
    .option("user", db_user) \
    .option("password", db_password) \
    .option("driver", "org.postgresql.Driver") \
    .load()

# =========================
# tAggregateRow_1 mapped to agg
# Group by manager_id, aggregate names as comma-separated string
# =========================
agg = df_employee.groupBy("manager_id") \
    .agg(
        F.concat_ws(",", F.collect_list("name")).alias("name")
    )
# tAggregateRow_1 mapped to agg

# =========================
# tNormalize_1 mapped to normalized
# Split the 'name' column by ',' and explode to get one row per name
# =========================
normalized = agg.withColumn(
    "name", F.explode(F.split(F.col("name"), ","))
)
normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
# tNormalize_1 mapped to normalized

# =========================
# tDBInput_2 mapped to df_salary
# =========================
df_salary = spark.read \
    .format("jdbc") \
    .option("url", db_url) \
    .option("dbtable", "(select id, salary from employee) as sal") \
    .option("user", db_user) \
    .option("password", db_password) \
    .option("driver", "org.postgresql.Driver") \
    .load()

# =========================
# tHashOutput_1/tHashInput_1 mapped to normalized (already in-memory)
# =========================

# =========================
# tMap_1 mapped to joined
# Join normalized (name, manager_id_norm) with df_salary on id
# Note: In Talend, the join is on id, but normalized does not have id.
# We'll assume the mapping is: name (from normalized) matches name in df_employee, so we need to get id.
# We'll reconstruct this by joining normalized back to df_employee to get id and employee, then join to df_salary.
# =========================

# Join normalized with df_employee to get id, employee, manager_id
normalized_with_id = normalized.join(
    df_employee.select("id", "name", "employee", "manager_id"),
    (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
    how="left"
)

# Join with df_salary to get salary
joined = normalized_with_id.join(
    df_salary,
    normalized_with_id["id"] == df_salary["id"],
    how="left"
).select(
    normalized_with_id["id"],
    normalized_with_id["name"],
    normalized_with_id["employee"],
    normalized_with_id["manager_id"],
    df_salary["salary"]
)
# tMap_1 mapped to joined

# =========================
# tLogRow_1 mapped to show output
# =========================
joined.show(truncate=False)

# =========================
# tFileOutputDelimited_1 mapped to CSV output
# =========================
joined.write \
    .option("header", True) \
    .option("delimiter", ";") \
    .mode("overwrite") \
    .csv(output_file_path)

# =========================
# tDBClose_1 handled automatically by Spark
# =========================

# Stop Spark session
spark.stop()

# =========================
# END OF SCRIPT
# =========================

# -- Variable/Component Mapping Comments --
# tDBInput_1      -> df_employee
# tAggregateRow_1 -> agg
# tNormalize_1    -> normalized
# tDBInput_2      -> df_salary
# tMap_1          -> joined
# tLogRow_1       -> joined.show()
# tFileOutputDelimited_1 -> joined.write.csv()
# tDBClose_1      -> spark.stop()

# -- Notes --
# - Replace <HOST>, <PORT>, <DATABASE>, <USER>, <PASSWORD>, <FILEPATH> with actual values or use environment/config.
# - The join logic is based on the assumption that 'name' is unique per id; adjust if needed.
# - All transformations are performed using PySpark DataFrame APIs for scalability and performance.
# - The output CSV will have columns: id, name, employee, manager_id, salary, matching the Talend job's output.
# - This script is ready for deployment or further customization.

# API cost for this call: 1 file read, 1 directory read.