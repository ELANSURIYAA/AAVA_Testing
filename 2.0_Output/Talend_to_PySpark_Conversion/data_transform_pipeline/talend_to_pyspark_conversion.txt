# ai_poc_postgre_etl.py
"""
PySpark ETL Pipeline - Talend to PySpark Conversion

This script performs the following steps:
1. Connects to a PostgreSQL database using parameters from context variables.
2. Reads the employee table (id, name, emp, manager_id).
3. Aggregates by manager_id, collecting a comma-separated list of names for each manager_id.
4. Normalizes the name list into individual rows.
5. Reads the employee table again (id, salary) as a lookup.
6. Joins the normalized data with the salary lookup on id.
7. Outputs the final DataFrame to a delimited file (semicolon separator), with columns: Id, Name, Employee, Manager_id, Salary.

Talend Components Mapping:
- tDBConnection_1: PostgreSQL connection
- tDBInput_1: Initial employee table read
- tAggregateRow_1: Aggregation by manager_id, collect names
- tNormalize_1: Split names into individual rows
- tDBInput_2/tAdvancedHash_row5: Salary lookup
- tMap_1: Join normalized data with salary
- tFileOutputDelimited_1: Output to file

Assumptions:
- The employee table contains columns: id, name, emp, manager_id, salary.
- All context variables are available as environment variables or can be set in the script.
- Output file path is provided in context['Filepath'].

Author: Data Engineer (Talend to PySpark Migration)
Date: 2025-04-07
"""

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, collect_list, concat_ws, split, explode
from pyspark.sql.types import StringType

# ---------------------------
# Context Variables (Replace with your values or use environment variables)
# ---------------------------
context = {
    'Host': os.getenv('PG_HOST', 'localhost'),
    'Port': os.getenv('PG_PORT', '5432'),
    'Database': os.getenv('PG_DATABASE', 'your_database'),
    'Useranme': os.getenv('PG_USERNAME', 'your_username'),
    'Password': os.getenv('PG_PASSWORD', 'your_password'),
    'Filepath': os.getenv('OUTPUT_FILEPATH', '/tmp/employee_output.csv')
}

# ---------------------------
# Spark Session Initialization
# ---------------------------
spark = SparkSession.builder \
    .appName("AI_POC_Postgre_ETL") \
    .getOrCreate()

# ---------------------------
# tDBConnection_1: PostgreSQL Connection Properties
# ---------------------------
pg_url = f"jdbc:postgresql://{context['Host']}:{context['Port']}/{context['Database']}"
pg_properties = {
    "user": context['Useranme'],
    "password": context['Password'],
    "driver": "org.postgresql.Driver"
}

# ---------------------------
# tDBInput_1: Read employee table (id, name, emp, manager_id)
# ---------------------------
employee_df = spark.read.jdbc(
    url=pg_url,
    table="employee",
    properties=pg_properties
).select(
    col("id").alias("Id"),
    col("name").alias("Name"),
    col("emp").alias("Employee"),
    col("manager_id").alias("Manager_id")
)

# ---------------------------
# tAggregateRow_1: Aggregate by manager_id, collect names
# ---------------------------
agg_df = employee_df.groupBy("Manager_id").agg(
    concat_ws(",", collect_list("Name")).alias("Name_list")
)
# Note: tAggregateRow_1 mapped to agg_df

# ---------------------------
# tNormalize_1: Split Name_list into individual rows
# ---------------------------
normalized_df = agg_df.withColumn("Name", explode(split(col("Name_list"), ","))) \
    .select("Manager_id", "Name")
# Note: tNormalize_1 mapped to normalized_df

# ---------------------------
# Join normalized names back to employee to get Id and Employee columns
# ---------------------------
# This assumes that Name is unique per employee (as in Talend mapping)
normalized_full_df = normalized_df.join(
    employee_df,
    (employee_df["Name"] == normalized_df["Name"]) & (employee_df["Manager_id"] == normalized_df["Manager_id"]),
    how="inner"
).select(
    employee_df["Id"],
    employee_df["Name"],
    employee_df["Employee"],
    employee_df["Manager_id"]
)
# Note: tNormalize_1 output mapped to normalized_full_df

# ---------------------------
# tDBInput_2/tAdvancedHash_row5: Salary lookup
# ---------------------------
salary_df = spark.read.jdbc(
    url=pg_url,
    table="employee",
    properties=pg_properties
).select(
    col("id").alias("Id"),
    col("salary").alias("Salary")
)
# Note: tDBInput_2/tAdvancedHash_row5 mapped to salary_df

# ---------------------------
# tMap_1: Join normalized data with salary on Id
# ---------------------------
final_df = normalized_full_df.join(
    salary_df,
    on="Id",
    how="left"
).select(
    "Id", "Name", "Employee", "Manager_id", "Salary"
)
# Note: tMap_1 mapped to final_df

# ---------------------------
# tFileOutputDelimited_1: Output to delimited file (semicolon separator)
# ---------------------------
final_df.write.csv(
    path=context['Filepath'],
    sep=";",
    header=True,
    mode="overwrite"
)
# Note: tFileOutputDelimited_1 mapped to final_df.write.csv

# ---------------------------
# End of ETL Pipeline
# ---------------------------

print(f"ETL completed. Output written to: {context['Filepath']}")

# ---------------------------
# Prompt for GitHub Repo and Token for Upload
# ---------------------------
print("\nTo upload this file to your GitHub repository, please provide:")
print("1. Your GitHub repository name (e.g., username/repo)")
print("2. Your GitHub API token (with repo write access)")

# Example usage with DI_Github_File_Writer tool:
# DI_Github_File_Writer(repo_name, api_token, file_path='ai_poc_postgre_etl.py')

# API cost for this call: 1 file read, 1 directory listing, 1 code generation (estimate: low).