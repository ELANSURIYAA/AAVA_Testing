# PySpark script converted from Talend Java job: AI_POC_Postgre
# This script performs the following workflow:
# 1. Connects to PostgreSQL and reads employee data (id, name, emp, manager_id)
# 2. Aggregates names by manager_id
# 3. Normalizes the name list
# 4. Performs a lookup to add salary from a second DB input
# 5. Joins and maps outputs
# 6. Writes the final result to a delimited file and logs output
# All component-to-variable mappings and complex logic are commented.
# Optimized for Spark performance and PEP8-compliant.

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StringType, StructType, StructField

# =========================
# CONFIGURATION
# =========================
# Map Talend context variables to Python variables
DB_HOST = "<your_host>"         # context.Host
DB_PORT = "<your_port>"         # context.Port
DB_NAME = "<your_db>"           # context.Database
DB_USER = "<your_user>"         # context.Useranme
DB_PASSWORD = "<your_password>" # context.Password
OUTPUT_PATH = "<your_output_file>"  # context.Filepath

# =========================
# SPARK SESSION
# =========================
spark = SparkSession.builder.appName("AI_POC_Postgre_ETL").getOrCreate()

# =========================
# 1. READ EMPLOYEE DATA (tDBInput_1)
# =========================
# Talend: select id, name, emp, manager_id from employee
employee_schema = StructType([
    StructField("Id", StringType(), True),
    StructField("Name", StringType(), True),
    StructField("Employee", StringType(), True),
    StructField("Manager_id", StringType(), True)
])

employee_df = spark.read \
    .format("jdbc") \
    .option("url", f"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}") \
    .option("dbtable", "employee") \
    .option("user", DB_USER) \
    .option("password", DB_PASSWORD) \
    .option("driver", "org.postgresql.Driver") \
    .load() \
    .select(
        F.col("id").alias("Id"),
        F.col("name").alias("Name"),
        F.col("emp").alias("Employee"),
        F.col("manager_id").alias("Manager_id")
    )

# =========================
# 2. AGGREGATE NAMES BY MANAGER_ID (tAggregateRow_1)
# =========================
# tAggregateRow_1 mapped to agg_df
agg_df = employee_df.groupBy("Manager_id") \
    .agg(
        F.collect_list("Name").alias("Name_list")
    )

# Convert list of names to comma-separated string
agg_df = agg_df.withColumn("Name", F.concat_ws(",", "Name_list"))

# =========================
# 3. NORMALIZE NAME LIST (tNormalize_1)
# =========================
# tNormalize_1 mapped to normalized_df
normalized_df = agg_df.select(
    "Manager_id",
    F.explode(F.split(F.col("Name"), ",")).alias("Name")
)

# Join back to employee_df to get other columns
normalized_df = normalized_df.join(
    employee_df.select("Id", "Name", "Employee", "Manager_id"),
    on=["Manager_id", "Name"],
    how="left"
)

# =========================
# 4. LOOKUP SALARY FROM SECOND DB INPUT (tDBInput_2, tAdvancedHash_row5)
# =========================
# tDBInput_2 mapped to salary_df
salary_schema = StructType([
    StructField("Id", StringType(), True),
    StructField("Salary", StringType(), True)
])

salary_df = spark.read \
    .format("jdbc") \
    .option("url", f"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}") \
    .option("dbtable", "employee") \
    .option("user", DB_USER) \
    .option("password", DB_PASSWORD) \
    .option("driver", "org.postgresql.Driver") \
    .load() \
    .select(
        F.col("id").alias("Id"),
        F.col("salary").alias("Salary")
    )

# =========================
# 5. JOIN AND MAP OUTPUTS (tMap_1)
# =========================
# tMap_1 mapped to final_df
final_df = normalized_df.join(
    salary_df,
    on="Id",
    how="left"
)

# =========================
# 6. WRITE FINAL RESULT TO DELIMITED FILE (tFileOutputDelimited_1)
# =========================
# tFileOutputDelimited_1 mapped to final_df.write
final_df = final_df.select(
    "Id", "Name", "Employee", "Manager_id", "Salary"
)

final_df.write \
    .option("header", True) \
    .option("delimiter", ";") \
    .mode("overwrite") \
    .csv(OUTPUT_PATH)

# =========================
# 7. LOG OUTPUT (tLogRow_1)
# =========================
# tLogRow_1 mapped to log_row function
def log_row(row):
    print(f"{row['Id']}|{row['Name']}|{row['Employee']}|{row['Manager_id']}|{row['Salary']}")

final_df.foreach(log_row)

# =========================
# CLEANUP
# =========================
spark.stop()

# =========================
# NOTES
# =========================
# - Replace <your_host>, <your_port>, <your_db>, <your_user>, <your_password>, <your_output_file> with actual values or use argparse/env/context.
# - All Talend component mappings are commented above each block.
# - Aggregation uses collect_list and concat_ws for names.
# - Normalization uses explode and split for name list.
# - Salary lookup uses DataFrame join.
# - Output columns match Talend job: Id, Name, Employee, Manager_id, Salary.
# - Logging mimics Talend tLogRow_1 output.
# - The script is modular, readable, and production-ready.

# API cost consumed for this call: 1 file list + 1 file read = 2 units.