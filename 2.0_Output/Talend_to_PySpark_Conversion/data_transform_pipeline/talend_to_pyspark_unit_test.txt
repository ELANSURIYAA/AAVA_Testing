Test Case List:

| Test Case ID | Test Case Description                                                                 | Expected Outcome                                                                                   |
|--------------|--------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|
| TC01         | Happy path: Aggregation and normalization of names by manager_id                      | Correct grouping and splitting of names; each manager_id has correct normalized rows               |
| TC02         | Happy path: Join normalized data with salary lookup                                   | Each employee's salary is correctly joined and present in output                                   |
| TC03         | Edge case: Null values in name, emp, manager_id, or salary                           | Nulls are handled gracefully; rows with nulls do not break transformations or joins                |
| TC04         | Edge case: Empty employee DataFrame                                                   | Pipeline runs without error; output file is empty with header only                                 |
| TC05         | Edge case: Multiple employees with same name under different managers                 | Names are correctly normalized and joined; no cross-manager contamination                          |
| TC06         | Boundary: Large number of employees under one manager                                | Aggregation and normalization scale; output row count matches input                                |
| TC07         | Error handling: Schema mismatch in salary lookup                                     | Exception is raised or handled if salary lookup schema is incorrect                                |
| TC08         | Error handling: Invalid data types in input                                          | Exception is raised or handled if input types are not as expected                                  |
| TC09         | Output validation: Output file is written with correct delimiter and header           | Output file exists, uses semicolon delimiter, and has correct header                               |
| TC10         | Transformation: Validate that normalization does not duplicate or drop employees      | No duplicate or missing employees in output                                                        |

Pytest Script (ai_poc_postgre_etl_test.py):

```python
import pytest
import os
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.functions import col, collect_list, concat_ws, split, explode

@pytest.fixture(scope="function")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("pytest_etl").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def employee_data():
    # Sample employee data for tests
    return [
        Row(id=1, name="Alice", emp="Engineer", manager_id=10, salary=1000),
        Row(id=2, name="Bob", emp="Analyst", manager_id=10, salary=1200),
        Row(id=3, name="Charlie", emp="Engineer", manager_id=20, salary=1100),
        Row(id=4, name="David", emp="Manager", manager_id=None, salary=1500)
    ]

@pytest.fixture
def employee_df(spark, employee_data):
    return spark.createDataFrame(employee_data)

@pytest.fixture
def salary_df(spark, employee_data):
    return spark.createDataFrame([Row(id=row.id, salary=row.salary) for row in employee_data])

def run_etl(employee_df, salary_df, output_path):
    # Step 1: Select relevant columns
    employee_df_sel = employee_df.select(
        col("id").alias("Id"),
        col("name").alias("Name"),
        col("emp").alias("Employee"),
        col("manager_id").alias("Manager_id")
    )
    # Step 2: Aggregate by manager_id, collect names
    agg_df = employee_df_sel.groupBy("Manager_id").agg(
        concat_ws(",", collect_list("Name")).alias("Name_list")
    )
    # Step 3: Normalize names
    normalized_df = agg_df.withColumn("Name", explode(split(col("Name_list"), ","))).select("Manager_id", "Name")
    # Step 4: Join normalized names back to employee to get Id and Employee columns
    normalized_full_df = normalized_df.join(
        employee_df_sel,
        (employee_df_sel["Name"] == normalized_df["Name"]) & (employee_df_sel["Manager_id"] == normalized_df["Manager_id"]),
        how="inner"
    ).select(
        employee_df_sel["Id"],
        employee_df_sel["Name"],
        employee_df_sel["Employee"],
        employee_df_sel["Manager_id"]
    )
    # Step 5: Join with salary
    final_df = normalized_full_df.join(
        salary_df.select(col("id").alias("Id"), col("salary").alias("Salary")),
        on="Id",
        how="left"
    ).select(
        "Id", "Name", "Employee", "Manager_id", "Salary"
    )
    # Step 6: Write to output
    final_df.write.csv(
        path=output_path,
        sep=";",
        header=True,
        mode="overwrite"
    )
    return final_df

def test_TC01_happy_path_aggregation_normalization(spark, employee_df):
    # Purpose: Test correct aggregation and normalization of names by manager_id
    employee_df_sel = employee_df.select(
        col("id").alias("Id"),
        col("name").alias("Name"),
        col("emp").alias("Employee"),
        col("manager_id").alias("Manager_id")
    )
    agg_df = employee_df_sel.groupBy("Manager_id").agg(
        concat_ws(",", collect_list("Name")).alias("Name_list")
    )
    result = agg_df.collect()
    assert any(row["Manager_id"] == 10 and set(row["Name_list"].split(",")) == {"Alice", "Bob"} for row in result)
    assert any(row["Manager_id"] == 20 and row["Name_list"] == "Charlie" for row in result)
    assert any(row["Manager_id"] is None and row["Name_list"] == "David" for row in result)

def test_TC02_happy_path_salary_join(spark, employee_df, salary_df):
    # Purpose: Test salary join correctness
    employee_df_sel = employee_df.select(
        col("id").alias("Id"),
        col("name").alias("Name"),
        col("emp").alias("Employee"),
        col("manager_id").alias("Manager_id")
    )
    normalized_full_df = employee_df_sel
    final_df = normalized_full_df.join(
        salary_df.select(col("id").alias("Id"), col("salary").alias("Salary")),
        on="Id",
        how="left"
    )
    result = final_df.collect()
    for row in result:
        assert row["Salary"] in [1000, 1200, 1100, 1500]

def test_TC03_null_values_handling(spark):
    # Purpose: Ensure nulls do not break pipeline
    data = [
        Row(id=1, name=None, emp="Engineer", manager_id=10, salary=1000),
        Row(id=2, name="Bob", emp=None, manager_id=None, salary=None)
    ]
    employee_df = spark.createDataFrame(data)
    salary_df = spark.createDataFrame([Row(id=1, salary=1000), Row(id=2, salary=None)])
    try:
        run_etl(employee_df, salary_df, "/tmp/test_nulls.csv")
    except Exception:
        pytest.fail("ETL failed on null values")

def test_TC04_empty_employee_df(spark):
    # Purpose: Ensure pipeline works with empty DataFrame
    employee_df = spark.createDataFrame([], "id INT, name STRING, emp STRING, manager_id INT, salary INT")
    salary_df = spark.createDataFrame([], "id INT, salary INT")
    output_path = "/tmp/test_empty.csv"
    final_df = run_etl(employee_df, salary_df, output_path)
    assert final_df.count() == 0
    assert os.path.exists(output_path)

def test_TC05_duplicate_names_different_managers(spark):
    # Purpose: Ensure normalization does not mix employees with same name under different managers
    data = [
        Row(id=1, name="Alex", emp="Engineer", manager_id=10, salary=1000),
        Row(id=2, name="Alex", emp="Analyst", manager_id=20, salary=1200)
    ]
    employee_df = spark.createDataFrame(data)
    salary_df = spark.createDataFrame([Row(id=1, salary=1000), Row(id=2, salary=1200)])
    final_df = run_etl(employee_df, salary_df, "/tmp/test_duplicate_names.csv")
    result = final_df.collect()
    assert len(result) == 2
    assert set(row["Manager_id"] for row in result) == {10, 20}

def test_TC06_large_group_aggregation(spark):
    # Purpose: Test aggregation and normalization with large group
    data = [Row(id=i, name=f"Emp{i}", emp="Staff", manager_id=99, salary=1000+i) for i in range(100)]
    employee_df = spark.createDataFrame(data)
    salary_df = spark.createDataFrame([Row(id=i, salary=1000+i) for i in range(100)])
    final_df = run_etl(employee_df, salary_df, "/tmp/test_large_group.csv")
    assert final_df.count() == 100

def test_TC07_salary_schema_mismatch(spark, employee_df):
    # Purpose: Test error handling for schema mismatch
    salary_df = spark.createDataFrame([Row(emp_id=1, amount=1000)])  # Wrong schema
    with pytest.raises(Exception):
        run_etl(employee_df, salary_df, "/tmp/test_schema_mismatch.csv")

def test_TC08_invalid_data_types(spark):
    # Purpose: Test error handling for invalid data types
    data = [Row(id="one", name=123, emp=True, manager_id="mgr", salary="high")]
    employee_df = spark.createDataFrame(data)
    salary_df = spark.createDataFrame([Row(id="one", salary="high")])
    with pytest.raises(Exception):
        run_etl(employee_df, salary_df, "/tmp/test_invalid_types.csv")

def test_TC09_output_file_validation(spark, employee_df, salary_df):
    # Purpose: Test output file correctness
    output_path = "/tmp/test_output_file.csv"
    run_etl(employee_df, salary_df, output_path)
    # Check file exists and delimiter
    files = [f for f in os.listdir(output_path) if f.endswith(".csv")]
    assert files
    # Optionally, read and check header
    import csv
    with open(os.path.join(output_path, files[0]), "r") as f:
        reader = csv.reader(f, delimiter=";")
        header = next(reader)
        assert header == ["Id", "Name", "Employee", "Manager_id", "Salary"]

def test_TC10_no_duplicate_or_missing_employees(spark, employee_df, salary_df):
    # Purpose: Ensure normalization does not duplicate or drop employees
    final_df = run_etl(employee_df, salary_df, "/tmp/test_no_duplicates.csv")
    ids = [row["Id"] for row in final_df.collect()]
    assert sorted(ids) == [1, 2, 3, 4]

# End of test script

# Instructions for uploading to GitHub:
# Please provide your GitHub repository name (e.g., username/repo), branch name, and API token.
# Example usage with DI_Github_File_Writer tool:
# DI_Github_File_Writer(repo_name, api_token, file_path='ai_poc_postgre_etl_test.py')

# API cost for this call: 1 directory listing, 1 file read, 1 code generation (estimate: low).
```