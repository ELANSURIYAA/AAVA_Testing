Test Case List:

Test Case ID: TC01
Description: Happy path - Aggregation and normalization of employee names by manager_id.
Expected Outcome: The output DataFrame should have one row per employee name per manager, with correct mapping and salary enrichment.

Test Case ID: TC02
Description: Edge case - Employee table contains null values in 'name', 'manager_id', or 'employee' columns.
Expected Outcome: Nulls are handled gracefully; rows with null names do not break aggregation or normalization.

Test Case ID: TC03
Description: Edge case - Salary table contains null values in 'salary' column.
Expected Outcome: Output DataFrame contains null salary values where appropriate, without errors.

Test Case ID: TC04
Description: Edge case - Empty employee table.
Expected Outcome: Output DataFrame is empty.

Test Case ID: TC05
Description: Edge case - Empty salary table.
Expected Outcome: Output DataFrame contains employees with null salary.

Test Case ID: TC06
Description: Error handling - Schema mismatch (e.g., salary table missing 'id' column).
Expected Outcome: The pipeline raises an appropriate exception.

Test Case ID: TC07
Description: Data correctness - Employee with multiple managers (boundary case).
Expected Outcome: Each (employee, manager) pair is represented correctly in the output.

Test Case ID: TC08
Description: Data correctness - Duplicate employee names under different managers.
Expected Outcome: Each name is associated with the correct manager and salary.

Test Case ID: TC09
Description: Data correctness - Large dataset (performance/sanity).
Expected Outcome: The pipeline completes successfully and produces the correct number of rows.

Test Case ID: TC10
Description: Input validation - Non-string types in 'name' or 'employee' columns.
Expected Outcome: The pipeline either coerces types or raises an informative error.

---

Pytest Script (test_data_transform_pipeline.py):

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

@pytest.fixture(scope="function")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("pytest-pyspark").getOrCreate()
    yield spark
    spark.stop()

def create_employee_df(spark, data):
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("name", StringType(), True),
        StructField("employee", StringType(), True),
        StructField("manager_id", IntegerType(), True)
    ])
    return spark.createDataFrame(data, schema=schema)

def create_salary_df(spark, data):
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("salary", FloatType(), True)
    ])
    return spark.createDataFrame(data, schema=schema)

# TC01: Happy path
def test_happy_path(spark):
    employee_data = [
        (1, "Alice", "E1", 10),
        (2, "Bob", "E2", 10),
        (3, "Charlie", "E3", 20)
    ]
    salary_data = [
        (1, 1000.0),
        (2, 1200.0),
        (3, 1100.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.orderBy("id").collect()
    assert len(result) == 3
    assert result[0]["name"] == "Alice"
    assert result[1]["name"] == "Bob"
    assert result[2]["name"] == "Charlie"
    assert result[0]["salary"] == 1000.0

# TC02: Nulls in employee table
def test_nulls_in_employee(spark):
    employee_data = [
        (1, None, "E1", 10),
        (2, "Bob", None, 10),
        (3, "Charlie", "E3", None)
    ]
    salary_data = [
        (1, 1000.0),
        (2, 1200.0),
        (3, 1100.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.collect()
    assert any(row["name"] is None for row in result)

# TC03: Nulls in salary table
def test_nulls_in_salary(spark):
    employee_data = [
        (1, "Alice", "E1", 10),
        (2, "Bob", "E2", 10)
    ]
    salary_data = [
        (1, None),
        (2, 1200.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.collect()
    assert any(row["salary"] is None for row in result)

# TC04: Empty employee table
def test_empty_employee_table(spark):
    employee_data = []
    salary_data = [
        (1, 1000.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    assert joined.count() == 0

# TC05: Empty salary table
def test_empty_salary_table(spark):
    employee_data = [
        (1, "Alice", "E1", 10)
    ]
    salary_data = []
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.collect()
    assert result[0]["salary"] is None

# TC06: Schema mismatch in salary table
def test_schema_mismatch_salary(spark):
    employee_data = [
        (1, "Alice", "E1", 10)
    ]
    schema = StructType([
        StructField("emp_id", IntegerType(), True),
        StructField("salary", FloatType(), True)
    ])
    salary_data = [(1, 1000.0)]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = spark.createDataFrame(salary_data, schema=schema)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    with pytest.raises(Exception):
        # This should fail because 'id' column is missing in salary
        normalized_with_id.join(
            df_salary,
            normalized_with_id["id"] == df_salary["id"],
            how="left"
        ).select(
            normalized_with_id["id"],
            normalized_with_id["name"],
            normalized_with_id["employee"],
            normalized_with_id["manager_id"],
            df_salary["salary"]
        ).collect()

# TC07: Employee with multiple managers
def test_employee_multiple_managers(spark):
    employee_data = [
        (1, "Alice", "E1", 10),
        (2, "Alice", "E1", 20)
    ]
    salary_data = [
        (1, 1000.0),
        (2, 1100.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.orderBy("id").collect()
    assert len(result) == 2
    assert result[0]["manager_id"] != result[1]["manager_id"]

# TC08: Duplicate employee names under different managers
def test_duplicate_names_different_managers(spark):
    employee_data = [
        (1, "Bob", "E1", 10),
        (2, "Bob", "E2", 20)
    ]
    salary_data = [
        (1, 1000.0),
        (2, 1100.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.orderBy("id").collect()
    assert len(result) == 2
    assert result[0]["manager_id"] != result[1]["manager_id"]

# TC09: Large dataset
def test_large_dataset(spark):
    employee_data = [(i, f"Emp{i}", f"E{i}", i % 5) for i in range(1000)]
    salary_data = [(i, float(1000 + i)) for i in range(1000)]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    assert joined.count() == 1000

# TC10: Non-string types in 'name' or 'employee'
def test_non_string_types_in_name(spark):
    employee_data = [
        (1, 123, 456, 10)
    ]
    salary_data = [
        (1, 1000.0)
    ]
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("name", IntegerType(), True),
        StructField("employee", IntegerType(), True),
        StructField("manager_id", IntegerType(), True)
    ])
    df_employee = spark.createDataFrame(employee_data, schema=schema)
    df_salary = create_salary_df(spark, salary_data)

    # Should either coerce to string or raise error during split/explode
    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    with pytest.raises(Exception):
        agg.withColumn("name", F.explode(F.split(F.col("name"), ","))).collect()
```

---
**API cost for this call:** 1 directory read, 1 file read.

---

To upload this test script to your GitHub repository, please provide:
- Your GitHub repo name ({{repo}})
- Your branch name
- Your GitHub API token ({{token}})