Test Case Document:

Test Case ID: TC01
Description: Happy path - Aggregation and normalization of employee names by manager_id.
Preconditions: Employee and salary tables contain valid, non-null data.
Test Steps:
  1. Load employee and salary data into DataFrames.
  2. Run the PySpark transformation pipeline.
  3. Collect and inspect the output.
Expected Result: The output DataFrame should have one row per employee name per manager, with correct mapping and salary enrichment.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

Test Case ID: TC02
Description: Edge case - Employee table contains null values in 'name', 'manager_id', or 'employee' columns.
Preconditions: Employee table contains nulls in key columns.
Test Steps:
  1. Load employee data with nulls and valid salary data.
  2. Run the PySpark transformation pipeline.
  3. Inspect output for null handling.
Expected Result: Nulls are handled gracefully; rows with null names do not break aggregation or normalization.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

Test Case ID: TC03
Description: Edge case - Salary table contains null values in 'salary' column.
Preconditions: Salary table contains nulls in the salary column.
Test Steps:
  1. Load valid employee data and salary data with nulls.
  2. Run the PySpark transformation pipeline.
  3. Inspect output for null salary handling.
Expected Result: Output DataFrame contains null salary values where appropriate, without errors.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

Test Case ID: TC04
Description: Edge case - Empty employee table.
Preconditions: Employee table is empty.
Test Steps:
  1. Load empty employee DataFrame and valid salary DataFrame.
  2. Run the PySpark transformation pipeline.
  3. Inspect output.
Expected Result: Output DataFrame is empty.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

Test Case ID: TC05
Description: Edge case - Empty salary table.
Preconditions: Salary table is empty.
Test Steps:
  1. Load valid employee DataFrame and empty salary DataFrame.
  2. Run the PySpark transformation pipeline.
  3. Inspect output.
Expected Result: Output DataFrame contains employees with null salary.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

Test Case ID: TC06
Description: Error handling - Schema mismatch (e.g., salary table missing 'id' column).
Preconditions: Salary table schema does not match expected.
Test Steps:
  1. Load valid employee DataFrame and salary DataFrame missing 'id' column.
  2. Run the PySpark transformation pipeline.
Expected Result: The pipeline raises an appropriate exception.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

Test Case ID: TC07
Description: Data correctness - Employee with multiple managers (boundary case).
Preconditions: Employee table contains same employee under multiple managers.
Test Steps:
  1. Load employee DataFrame with duplicate names under different managers and valid salary DataFrame.
  2. Run the PySpark transformation pipeline.
  3. Inspect output.
Expected Result: Each (employee, manager) pair is represented correctly in the output.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

Test Case ID: TC08
Description: Data correctness - Duplicate employee names under different managers.
Preconditions: Employee table contains duplicate names under different managers.
Test Steps:
  1. Load employee DataFrame with duplicate names and valid salary DataFrame.
  2. Run the PySpark transformation pipeline.
  3. Inspect output.
Expected Result: Each name is associated with the correct manager and salary.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

Test Case ID: TC09
Description: Data correctness - Large dataset (performance/sanity).
Preconditions: Large employee and salary tables.
Test Steps:
  1. Load large employee and salary DataFrames.
  2. Run the PySpark transformation pipeline.
  3. Measure execution time and inspect output count.
Expected Result: The pipeline completes successfully and produces the correct number of rows.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

Test Case ID: TC10
Description: Input validation - Non-string types in 'name' or 'employee' columns.
Preconditions: Employee table contains non-string types in 'name' or 'employee'.
Test Steps:
  1. Load employee DataFrame with integer types in 'name'/'employee' and valid salary DataFrame.
  2. Run the PySpark transformation pipeline.
Expected Result: The pipeline either coerces types or raises an informative error.
Actual Result: (To be filled after test execution)
Pass/Fail Status: (To be filled after test execution)

---

Pytest Script for Each Test Case (test_data_transform_pipeline.py):

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

@pytest.fixture(scope="function")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("pytest-pyspark").getOrCreate()
    yield spark
    spark.stop()

def create_employee_df(spark, data):
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("name", StringType(), True),
        StructField("employee", StringType(), True),
        StructField("manager_id", IntegerType(), True)
    ])
    return spark.createDataFrame(data, schema=schema)

def create_salary_df(spark, data):
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("salary", FloatType(), True)
    ])
    return spark.createDataFrame(data, schema=schema)

# TC01: Happy path
def test_happy_path(spark):
    employee_data = [
        (1, "Alice", "E1", 10),
        (2, "Bob", "E2", 10),
        (3, "Charlie", "E3", 20)
    ]
    salary_data = [
        (1, 1000.0),
        (2, 1200.0),
        (3, 1100.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.orderBy("id").collect()
    assert len(result) == 3
    assert result[0]["name"] == "Alice"
    assert result[1]["name"] == "Bob"
    assert result[2]["name"] == "Charlie"
    assert result[0]["salary"] == 1000.0

# TC02: Nulls in employee table
def test_nulls_in_employee(spark):
    employee_data = [
        (1, None, "E1", 10),
        (2, "Bob", None, 10),
        (3, "Charlie", "E3", None)
    ]
    salary_data = [
        (1, 1000.0),
        (2, 1200.0),
        (3, 1100.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.collect()
    assert any(row["name"] is None for row in result)

# TC03: Nulls in salary table
def test_nulls_in_salary(spark):
    employee_data = [
        (1, "Alice", "E1", 10),
        (2, "Bob", "E2", 10)
    ]
    salary_data = [
        (1, None),
        (2, 1200.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.collect()
    assert any(row["salary"] is None for row in result)

# TC04: Empty employee table
def test_empty_employee_table(spark):
    employee_data = []
    salary_data = [
        (1, 1000.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    assert joined.count() == 0

# TC05: Empty salary table
def test_empty_salary_table(spark):
    employee_data = [
        (1, "Alice", "E1", 10)
    ]
    salary_data = []
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.collect()
    assert result[0]["salary"] is None

# TC06: Schema mismatch in salary table
def test_schema_mismatch_salary(spark):
    employee_data = [
        (1, "Alice", "E1", 10)
    ]
    schema = StructType([
        StructField("emp_id", IntegerType(), True),
        StructField("salary", FloatType(), True)
    ])
    salary_data = [(1, 1000.0)]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = spark.createDataFrame(salary_data, schema=schema)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    with pytest.raises(Exception):
        # This should fail because 'id' column is missing in salary
        normalized_with_id.join(
            df_salary,
            normalized_with_id["id"] == df_salary["id"],
            how="left"
        ).select(
            normalized_with_id["id"],
            normalized_with_id["name"],
            normalized_with_id["employee"],
            normalized_with_id["manager_id"],
            df_salary["salary"]
        ).collect()

# TC07: Employee with multiple managers
def test_employee_multiple_managers(spark):
    employee_data = [
        (1, "Alice", "E1", 10),
        (2, "Alice", "E1", 20)
    ]
    salary_data = [
        (1, 1000.0),
        (2, 1100.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.orderBy("id").collect()
    assert len(result) == 2
    assert result[0]["manager_id"] != result[1]["manager_id"]

# TC08: Duplicate employee names under different managers
def test_duplicate_names_different_managers(spark):
    employee_data = [
        (1, "Bob", "E1", 10),
        (2, "Bob", "E2", 20)
    ]
    salary_data = [
        (1, 1000.0),
        (2, 1100.0)
    ]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    result = joined.orderBy("id").collect()
    assert len(result) == 2
    assert result[0]["manager_id"] != result[1]["manager_id"]

# TC09: Large dataset
def test_large_dataset(spark):
    employee_data = [(i, f"Emp{i}", f"E{i}", i % 5) for i in range(1000)]
    salary_data = [(i, float(1000 + i)) for i in range(1000)]
    df_employee = create_employee_df(spark, employee_data)
    df_salary = create_salary_df(spark, salary_data)

    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    normalized = agg.withColumn("name", F.explode(F.split(F.col("name"), ",")))
    normalized = normalized.withColumnRenamed("manager_id", "manager_id_norm")
    normalized_with_id = normalized.join(
        df_employee.select("id", "name", "employee", "manager_id"),
        (normalized["name"] == df_employee["name"]) & (normalized["manager_id_norm"] == df_employee["manager_id"]),
        how="left"
    )
    joined = normalized_with_id.join(
        df_salary,
        normalized_with_id["id"] == df_salary["id"],
        how="left"
    ).select(
        normalized_with_id["id"],
        normalized_with_id["name"],
        normalized_with_id["employee"],
        normalized_with_id["manager_id"],
        df_salary["salary"]
    )
    assert joined.count() == 1000

# TC10: Non-string types in 'name' or 'employee'
def test_non_string_types_in_name(spark):
    employee_data = [
        (1, 123, 456, 10)
    ]
    salary_data = [
        (1, 1000.0)
    ]
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("name", IntegerType(), True),
        StructField("employee", IntegerType(), True),
        StructField("manager_id", IntegerType(), True)
    ])
    df_employee = spark.createDataFrame(employee_data, schema=schema)
    df_salary = create_salary_df(spark, salary_data)

    # Should either coerce to string or raise error during split/explode
    agg = df_employee.groupBy("manager_id").agg(F.concat_ws(",", F.collect_list("name")).alias("name"))
    with pytest.raises(Exception):
        agg.withColumn("name", F.explode(F.split(F.col("name"), ","))).collect()
```

---

Test Execution Report Template:

| Test Case ID | Description | Preconditions | Test Steps | Expected Result | Actual Result | Pass/Fail Status |
|--------------|-------------|---------------|------------|----------------|--------------|------------------|
| TC01         | Happy path  | Employee and salary tables contain valid, non-null data. | ... | Output DataFrame should have one row per employee name per manager, with correct mapping and salary enrichment. | (To be filled) | (To be filled) |
| ...          | ...         | ...           | ...        | ...            | ...          | ...              |

---

**API cost for this call:** 0.0125 USD