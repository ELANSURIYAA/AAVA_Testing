=============================================
Author:        Ascendion AVA+
Created on:   
Description:   This document details the SAS script for extracting, transforming, and exporting healthcare plan data for marketing campaign targeting.
=============================================

# SAS to Python Migration Analysis Report

## Syntax Comparison

### Key Differences Between SAS and Python (pandas)

| Feature/Step                | SAS Example                                                                 | Python (pandas) Example                                                      | Notes                                                                                      |
|-----------------------------|-----------------------------------------------------------------------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|
| **Database Connection**     | `libname tdata teradata ...`                                                | `sqlalchemy.create_engine`, `teradatasql.connect`, or `pandas.read_sql`      | Python uses explicit connection objects, not librefs.                                      |
| **SQL Query**               | `proc sql; create table ... as select ... from ... left join ...; quit;`    | `pd.read_sql(query, conn)` or pandas merge/transformations                   | SQL can be run directly, or logic can be refactored as pandas code.                       |
| **String Concatenation**    | `trim(a) || ' ' || trim(b)`                                                 | `df['a'].str.strip() + ' ' + df['b'].str.strip()`                            | Use pandas `.str` methods and `+` for concatenation.                                      |
| **CASE/WHEN Logic**         | `case when ... then ... else ... end as ...`                                 | `np.select`, `np.where`, or pandas `.apply()`                                | Use numpy or pandas for conditional logic.                                                 |
| **UPCASE**                  | `upcase(plan_type)`                                                         | `df['plan_type'].str.upper()`                                                | Use `.str.upper()` in pandas.                                                             |
| **Filtering**               | `where ...` in SQL, `if ...;` in DATA step                                  | `df = df[condition]`                                                         | Boolean indexing in pandas.                                                               |
| **Export to CSV**           | `proc export data=... outfile=... dbms=csv; run;`                           | `df.to_csv('path', index=False)`                                             | Direct method in pandas.                                                                  |
| **Macro Variables**         | `%let outpath = ...;`                                                       | `outpath = '...'`                                                            | Standard Python variables.                                                                |
| **DATA Step**               | `data ...; set ...; if ...; run;`                                           | `df = df[df['col'] == value]`                                                | Filtering and assignment in pandas.                                                       |
| **Joins**                   | `left join ... on ...`                                                      | `pd.merge(left, right, how='left', on='col')`                                | Use `pd.merge` for joins.                                                                 |
| **Column Aliasing**         | `select ... as colname`                                                     | `df.rename(columns={'old':'new'})` or assignment                             | Use assignment or `rename`.                                                               |

---

## Manual Adjustments Required Post-Automated Conversion

1. **Database Connection**
   - SAS: `libname tdata teradata ...`
   - Python: Must use `sqlalchemy`, `teradatasql`, or other connectors. Credentials and connection string must be handled explicitly.
   - **Manual Adjustment:** Replace libname with Python DB connection code.

2. **SQL to pandas Transformations**
   - Automated tools may convert `proc sql` to a raw SQL string, but pandas logic may be more efficient.
   - **Manual Adjustment:** Refactor SQL logic (CASE, JOIN, WHERE) into pandas/numpy operations for better readability and performance.

3. **String Operations**
   - SAS uses `trim()` and `||` for concatenation.
   - Python uses `.str.strip()` and `+`.
   - **Manual Adjustment:** Update all string concatenations and trims.

4. **CASE Statements**
   - SAS CASE logic must be mapped to `np.select`, `np.where`, or `.apply()` in pandas.
   - **Manual Adjustment:** Rewrite all CASE logic using pandas/numpy.

5. **UPCASE**
   - SAS: `upcase()`
   - Python: `.str.upper()`
   - **Manual Adjustment:** Replace with pandas string methods.

6. **Filtering**
   - SAS DATA step: `if is_campaign_eligible = 1;`
   - Python: `df = df[df['is_campaign_eligible'] == 1]`
   - **Manual Adjustment:** Use boolean indexing.

7. **Export**
   - SAS PROC EXPORT: `proc export ...`
   - Python: `df.to_csv(...)`
   - **Manual Adjustment:** Use pandas export.

8. **Macro Variables**
   - `%let outpath = ...;`
   - Python: `outpath = '...'`
   - **Manual Adjustment:** Replace all macro variables with Python variables.

9. **Column Naming and Aliasing**
   - SAS: `as colname`
   - Python: Assign new columns or use `rename`.
   - **Manual Adjustment:** Ensure all columns are correctly named.

10. **Null Handling**
    - SAS: `is not null`
    - Python: `.notnull()`
    - **Manual Adjustment:** Replace all null checks with pandas equivalents.

11. **Data Types**
    - SAS may implicitly cast types; Python requires explicit conversion.
    - **Manual Adjustment:** Use `.astype()` as needed.

**Example Manual Adjustments:**

- SAS: `trim(hp.member_first_name) || ' ' || trim(hp.member_last_name)`
- Python: `df['member_full_name'] = df['member_first_name'].str.strip() + ' ' + df['member_last_name'].str.strip()`

- SAS: 
  ```
  case 
    when hp.member_age between 25 and 35 then 'YOUNG_ADULT'
    when hp.member_age between 36 and 50 then 'MID_AGE'
    when hp.member_age > 50 then 'SENIOR'
    else 'UNKNOWN'
  end as age_group
  ```
- Python:
  ```python
  conditions = [
      df['member_age'].between(25, 35),
      df['member_age'].between(36, 50),
      df['member_age'] > 50
  ]
  choices = ['YOUNG_ADULT', 'MID_AGE', 'SENIOR']
  df['age_group'] = np.select(conditions, choices, default='UNKNOWN')
  ```

---

## Complexity Evaluation

**Complexity Score:** **35/100**

### Justification

- **SQL to pandas Mapping:** Moderate complexity. The SQL logic is straightforward, with a single join, simple CASE logic, and basic string manipulation.
- **ETL Steps:** The ETL flow is linear and modular, which eases conversion.
- **Library Availability:** All required operations (join, filter, string ops, CASE logic, export) are natively supported in pandas/numpy.
- **Manual Adjustments:** Required for string handling, CASE logic, and connection code, but these are standard in migration projects.
- **No Advanced Features:** No macros, arrays, or advanced SAS features (e.g., hash objects, formats, custom procedures).
- **Data Volume:** Not specified, but if large, may require chunking or optimization in Python.
- **External Dependency:** Teradata connection requires correct driver and credentials setup in Python.

**Summary:** The conversion is straightforward for an experienced data engineer familiar with both SAS and pandas. The main complexity is in translating SQL CASE logic and ensuring all string and type handling is explicit.

---

## Best Practices for Python Implementation

1. **Use pandas for DataFrames:** All transformations should use pandas DataFrames for readability and efficiency.
2. **Explicit Database Connections:** Use SQLAlchemy or teradatasql for Teradata connections. Store credentials securely (e.g., environment variables, config files).
3. **Vectorized Operations:** Prefer vectorized pandas/numpy operations over `.apply()` for performance.
4. **Column Naming:** Match output column names exactly for downstream compatibility.
5. **Null Handling:** Use `.notnull()` and `.fillna()` as appropriate.
6. **Logging and Error Handling:** Add logging for each ETL step and handle exceptions (especially for DB connections and file I/O).
7. **Configurable Paths:** Use variables or config files for file paths and parameters.
8. **Code Modularity:** Split ETL steps into functions or modules for maintainability.
9. **Documentation:** Add docstrings and comments, and use the metadata header as shown above.
10. **Testing:** Validate output against SAS results for a sample run.

---

## Performance Optimization Strategies

1. **Batch Processing:** For large datasets, use chunked reads/writes with pandas.
2. **Pushdown SQL:** Where possible, push filtering and joins to the database (`WHERE`, `JOIN` in SQL) before loading into pandas.
3. **Efficient Data Types:** Use `category` dtype for columns with limited unique values (e.g., `plan_type`, `region`).
4. **Avoid `.apply()` When Possible:** Use vectorized pandas/numpy operations for CASE logic and transformations.
5. **Parallel Processing:** Consider `dask` or `modin` for very large datasets.
6. **Connection Pooling:** Use connection pooling for repeated DB access.
7. **Resource Management:** Close DB connections and file handles promptly.
8. **Profiling:** Use tools like `memory_profiler` and `cProfile` to identify bottlenecks.

---

## Refactor vs Rebuild Recommendation

### Recommendation: **Refactor into pandas with Minimal Changes**

**Justification:**
- The SAS code is modular, linear, and does not use advanced features or complex macro logic.
- The SQL logic can be expressed directly in pandas (or as a SQL query using `read_sql`).
- Minimal changes will ensure business logic is preserved and facilitate validation against SAS output.
- A full rebuild is unnecessary unless there is a need for significant business logic changes or scalability requirements (e.g., moving to Spark for big data).

**When to Rebuild:**
- If the data volume is massive and pandas cannot handle it in memory.
- If there is a need to integrate with a larger Python-based data pipeline or orchestration framework (e.g., Airflow, dbt).

---

## Example Python Skeleton (with Metadata Header)

```python
=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Extract, transform, and export healthcare plan data for marketing campaign targeting.
=============================================

import pandas as pd
import numpy as np
import teradatasql

# Step 1: Connect to Teradata
conn = teradatasql.connect(host='your_server_name', user='your_user', password='your_password', database='tdata')

# Step 2: Extract and join data (push as much logic as possible to SQL)
query = """
SELECT 
    hp.member_id,
    TRIM(hp.member_first_name) AS member_first_name,
    TRIM(hp.member_last_name) AS member_last_name,
    hp.member_age,
    hp.plan_id,
    hp.plan_type,
    hp.region,
    pn.network_type,
    hp.plan_status
FROM tdata.healthcare_plans hp
LEFT JOIN tdata.provider_network pn
    ON hp.plan_id = pn.plan_id
WHERE hp.plan_status = 'ACTIVE'
  AND hp.member_age IS NOT NULL
"""
df = pd.read_sql(query, conn)

# Step 3: Transformations
df['member_full_name'] = df['member_first_name'].str.strip() + ' ' + df['member_last_name'].str.strip()

conditions = [
    df['member_age'].between(25, 35),
    df['member_age'].between(36, 50),
    df['member_age'] > 50
]
choices = ['YOUNG_ADULT', 'MID_AGE', 'SENIOR']
df['age_group'] = np.select(conditions, choices, default='UNKNOWN')

df['marketing_segment'] = df['plan_type'].str.upper() + '-' + df['region']

df['is_campaign_eligible'] = np.where(
    (df['plan_status'] == 'ACTIVE') &
    (df['member_age'].between(25, 60)) &
    (df['plan_type'].isin(['PPO', 'HMO'])) &
    (df['region'].isin(['WEST', 'SOUTH'])) &
    (df['network_type'] == 'IN_NETWORK'),
    1, 0
)

# Step 4: Filter eligible members
final_df = df[df['is_campaign_eligible'] == 1]

# Step 5: Export to CSV
outpath = '/folders/myfolders/final_marketing_campaign_extract_2025.csv'
final_df.to_csv(outpath, index=False)
```

---

## API Cost

"apiCost": 0.01