=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads cleaned and validated sales transactions from staging into the sales fact table, applies data quality checks, logs audit and validation results, and manages batch processing.
=============================================

---

**1. Procedure Overview**

The Azure Synapse stored procedure `dw.sp_load_sales_fact` automates the ETL workflow for loading sales transaction data from a staging table into the enterprise data warehouse’s sales fact table. It performs data quality validation, logs audit information, manages batch processing, and ensures only clean and validated data is loaded into the target table. This supports the business objective of high-quality, auditable sales reporting and analytics.

- **Number of mappings per workflow/session:** 1 main mapping (staging to fact table with enrichment and logging).

---

**2. Complexity Metrics**

| Metric                        | Value / Type                                                                                 |
|-------------------------------|---------------------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (stg.Sales_Transactions)                                                                  |
| Number of Transformations     | 5 (validation, calculation, enrichment, timestamp, batch ID)                                |
| Lookup Usage                  | 2 (joins to dw.Dim_Customer, dw.Dim_Date; both connected lookups)                          |
| Expression Logic              | 2 (Total_Sales_Amount = Quantity * Unit_Price; SYSDATETIME(); CONCAT for audit messages)   |
| Join Conditions               | 2 (INNER JOIN: Customer_ID and Sales_Date/Date_Value)                                      |
| Conditional Logic             | 2 (WHERE Customer_ID IS NULL, WHERE Quantity <= 0)                                         |
| Reusable Components           | 0 (no reusable transformations or mapplets)                                                 |
| Data Sources                  | 1 (SQL Server: stg.Sales_Transactions; joined to dw.Dim_Customer, dw.Dim_Date)             |
| Data Targets                  | 3 (SQL Server: dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures)                                |
| Pre/Post SQL Logic            | 0 (no explicit pre/post SQL, but audit and failure logging)                                 |
| Session/Workflow Controls     | 1 (TRY/CATCH block for error handling, audit status update)                                 |
| DML Logic                     | INSERT (Fact_Sales, Audit_Log, DQ_Failures), DELETE (staging), TRUNCATE (staging)          |
| Complexity Score (0–100)      | 65                                                                                          |

**High-complexity areas:**
- Data quality validation and conditional deletion from staging.
- Multi-table joins for enrichment.
- Audit and error logging with dynamic batch/timestamp assignment.
- Use of temporary tables for validation failures.

---

**3. Syntax Differences**

- **Temporary Tables:** Azure Synapse uses `#InvalidRows` (session-scoped temp table); BigQuery does not support temp tables in the same way. Use scripting with temporary tables via `CREATE TEMP TABLE` or CTEs.
- **Variables:** T-SQL variables (`@batch_id`, `@start_time`, etc.) must be replaced with BigQuery scripting variables (`DECLARE ... DEFAULT ...;`).
- **SYSDATETIME():** Replace with `CURRENT_TIMESTAMP()` in BigQuery.
- **TRY/CATCH:** BigQuery scripting uses `BEGIN ... EXCEPTION WHEN ERROR THEN ... END;` for error handling.
- **@@ROWCOUNT:** Use `ROW_COUNT()` after DML in BigQuery scripting.
- **OBJECT_NAME(@@PROCID):** No direct equivalent; procedure name may need to be hardcoded or passed as a parameter.
- **TRUNCATE TABLE:** Supported in BigQuery, but semantics may differ (ensure table partitioning/streaming compatibility).
- **CONCAT:** Use `CONCAT()` in BigQuery, but string handling may differ slightly.
- **GO:** Not used in BigQuery scripts.

---

**4. Manual Adjustments**

- **Temporary Table Logic:** Manual conversion needed for temp table usage; consider using scripting blocks or CTEs.
- **Audit/Error Logging:** Ensure BigQuery tables (dw.Audit_Log, dw.DQ_Failures) exist and support the required schema.
- **Batch ID Generation:** Use `GENERATE_UUID()` in BigQuery.
- **Error Handling:** Manual implementation of exception handling and logging.
- **Session Variables:** All variable assignments and usage must be restructured for BigQuery scripting.
- **Data Type Differences:** Review and adjust data types (e.g., DATETIME vs. TIMESTAMP).
- **External Dependencies:** Confirm logging tables and batch management are handled outside the procedure if needed.
- **Business Logic Validation:** After migration, validate that all data quality and enrichment logic produces identical results.

---

**5. Optimization Techniques**

- **Partitioning:** Partition target tables (Fact_Sales, Audit_Log, DQ_Failures) by date or batch_id for efficient querying and loading.
- **Pipeline Joins:** Replace multiple joins and filters with a single pipeline using CTEs or subqueries.
- **Window Functions:** Use window functions for aggregations or row counts if needed.
- **Batch Processing:** Use scripting to manage batch variables and error handling.
- **Broadcast Joins:** For small dimension tables, use broadcast joins to optimize performance.
- **Refactor vs. Rebuild:** Refactor is recommended—retain most of the original logic, but restructure for BigQuery’s scripting and DML patterns. Consider a full rebuild only if performance or maintainability issues arise.

---

**Summary of Key Transformation Patterns, Constraints, and Automation Feasibility:**

- Most logic (validation, enrichment, logging) can be automated with scripting templates.
- Manual intervention required for temp table logic, error handling, and variable management.
- Data type and function mapping must be reviewed for each step.
- Automation is feasible for standard ETL patterns, but audit/error handling and temp table usage may require custom scripting.

---