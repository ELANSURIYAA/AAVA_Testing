```
=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads cleaned and validated sales transactions from staging into the sales fact table, applying business rules, audit logging, and data quality checks.
=============================================

---

**1. Procedure Overview**

This workflow consists of a single stored procedure: `dw.sp_load_sales_fact`. Its primary business objective is to automate the ETL (Extract, Transform, Load) process for sales transactions. The procedure extracts data from the staging table (`stg.Sales_Transactions`), applies data quality checks and business transformations, and loads the cleaned data into the fact table (`dw.Fact_Sales`). It also logs audit information, handles errors, and tracks data quality failures. This ensures only valid and high-quality sales data is loaded into the data warehouse, supporting accurate reporting and analytics for business decision-making.

- **Number of Mappings per Workflow/Session:** 1 (single procedure handling the entire ETL flow)

---

**2. Complexity Metrics**

| Metric                      | Value/Type                                                                                       |
|-----------------------------|--------------------------------------------------------------------------------------------------|
| Number of Source Qualifiers | 1 (stg.Sales_Transactions)                                                                       |
| Number of Transformations   | 4 (Data validation, calculation, enrichment via joins, audit logging)                            |
| Lookup Usage                | 2 (Joins to dw.Dim_Customer and dw.Dim_Date; equivalent to connected lookups)                   |
| Expression Logic            | 2 (Total_Sales_Amount = Quantity * Unit_Price; SYSDATETIME() usage; simple, not deeply nested)  |
| Join Conditions             | 2 (INNER JOINs: Customer_ID, Sales_Date=Date_Value; both normal joins)                          |
| Conditional Logic           | 2 (Validation: WHERE Customer_ID IS NULL; WHERE Quantity <= 0)                                  |
| Reusable Components         | 0 (no reusable transformations, mapplets, or sessions)                                          |
| Data Sources                | 3 (SQL Server tables: stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date)                     |
| Data Targets                | 3 (SQL Server tables: dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures)                              |
| Pre/Post SQL Logic          | 1 (Audit logging at start/end, error handling)                                                  |
| Session/Workflow Controls   | 1 (TRY/CATCH block for error handling; audit logging)                                           |
| DML Logic                   | 4 (INSERT, DELETE, TRUNCATE, UPDATE)                                                            |
| Complexity Score (0–100)    | 65 (Moderate; aligns with DI_Azure_Synapse_To_Bigquery_Documentation)                           |

**High-Complexity Areas:**
- Multiple joins (dimension enrichment)
- Audit and error handling logic
- Data quality validation and logging
- No deeply nested expressions or multiple lookups; logic is linear

---

**3. Syntax Differences**

- **Functions:** 
  - `SYSDATETIME()` (SQL Server) → `CURRENT_TIMESTAMP()` (BigQuery)
  - `NEWID()` (SQL Server) → `GENERATE_UUID()` (BigQuery)
  - `OBJECT_NAME(@@PROCID)` (SQL Server) → No direct equivalent; may need to hardcode or use session variables in BigQuery scripting
  - `@@ROWCOUNT` (SQL Server) → Use `ROW_COUNT()` or variable assignment post-DML in BigQuery scripting
  - `TRY/CATCH` (SQL Server) → `BEGIN ... EXCEPTION` block in BigQuery scripting
  - Temporary tables (`#InvalidRows`) → Use temporary tables with `CREATE TEMP TABLE` in BigQuery scripting

- **Data Types:**
  - `UNIQUEIDENTIFIER` → `STRING` (UUID) in BigQuery
  - `DATETIME` → `TIMESTAMP` in BigQuery
  - `NVARCHAR` → `STRING` in BigQuery
  - `BIGINT` → `INT64` in BigQuery

- **Control/Workflow Logic:**
  - No direct equivalent for session variables; must use BigQuery scripting variables (`DECLARE`, `SET`)
  - No direct equivalent for `TRUNCATE TABLE` in BigQuery; use `DELETE FROM` or `CREATE OR REPLACE TABLE` for full refresh

---

**4. Manual Adjustments**

- **Components Requiring Manual Implementation:**
  - Audit logging logic: BigQuery does not support triggers or procedural hooks; must be implemented in script steps
  - Error handling: BigQuery scripting supports `EXCEPTION` blocks, but error details and logging must be manually coded
  - Temporary tables: Use `CREATE TEMP TABLE` and ensure scope is managed within the script
  - Variable assignment and usage: All procedural variables must be explicitly declared and set in BigQuery scripting
  - No direct support for `@@ROWCOUNT`; must use `SELECT COUNT(*)` or track affected rows via scripting logic

- **External Dependencies:**
  - None detected (no external shell scripts, only in-database logic)
  - All logging and DQ failure tracking is in-database

- **Business Logic Review:**
  - Validation rules (missing Customer_ID, invalid Quantity) should be reviewed for completeness
  - Joins to dimension tables must be validated for referential integrity in BigQuery

---

**5. Optimization Techniques**

- **Partitioning:** Partition target tables (e.g., `dw.Fact_Sales`) by date or batch_id for efficient querying and loading
- **Caching:** Use BigQuery’s result caching for repeated audit or DQ queries
- **Broadcast Joins:** Not directly applicable, but ensure small dimension tables are joined efficiently (BigQuery auto-optimizes)
- **Pipeline Joins/Filters:** Combine validation, enrichment, and transformation steps into a single `INSERT ... SELECT` pipeline to minimize intermediate storage
- **Window Functions:** Use window functions for audit metrics or advanced DQ checks if needed
- **Refactor vs. Rebuild:** 
  - **Recommendation:** Refactor. The logic is clear and modular; BigQuery scripting can replicate the flow with minor adjustments. No need for a full rebuild unless future scalability or advanced DQ requirements arise.

---

This analysis provides a comprehensive mapping and transformation guide for converting the Azure Synapse stored procedure `dw.sp_load_sales_fact` to BigQuery SQL, highlighting all key transformation patterns, constraints, blockers, and automation feasibility.