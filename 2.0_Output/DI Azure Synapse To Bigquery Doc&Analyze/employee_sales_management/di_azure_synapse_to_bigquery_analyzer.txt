```
=============================================
Author:        Ascendion AVA
Date:   
Description:   Analysis and mapping of Azure Synapse ETL procedure for loading, auditing, and validating sales transactions to BigQuery SQL, including complexity metrics, syntax differences, manual adjustments, and optimization guidance.
=============================================

---

**1. Procedure Overview**

This document analyzes the Azure Synapse stored procedure `dw.sp_load_sales_fact`, which orchestrates the ETL process for loading sales transactions from the staging table (`stg.Sales_Transactions`) into the data warehouse fact table (`dw.Fact_Sales`). The procedure performs data quality checks, audit logging, manages rejected records, and robust error handling. Its key business objective is to ensure only clean, validated sales data is loaded into analytics-ready tables, supporting accurate reporting and downstream analytics.

- **Number of mappings per workflow/session:** 1 main mapping (from staging to fact with joins to dimensions and audit/error logging).

---

**2. Complexity Metrics**

| Metric                      | Value / Type                                                                 |
|-----------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers | 1 (stg.Sales_Transactions)                                                   |
| Number of Transformations   | 5 (Expression, Aggregator, Joiner, Filter, Audit/Error Logging)              |
| Lookup Usage                | 2 (Joins to dw.Dim_Customer, dw.Dim_Date)                                    |
| Expression Logic            | 2 (Total_Sales_Amount calc, audit message concat)                            |
| Join Conditions             | 2 (Inner joins: Customer_ID, Sales_Date/Date_Value)                          |
| Conditional Logic           | 2 (Validation filters: Customer_ID IS NULL, Quantity <= 0)                   |
| Reusable Components         | 0 (no explicit reusable transformations or mapplets)                         |
| Data Sources                | 3 (stg.Sales_Transactions [SQL Server], dw.Dim_Customer [SQL Server], dw.Dim_Date [SQL Server]) |
| Data Targets                | 3 (dw.Fact_Sales [SQL Server], dw.Audit_Log [SQL Server], dw.DQ_Failures [SQL Server]) |
| Pre/Post SQL Logic          | 1 (Audit log update, error handling)                                         |
| Session/Workflow Controls   | 1 (TRY/CATCH error handling, audit status updates)                           |
| DML Logic                   | Frequent (INSERT, DELETE, TRUNCATE, UPDATE)                                  |
| Complexity Score (0–100)    | 65                                                                           |

**High Complexity Areas:**
- Nested CTE for transformation and enrichment before insert.
- Multiple joins for dimension enrichment.
- Branching logic for validation and error handling.
- Audit and DQ logging with dynamic message construction.

---

**3. Syntax Differences**

- **Temporary Tables:** `#InvalidRows` (SQL Server temp table) must be replaced with BigQuery temporary tables (e.g., `CREATE TEMP TABLE`).
- **Variables/Parameters:** SQL Server-style variable declarations (`DECLARE @var`) and assignments must be replaced with BigQuery scripting variables (`DECLARE var`).
- **Error Handling:** TRY...CATCH/THROW blocks are not directly supported in BigQuery; use BEGIN...EXCEPTION blocks or script-level error handling.
- **System Functions:** 
  - `SYSDATETIME()` → `CURRENT_TIMESTAMP()` in BigQuery.
  - `OBJECT_NAME(@@PROCID)` → Use session variables or script parameters.
  - `NEWID()` → `GENERATE_UUID()` in BigQuery.
  - `@@ROWCOUNT` → Use `ROW_COUNT()` after DML in BigQuery scripting.
- **String Functions:** `CONCAT()` is supported but ensure compatibility.
- **CTE Syntax:** Largely compatible, but BigQuery has some restrictions (e.g., no DML inside CTE).
- **TRUNCATE TABLE:** Supported in BigQuery, but be aware of partitioned tables.
- **GO Statement:** Not used in BigQuery; scripts are separated by `;`.

**Data Type Conversions:**
- `DATETIME`/`DATE` types may need explicit casting.
- Ensure `NVARCHAR`/`VARCHAR` compatibility with BigQuery `STRING`.

**Workflow/Control Logic:**
- No direct equivalent for session-based audit logging; must be implemented via scripting and logging tables.
- No direct equivalent for SQL Server's batch/transaction scope; BigQuery scripting is more linear.

---

**4. Manual Adjustments**

- **Temporary Table Handling:** All temp tables must be created with `CREATE TEMP TABLE` and dropped explicitly or allowed to auto-drop at session end.
- **Error Handling:** Manual implementation of error logging and rethrowing is required; BigQuery scripting supports `EXCEPTION` blocks but with different semantics.
- **Audit Logging:** All audit log updates must be explicitly scripted; no implicit session or transaction context.
- **Row Count Tracking:** Use `ROW_COUNT()` after each DML operation to capture affected row counts.
- **External Dependencies:** Ensure all references to audit and DQ tables exist in BigQuery and are properly structured.
- **Business Logic Review:** Validate all transformation logic, especially calculations and joins, after migration for correctness.
- **Batch ID Generation:** Use `GENERATE_UUID()` or equivalent for unique batch IDs.

---

**5. Optimization Techniques**

- **Partitioning:** Partition large tables (e.g., `Fact_Sales`, staging) by date or batch for efficient querying and loading.
- **Clustering:** Cluster on frequently filtered columns (e.g., `Customer_ID`, `Sales_Date`) to optimize join and filter performance.
- **Pipeline Joins:** Chain filters and joins in a single pipeline query to minimize intermediate storage and improve performance.
- **Window Functions:** Use window functions for aggregations or row numbering if needed.
- **Broadcast Joins:** For small dimension tables, use broadcast joins to optimize performance.
- **Caching:** Not directly available in BigQuery, but repeated subqueries can be materialized as temp tables for efficiency.
- **Cost Awareness:** Monitor query size and optimize to minimize on-demand query costs (as per the provided cost/storage context).
- **Refactor vs. Rebuild:** 
  - **Recommendation:** Refactor if the current logic is performant and readable; Rebuild if significant performance gains or simplification is possible using BigQuery features (e.g., scripting, partitioned tables, or materialized views).

---

**Summary Table: Data Mapping**

| Target Table Name | Target Column Name    | Source Table Name      | Source Column Name    | Remarks (1 to 1 mapping, Transformation, Validation)                 |
|-------------------|----------------------|-----------------------|----------------------|-----------------------------------------------------------------------|
| dw.Fact_Sales     | Transaction_ID       | stg.Sales_Transactions| Transaction_ID       | 1 to 1 mapping                                                        |
| dw.Fact_Sales     | Customer_ID          | stg.Sales_Transactions| Customer_ID          | 1 to 1 mapping, validated (not null)                                  |
| dw.Fact_Sales     | Product_ID           | stg.Sales_Transactions| Product_ID           | 1 to 1 mapping                                                        |
| dw.Fact_Sales     | Sales_Date           | stg.Sales_Transactions| Sales_Date           | 1 to 1 mapping                                                        |
| dw.Fact_Sales     | Quantity             | stg.Sales_Transactions| Quantity             | 1 to 1 mapping, validated (>0)                                        |
| dw.Fact_Sales     | Unit_Price           | stg.Sales_Transactions| Unit_Price           | 1 to 1 mapping                                                        |
| dw.Fact_Sales     | Total_Sales_Amount   | stg.Sales_Transactions| Quantity, Unit_Price | Transformation: Quantity * Unit_Price                                 |
| dw.Fact_Sales     | Region_ID            | dw.Dim_Date           | Region_ID            | Transformation: join on Sales_Date                                    |
| dw.Fact_Sales     | Customer_Segment     | dw.Dim_Customer       | Customer_Segment     | Transformation: join on Customer_ID                                   |
| dw.Fact_Sales     | Load_Timestamp       | System                | SYSDATETIME()        | Transformation: current timestamp                                     |
| dw.Fact_Sales     | Batch_ID             | System                | @batch_id            | Transformation: batch identifier                                      |

---

**Storage and Cost Context**

- Source/target tables are large (up to 1 TB for Fact_Sales).
- Estimated data processed per run: 200–500 GB.
- BigQuery on-demand query pricing: $1.00–$2.50 per run (at $5.00/TB processed).
- Optimization is critical to control costs and ensure efficient processing.

---

**Conclusion**

This document provides a comprehensive analysis for converting the Azure Synapse stored procedure `dw.sp_load_sales_fact` to BigQuery SQL, including complexity metrics, mapping, syntax differences, manual adjustment requirements, and optimization recommendations. Use this as a blueprint for migration, ensuring all business logic, audit, and data quality requirements are preserved and optimized for the BigQuery environment.
```