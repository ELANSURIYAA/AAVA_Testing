=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads cleaned and validated sales transaction data from staging into the sales fact table, performing data quality checks, transformations, and audit logging.
=============================================

---

**1. Procedure Overview**

This workflow consists of a single main mapping/session, implemented as the stored procedure `dw.sp_load_sales_fact`. Its primary business objective is to orchestrate the ETL (Extract, Transform, Load) process for sales transaction data, ensuring only high-quality, business-relevant data is loaded into the data warehouse. The procedure validates, cleans, and transforms raw sales data from the staging table (`stg.Sales_Transactions`), loads the processed data into the sales fact table (`dw.Fact_Sales`), and logs both successful and failed records for auditing and data quality purposes. It also maintains an audit trail for traceability and error handling, aligning with organizational data governance standards.

---

**2. Complexity Metrics**

| Metric                       | Value / Type                                                                                       |
|------------------------------|----------------------------------------------------------------------------------------------------|
| Number of Mappings/Workflows | 1                                                                                                  |
| Number of Source Qualifiers  | 1 (stg.Sales_Transactions)                                                                         |
| Number of Transformations    | 5 (Validation, Join, Calculation, Enrichment, Timestamping)                                        |
| Lookup Usage                 | 2 (dw.Dim_Customer, dw.Dim_Date)                                                                   |
| Expression Logic             | 2 (Quantity * Unit_Price, SYSDATETIME(), CONCAT for audit log message)                             |
| Join Conditions              | 2 (INNER JOIN: Customer_ID, INNER JOIN: Sales_Date to Date_Value)                                  |
| Conditional Logic            | 2 (WHERE Customer_ID IS NULL, WHERE Quantity <= 0)                                                 |
| Reusable Components          | 0 (No explicit reusable transformations or mapplets)                                                |
| Data Sources                 | 3 (SQL Server: stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date)                               |
| Data Targets                 | 3 (SQL Server: dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures)                                        |
| Pre/Post SQL Logic           | 0 (No explicit pre/post SQLs outside procedure)                                                    |
| Session/Workflow Controls    | 1 (TRY/CATCH for error handling and audit logging)                                                 |
| DML Logic                    | INSERT (Audit_Log, DQ_Failures, Fact_Sales, #InvalidRows), DELETE (staging), TRUNCATE (staging)    |
| Complexity Score (0–100)     | 65 (Moderate complexity: multi-step ETL, validation, enrichment, audit, error handling)            |

**High-Complexity Areas:**
- Multiple joins for enrichment (Customer, Date dimensions)
- Data quality validation with error logging and batch audit
- Use of temporary tables for invalid row tracking
- Error handling and audit log update logic

---

**3. Syntax Differences**

- **Functions:**
  - `SYSDATETIME()` (Azure Synapse) → `CURRENT_TIMESTAMP` (BigQuery)
  - `NEWID()` (Azure Synapse) → `GENERATE_UUID()` (BigQuery)
  - `OBJECT_NAME(@@PROCID)` (Azure Synapse) → No direct equivalent; use string literal or session variable in BigQuery
  - `@@ROWCOUNT` (Azure Synapse) → Use `ROW_COUNT()` after DML in BigQuery scripting
  - `CONCAT()` (Azure Synapse) → `CONCAT()` (BigQuery, compatible)
- **Temporary Tables:**
  - `#InvalidRows` (Azure temp table) → Use BigQuery temporary tables or scripting arrays/CTEs
- **Variables:**
  - T-SQL style `DECLARE @var TYPE` → BigQuery scripting uses `DECLARE var TYPE;`
- **Error Handling:**
  - `TRY...CATCH` (Azure) → BigQuery scripting uses `BEGIN ... EXCEPTION WHEN ERROR THEN ... END;`
- **DML:**
  - `TRUNCATE TABLE` (Azure) → `TRUNCATE TABLE` (BigQuery, but with some restrictions)
  - `DELETE ... FROM ... INNER JOIN` (Azure) → BigQuery supports `DELETE FROM ... USING ... WHERE ...`
- **Batch/Audit Logging:**
  - Use of explicit batch IDs and audit tables is supported, but session variables and function calls must be adapted.

**Data Type Conversions:**
- `UNIQUEIDENTIFIER` → `STRING` (UUID format) in BigQuery
- `DATETIME` → `TIMESTAMP` in BigQuery
- `NVARCHAR` → `STRING` in BigQuery

**Workflow/Control Logic:**
- TRY/CATCH and variable-based audit logging must be restructured using BigQuery scripting blocks.

---

**4. Manual Adjustments**

- **Temporary Table Handling:** 
  - Replace `#InvalidRows` with a temporary table or CTE in BigQuery scripting.
- **Audit Log Table:**
  - Ensure audit log and DQ failure tables exist in BigQuery schema.
- **Error Handling:**
  - Manual translation of TRY/CATCH to BigQuery's scripting error handling.
- **Batch ID Generation:**
  - Replace `NEWID()` with `GENERATE_UUID()` and ensure consistent usage.
- **Session Variables:**
  - All T-SQL variables must be declared and used per BigQuery scripting syntax.
- **Row Count Tracking:**
  - Use `ROW_COUNT()` after DML statements to capture affected row counts.
- **Date Handling:**
  - Ensure date comparisons and casts are compatible (e.g., `CAST(s.Sales_Date AS DATE)`).
- **Audit/Monitoring Integration:**
  - Review and validate audit and error logging logic post-migration.

**External Dependencies:**
- None detected (no external stored procedures, shell scripts, or pre/post SQLs referenced).

**Business Logic Review:**
- Validation criteria (e.g., what constitutes an invalid row) should be reviewed for business rule alignment post-migration.

---

**5. Optimization Techniques**

- **Partitioning:** 
  - Partition large tables (Fact_Sales, Sales_Transactions) by date or batch ID in BigQuery for efficient querying and loading.
- **Broadcast Joins:** 
  - If dimension tables are small, use broadcast joins (BigQuery auto-optimizes, but validate join order).
- **Pipeline Joins/Filters:** 
  - Chain validation, enrichment, and transformation logic using CTEs for a single pipeline execution.
- **Window Functions:** 
  - Use window functions for aggregations or deduplication if needed.
- **Caching:** 
  - Not directly applicable in BigQuery, but ensure dimension tables are optimized for join performance.
- **Refactor vs. Rebuild:** 
  - **Recommendation:** Refactor. The logic is clear and modular; a direct translation with BigQuery scripting and CTEs will retain maintainability and auditability. Rebuilding is not necessary unless further performance or business rule changes are required.

---

**BigQuery Sizing and Cost Considerations (from provided estimates):**
- Source tables: ~950 GB, Target table: ~1 TB
- Estimated per-run data processed: 200–500 GB
- On-demand query cost: $1.00–$2.50 per run (at $5/TB processed)

---

This analysis provides a comprehensive mapping and migration plan for converting the Azure Synapse stored procedure to BigQuery SQL, highlighting transformation patterns, constraints, blockers, and automation feasibility.