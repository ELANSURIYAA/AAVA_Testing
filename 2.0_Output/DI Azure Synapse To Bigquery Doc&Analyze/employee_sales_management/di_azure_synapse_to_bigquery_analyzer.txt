=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads cleaned and validated sales transactions from staging to the sales fact table, with audit logging and data quality checks.
=============================================

---

**1. Procedure Overview**

This workflow consists of a single mapping/session: the stored procedure `dw.sp_load_sales_fact`.  
It automates the ETL process for loading sales transaction data from the staging table (`stg.Sales_Transactions`) into the data warehouse fact table (`dw.Fact_Sales`).  
The procedure supports the business objective of reliable, accurate, and auditable sales data for downstream reporting, analytics, and decision-making.  
Key functions include:  
- Audit logging of ETL runs  
- Data quality validation and rejection of invalid records  
- Enrichment via joins to dimension tables  
- Transformation and loading of cleaned data  
- Data quality failure logging  
- Error handling and cleanup

---

**2. Complexity Metrics**

| Metric                    | Value & Type                                                                 |
|---------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers | 1 (stg.Sales_Transactions; SQL Server table)                                |
| Number of Transformations  | 5 (Expression, Aggregator, Joiner, Filter, CTE)                             |
| Lookup Usage               | 2 (Joins to dw.Dim_Customer, dw.Dim_Date; both SQL Server tables)            |
| Expression Logic           | 2 (Total_Sales_Amount = Quantity * Unit_Price; SYSDATETIME() for timestamp)  |
| Join Conditions            | 2 (Inner joins: Customer_ID, Sales_Date=Date_Value)                          |
| Conditional Logic          | 2 (WHERE clauses for validation, error handling via TRY/CATCH)               |
| Reusable Components        | 1 (CTE for transformed data)                                                 |
| Data Sources               | 3 (stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date; all SQL Server)     |
| Data Targets               | 2 (dw.Fact_Sales, dw.DQ_Failures; SQL Server tables)                         |
| Pre/Post SQL Logic         | 2 (Audit log insert/update, error log update)                                |
| Session/Workflow Controls  | 1 (TRY/CATCH for error handling)                                             |
| DML Logic                  | 5 (INSERT, DELETE, TRUNCATE, UPDATE, THROW)                                  |
| Complexity Score           | 65 (Medium; moderate joins, validation, audit, error handling)               |

**High-complexity areas:**  
- Data quality validation with branching logic (invalid rows captured and processed separately)  
- Multiple joins for enrichment  
- Audit and error handling logic  
- Use of temporary tables and CTEs

---

**3. Syntax Differences**

- **Temporary Tables:**  
  Azure Synapse uses `CREATE TABLE #InvalidRows` for temp tables.  
  BigQuery does not support session-scoped temp tables in the same way; use `WITH` clauses or persistent staging tables.

- **Variables and Control Flow:**  
  Azure Synapse uses T-SQL variables (`DECLARE @batch_id`, etc.) and TRY/CATCH blocks.  
  BigQuery SQL does not support procedural variables or TRY/CATCH; use scripting (`DECLARE`, `BEGIN ... EXCEPTION ... END`) or orchestrate error handling externally (e.g., with Cloud Composer/Airflow).

- **Audit Logging:**  
  Direct `INSERT`/`UPDATE` to audit tables with variables.  
  In BigQuery, use scripting for multi-step logic, or orchestrate audit logging outside SQL.

- **Functions:**  
  - `SYSDATETIME()` → BigQuery equivalent is `CURRENT_TIMESTAMP()`  
  - `NEWID()` → BigQuery equivalent is `GENERATE_UUID()`  
  - `OBJECT_NAME(@@PROCID)` → No direct equivalent; procedure name may need to be hardcoded or passed as a parameter.

- **Row Count:**  
  - `@@ROWCOUNT` → In BigQuery, use `COUNT(*)` or capture affected rows via scripting.

- **Error Handling:**  
  - `THROW`/`CATCH` → BigQuery scripting supports `EXCEPTION` blocks, but with different syntax.

- **Joins:**  
  - T-SQL inner joins are supported in BigQuery, but data type casting and date handling may differ.

- **Table Truncation:**  
  - `TRUNCATE TABLE` → In BigQuery, use `DELETE FROM` or `TRUNCATE TABLE` (if supported).

- **CTE Usage:**  
  - `;WITH transformed AS (...)` → BigQuery supports `WITH` clauses.

- **String Functions:**  
  - `CONCAT()` is supported in both, but syntax may differ for some string manipulations.

---

**4. Manual Adjustments**

- **Temp Table Logic:**  
  Must be refactored to use CTEs or persistent staging tables in BigQuery.

- **Audit/Error Logging:**  
  Multi-step audit logging (insert/update with variables) requires BigQuery scripting (`DECLARE`, `SET`, etc.) or orchestration via external tools.

- **Variable Management:**  
  All T-SQL variable logic (`DECLARE`, `SET`) must be rewritten using BigQuery scripting or handled in orchestration.

- **Error Handling:**  
  TRY/CATCH and THROW logic must be refactored to BigQuery scripting’s `EXCEPTION` blocks, or handled externally.

- **Row Count Tracking:**  
  `@@ROWCOUNT` must be replaced with explicit count queries.

- **Procedure Metadata:**  
  `OBJECT_NAME(@@PROCID)` must be hardcoded or passed in BigQuery.

- **External Dependencies:**  
  Any shell scripts, pre/post SQLs, or external triggers must be identified and migrated to GCP orchestration (Cloud Composer, Dataflow, etc.).

- **Business Logic Validation:**  
  All validation rules (e.g., missing Customer_ID, invalid Quantity) must be reviewed for correctness post-migration.

---

**5. Optimization Techniques**

- **Partitioning:**  
  Partition large tables (e.g., `dw.Fact_Sales`) by date or batch ID in BigQuery to improve query performance.

- **Broadcast Joins:**  
  For small dimension tables (`dw.Dim_Customer`, `dw.Dim_Date`), leverage BigQuery’s automatic broadcast join optimization.

- **Pipeline Transformation:**  
  Chain filters and joins into a single SQL pipeline using CTEs to minimize intermediate writes.

- **Window Functions:**  
  Use window functions for aggregations and running totals if needed, instead of nested queries.

- **Caching:**  
  Use BigQuery’s result caching for repeated queries where appropriate.

- **Refactor vs. Rebuild:**  
  Recommend **Refactor**: Retain most of the original logic, as the procedure is modular and business rules are well-defined.  
  Consider **Rebuild** only if performance profiling in BigQuery suggests further optimization is needed (e.g., for very large data volumes or complex audit/error logic).

---

**Summary of Transformation Patterns, Constraints, and Automation Feasibility:**

- Most logic can be automated using BigQuery scripting and SQL, but variable management, error handling, and audit logging require careful refactoring.
- Temp table logic must be replaced with CTEs or persistent tables.
- All data type conversions (e.g., DATETIME, UUID) must be mapped to BigQuery types.
- External orchestration may be required for multi-step workflows and error handling.
- Automation is feasible for the main ETL logic, but manual intervention is needed for audit/error handling and validation logic migration.

**End of Analysis**