```
=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads cleaned and validated sales transaction data from staging into the Fact_Sales table, applying data quality checks, transformations, and audit logging.
=============================================

---

**1. Procedure Overview**

This workflow consists of a single mapping/session encapsulated in the stored procedure `dw.sp_load_sales_fact`. Its primary objective is to automate the ETL process for loading sales transaction data from the staging area (`stg.Sales_Transactions`) into the data warehouse fact table (`dw.Fact_Sales`). The procedure performs data quality checks, applies business logic transformations, manages audit logging, and handles error reporting. This ensures only clean, validated data is loaded into the warehouse, supporting accurate reporting and analytics. The process is critical for maintaining high data quality and traceability in the organization's sales reporting pipeline.

**Number of mappings per workflow/session:** 1

---

**2. Complexity Metrics**

| Metric                        | Value/Type                                                                                 |
|-------------------------------|-------------------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (stg.Sales_Transactions)                                                                |
| Number of Transformations     | 5 (Data Quality checks, Calculation, Enrichment, Audit Logging, Error Handling)           |
| Lookup Usage                  | 2 (Joins to dw.Dim_Customer and dw.Dim_Date as lookups)                                   |
| Expression Logic              | 2 (Total_Sales_Amount calculation, audit message concatenation)                           |
| Join Conditions               | 2 (INNER JOIN: Customer_ID, INNER JOIN: Sales_Date to Date_Value)                         |
| Conditional Logic             | 2 (WHERE clauses for DQ, DELETE with JOIN for invalid rows)                               |
| Reusable Components           | 0 (No reusable transformations, mapplets, or sessions)                                    |
| Data Sources                  | 3 (SQL Server tables: stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date)               |
| Data Targets                  | 3 (SQL Server tables: dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures)                        |
| Pre/Post SQL Logic            | 1 (Audit log insert/update, DQ failure log, error handling)                               |
| Session/Workflow Controls     | 1 (TRY/CATCH block for error handling and logging)                                        |
| DML Logic                     | 4 (INSERT, DELETE, TRUNCATE, UPDATE)                                                      |
| Complexity Score (0–100)      | 65 (Moderate: multiple steps, DQ, logging, joins, but clear structure)                    |

**High-complexity areas:**
- Data quality logic using temporary tables and multi-step validation
- Multiple joins for enrichment
- Error handling and audit logging with dynamic message generation
- Use of temporary tables and batch variables

---

**3. Syntax Differences**

- **Temporary Tables:** Synapse uses `#InvalidRows` (SQL Server temp table). In BigQuery, use temporary tables with `CREATE TEMP TABLE` or CTEs/subqueries.
- **Variables and Batch Logic:** SQL Server variables (`@batch_id`, `@start_time`, etc.) must be replaced with BigQuery scripting variables (`DECLARE`, `SET`).
- **Audit Logging:** SQL Server uses `SYSDATETIME()`, `NEWID()`. In BigQuery, use `CURRENT_TIMESTAMP()` and `GENERATE_UUID()`.
- **Joins and DML:** Syntax is similar, but BigQuery does not support `DELETE ... FROM ... JOIN ...` directly; must use `MERGE` or subqueries.
- **TRY/CATCH:** BigQuery scripting supports `BEGIN ... EXCEPTION ... END;` for error handling, but with different syntax.
- **@@ROWCOUNT:** Replace with `ROW_COUNT()` in BigQuery scripting or capture row counts via scripting logic.
- **String Concatenation:** SQL Server uses `CONCAT()`, BigQuery also supports `CONCAT()` but may require explicit casting.
- **TRUNCATE TABLE:** Supported in BigQuery, but permissions and behavior may differ.

---

**4. Manual Adjustments**

- **Temporary Table Logic:** Manual conversion required for all temp table operations (`#InvalidRows`) to BigQuery temp tables or CTEs.
- **Error Handling:** Manual rewrite of TRY/CATCH logic to BigQuery scripting's error handling.
- **Audit Logging:** Ensure audit log tables and DQ failure tables exist and are compatible in BigQuery.
- **Variable Initialization:** All variable declarations and assignments must be manually mapped to BigQuery scripting.
- **DELETE with JOIN:** Must be rewritten using `MERGE` or `DELETE` with subqueries.
- **Identity/UUID Generation:** Replace `NEWID()` with `GENERATE_UUID()`.
- **Date Functions:** Replace `SYSDATETIME()` with `CURRENT_TIMESTAMP()`; ensure date casting aligns with BigQuery's requirements.
- **Pipeline Integration:** If part of a larger orchestration, ensure BigQuery scripting integrates with orchestration tools (e.g., Cloud Composer, Dataflow).

**Areas for Business Logic Review:**
- Data quality rules (e.g., what constitutes invalid data) should be validated post-migration.
- Audit and error messages should be checked for consistency after conversion.

---

**5. Optimization Techniques**

- **Partitioning:** Partition large tables (Fact_Sales, staging) on appropriate columns (e.g., Sales_Date, Batch_ID) for efficient querying and loading.
- **Pipeline Joins:** Convert multi-step filters and joins into a single pipeline using CTEs to minimize data scans.
- **Window Functions:** Use window functions for aggregations or audit calculations where possible.
- **Broadcast Joins:** For small dimension tables (Dim_Customer, Dim_Date), leverage broadcast joins to optimize performance.
- **Caching:** Use BigQuery's result cache for repeated queries within the ETL pipeline.
- **Refactor vs. Rebuild:** Refactor is recommended—retain most of the original logic but optimize for BigQuery's set-based operations and scripting features. Rebuild only if significant performance or maintainability gains are identified.

---

**Summary of Key Points:**
- The procedure is moderately complex, with clear, auditable ETL logic.
- Manual conversion is required for temp tables, error handling, and some DML patterns.
- Syntax and function differences must be carefully mapped, especially for variables, date/time, and UUIDs.
- Optimization should focus on partitioning, pipeline joins, and leveraging BigQuery's set-based processing.
- Business logic and audit requirements must be validated post-migration for data quality and compliance.

```