=============================================
Author:        Ascendion AVA
Created on:    
Description:   Analysis and conversion guidance for migrating Azure Synapse stored procedure dw.sp_load_sales_fact to BigQuery SQL, covering mapping, complexity, syntax differences, manual adjustments, and optimization.
=============================================

---

**1. Procedure Overview**

The workflow consists of a single mapping/session: the stored procedure `dw.sp_load_sales_fact`.

- **Business Objective:** ETL process for loading cleaned and validated sales transactions from the staging table (`stg.Sales_Transactions`) into the sales fact table (`dw.Fact_Sales`). Supports data integration, cleansing, enrichment, and audit logging for reliable analytics and reporting.
- **Workflow Steps:** Audit logging initiation, data validation (quality checks), cleaning staging data, transformation and loading into fact table, handling rejected records, updating audit logs, error handling, and cleanup.
- **Number of mappings per workflow/session:** 1 (single stored procedure encapsulating all logic).

---

**2. Complexity Metrics**

| Metric                        | Value & Type                                                                                  |
|-------------------------------|----------------------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (stg.Sales_Transactions)                                                                   |
| Number of Transformations     | 5 (validation, calculation, joiner, timestamp, batch ID)                                     |
| Lookup Usage                  | 2 (joins to dw.Dim_Customer and dw.Dim_Date; both connected lookups)                         |
| Expression Logic              | 2 (Total_Sales_Amount calculation, SYSDATETIME for timestamp)                                |
| Join Conditions               | 2 (INNER JOIN: Customer_ID, INNER JOIN: Sales_Date)                                          |
| Conditional Logic             | 2 (WHERE for validation, error handling via TRY/CATCH)                                       |
| Reusable Components           | 0 (no reusable transformations or mapplets)                                                  |
| Data Sources                  | 3 (SQL Server tables: stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date)                  |
| Data Targets                  | 3 (SQL Server tables: dw.Fact_Sales, dw.DQ_Failures, dw.Audit_Log)                          |
| Pre/Post SQL Logic            | 2 (Audit log insert/update, error handling block)                                            |
| Session/Workflow Controls     | 1 (TRY/CATCH for error control, audit logging)                                               |
| DML Logic                     | 5 (INSERT, DELETE, SELECT, TRUNCATE, UPDATE)                                                 |
| Complexity Score (0–100)      | 60 (matches DI_Azure_Synapse_To_Bigquery_Documentation Complexity Score)                     |

**High-Complexity Areas:**
- Deeply nested error handling (TRY/CATCH, audit log updates)
- Multiple lookups via joins (Customer and Date dimensions)
- Branching logic (validation, error handling)
- Data quality checks and rejected record handling

---

**3. Syntax Differences**

- **Functions:**
  - `SYSDATETIME()` (SQL Server) → `CURRENT_TIMESTAMP()` (BigQuery)
  - `NEWID()` (SQL Server) → `GENERATE_UUID()` (BigQuery)
  - `OBJECT_NAME(@@PROCID)` (SQL Server) → No direct equivalent; can be hardcoded or managed via scripting in BigQuery.
  - `@@ROWCOUNT` (SQL Server) → Use `COUNT(*)` in BigQuery after DML operations.
  - `TRY/CATCH` (SQL Server) → BigQuery scripting supports `BEGIN ... EXCEPTION ... END` blocks for error handling.
  - `CONCAT()` (SQL Server) → `CONCAT()` (BigQuery), but syntax may differ slightly.

- **Data Types:**
  - `DATETIME` (SQL Server) → `TIMESTAMP` (BigQuery)
  - `NVARCHAR` (SQL Server) → `STRING` (BigQuery)
  - `BIGINT` (SQL Server) → `INT64` (BigQuery)
  - `UNIQUEIDENTIFIER` (SQL Server) → `STRING` (BigQuery, using UUIDs)

- **Temporary Tables:**
  - `#InvalidRows` (SQL Server temp table) → BigQuery temporary tables (using `CREATE TEMP TABLE`) or CTEs.

- **DML Operations:**
  - `TRUNCATE TABLE` (SQL Server) → `TRUNCATE TABLE` (BigQuery, supported)
  - `INSERT INTO ... SELECT ...` (SQL Server) → Same in BigQuery, but table references must be fully qualified.

- **Control Flow:**
  - No direct equivalent for stored procedure parameters (none used here).
  - Audit logging and error handling must be adapted to BigQuery scripting.

---

**4. Manual Adjustments**

- **Components Requiring Manual Implementation:**
  - Audit logging logic (insert/update) must be adapted to BigQuery scripting.
  - Error handling: BigQuery scripting supports `EXCEPTION` blocks, but logic must be rewritten.
  - Temporary table logic (`#InvalidRows`) should use BigQuery temp tables or CTEs.
  - Batch ID generation and usage (`@batch_id`) must be handled with `GENERATE_UUID()` and variable assignment in BigQuery scripting.
  - No external dependencies (no shell scripts or external procedures), but business logic for validation and audit must be validated post-conversion.

- **Areas for Business Logic Review:**
  - Validation rules (Customer_ID not null, Quantity > 0) should be confirmed for business requirements.
  - Audit log schema and update logic must be verified for compatibility.
  - Error message handling and reporting may need adjustment for BigQuery's scripting and logging capabilities.

---

**5. Optimization Techniques**

- **Partitioning:** Use partitioned tables for `dw.Fact_Sales` and staging tables to optimize query performance and cost.
- **Caching:** Leverage BigQuery's caching for repeated queries and intermediate results.
- **Broadcast Joins:** For small dimension tables (e.g., `dw.Dim_Date`, `dw.Dim_Customer`), BigQuery automatically optimizes joins; ensure statistics are up-to-date.
- **Pipeline Conversion:** Chain filters and joins in a single SQL pipeline using CTEs or subqueries to minimize intermediate data writes.
- **Window Functions:** Use window functions for aggregations if needed, to simplify nested logic.
- **Refactor vs. Rebuild:** Recommend **Refactor**—retain most of the original logic, as the ETL flow is straightforward and can be mapped to BigQuery scripting with moderate changes. Consider **Rebuild** only if performance or cost optimization is a major concern (e.g., for very large data volumes or more complex validation).

---

**Additional Notes:**
- **Estimated BigQuery Cost:** Based on provided documentation, each run processes ~200–500 GB, costing ~$1.00–$2.50 per run.
- **Data Source/Target Types:** All sources and targets are SQL Server tables; will become BigQuery tables post-migration.

---

This document provides a comprehensive mapping and analysis for converting the Azure Synapse stored procedure `dw.sp_load_sales_fact` to BigQuery SQL, highlighting transformation patterns, constraints, blockers, and automation feasibility. No code is included per instructions.