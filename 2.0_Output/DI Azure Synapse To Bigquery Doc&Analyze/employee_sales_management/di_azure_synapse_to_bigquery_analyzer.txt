```
=============================================
Author:        Ascendion AVA
Date:   
Description:   Analysis and conversion guidance for Azure Synapse stored procedure 'dw.sp_load_sales_fact' to BigQuery SQL, covering mapping, complexity, syntax differences, manual adjustments, and optimization.
=============================================

**1. Procedure Overview**

The stored procedure `dw.sp_load_sales_fact` automates the ETL workflow for loading sales transactions from the staging table (`stg.Sales_Transactions`) into the sales fact table (`dw.Fact_Sales`). It performs data cleansing, enrichment via joins with dimension tables (`dw.Dim_Customer`, `dw.Dim_Date`), audit logging, and manages data quality failures. The business objective is to ensure only validated, enriched, and traceable sales data is loaded for analytics and reporting.

- Number of mappings per workflow/session: 1 main mapping (staging to fact table with enrichment and logging).

---

**2. Complexity Metrics**

| Metric                     | Value & Type                                                                 |
|----------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers| 1 (stg.Sales_Transactions)                                                   |
| Number of Transformations  | 6 (validation, joins, calculation, enrichment, truncation, audit logging)    |
| Lookup Usage               | 2 (joins to dw.Dim_Customer, dw.Dim_Date; both act as lookups)               |
| Expression Logic           | 2 (Quantity * Unit_Price, SYSDATETIME(), CONCAT for messages)                |
| Join Conditions            | 2 (INNER JOIN: Customer_ID, Sales_Date/Date_Value)                           |
| Conditional Logic          | 2 (WHERE filters for validation, error handling via TRY/CATCH)               |
| Reusable Components        | 0 (no reusable transformations or mapplets)                                  |
| Data Sources               | 3 (SQL Server tables: stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date)  |
| Data Targets               | 3 (SQL Server tables: dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures)           |
| Pre/Post SQL Logic         | 1 (audit log insert/update, error handling)                                  |
| Session/Workflow Controls  | 1 (TRY/CATCH block for error handling)                                       |
| DML Logic                  | Frequent (INSERT, DELETE, TRUNCATE, UPDATE)                                  |
| Complexity Score (0–100)   | 65 (moderate: multi-step ETL, audit, validation, enrichment, error handling)  |

**High-complexity areas:**
- Data quality validation and error handling (TRY/CATCH).
- Enrichment via joins (multi-table).
- Audit and data quality logging.
- Use of temporary table for invalid rows.

---

**3. Syntax Differences**

- **Temporary Tables**: Synapse uses `#InvalidRows` (local temp table); BigQuery does not support temp tables in the same way. Use CTEs or temp tables via scripting.
- **Variables**: SQL Server variables (`@batch_id`, `@start_time`, etc.) must be replaced with BigQuery scripting variables (`DECLARE`, `SET`).
- **Audit Logging**: SQL Server uses `INSERT`/`UPDATE` with variables; BigQuery scripting supports similar DML but lacks direct `@@ROWCOUNT` equivalent (use `SELECT COUNT(*)`).
- **Error Handling**: TRY/CATCH and `THROW` in T-SQL; BigQuery scripting uses `BEGIN ... EXCEPTION ... END`.
- **Functions**: 
    - `SYSDATETIME()` → `CURRENT_TIMESTAMP()` in BigQuery.
    - `NEWID()` → `GENERATE_UUID()` in BigQuery.
    - `OBJECT_NAME(@@PROCID)` → No direct equivalent; procedure name may need to be hardcoded or passed as a variable.
    - `CONCAT()` is supported in BigQuery.
- **Joins**: Standard SQL joins; compatible, but ensure date casting (`CAST(s.Sales_Date AS DATE)`) matches BigQuery syntax.
- **Truncate Table**: `TRUNCATE TABLE` is supported in BigQuery, but permissions and semantics may differ.
- **Batch Metadata**: BigQuery scripting supports variable assignment and usage.

---

**4. Manual Adjustments**

- **Temp Table Replacement**: Replace `#InvalidRows` with CTEs or arrays in BigQuery scripting.
- **Row Count Tracking**: Replace `@@ROWCOUNT` with explicit `SELECT COUNT(*)` queries post DML.
- **Error Handling**: Rewrite TRY/CATCH logic using BigQuery scripting's `EXCEPTION` block.
- **Audit Logging**: Ensure audit log table exists and supports required columns; DML must be adapted for BigQuery.
- **External Dependencies**: Validate existence and schema of target tables (`dw.Fact_Sales`, `dw.Audit_Log`, `dw.DQ_Failures`) in BigQuery.
- **Business Logic Review**: Validate join conditions, enrichment logic, and error handling post-migration for semantic equivalence.

---

**5. Optimization Techniques**

- **Partitioning**: Partition target tables (`Fact_Sales`, `Audit_Log`, `DQ_Failures`) by batch or date for efficient querying.
- **Pipeline Conversion**: Chain validation, enrichment, and insert steps using CTEs for readability and performance.
- **Window Functions**: Use window functions for aggregations if needed (not present in current logic, but useful for future extensions).
- **Broadcast Joins**: For small dimension tables, BigQuery automatically optimizes joins; ensure statistics are up to date.
- **Caching**: Not directly applicable in BigQuery, but repeated subqueries can be materialized as temp tables or CTEs.
- **Refactor vs. Rebuild**: **Refactor** recommended—logic is clear and modular; only syntactic and control flow changes needed for BigQuery compatibility.

---

**Additional Notes:**

- Estimated data processed per run: 200–500 GB/query (based on provided context).
- Storage and pricing: BigQuery on-demand query pricing applies; optimize queries to minimize processed data.
- No external scripts or Java transformations detected; migration is feasible with moderate manual effort.

```