=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads cleaned sales transaction data from staging into the sales fact table, with audit logging and data quality validation.
=============================================

--------------------------------------------------------------------------------
1. Procedure Overview
--------------------------------------------------------------------------------
This documentation analyzes the Azure Synapse stored procedure `dw.sp_load_sales_fact`, which automates the ETL workflow for sales transaction data. The procedure validates, cleanses, and loads sales data from a staging table (`stg.Sales_Transactions`) into the sales fact table (`dw.Fact_Sales`), while maintaining audit logs (`dw.Audit_Log`) and capturing data quality failures (`dw.DQ_Failures`). The business objective is to ensure only high-quality, validated sales transactions are loaded into the data warehouse, supporting reliable analytics, operational monitoring, and compliance.

Number of mappings per workflow/session:  
- 1 main mapping (staging to fact table with enrichment and validation)
- 2 supporting mappings (audit log, data quality failures)

--------------------------------------------------------------------------------
2. Complexity Metrics
--------------------------------------------------------------------------------
| Metric                     | Value / Type                                                                 |
|----------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers| 1 (stg.Sales_Transactions)                                                   |
| Number of Transformations  | 4 (Validation, Enrichment, Calculation, Metadata Addition)                   |
| Lookup Usage               | 2 (Customer Segment from dw.Dim_Customer, Region_ID from dw.Dim_Date)        |
| Expression Logic           | 2 (Quantity * Unit_Price, CONCAT for audit message)                          |
| Join Conditions            | 2 (INNER JOIN: Customer, Date dimensions)                                    |
| Conditional Logic          | 2 (Validation WHERE clauses, TRY/CATCH error handling)                       |
| Reusable Components        | 0 (No reusable mapplets or sessions)                                         |
| Data Sources               | 3 (SQL Server tables: stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date)  |
| Data Targets               | 3 (SQL Server tables: dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures)           |
| Pre/Post SQL Logic         | 1 (Audit log, DQ failure logging, error handling)                            |
| Session/Workflow Controls  | 1 (TRY/CATCH for error control)                                              |
| DML Logic                  | 4 (INSERT, DELETE, UPDATE, TRUNCATE)                                         |
| Complexity Score (0–100)   | 65 (Moderate: multi-step ETL, audit, error handling, enrichment, validation) |

**High-complexity areas:**
- Nested expressions for audit messaging and sales amount calculation
- Multiple lookups (dimension joins)
- Branching logic via error handling (TRY/CATCH)
- Temporary table usage for validation failures

--------------------------------------------------------------------------------
3. Syntax Differences
--------------------------------------------------------------------------------
- **Temporary Tables:**  
  - Azure Synapse uses `#InvalidRows` (local temp table). BigQuery does not support session-scoped temp tables; instead, use temporary tables (`CREATE TEMP TABLE`) or CTEs/subqueries.
- **Variables and Control Flow:**  
  - T-SQL variables (`DECLARE @batch_id ...`) must be replaced with BigQuery scripting variables (`DECLARE ... DEFAULT ...;`).
  - BigQuery scripting supports `BEGIN ... EXCEPTION ... END` for error handling, but syntax differs from T-SQL's `TRY/CATCH`.
- **System Functions:**  
  - `SYSDATETIME()` → `CURRENT_TIMESTAMP()` in BigQuery.
  - `NEWID()` → `GENERATE_UUID()` in BigQuery.
  - `OBJECT_NAME(@@PROCID)` → Use scripting variable or hardcoded procedure name.
- **Row Count:**  
  - `@@ROWCOUNT` is not available; use `SELECT COUNT(*)` or scripting variables to track affected rows.
- **String Concatenation:**  
  - `CONCAT()` is available in BigQuery, but syntax may differ slightly.
- **Error Handling:**  
  - BigQuery supports `BEGIN ... EXCEPTION ... END`, but error reporting and logging must be adapted.
- **DML Operations:**  
  - BigQuery supports `INSERT`, `DELETE`, `UPDATE`, but `TRUNCATE TABLE` is not supported; use `DELETE FROM table` without a WHERE clause.
- **Joins:**  
  - Standard SQL joins are supported, but explicit casting (`CAST(s.Sales_Date AS DATE)`) may require type conversion functions in BigQuery.

--------------------------------------------------------------------------------
4. Manual Adjustments
--------------------------------------------------------------------------------
- **Temp Table Replacement:**  
  - Replace `#InvalidRows` with a temporary table or CTE/subquery in BigQuery.
- **Audit and DQ Logging:**  
  - Ensure audit and DQ log tables exist in BigQuery and adapt schema as needed.
- **Error Handling:**  
  - Manual review required to ensure error handling logic is correctly mapped to BigQuery scripting.
- **Row Count Tracking:**  
  - Replace `@@ROWCOUNT` with explicit counting logic.
- **External Dependencies:**  
  - Review any external scripts or stored procedures referenced in audit or DQ logging.
- **Business Logic Validation:**  
  - Post-migration, validate that all business rules (validation, enrichment, error handling) are producing expected results.

--------------------------------------------------------------------------------
5. Optimization Techniques
--------------------------------------------------------------------------------
- **Partitioning:**  
  - Partition large tables (Fact_Sales, Sales_Transactions) by date or batch_id for efficient querying and loading.
- **Pipeline Conversion:**  
  - Convert chained filters and joins into a single pipeline using CTEs or subqueries.
- **Window Functions:**  
  - Use window functions for aggregations or running totals if needed.
- **Broadcast Joins:**  
  - For small dimension tables (Dim_Date, Dim_Customer), leverage broadcast joins to optimize performance.
- **Caching:**  
  - Use BigQuery's caching features for repeated queries.
- **Refactor vs. Rebuild:**  
  - **Recommendation:** Refactor the procedure for BigQuery SQL, retaining core logic but adapting for syntax and platform differences. Rebuild only if significant performance or maintainability improvements are identified during migration.

--------------------------------------------------------------------------------
Summary
--------------------------------------------------------------------------------
This analysis provides a comprehensive mapping of the Azure Synapse stored procedure to BigQuery SQL, identifying transformation patterns, syntax differences, manual adjustments, and optimization opportunities. The procedure supports high-quality, auditable data integration for sales analytics, with moderate complexity due to validation, enrichment, audit logging, and error handling. Manual review is required for temp table usage, error handling, and row count logic. Optimization recommendations include partitioning, pipeline conversion, and leveraging BigQuery-specific features.

**No code is included in this output per instructions.**