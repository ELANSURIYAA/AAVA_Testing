=============================================
Author:        Ascendion AVA
Date:   
Description:   Detailed analysis of Azure Synapse stored procedure for sales fact ETL, supporting conversion to BigQuery SQL.
=============================================

---

**1. Procedure Overview**

The stored procedure `dw.sp_load_sales_fact` automates the ETL workflow for sales transaction data. It validates, transforms, and loads sales records from the staging table into the `Fact_Sales` table, while capturing audit logs and data quality failures. The business objective is to ensure only clean, validated, and enriched data is loaded into the data warehouse, supporting accurate analytics, reporting, and compliance with data governance standards.

- **Mappings per workflow/session:** 1 main mapping (Sales Transaction ETL) per procedure execution.
- **Key objectives:** Data integration, cleansing, enrichment, audit logging, and data quality tracking.

---

**2. Complexity Metrics**

| Metric                      | Value & Type                                                                 |
|-----------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers | 1 (stg.Sales_Transactions)                                                   |
| Number of Transformations   | 5 (Validation, Calculation, 2 Joins, Timestamp, Batch_ID)                    |
| Lookup Usage                | 2 (Dim_Customer, Dim_Date via INNER JOIN; equivalent to connected lookups)   |
| Expression Logic            | 2 (Total_Sales_Amount = Quantity * Unit_Price; SYSDATETIME for timestamp)    |
| Join Conditions             | 2 (INNER JOIN: Customer_ID, INNER JOIN: Sales_Date/Date_Value)               |
| Conditional Logic           | 2 (WHERE filters for validation, error handling via TRY...CATCH)             |
| Reusable Components         | 0 (No reusable transformations, mapplets, or sessions)                       |
| Data Sources                | 3 (SQL Server tables: stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date)  |
| Data Targets                | 3 (SQL Server tables: dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures)           |
| Pre/Post SQL Logic          | 0 (No explicit pre/post SQL in sessions; all logic internal to procedure)    |
| Session/Workflow Controls   | 1 (TRY...CATCH for error handling and audit status updates)                  |
| DML Logic                   | Frequent (INSERT, DELETE, TRUNCATE, UPDATE)                                  |
| Complexity Score (0–100)    | 65 (Moderate complexity: validation, joins, audit, error handling)           |

**High-complexity areas:**
- Validation logic with temporary tables and multiple filters.
- Enrichment via multiple joins.
- Audit and error handling with dynamic status updates.
- Data quality failure capture and logging.

---

**3. Syntax Differences**

- **Temporary Tables:** Synapse uses `#InvalidRows`; BigQuery does not support temp tables in the same way. Use CTEs or subqueries.
- **Variables:** Synapse uses T-SQL variables (`@batch_id`, `@rows_inserted`); BigQuery scripting uses DECLARE/SET but with different syntax.
- **Error Handling:** Synapse uses `TRY...CATCH`; BigQuery supports `BEGIN...EXCEPTION...END` blocks, but with different semantics.
- **System Functions:** 
  - `SYSDATETIME()` → BigQuery: `CURRENT_TIMESTAMP()`
  - `NEWID()` → BigQuery: `GENERATE_UUID()`
  - `OBJECT_NAME(@@PROCID)` → BigQuery: No direct equivalent; may need to hardcode or use scripting context.
- **Joins:** Synapse uses explicit INNER JOINs; BigQuery supports similar syntax.
- **Row Count:** Synapse uses `@@ROWCOUNT`; BigQuery uses `ROW_COUNT()` after DML in scripting.
- **String Concatenation:** Synapse uses `CONCAT`; BigQuery uses `CONCAT` or `||`.
- **Transaction Control:** Synapse supports explicit transaction control; BigQuery scripting supports transactions but with different syntax.

---

**4. Manual Adjustments**

- **Temp Table Logic:** Replace `#InvalidRows` with CTEs or array structures in BigQuery.
- **Variable Management:** Refactor variable declarations and assignments to BigQuery scripting style.
- **Error Handling:** Rewrite `TRY...CATCH` blocks using BigQuery’s scripting error handling.
- **Audit Logging:** Manual review needed to ensure audit logic aligns with BigQuery’s DML and scripting capabilities.
- **Row Count Tracking:** Replace `@@ROWCOUNT` with BigQuery’s `ROW_COUNT()` or equivalent logic.
- **Staging Table Truncation:** BigQuery uses `TRUNCATE TABLE` but ensure permissions and transactional consistency.
- **External Dependencies:** No explicit pre/post SQL or external scripts, but audit and DQ tables must exist in BigQuery.
- **Business Logic Validation:** Post-conversion, validate that all data quality and audit checks function as intended.

---

**5. Optimization Techniques**

- **Partitioning:** Partition target tables (`Fact_Sales`, `Audit_Log`, `DQ_Failures`) by date or batch ID for performance.
- **Pipeline Processing:** Convert chain filters and joins into a single pipeline using CTEs for efficiency.
- **Window Functions:** Use window functions for aggregations or row counts if needed.
- **Broadcast Joins:** For small dimension tables, use broadcast joins to optimize join performance.
- **Caching:** Use BigQuery’s caching for repeated queries where applicable.
- **Refactor vs. Rebuild:** 
  - **Refactor:** Retain most original logic, translating T-SQL constructs to BigQuery scripting and SQL.
  - **Rebuild:** Consider rebuilding if BigQuery-native features (e.g., partitioned tables, streaming inserts) offer significant performance or maintainability improvements.

---

This analysis provides a comprehensive mapping and transformation guide for converting the Azure Synapse stored procedure to BigQuery SQL, highlighting areas requiring manual intervention, syntax adjustments, and optimization opportunities.