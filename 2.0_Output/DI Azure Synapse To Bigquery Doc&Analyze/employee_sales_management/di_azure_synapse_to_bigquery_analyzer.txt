=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads cleaned and validated sales transactions from staging into the sales fact table, with audit logging and data quality checks.
=============================================

---

**1. Procedure Overview**

The Azure Synapse stored procedure `[dw].[sp_load_sales_fact]` automates the ETL process for loading sales transaction data from the staging area into the enterprise sales fact table. It performs data validation, transformation, and audit logging to ensure high data quality and traceability. The procedure identifies and removes invalid records, transforms and loads clean data into the target table, and logs both successful and failed operations for compliance and troubleshooting. This process supports the business need for accurate, timely, and auditable sales reporting, enabling reliable analytics and decision-making.

- **Number of mappings per workflow/session:** 1 main ETL mapping within a single stored procedure.
- **Key business objective:** Data integration, cleansing, enrichment, and loading with audit and data quality controls.

---

**2. Complexity Metrics**

| Metric                    | Value / Type                                                                 |
|---------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers | 1 (stg.Sales_Transactions) [SQL Server]                                    |
| Number of Transformations | 6 (Validation, Expression, Aggregation, Join, Filter, Audit Logging)         |
| Lookup Usage              | 2 (Joins to dw.Dim_Customer and dw.Dim_Date) [Connected Lookups]             |
| Expression Logic          | 2 (Total_Sales_Amount calculation, Load_Timestamp, Batch_ID assignment)      |
| Join Conditions           | 2 (Inner joins: Customer_ID, Sales_Date=Date_Value)                          |
| Conditional Logic         | 2 (WHERE clauses for validation, error handling with TRY/CATCH)              |
| Reusable Components       | 0 (All logic is inline within the procedure)                                 |
| Data Sources              | 3 (stg.Sales_Transactions [SQL Server Table], dw.Dim_Customer [SQL Server Table], dw.Dim_Date [SQL Server Table]) |
| Data Targets              | 2 (dw.Fact_Sales [SQL Server Table], dw.DQ_Failures [SQL Server Table])      |
| Pre/Post SQL Logic        | 1 (Audit log updates before and after load)                                  |
| Session/Workflow Controls | 1 (TRY/CATCH for error handling and audit)                                   |
| DML Logic                 | 4 (INSERT, DELETE, TRUNCATE, UPDATE)                                         |
| Complexity Score (0–100)  | 70                                                                           |

**High-complexity areas:**
- Deeply nested expressions: Moderate (windowed CTE for transformation, inline calculations)
- Multiple lookups: Yes (joins to two dimension tables)
- Branching logic: Error handling via TRY/CATCH, validation branching
- Unstructured sources/external scripts: None

---

**3. Syntax Differences**

- **Functions without direct BigQuery equivalents:**
  - `SYSDATETIME()` → Use `CURRENT_TIMESTAMP()` in BigQuery.
  - `NEWID()` → Use `GENERATE_UUID()` in BigQuery.
  - `OBJECT_NAME(@@PROCID)` → No direct equivalent; may need to hardcode or use a parameter.
  - `@@ROWCOUNT` → Use `ROW_COUNT()` after DML in BigQuery scripting.

- **Data type conversions:**
  - `DATETIME` (SQL Server) → `TIMESTAMP` or `DATETIME` (BigQuery).
  - `UNIQUEIDENTIFIER` (SQL Server) → `STRING` (UUID) in BigQuery.
  - Temporary tables (`#InvalidRows`) → Use temporary tables or scripting arrays in BigQuery.

- **Workflow/control logic:**
  - `TRY/CATCH` blocks → Use BigQuery scripting `BEGIN ... EXCEPTION ... END`.
  - Variable declarations and assignment syntax differ.
  - Table truncation (`TRUNCATE TABLE`) is supported in BigQuery, but permissions and behavior may differ.
  - Inline CTEs and multi-step DML are supported but require scripting blocks.

---

**4. Manual Adjustments**

- **Components requiring manual implementation:**
  - Temporary table logic (`#InvalidRows`) must be restructured using BigQuery scripting (DECLARE/CREATE TEMP TABLE or ARRAY).
  - Audit logging and error handling must be rewritten using BigQuery scripting constructs.
  - The use of `OBJECT_NAME(@@PROCID)` for dynamic procedure name logging needs manual adjustment.
  - All references to SQL Server-specific system functions and variables must be mapped or hardcoded.

- **External dependencies:**
  - None detected (no external stored procedures or shell scripts invoked).

- **Business logic review/validation:**
  - Data quality rules (e.g., what constitutes "invalid" data) should be validated post-migration.
  - Audit log and DQ failure table structures must be reviewed to ensure compatibility with BigQuery schemas.

---

**5. Optimization Techniques**

- **Partitioning:** Partition target tables (e.g., `Fact_Sales`) by date or batch_id for efficient querying and loading.
- **Caching/Broadcast Joins:** Use BigQuery's automatic join optimization; ensure dimension tables are small or use `WITH` clauses for efficient lookups.
- **Pipeline Filters/Joins:** Chain validation and join logic in a single SQL pipeline to minimize intermediate writes.
- **Window Functions:** Use window functions for aggregations if needed, though not required in current logic.
- **Refactor vs. Rebuild:** Recommend **Refactor**—retain most original logic but restructure for BigQuery scripting, as the logic is clear and modular. However, consider **Rebuild** if further optimization or modularization is desired for larger-scale or more complex ETL.

---

**Summary Table: Complexity Metrics with Data Source/Target Types**

| Parameter             | Value / Type                                   |
|-----------------------|------------------------------------------------|
| Procedure Name        | dw.sp_load_sales_fact                          |
| Source Tables         | 3 (SQL Server: stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date) |
| Target Tables         | 2 (SQL Server: dw.Fact_Sales, dw.DQ_Failures)  |
| Data Flows            | 4                                              |
| Transformations       | 6                                              |
| Joins and Filters     | 4                                              |
| Variables             | 7                                              |
| Parameters            | 0                                              |
| Dependencies          | 0                                              |
| Complexity Score      | 70                                             |

---

**Key Takeaways for Migration:**
- Most logic can be refactored into BigQuery scripting with moderate effort.
- Manual intervention is required for temporary tables, error handling, and system function mapping.
- Data quality and audit logic should be validated after migration.
- Performance can be optimized using BigQuery best practices for partitioning and join strategies.