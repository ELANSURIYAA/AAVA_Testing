```
=============================================
Author:        Ascendion AVA
Date:   
Description:   Analysis of Azure Synapse stored procedure for loading, cleansing, transforming, and auditing sales transaction data, with guidance for BigQuery SQL migration.
=============================================

1. Procedure Overview

The stored procedure [dw.sp_load_sales_fact] orchestrates the ETL process for sales transactions, supporting the business objective of integrating, cleansing, enriching, and auditing sales data. It validates and removes invalid records, enriches valid data with customer and date dimension information, loads cleansed data into the sales fact table, logs all actions for auditing, and records any data quality failures. This ensures only high-quality, enriched sales data is integrated into the data warehouse, supporting accurate reporting and analytics.

- Number of mapping per workflow/session: 1 main ETL mapping (validation, enrichment, load, audit, DQ logging)

2. Complexity Metrics

| Metric                    | Value & Type/Details                                                                 |
|---------------------------|--------------------------------------------------------------------------------------|
| Number of Source Qualifiers | 1 (stg.Sales_Transactions)                                                        |
| Number of Transformations  | 5 (validation, joins, calculation, enrichment, audit/error handling)                |
| Lookup Usage               | 2 (joins to dw.Dim_Customer and dw.Dim_Date)                                        |
| Expression Logic           | 1 (Total_Sales_Amount = Quantity * Unit_Price; some basic string concatenation)     |
| Join Conditions            | 2 (INNER JOIN: Customer_ID, Sales_Date to Date_Value)                               |
| Conditional Logic          | 2 (WHERE Customer_ID IS NULL, WHERE Quantity <= 0)                                  |
| Reusable Components        | 0 (no mapplets, no reusable transformations)                                        |
| Data Sources               | 3 (stg.Sales_Transactions [SQL Server], dw.Dim_Customer [SQL Server], dw.Dim_Date [SQL Server]) |
| Data Targets               | 3 (dw.Fact_Sales [SQL Server], dw.Audit_Log [SQL Server], dw.DQ_Failures [SQL Server]) |
| Pre/Post SQL Logic         | 2 (Audit log insert/update, error handling)                                         |
| Session/Workflow Controls  | 1 (TRY/CATCH block for error handling and audit)                                    |
| DML Logic                  | Frequent (INSERT, DELETE, TRUNCATE, UPDATE)                                         |
| Complexity Score           | 65 (Moderate: multiple joins, validation, audit, error handling)                    |

High-complexity areas:
- Error handling with TRY/CATCH and audit log updates
- Use of temporary tables for validation failures
- Multiple DML operations (DELETE, INSERT, TRUNCATE, UPDATE)
- Data enrichment via dimension joins

3. Syntax Differences

- Temporary tables (#InvalidRows): BigQuery does not support session-scoped temp tables in the same way; use WITH clauses or persistent staging tables.
- Variables (DECLARE/SET): BigQuery scripting supports DECLARE/SET, but with different syntax and scoping.
- System functions: 
    - SYSDATETIME() → CURRENT_TIMESTAMP() in BigQuery.
    - @@ROWCOUNT → Use row count variables or SELECT COUNT(*) as needed.
    - OBJECT_NAME(@@PROCID) → Use SESSION_USER() or hardcode procedure name.
    - ERROR_MESSAGE() → Use ERROR_MESSAGE() in BigQuery scripting, but error handling is different.
- String concatenation: CONCAT() is supported in both, but syntax may differ.
- TRY/CATCH: BigQuery uses BEGIN...EXCEPTION blocks for error handling.
- TRUNCATE TABLE: Supported in BigQuery, but permissions and behavior differ.
- INSERT INTO ... SELECT ...: Supported, but BigQuery requires explicit column lists and may need type casting.
- CTEs (WITH transformed AS ...): Supported in BigQuery, but scoping and variable usage differ.

4. Manual Adjustments

- Temporary table logic (#InvalidRows) must be restructured using CTEs or persistent tables in BigQuery.
- Audit logging (dw.Audit_Log) and DQ logging (dw.DQ_Failures) require explicit scripting for insert/update, as procedural flow differs.
- Error handling: BigQuery scripting supports EXCEPTION blocks, but logic must be adapted.
- Variable assignment and usage: Must be refactored for BigQuery scripting.
- System functions (e.g., @@ROWCOUNT, OBJECT_NAME): Need manual mapping or replacement.
- Any pre/post SQL logic (e.g., pipeline triggers) must be implemented in orchestration layer (e.g., Cloud Composer, Dataform, or dbt).
- Business logic for validation and enrichment should be validated post-migration for correctness.
- Cost and storage considerations: BigQuery is billed per TB processed; query optimization is critical.

5. Optimization Techniques

- Partition target tables (dw.Fact_Sales) by date or batch_id for efficient querying and loading.
- Use WITH clauses (CTEs) to replace temp tables and chain validation, enrichment, and transformation steps.
- Minimize data scanned by filtering early (e.g., WHERE clauses before joins).
- Use window functions for aggregations if needed.
- Consider MERGE statements for upserts if deduplication or SCD logic is required.
- Refactor: Retain most of the original logic, but refactor temp table and error handling patterns for BigQuery compatibility.
- Rebuild: If performance or cost is a concern (as per the cost analysis), consider redesigning the workflow to minimize data processed and leverage BigQuery’s strengths (e.g., ELT with Dataflow or dbt).

---
**BigQuery Cost/Storage Context (from file):**
- Estimated storage used per run: ~1.95 TB (source + dimension + target tables).
- Estimated data processed per run: 200–500 GB/query (10% of total).
- On-demand query cost: $1.00–$2.50 per run (at $5.00/TB).
- Recommendation: Optimize queries to minimize data processed and leverage partitioning.

---
```