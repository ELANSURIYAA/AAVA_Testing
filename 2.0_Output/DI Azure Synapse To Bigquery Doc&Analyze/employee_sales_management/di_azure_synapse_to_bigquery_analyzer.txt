```
=============================================
Author:        Ascendion AVA
Date:   
Description:   Analysis of Azure Synapse stored procedure for loading, validating, and auditing sales transaction data, with guidance for BigQuery SQL migration.
=============================================

**1. Procedure Overview**

The stored procedure `dw.sp_load_sales_fact` orchestrates the ETL workflow for sales transaction data. It performs data validation, cleansing, enrichment, and loading from the staging table to the fact table, while maintaining audit and data quality logs. This process ensures that only high-quality, enriched sales data is loaded into the analytics layer, supporting business intelligence, compliance, and reporting objectives.

- Number of mappings per workflow/session: 1 main mapping (staging → validation → enrichment → fact/DQ/audit).

---

**2. Complexity Metrics**

| Metric                    | Value & Type                                                                                   |
|---------------------------|-----------------------------------------------------------------------------------------------|
| Number of Source Qualifiers | 1 (stg.Sales_Transactions) [SQL Server]                                                      |
| Number of Transformations  | 5 (Validation, Enrichment, Calculation, Audit Logging, Error Handling)                        |
| Lookup Usage               | 2 (Joins with dw.Dim_Customer, dw.Dim_Date) [Connected Lookups]                              |
| Expression Logic           | 2 (Calculation: Quantity * Unit_Price, use of SYSDATETIME(), CONCAT, etc.)                   |
| Join Conditions            | 2 (Inner Joins: Customer_ID, Sales_Date)                                                     |
| Conditional Logic          | 2 (WHERE clauses for validation, error handling block)                                       |
| Reusable Components        | 0 (No reusable transformations or mapplets)                                                   |
| Data Sources               | 3 (stg.Sales_Transactions [SQL Server], dw.Dim_Customer [SQL Server], dw.Dim_Date [SQL Server]) |
| Data Targets               | 3 (dw.Fact_Sales [SQL Server], dw.Audit_Log [SQL Server], dw.DQ_Failures [SQL Server])       |
| Pre/Post SQL Logic         | 1 (Audit log and error handling updates)                                                     |
| Session/Workflow Controls  | 1 (TRY/CATCH error handling, batch/audit variables)                                          |
| DML Logic                  | Frequent (INSERT, DELETE, TRUNCATE, UPDATE)                                                  |
| Complexity Score (0–100)   | 65                                                                                           |

**High-complexity areas:**
- Use of temporary tables for validation failures.
- Multi-step validation and enrichment logic.
- Error handling and audit logging with dynamic messages.
- Multiple DML operations in a single procedure.

---

**3. Syntax Differences**

- **Temporary Tables:** Synapse uses `#InvalidRows` (temp table); BigQuery does not support session-scoped temp tables. Use WITH clauses (CTEs) or temp tables with `CREATE TEMP TABLE` (scoped to session).
- **Variables:** SQL Server variables (`@batch_id`, `@start_time`, etc.) must be replaced with BigQuery scripting variables (`DECLARE`, `SET`).
- **Procedural Blocks:** Synapse uses `BEGIN TRY...END TRY` and `BEGIN CATCH...END CATCH`; BigQuery uses `BEGIN...EXCEPTION...END`.
- **Functions:** 
    - `NEWID()` → `GENERATE_UUID()`
    - `SYSDATETIME()` → `CURRENT_TIMESTAMP()`
    - `OBJECT_NAME(@@PROCID)` → Use a string literal or session variable.
    - `@@ROWCOUNT` → Use `ROW_COUNT()` in BigQuery scripting.
    - `CONCAT()` is supported in both, but argument handling may differ.
- **DML Statements:** 
    - `TRUNCATE TABLE` is supported in BigQuery.
    - `DELETE ... FROM ... INNER JOIN` must be rewritten as `DELETE FROM ... WHERE ... IN (SELECT ...)`.
- **Error Handling:** BigQuery scripting uses `BEGIN ... EXCEPTION ... END;` with `RAISE` for error handling.
- **GO statement:** Not required in BigQuery.

---

**4. Manual Adjustments**

- **Temp Table Logic:** Must be restructured using CTEs or session-scoped temp tables.
- **Audit Logging:** Update logic for capturing procedure name and timestamps.
- **Error Handling:** Manual translation of TRY/CATCH to BigQuery's scripting error handling.
- **Row Count Tracking:** Replace `@@ROWCOUNT` with `ROW_COUNT()` or equivalent.
- **External Dependencies:** None detected (no external stored procedures or shell scripts).
- **Business Logic Validation:** Ensure all validation and enrichment steps are preserved and tested post-migration.
- **Batch Metadata:** Ensure UUIDs and timestamps are generated and used consistently.

---

**5. Optimization Techniques**

- **Partitioning:** Partition large tables (e.g., `dw.Fact_Sales`) by date or batch for efficient querying and loading.
- **Pipeline Joins/Filters:** Chain validation and enrichment steps using CTEs to avoid multiple scans.
- **Window Functions:** Use window functions for aggregations if needed (not present in current logic, but useful for future enhancements).
- **Broadcast Joins:** For small dimension tables, use broadcast joins to optimize performance.
- **Caching:** Not directly applicable in BigQuery, but minimize repeated scans by structuring queries efficiently.
- **Refactor vs. Rebuild:** Recommend **Refactor**—the original logic is clear and modular, but restructuring temp table and error handling logic for BigQuery is required.

---

**BigQuery Cost & Storage Considerations**

- Source table sizes:
    - stg.Sales_Transactions: ~700 GB
    - dw.Dim_Customer: ~150 GB
    - dw.Dim_Date: ~100 GB
    - dw.Fact_Sales (target): ~1 TB
    - Estimated data processed per run: 200–500 GB
    - Estimated BigQuery cost per run: $1.00–$2.50 (on-demand, based on 10% data processed per query)

---

**Summary**

This document provides a comprehensive mapping and analysis of the Azure Synapse stored procedure `dw.sp_load_sales_fact` for migration to BigQuery SQL. Key transformation patterns, syntax differences, manual adjustment areas, and optimization recommendations are outlined to ensure a smooth, cost-effective, and performant migration.

```