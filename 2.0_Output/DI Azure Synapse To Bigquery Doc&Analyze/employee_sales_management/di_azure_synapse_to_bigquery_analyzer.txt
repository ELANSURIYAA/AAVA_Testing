=============================================
Author:        Ascendion AVA
Date:   
Description:   Analysis of Azure Synapse stored procedure for ETL of sales transactions, supporting conversion to BigQuery SQL.
=============================================

---

**1. Procedure Overview**

The stored procedure `dw.sp_load_sales_fact` automates the ETL workflow for sales transaction data. It extracts records from the staging table (`stg.Sales_Transactions`), performs data quality validation, enriches data via dimension lookups, loads cleansed and transformed records into the `dw.Fact_Sales` fact table, and logs audit and data quality information. The business objective is to ensure only high-quality, validated, and enriched sales data is loaded for analytics, reporting, and compliance.

- **Mappings per workflow/session:** 1 main mapping per procedure execution, encompassing validation, enrichment, loading, and logging.
- **Key business objective:** Data integration, cleansing, enrichment, and loading with audit traceability.

---

**2. Complexity Metrics**

| Metric                      | Value & Type                                                                 |
|-----------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers | 1 (stg.Sales_Transactions; SQL Server staging table)                         |
| Number of Transformations   | 5 (Validation, Enrichment via Join, Calculation, Audit Logging, DQ Logging)  |
| Lookup Usage                | 2 (Connected: dw.Dim_Customer, dw.Dim_Date)                                  |
| Expression Logic            | 2 (Calculation: Quantity * Unit_Price, SYSDATETIME(), NEWID())               |
| Join Conditions             | 2 (Inner joins: Customer_ID, Sales_Date/Date_Value)                          |
| Conditional Logic           | 2 (Filters: Customer_ID IS NULL, Quantity <= 0)                              |
| Reusable Components         | 0 (No reusable mapplets/sessions; all logic in single procedure)             |
| Data Sources                | 1 (SQL Server table: stg.Sales_Transactions)                                 |
| Data Targets                | 3 (SQL Server tables: dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures)           |
| Pre/Post SQL Logic          | 2 (Audit log insert/update, DQ log insert)                                   |
| Session/Workflow Controls   | 1 (Error handling via TRY/CATCH, audit status updates)                       |
| DML Logic                   | Frequent (INSERT, DELETE, TRUNCATE, UPDATE)                                  |
| Complexity Score (0–100)    | 65 (Moderate complexity: multi-step ETL, validation, enrichment, logging)    |

**High-complexity areas:**
- Data validation and error handling with temporary tables.
- Multiple lookups (dimension joins).
- Audit and DQ logging with dynamic variables.
- Branching logic for error handling and status updates.

---

**3. Syntax Differences**

- **Functions without direct BigQuery equivalents:**
  - `SYSDATETIME()` → Use `CURRENT_TIMESTAMP()` or `CURRENT_DATETIME()` in BigQuery.
  - `NEWID()` → Use `GENERATE_UUID()` in BigQuery.
  - `OBJECT_NAME(@@PROCID)` → No direct equivalent; procedure name must be handled differently.
  - `@@ROWCOUNT` → Use `ROW_COUNT()` or track row counts via subqueries in BigQuery.
  - `TRY/CATCH` block → BigQuery scripting uses `BEGIN ... EXCEPTION ... END` for error handling.
  - Temporary tables (`#InvalidRows`) → Use temporary tables via scripting or CTEs in BigQuery.
  - `TRUNCATE TABLE` → Use `TRUNCATE TABLE` in BigQuery, but permissions/behavior may differ.

- **Data type conversions:**
  - `DATETIME`/`NVARCHAR` → Use `DATETIME`/`STRING` in BigQuery.
  - Explicit casts (e.g., `CAST(s.Sales_Date AS DATE)`) may need adjustment for BigQuery’s date/time functions.

- **Workflow/control logic:**
  - Audit logging and status updates must be restructured using BigQuery scripting and DML.
  - Error handling and logging require BigQuery scripting constructs.

---

**4. Manual Adjustments**

- **Components requiring manual implementation:**
  - Audit logging logic (insert/update with dynamic status and messages).
  - Data quality failure logging (insert into DQ_Failures).
  - Error handling (TRY/CATCH → BigQuery scripting EXCEPTION block).
  - Temporary table logic (`#InvalidRows`) → Use scripting variables, arrays, or CTEs.
  - Batch variables (`@batch_id`, `@start_time`, etc.) → Use scripting variables in BigQuery.

- **External dependencies:**
  - Audit and DQ log tables must exist and be compatible with BigQuery schema.
  - Any downstream processes relying on SQL Server-specific features must be reviewed.

- **Areas for business logic review:**
  - Validation rules for Customer_ID and Quantity.
  - Enrichment logic via joins (ensure dimension tables are available in BigQuery).
  - Audit and error message formatting.

---

**5. Optimization Techniques**

- **Partitioning:** Partition `Fact_Sales` and dimension tables on appropriate keys (e.g., Sales_Date, Customer_ID) to improve query performance.
- **Caching:** Use BigQuery’s caching for repeated queries; avoid unnecessary recomputation.
- **Broadcast joins:** For small dimension tables, leverage broadcast joins (BigQuery automatically optimizes joins).
- **Pipeline conversion:** Chain filters and joins in a single SQL pipeline using CTEs or scripting blocks.
- **Window functions:** Use window functions for aggregations or row numbering if needed.
- **Refactor vs. Rebuild:** 
  - **Refactor** if the original logic is efficient and aligns with BigQuery best practices.
  - **Rebuild** if the ETL can be simplified using BigQuery’s native features (e.g., MERGE, scripting, partitioned tables).

---

**Summary**

This document provides a comprehensive analysis of the Azure Synapse stored procedure `dw.sp_load_sales_fact` for conversion to BigQuery SQL, detailing the procedure workflow, complexity metrics, syntax differences, manual adjustments, and optimization recommendations. The analysis supports a smooth migration by highlighting transformation patterns, constraints, blockers, and automation feasibility for organizations transitioning their ETL processes to Google Cloud Platform.