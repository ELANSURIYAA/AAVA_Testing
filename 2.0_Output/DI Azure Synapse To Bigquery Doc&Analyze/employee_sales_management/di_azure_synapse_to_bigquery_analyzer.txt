=============================================
Author:        Ascendion AVA
Date:   
Description:   Detailed analysis for converting Azure Synapse stored procedure 'dw.sp_load_sales_fact' to BigQuery SQL, covering mapping, complexity, syntax differences, manual adjustments, and optimization strategies.
=============================================

**1. Procedure Overview**

The stored procedure `dw.sp_load_sales_fact` orchestrates the ETL workflow for loading, validating, transforming, and auditing sales transaction data from the staging table (`stg.Sales_Transactions`) into the data warehouse fact table (`dw.Fact_Sales`). It supports the business objectives of data integration, cleansing, enrichment, and operational compliance. The workflow includes audit logging, data quality checks, enrichment via dimension joins, error handling, and metrics tracking for both successful and failed loads.

- Number of mappings per workflow/session: 1 main mapping (from staging to fact table), with supporting mappings for audit log and DQ failures.

**2. Complexity Metrics**

| Metric                      | Value & Type                                                                 |
|-----------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers | 1 (stg.Sales_Transactions: SQL Server/ Synapse table)                        |
| Number of Transformations   | 5 (Validation, Calculation, Enrichment via Join, Audit Logging, Error Handling)|
| Lookup Usage                | 2 (Joins to dw.Dim_Customer and dw.Dim_Date: SQL Server/ Synapse tables)     |
| Expression Logic            | 2 (Total_Sales_Amount calculation, SYSDATETIME(), NEWID())                   |
| Join Conditions             | 2 (Inner joins: Customer_ID, Sales_Date)                                     |
| Conditional Logic           | 2 (WHERE filters for validation, error handling via TRY/CATCH)               |
| Reusable Components         | 0 (No mapplets or reusable transformations)                                  |
| Data Sources                | 3 (stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date: SQL Server tables)  |
| Data Targets                | 3 (dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures: SQL Server tables)           |
| Pre/Post SQL Logic          | 0 (No explicit pre/post SQL, but audit log and error handling act as control)|
| Session/Workflow Controls   | 1 (TRY/CATCH block for error handling)                                       |
| DML Logic                   | Frequent (INSERT, DELETE, TRUNCATE, UPDATE)                                  |
| Complexity Score (0–100)    | 75 (Moderate-high: multi-step ETL, audit, error handling, enrichment)        |

**High-Complexity Areas:**
- Deeply nested workflow with multiple DML operations.
- Branching logic via TRY/CATCH for error handling.
- Data enrichment via joins to dimension tables.
- Temporary table usage for validation failures.
- Audit and DQ failure logging for compliance.

**3. Syntax Differences**

- **Functions:**
  - `SYSDATETIME()` → Use `CURRENT_TIMESTAMP()` in BigQuery.
  - `NEWID()` → Use `GENERATE_UUID()` in BigQuery.
  - `OBJECT_NAME(@@PROCID)` → No direct equivalent; procedure name may need to be hardcoded or passed as a parameter.
  - `@@ROWCOUNT` → Use `ROW_COUNT()` or count in BigQuery via subqueries.
  - `TRY/CATCH` block → BigQuery scripting uses `BEGIN ... EXCEPTION ... END` for error handling.
  - Temporary tables (`#InvalidRows`) → Use temporary tables or CTEs in BigQuery; BigQuery supports scripting variables and temp tables with `CREATE TEMP TABLE`.
  - `TRUNCATE TABLE` → Use `DELETE FROM ... WHERE TRUE` or overwrite in BigQuery (since TRUNCATE is not supported).

- **Data Type Conversions:**
  - `DATETIME` → Use `TIMESTAMP` in BigQuery.
  - `NVARCHAR` → Use `STRING` in BigQuery.
  - `BIGINT` → Use `INT64` in BigQuery.

- **Workflow/Control Logic:**
  - Audit logging and error handling must be restructured using BigQuery scripting constructs.
  - Batch tracking via variables should use BigQuery script variables.

**4. Manual Adjustments**

- **Components requiring manual implementation:**
  - Audit log and DQ failure logging logic must be manually scripted in BigQuery.
  - Error handling must be adapted to BigQuery scripting (`EXCEPTION` blocks).
  - Temporary table logic for validation failures needs to be implemented via CTEs or temp tables.
  - Procedure name retrieval (`OBJECT_NAME(@@PROCID)`) must be hardcoded or parameterized.
  - Row count tracking (`@@ROWCOUNT`) must be implemented using explicit counting logic.
  - Truncation of staging table must be replaced with a full-table delete or overwrite.

- **External dependencies:**
  - No external shell scripts or Java transformations detected.
  - All dependencies are internal to the database (dimension tables, audit log, DQ failures).

- **Business logic review:**
  - Validation rules (missing customer, invalid quantity) should be reviewed for completeness.
  - Audit and error handling logic must be validated post-migration for operational compliance.

**5. Optimization Techniques**

- **Partitioning:** Partition large tables (Fact_Sales, Sales_Transactions) by date or batch_id for efficient querying.
- **Caching:** Use BigQuery’s built-in caching for repeated queries; avoid unnecessary recomputation.
- **Broadcast Joins:** Not directly applicable, but ensure dimension tables are small for efficient joins.
- **Pipeline Conversion:** Convert chained filters and joins into CTEs or subqueries for pipeline processing.
- **Window Functions:** Use window functions for aggregations if needed (not present in current logic).
- **Refactor vs. Rebuild:** Recommend **Refactor** approach—retain most original logic, but restructure for BigQuery scripting and optimize for cost/performance. Consider **Rebuild** only if business logic needs major changes or if performance bottlenecks are detected.

---

This analysis provides a comprehensive mapping and conversion guide for migrating the Azure Synapse stored procedure to BigQuery SQL, highlighting transformation patterns, constraints, blockers, and automation feasibility.