=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads, validates, and transforms sales transaction data from staging to the sales fact table, with audit logging and data quality checks.
=============================================

---

**1. Procedure Overview**

The Azure Synapse stored procedure `[dw.sp_load_sales_fact]` automates the ETL workflow for sales transaction data. It extracts raw transactions from the staging table (`stg.Sales_Transactions`), performs data validation and cleansing, applies business logic transformations (including enrichment from dimension tables), and loads the processed records into the enterprise sales fact table (`dw.Fact_Sales`). The procedure also logs audit information (`dw.Audit_Log`), captures data quality failures (`dw.DQ_Failures`), and ensures traceability for compliance and troubleshooting. This workflow supports the business objective of maintaining a reliable, analytics-ready sales dataset for reporting and decision-making.

- **Number of Mappings per Workflow/Session:** 1 stored procedure encapsulating the entire ETL process.
- **Key Business Objective:** Data integration, cleansing, enrichment, and loading with audit and data quality logging.

---

**2. Complexity Metrics**

| Metric                     | Value & Type                                                                                   |
|----------------------------|-----------------------------------------------------------------------------------------------|
| Number of Source Qualifiers| 1 (stg.Sales_Transactions; SQL Server/Azure Synapse table)                                    |
| Number of Transformations  | 5 (validation, enrichment, calculation, audit logging, DQ logging)                            |
| Lookup Usage               | 2 (joins to dw.Dim_Customer and dw.Dim_Date; both are connected lookups via INNER JOIN)       |
| Expression Logic           | 2 (Total_Sales_Amount = Quantity * Unit_Price; SYSDATETIME() usage; no deep nesting)          |
| Join Conditions            | 2 (INNER JOIN: Customer_ID to Dim_Customer, Sales_Date to Dim_Date)                           |
| Conditional Logic          | 2 (validation WHERE clauses for missing/invalid data; error handling with TRY...CATCH)        |
| Reusable Components        | 0 (no reusable transformations or mapplets; all logic in one procedure)                       |
| Data Sources               | 3 (stg.Sales_Transactions [Azure Synapse], dw.Dim_Customer [Azure Synapse], dw.Dim_Date [Azure Synapse]) |
| Data Targets               | 3 (dw.Fact_Sales [Azure Synapse], dw.Audit_Log [Azure Synapse], dw.DQ_Failures [Azure Synapse]) |
| Pre/Post SQL Logic         | 2 (audit log insert/update, DQ failure insert, error handling)                                |
| Session/Workflow Controls  | 1 (TRY...CATCH error handling, status updates in audit log)                                   |
| DML Logic                  | 4 (INSERT, DELETE, TRUNCATE, UPDATE)                                                          |
| Complexity Score (0–100)   | 65                                                                                            |

**High-Complexity Areas:**
- Multiple joins for enrichment (customer and date dimensions)
- Data quality validation and error handling logic
- Audit and DQ logging for traceability
- Use of temporary tables for invalid data capture

---

**3. Syntax Differences**

- **Functions without Direct BigQuery Equivalents:**
  - `SYSDATETIME()` (Azure Synapse) → `CURRENT_TIMESTAMP()` (BigQuery)
  - `NEWID()` (Azure Synapse) → `GENERATE_UUID()` (BigQuery)
  - `OBJECT_NAME(@@PROCID)` (Azure Synapse) → No direct equivalent; may require hardcoding or session variables in BigQuery scripting.
  - `@@ROWCOUNT` (Azure Synapse) → Use `ROW_COUNT()` in BigQuery scripting (with procedural logic).
  - `TRY...CATCH` (Azure Synapse) → Use `BEGIN ... EXCEPTION ... END` in BigQuery scripting.

- **Data Type Conversions:**
  - `DATETIME` (Azure Synapse) → `TIMESTAMP` (BigQuery)
  - `UNIQUEIDENTIFIER` (Azure Synapse) → `STRING` (UUID format in BigQuery)
  - `NVARCHAR` (Azure Synapse) → `STRING` (BigQuery)
  - Temporary tables (`#InvalidRows`) → Use temporary tables or scripting arrays/structs in BigQuery.

- **Workflow/Control Logic:**
  - No direct equivalent for session variables; use BigQuery scripting variables.
  - No direct support for `TRUNCATE TABLE` in BigQuery; use `DELETE FROM` or overwrite strategy.
  - Error handling must be restructured using BigQuery scripting's `EXCEPTION` blocks.

---

**4. Manual Adjustments**

- **Components Requiring Manual Implementation:**
  - Temporary table logic (`#InvalidRows`) must be rewritten using BigQuery scripting constructs (e.g., temporary tables, arrays, or CTEs).
  - Error handling (`TRY...CATCH`) must be manually mapped to BigQuery scripting's `EXCEPTION` handling.
  - Audit log updates and DQ failure inserts may need to be split into separate statements due to BigQuery's procedural limitations.
  - `OBJECT_NAME(@@PROCID)` logic for procedure name tracking must be hardcoded or handled via scripting variable.
  - `@@ROWCOUNT` logic must be replaced with explicit row counting in BigQuery.

- **External Dependencies:**
  - None detected (no external stored procedures or shell scripts invoked).
  - All logic is self-contained within the procedure.

- **Business Logic Validation:**
  - Ensure that joins to dimension tables and validation logic produce the same results post-migration.
  - Validate that audit and DQ logging semantics are preserved in BigQuery.

---

**5. Optimization Techniques**

- **Partitioning:** Partition `Fact_Sales` and large dimension tables by date or batch ID to improve query and load performance.
- **Pipeline Transformation:** Convert chained filters and joins into a single pipeline using CTEs or subqueries in BigQuery.
- **Window Functions:** Use window functions for aggregations or row numbering if needed (not present in this logic, but recommended for future enhancements).
- **Broadcast Joins:** Not directly applicable in BigQuery, but ensure that dimension tables are small enough for efficient join processing.
- **Caching:** Use materialized views or result caching for frequently accessed dimension data.
- **Refactor vs. Rebuild:** 
  - **Refactor** is recommended since the logic is clear, modular, and maps well to BigQuery scripting with moderate manual adjustments.
  - **Rebuild** only if significant performance or maintainability improvements are identified during testing.

---

**Summary:**  
This stored procedure is moderately complex (score: 65/100), with key challenges in temporary table handling, error management, and audit/DQ logging. Most logic can be refactored for BigQuery, but manual intervention is needed for procedural constructs and data type conversions. Optimization should focus on partitioning, pipeline transformation, and leveraging BigQuery scripting best practices.