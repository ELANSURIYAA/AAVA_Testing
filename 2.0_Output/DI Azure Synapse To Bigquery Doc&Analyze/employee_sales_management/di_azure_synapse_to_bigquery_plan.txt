=============================================
Author:        Ascendion AVA
Created on:    
Description:   Effort and cost estimation for converting and testing Azure Synapse ETL procedure to GCP BigQuery, including runtime cost and manual intervention effort.
=============================================

1. Cost Estimation

   1.1 BigQuery Runtime Cost

   **Pricing Reference:**  
   - BigQuery On-Demand Query Pricing: $5.00 per TB processed

   **Data Volume Reference:**  
   - stg.Sales_Transactions: ~700 GB  
   - dw.Dim_Customer: ~150 GB  
   - dw.Dim_Date: ~100 GB  
   - dw.Fact_Sales (Target Table): ~1 TB  
   - Total Estimated Storage Used: ~1.95 TB

   **Estimated Data Processed Per Run:**  
   - 10% of total storage per query (ETL load)  
   - Range: ~200–500 GB/query

   **Calculation:**  
   - Lower Bound: 200 GB = 0.2 TB × $5.00 = $1.00 per run  
   - Upper Bound: 500 GB = 0.5 TB × $5.00 = $2.50 per run

   **Breakup & Reasoning:**  
   - The ETL process involves reading from the staging table, joining with two dimension tables, performing transformations, and writing to the fact table.
   - The largest scan is from stg.Sales_Transactions (~700 GB), but only clean/valid records are loaded, and joins are with relatively small dimension tables.
   - Temporary tables and DQ failure logs add minor overhead, but most processing is set-based and can be optimized with partitioning and CTEs in BigQuery.
   - Audit logging and error handling add negligible data scan cost.
   - Estimated cost per ETL batch run: **$1.00–$2.50 USD**

   **Assumptions:**  
   - No excessive cross joins or full scans of the fact table.
   - No materialized views or additional intermediate storage costs.
   - Only one ETL batch run per day (adjust cost if frequency increases).

   **apiCost:** 0.0125 USD

---

2. Code Fixing and Testing Effort Estimation

   2.1 BigQuery Identified Manual Code Fixes and Unit Testing Effort

   **Manual Code Fixes Required:**
   - Temporary table logic (`#InvalidRows`) → BigQuery temp tables or CTEs
   - Variable declarations/assignments (`@batch_id`, `@start_time`, etc.) → BigQuery scripting variables
   - Error handling (TRY/CATCH) → BigQuery scripting EXCEPTION blocks
   - Audit logging (SYSDATETIME(), NEWID()) → CURRENT_TIMESTAMP(), GENERATE_UUID()
   - DELETE with JOIN → MERGE or DELETE with subquery
   - Row count tracking (`@@ROWCOUNT`) → BigQuery scripting logic
   - String concatenation and date functions mapping

   **Unit Testing Effort:**
   - Data quality checks: validate removal of invalid records (missing Customer_ID, invalid Quantity)
   - Joins and enrichment: verify correct mapping from dimension tables
   - Calculation: ensure Total_Sales_Amount is correct
   - Audit and DQ failure logging: confirm correct entries and error handling
   - Staging table truncation and resource cleanup
   - End-to-end batch test: validate counts, error paths, and logging

   **Effort Estimation Table:**

   | Activity                            | Effort (Hours) |
   |--------------------------------------|----------------|
   | Temp Table Conversion                | 2              |
   | Variable/Script Logic Mapping        | 1.5            |
   | Error Handling Rewrite               | 1.5            |
   | Audit Logging Adaptation             | 1              |
   | DELETE/MERGE Logic                   | 1.5            |
   | Row Count/Message Logic              | 1              |
   | Testing DQ Checks                    | 2              |
   | Testing Joins/Enrichment             | 1.5            |
   | Testing Calculations                 | 1              |
   | Testing Audit/DQ Logging             | 1              |
   | End-to-End Batch Test                | 2              |
   | Regression/Edge Case Testing         | 1.5            |
   | Documentation/Metadata Update        | 1              |
   | **Total Estimated Effort**           | **18 hours**   |

   **Effort Rationale:**
   - Moderate complexity due to multiple ETL steps, DQ logic, and audit/error handling.
   - Each area requires both code conversion and targeted unit testing.
   - End-to-end and regression testing are critical for data quality and compliance.

---

**Summary Table**

| Item                        | Value/Estimate         |
|-----------------------------|-----------------------|
| BigQuery Runtime Cost/Run    | $1.00–$2.50 USD       |
| API Cost (for analysis)      | 0.0125 USD            |
| Manual Code Fixing Effort    | 9.5 hours             |
| Unit/Integration Testing     | 8.5 hours             |
| **Total Effort**             | **18 hours**          |

---

**Notes:**
- Metadata headers should be updated in all converted scripts as per the required format.
- All business logic and audit requirements must be validated after migration.
- Cost and effort estimates are based on current data volumes and moderate complexity; adjust for future scale or additional requirements.

apiCost: 0.0125 USD