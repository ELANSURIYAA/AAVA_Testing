=============================================
Author:        Ascendion AVA
Created on:   
Description:   Effort and cost estimation for migrating and testing the ETL stored procedure 'dw.sp_load_sales_fact' from Azure Synapse to GCP BigQuery, including manual code fixes, unit/data recon testing, and BigQuery runtime cost analysis.
=============================================

1. Cost Estimation

   1.1 BigQuery Runtime Cost

   **Calculation Breakup:**
   - BigQuery On-Demand Query Pricing: $5.00 per TB processed (from env_variable.txt)
   - Source Table Sizes:
     - stg.Sales_Transactions: ~700 GB
     - dw.Dim_Customer: ~150 GB
     - dw.Dim_Date: ~100 GB
     - dw.Fact_Sales (Target): ~1 TB
     - Total Storage: ~1.95 TB
   - Estimated Data Processed Per Run: 10% of total (~200–500 GB/query)
   - Cost Per Run: $1.00–$2.50 (as per GCP estimate)
   - For one full ETL batch run:
     - Lower bound: 200 GB * $5 / 1024 = ~$0.98
     - Upper bound: 500 GB * $5 / 1024 = ~$2.44
   - **Assumption:** Each ETL run processes 200–500 GB (10% of total), matching the GCP estimate.
   - **Number of Queries:** The procedure logic is encapsulated in a single batch ETL query, with possible additional queries for audit and DQ logging.
   - **Total Estimated Cost per Run:** **$1.00 – $2.50 USD**
   - **Reasons:** Cost is driven by the volume of data scanned (input tables), not by the number of output rows. Use of CTEs, temp tables, and joins will not significantly increase cost unless they cause repeated full scans.

   **apiCost: 0.0125 USD**

2. Code Fixing and Testing Effort Estimation

   2.1 BigQuery Identified Manual Code Fixes and Unit Testing Effort

   **Manual Code Fixes (excluding pure syntax translation):**
   - Replace T-SQL error handling (TRY/CATCH, THROW) with BigQuery scripting blocks and/or external orchestration.
   - Audit log and DQ failure logging must be explicitly coded in BigQuery scripting (no implicit variable assignment).
   - Temporary table logic (#InvalidRows) must be replaced with CREATE TEMP TABLE or CTEs.
   - Batch ID and procedure name assignment must be handled via scripting variables.
   - Row count tracking (@@ROWCOUNT) must be replaced with SELECT COUNT(*) or ROW_COUNT().
   - Remove/rewrite OBJECT_NAME(@@PROCID) logic (hardcode or pass as parameter).
   - Ensure all references to system variables and SQL Server-specific metadata are removed or replaced.
   - Validate and adjust all data type mappings (e.g., UNIQUEIDENTIFIER → STRING, DATETIME → TIMESTAMP).
   - Review and adjust joins and enrichment logic for BigQuery compatibility.
   - Test and validate all transformations, especially calculation of Total_Sales_Amount and enrichment columns.

   **Testing Effort (Unit & Data Reconciliation):**
   - Develop and execute unit tests for each transformation step (validation, calculation, joins, enrichment).
   - Perform data reconciliation between Synapse and BigQuery outputs for Fact_Sales, Audit_Log, and DQ_Failures.
   - Validate audit logging and error handling in the new environment.
   - Test full ETL batch run, including edge cases (missing Customer_ID, invalid Quantity, error scenarios).
   - Regression test with production-scale data volumes (200–500 GB per run).

   **Effort Estimate Table:**

   | Task                                               | Estimated Hours |
   |----------------------------------------------------|-----------------|
   | Manual code fixes (logic, variables, error handling)| 8               |
   | Temp table/CTE conversion                          | 2               |
   | Audit/DQ logging adjustment                        | 2               |
   | Data type mapping and validation                   | 2               |
   | Unit test development (all transformations)        | 4               |
   | Data reconciliation (Fact, Audit, DQ tables)       | 4               |
   | Regression testing (full batch, edge cases)        | 4               |
   | Documentation and review                           | 2               |
   | **Total**                                          | **28 hours**    |

   **Breakdown:**
   - Manual code fixes: 14 hours (core logic, temp tables, audit/DQ logging, data types)
   - Testing (unit, data recon, regression): 12 hours
   - Documentation/review: 2 hours

   **Assumptions:**
   - The migration is a direct refactor (not a full redesign).
   - All required tables exist in BigQuery with compatible schemas.
   - No advanced orchestration or external dependencies outside the ETL logic.
   - Data volumes and complexity are as described.

---

**Summary Table**

| Category                | Estimate                |
|-------------------------|------------------------|
| BigQuery Runtime Cost   | $1.00 – $2.50 per run  |
| Manual Code Fix Effort  | 14 hours               |
| Testing Effort          | 12 hours               |
| Documentation/Review    | 2 hours                |
| **Total Effort**        | **28 hours**           |
| API Cost (for this call)| 0.0125 USD             |

---

**Notes:**
- The cost and effort estimates are for a single ETL batch migration and test cycle. For ongoing production, multiply runtime cost by expected frequency.
- The effort estimate is for an experienced data engineer familiar with both Synapse and BigQuery.
- All syntax-only changes (e.g., function names) are excluded from manual effort, as per instructions.

apiCost: 0.0125 USD