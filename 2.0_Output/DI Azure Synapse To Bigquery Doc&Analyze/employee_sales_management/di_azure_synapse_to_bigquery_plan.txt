=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads cleaned and validated sales transactions from staging into the sales fact table, applying business rules, audit logging, and data quality checks.
=============================================

1. Cost Estimation

   2.1 BigQuery Runtime Cost

   **BigQuery On-Demand Query Pricing:** $5.00 per TB processed

   **Table Sizes:**
   - stg.Sales_Transactions: ~700 GB
   - dw.Dim_Customer: ~150 GB
   - dw.Dim_Date: ~100 GB
   - dw.Fact_Sales (Target Table): ~1 TB
   - Total Estimated Storage Used: ~1.95 TB

   **Estimated Data Processed Per Run:** 10% of total (~200–500 GB/query)

   **Cost Calculation:**
   - Data processed per run: 200–500 GB (0.2–0.5 TB)
   - Cost per run = Data Processed (TB) × $5.00
     - Minimum: 0.2 TB × $5.00 = $1.00
     - Maximum: 0.5 TB × $5.00 = $2.50

   **Breakdown & Reasons:**
   - The main cost driver is the volume of data scanned by BigQuery during ETL (joins, DQ checks, and inserts).
   - Temporary tables and intermediate calculations are handled in-memory and do not incur additional storage costs, but all data scanned (including joins with dimension tables) counts toward the query cost.
   - The cost is per query execution; if multiple test runs or iterations are needed, multiply accordingly.

   **Estimated Cost per Full ETL Run:** $1.00 – $2.50 USD

2. Code Fixing and Testing Effort Estimation

   2.1 BigQuery Identified Manual Code Fixes and Unit Testing Effort (in hours)

   **Manual Code Fixes Required:**
   - Audit logging logic: Refactor to explicit BigQuery scripting steps (1 hour)
   - Error handling: Convert TRY/CATCH to BigQuery EXCEPTION block and manual error logging (1 hour)
   - Temporary table conversion: Refactor #InvalidRows to CREATE TEMP TABLE and manage scope (0.5 hour)
   - Variable declarations and assignments: Convert all procedural variables to BigQuery DECLARE/SET (0.5 hour)
   - Row count tracking: Replace @@ROWCOUNT with SELECT COUNT(*) or scripting logic (0.5 hour)
   - Truncate logic: Replace TRUNCATE TABLE with DELETE FROM or CREATE OR REPLACE TABLE (0.5 hour)
   - Metadata header update: Apply standard metadata header to the converted script (0.25 hour)
   - Test and validate all business logic and joins for correctness in BigQuery (1.5 hours)
   - Data quality failure logging: Ensure correct mapping and insert into DQ_Failures (0.5 hour)

   **Unit Testing and Data Reconciliation:**
   - Prepare test data in staging and dimension tables (0.5 hour)
   - Execute ETL and validate row counts, transformation logic, and error handling (1 hour)
   - Validate audit log and DQ_Failures entries (0.5 hour)
   - Compare output with expected results for at least 2–3 test scenarios (1 hour)
   - Review and document test results (0.5 hour)

   **Total Estimated Effort:**
   - Manual Code Fixes: 5.75 hours
   - Unit Testing & Data Reconciliation: 3.5 hours
   - **Total Effort:** ~9.25 hours

   **Effort Breakdown:**
   - The effort covers all manual intervention areas identified in the analysis (excluding pure syntax conversion).
   - Includes time for code refactoring, BigQuery-specific scripting, and comprehensive unit/data reconciliation testing.
   - Assumes moderate familiarity with both Synapse and BigQuery scripting paradigms.

---

**apiCost: 0.0125 USD**