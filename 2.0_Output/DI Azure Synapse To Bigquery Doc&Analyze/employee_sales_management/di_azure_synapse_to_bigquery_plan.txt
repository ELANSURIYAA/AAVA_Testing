=============================================
Author:        Ascendion AVA
Created on:   
Description:   Effort and cost estimation for migrating and testing the ETL process that loads, validates, and audits sales transactions from staging into the sales fact table in BigQuery, converted from Azure Synapse.
=============================================

1. Cost Estimation

   1.1 BigQuery Runtime Cost

   **Data Processed per Run:**
   - stg.Sales_Transactions: ~700 GB
   - dw.Dim_Customer: ~150 GB
   - dw.Dim_Date: ~100 GB
   - dw.Fact_Sales (Target Table): ~1 TB
   - Total Estimated Storage Used: ~1.95 TB

   **Estimated Data Processed per Query Run:** 10% of total = ~200–500 GB/query

   **BigQuery On-Demand Pricing:** $5.00 per TB processed

   **Calculation:**
   - Lower bound: 200 GB = 0.2 TB × $5.00 = $1.00 per run
   - Upper bound: 500 GB = 0.5 TB × $5.00 = $2.50 per run

   **Assumptions:**
   - Each ETL/test run processes 200–500 GB (based on join/filter logic and table scan estimates).
   - Cost includes all DML (validation, joins, inserts, deletes, truncates, logging).
   - No additional cost for storage or streaming inserts is included (query cost only).
   - Multiple test iterations will multiply this cost.

   **BigQuery Runtime Cost per Test Run:** **$1.00 – $2.50 USD**

   **If 5 test runs are performed:** $5.00 – $12.50 USD total

   **apiCost:** 0.0125 USD (for this estimation API call)

---

2. Code Fixing and Testing Effort Estimation

   2.1 BigQuery Identified Manual Code Fixes and Unit Testing Effort

   **Manual Code Fixes Required:**
   - Refactor temporary table logic (`#InvalidRows`) to BigQuery scripting (DECLARE/CREATE TEMP TABLE or ARRAY).
   - Rewrite audit logging and error handling using BigQuery scripting (`BEGIN ... EXCEPTION ... END`).
   - Replace SQL Server system functions:
     - `SYSDATETIME()` → `CURRENT_TIMESTAMP()`
     - `NEWID()` → `GENERATE_UUID()`
     - `OBJECT_NAME(@@PROCID)` → Hardcode procedure name or use parameter.
     - `@@ROWCOUNT` → Use `ROW_COUNT()` in BigQuery scripting.
   - Adjust variable declarations and assignments.
   - Review and adapt table truncation and DML syntax.
   - Validate and adapt joins and data type conversions (e.g., DATETIME to TIMESTAMP).
   - Review and update audit log and DQ failure table schema compatibility.

   **Testing Effort:**
   - Unit test all validation logic (missing Customer_ID, invalid Quantity).
   - Data reconciliation: Ensure all valid records are loaded, invalid records are logged.
   - Validate audit log and DQ failure log entries.
   - Test error handling and pipeline monitoring.
   - Validate all transformations (joins, calculations, metadata tagging).
   - At least 2–3 test iterations for each logic branch (success, validation fail, error).

   **Effort Estimation Table:**

   | Task/Area                                  | Estimated Effort (hrs) |
   |--------------------------------------------|-----------------------|
   | Manual code refactoring (temp tables,      |                       |
   | error handling, system functions, DML)     | 6                     |
   | Schema review and adjustments              | 1                     |
   | Unit test case design & implementation     | 2                     |
   | Data reconciliation & validation           | 2                     |
   | Audit log and DQ failure log validation    | 1                     |
   | Error handling & pipeline monitoring tests | 1                     |
   | Test execution (3 full cycles)             | 3                     |
   | **Total**                                 | **16 hours**          |

   **Summary:**
   - **Manual code fixes & migration:** ~7 hours
   - **Testing (unit, data recon, logs, errors, execution):** ~9 hours
   - **Total Estimated Effort:** **16 hours**

---

**apiCost:** 0.0125 USD

---