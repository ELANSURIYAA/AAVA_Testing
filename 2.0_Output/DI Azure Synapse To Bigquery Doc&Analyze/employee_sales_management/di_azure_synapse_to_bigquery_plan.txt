=============================================
Author:        Ascendion AVA
Created on:    
Description:   Loads, validates, and transforms sales transaction data from staging to the sales fact table, with audit logging and data quality checks.
=============================================

1. Cost Estimation

   1.1 BigQuery Runtime Cost

   **Calculation Breakup:**
   - BigQuery On-Demand Query Pricing: $5.00 per TB processed.
   - Source Tables: 
     - stg.Sales_Transactions: ~700 GB
     - dw.Dim_Customer: ~150 GB
     - dw.Dim_Date: ~100 GB
     - dw.Fact_Sales (Target): ~1 TB
   - Estimated Data Processed Per Run: 10% of total (~1.95 TB) = ~200–500 GB/query.
   - Cost per run: 
     - 200 GB processed: (200/1024) TB * $5 = ~$0.98
     - 500 GB processed: (500/1024) TB * $5 = ~$2.44
   - **Estimated Cost Per Run:** $1.00 – $2.50 (rounded for operational overhead and possible variance in query optimization).
   - **Reasoning:** The cost is determined by the amount of data scanned by each query, which includes joining and filtering source and dimension tables, as well as writing to the target table. Temporary tables and multiple DML operations in BigQuery scripting may increase the processed data slightly, but the main driver is the data volume in the SELECT/INSERT operations.

2. Code Fixing and Testing Effort Estimation

   2.1 BigQuery Identified Manual Code Fixes and Unit Testing Effort (in hours):

   **Manual Code Fixes:**
   - Temporary table logic (`#InvalidRows`) → Rewrite using BigQuery scripting temporary tables or arrays/structs: **2 hours**
   - Error handling (`TRY...CATCH`) → Map to BigQuery scripting `EXCEPTION` blocks: **1 hour**
   - Audit log and DQ failure inserts split due to procedural limitations: **1 hour**
   - Replace `OBJECT_NAME(@@PROCID)` and `@@ROWCOUNT` with BigQuery equivalents: **1 hour**
   - Data type conversions (UNIQUEIDENTIFIER, DATETIME, NVARCHAR): **1 hour**
   - General validation of business logic mapping (joins, calculations, enrichment): **2 hours**

   **Unit Testing & Data Recon:**
   - Develop and run unit tests for each transformation and validation step (including temp table logic, joins, and calculations): **3 hours**
   - Data reconciliation between Synapse output and BigQuery output for a sample batch: **2 hours**
   - Audit and DQ log verification: **1 hour**

   **Total Estimated Effort:**  
   - Manual Code Fixes: **8 hours**
   - Testing & Data Recon: **6 hours**
   - **Grand Total:** **14 hours**

   **Breakdown by Activity:**
   - Manual code refactoring for BigQuery scripting: 6 hours
   - Data type and control logic adjustments: 2 hours
   - Unit test development and execution: 3 hours
   - Data reconciliation and audit/DQ log validation: 3 hours

---

**apiCost: 0.0100 USD**

---

**Summary Table**

| Item                                 | Estimate                  |
|-------------------------------------- |--------------------------|
| BigQuery Runtime Cost (per run)       | $1.00 – $2.50 USD        |
| Manual Code Fixes (hours)             | 8                        |
| Unit Testing & Data Recon (hours)     | 6                        |
| **Total Effort (hours)**              | **14**                   |
| API Cost for this call                | 0.0100 USD               |

**Notes:**
- The cost and effort estimates are based on the provided table sizes, query logic, and typical BigQuery pricing. Actual costs may vary depending on query optimization and data growth.
- Effort hours are based on a moderately experienced data engineer familiar with both Synapse and BigQuery environments.