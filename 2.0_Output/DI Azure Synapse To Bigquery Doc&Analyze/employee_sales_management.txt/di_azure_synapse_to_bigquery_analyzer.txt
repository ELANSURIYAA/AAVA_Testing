=============================================
Author:        Ascendion AVA
Date:   
Description:   Analysis and conversion guidance for Azure Synapse stored procedure dw.sp_load_sales_fact to BigQuery SQL, including complexity metrics, syntax differences, manual adjustments, and optimization techniques.
=============================================

---

**1. Procedure Overview**

The stored procedure `dw.sp_load_sales_fact` orchestrates the ETL workflow for loading sales transaction data from the staging area (`stg.Sales_Transactions`) into the sales fact table (`dw.Fact_Sales`). It performs data quality checks, applies business logic transformations, logs audit information, and manages validation failures. The business objective is to ensure only high-quality, validated sales data is loaded for analytics and reporting, supporting reliable downstream decision-making.

- **Mappings per workflow/session:** 1 main mapping (staging to fact table), with supporting mappings for audit and data quality logs.
- **Key business objective:** Data integration, cleansing, enrichment, and audit logging for sales transactions.

---

**2. Complexity Metrics**

| Metric                        | Value & Type                                                                 |
|-------------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (stg.Sales_Transactions, SQL Server)                                       |
| Number of Transformations     | 5 (validation, enrichment, calculation, timestamp, batch tracking)           |
| Lookup Usage                  | 2 (joins with dw.Dim_Customer and dw.Dim_Date; both connected lookups)       |
| Expression Logic              | 2 (Total_Sales_Amount calculation, SYSDATETIME, NEWID for batch)             |
| Join Conditions               | 2 (INNER JOIN: Customer_ID, INNER JOIN: Sales_Date/Date_Value)               |
| Conditional Logic             | 2 (WHERE clauses for validation, error handling via TRY/CATCH)               |
| Reusable Components           | 0 (no reusable transformations or mapplets)                                  |
| Data Sources                  | 3 (stg.Sales_Transactions [SQL Server], dw.Dim_Customer [SQL Server], dw.Dim_Date [SQL Server]) |
| Data Targets                  | 3 (dw.Fact_Sales [SQL Server], dw.Audit_Log [SQL Server], dw.DQ_Failures [SQL Server]) |
| Pre/Post SQL Logic            | 1 (audit log updates, error handling, batch tracking)                        |
| Session/Workflow Controls     | 1 (TRY/CATCH for error handling)                                             |
| DML Logic                     | 4 (INSERT, DELETE, TRUNCATE, UPDATE)                                         |
| Complexity Score (0–100)      | 65                                                                           |

**High-complexity areas:**
- Data validation with temporary tables
- Multiple joins for enrichment
- Audit and error handling logic
- Batch and timestamp tracking

---

**3. Syntax Differences**

- **Functions without direct BigQuery equivalents:**
  - `SYSDATETIME()` → Use `CURRENT_TIMESTAMP()` in BigQuery.
  - `NEWID()` → Use `GENERATE_UUID()` in BigQuery.
  - `TRY/CATCH` block → BigQuery does not support procedural error handling; must use scripting constructs or handle errors via query logic.
  - Temporary tables (`#InvalidRows`) → BigQuery does not support session-scoped temp tables; use CTEs or staging tables.
  - `@@ROWCOUNT` → Use `COUNT(*)` or array_length for affected rows in BigQuery.

- **Data type conversions:**
  - `DATETIME` (SQL Server) → `TIMESTAMP` (BigQuery)
  - `NVARCHAR` → `STRING`
  - `BIGINT` → `INT64`

- **Workflow/control logic:**
  - Audit logging and batch tracking must be implemented via scripting or separate queries.
  - Error handling (TRY/CATCH) must be refactored using BigQuery scripting (BEGIN...EXCEPTION...END) or handled externally.

---

**4. Manual Adjustments**

- **Components requiring manual implementation:**
  - Temporary table logic (`#InvalidRows`) should be replaced with CTEs or intermediate tables in BigQuery.
  - Audit log updates and error handling must be restructured for BigQuery scripting.
  - Batch ID generation and timestamping need to use BigQuery functions.
  - DML operations (DELETE, TRUNCATE) must be adapted to BigQuery syntax and permissions.

- **External dependencies:**
  - Audit logging and data quality failure logging may require separate BigQuery tables and scripts.
  - Any downstream processes relying on SQL Server-specific features (e.g., triggers, extended properties) must be reviewed.

- **Business logic review areas:**
  - Validation rules (e.g., missing Customer_ID, invalid Quantity) should be confirmed for completeness.
  - Joins for enrichment must be validated for referential integrity in BigQuery.
  - Data type mappings and null-handling should be tested post-migration.

---

**5. Optimization Techniques**

- **Partitioning:** Partition target tables (e.g., `dw.Fact_Sales`) by date or batch ID for efficient querying.
- **Broadcast joins:** Use BigQuery's broadcast join optimization for small dimension tables (`dw.Dim_Customer`, `dw.Dim_Date`).
- **Pipeline filters/joins:** Convert chained WHERE clauses and joins into a single pipeline using CTEs.
- **Window functions:** Use window functions to simplify aggregations if needed.
- **Caching:** Use materialized views for frequently accessed dimension tables.
- **Refactor vs. Rebuild:** Recommend **Refactor** if the current logic is sufficient and performance is acceptable; **Rebuild** if BigQuery's native features (e.g., partitioned tables, streaming inserts) can provide significant optimization.

---

**Additional Notes:**

- **BigQuery Cost Considerations:** Estimated per-run query cost is $1.00–$2.50, with ~200–500 GB processed per run. Monitor query costs and optimize data scans.
- **Session/Workflow Controls:** BigQuery scripting can implement batch control, but lacks direct equivalents for SQL Server's procedural constructs.
- **Audit and Data Quality:** Ensure audit and DQ tables are migrated and compatible with BigQuery's data model.

---

This document provides a comprehensive analysis and conversion guidance for migrating the Azure Synapse stored procedure `dw.sp_load_sales_fact` to BigQuery SQL, addressing all transformation patterns, constraints, blockers, and automation feasibility.