```
=============================================
Author:        AAVA
Created on:   
Description:   Loads the FACT_EXECUTIVE_SUMMARY table with summarized holding metrics from staging, applying business rules, data quality checks, and referential integrity via joins to dimension tables.
=============================================

# 1. Procedure Overview

This Azure Synapse stored procedure (`dbo.LOAD_FACT_EXECUTIVE_SUMMARY`) loads the `FACT_EXECUTIVE_SUMMARY` fact table from the staging table `STG_HOLDING_METRICS`. It performs data quality validation, applies business rule checks (such as normalizing income amounts), and ensures referential integrity by joining to dimension tables (`DIM_DATE`, `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`). The procedure supports regulatory reporting and advanced analytics by summarizing holdings data for downstream consumption.

**Key Business Objective:**  
Data integration, cleansing, enrichment, transformation, and reporting for executive summary analytics.

---

# 2. Complexity Metrics

| Metric                                   | Description                                                                 | Value/Details                      |
|-------------------------------------------|-----------------------------------------------------------------------------|------------------------------------|
| Number of Input Tables                    | Count of distinct source tables used in the procedure.                      | 1 (`STG_HOLDING_METRICS`)          |
| Number of Output Tables                   | Count of target or intermediate tables modified or populated.                | 1 (`FACT_EXECUTIVE_SUMMARY`)       |
| Variable Declarations                     | Number of declared variables and their usage complexity.                    | 2 (`@v_row_count`, `@error_message`) - simple usage |
| Conditional Logic                        | Number of IF, CASE, or nested conditional blocks.                           | 2 (IF for temp table drop, CASE for income_amount) |
| Loop Constructs                           | Number of WHILE or FOR loops, if present.                                   | 0                                  |
| Join Conditions                           | Count and types of joins (INNER, LEFT, RIGHT, FULL).                        | 4 INNER JOINs                      |
| Aggregations                              | Number of aggregation operations (SUM, COUNT, AVG, etc.).                   | 0                                  |
| Subqueries / CTEs                         | Number of subqueries or Common Table Expressions used.                      | 0                                  |
| Procedural Calls                          | Number of stored procedure or function invocations.                         | 0                                  |
| DML Operations                            | Frequency of INSERT, UPDATE, DELETE, MERGE operations.                      | 1 INSERT, 2 SELECT INTO, 2 DROP TABLE |
| Temporary Tables / Table Variables         | Number of temp tables or table variables created and used.                  | 1 temp table (`#staging_metrics`)  |
| Transaction Handling                      | Count of BEGIN TRAN, COMMIT, ROLLBACK statements.                           | 0                                  |
| Error Handling Blocks                     | Presence and count of TRY...CATCH logic.                                    | 0                                  |
| Complexity Score (0–100)                  | Based on nested logic, control flow, DML count, and procedural depth.       | 35                                 |

**High-Complexity Areas:**
- Use of temporary table for staging data.
- Multiple joins to dimension tables for referential integrity.
- Conditional logic for data normalization (CASE for `income_amount`).
- Minimal procedural control flow; no dynamic SQL or nested blocks.

---

# 3. Syntax Differences

**T-SQL Constructs Without Direct Databricks Declarative SQL Equivalents:**
- **Variable Declarations (DECLARE, SET):**  
  Not supported in Databricks declarative SQL. Must be replaced with CTEs or derived columns.
- **PRINT Statements:**  
  Not supported; use logging mechanisms outside SQL or notebook markdown for status reporting.
- **Temporary Tables (`#staging_metrics`):**  
  Not directly supported. Use CTEs or views to stage data in Databricks SQL.
- **IF OBJECT_ID... DROP TABLE:**  
  Not supported. Table existence checks and drops must be handled outside SQL or with error-tolerant logic.
- **Procedural Blocks (BEGIN...END, GO):**  
  Databricks SQL is declarative; procedural segmentation is not supported.
- **@@ROWCOUNT:**  
  Not available. Row counts must be derived via queries or notebook logic.

**Necessary Syntax Changes:**
- Replace variable logic with CTEs or SELECT statements.
- Replace temp tables with CTEs or inline views.
- Remove or externalize PRINT/logging.
- Remove procedural IF checks; manage table lifecycle externally.
- Remove or refactor procedural blocks.

**Data Type Conversions:**
- Validate types such as `DATETIME` (Synapse) → `TIMESTAMP` (Databricks).
- Ensure numeric/decimal precision matches between platforms.

---

# 4. Manual Adjustments

**Components Requiring Manual Implementation:**
- **Variable Logic:**  
  All variable assignments (e.g., row count, error message) must be rewritten as queries or handled in orchestration (e.g., notebooks or external scripts).
- **Audit Logging:**  
  PRINT statements and row count reporting must be implemented outside SQL, such as in Databricks notebooks or orchestration pipelines.
- **Temporary Table Logic:**  
  The temp table `#staging_metrics` must be replaced with a CTE or view. If intermediate materialization is required, use a managed table or view.
- **Table Existence Checks and Drops:**  
  Must be handled outside SQL or with orchestration tools, as Databricks SQL does not support IF OBJECT_ID or DROP TABLE IF EXISTS for temp tables.
- **Error Handling:**  
  No TRY...CATCH logic present, but any error handling must be implemented at the orchestration level.
- **Business Rule Validation:**  
  The CASE logic for `income_amount` must be validated post-conversion to ensure parity.

**External Dependencies:**
- None detected (no external function calls or system procedures).

---

# 5. Optimization Techniques

**Databricks Declarative SQL Best Practices:**
- **Replace Temporary Tables with CTEs:**  
  Use sequential CTEs to stage and transform data instead of temp tables.
- **Simplify Logic:**  
  Move all conditional logic (e.g., income normalization) into SELECT/CTE expressions.
- **Optimize Joins:**  
  Ensure join predicates are selective and leverage predicate pushdown.
- **Reduce Redundant Materialization:**  
  Avoid unnecessary intermediate tables; chain transformations in a single query.
- **Audit Logging:**  
  If row counts are needed, use a SELECT COUNT(*) after the INSERT, handled outside SQL.
- **Partition Pruning:**  
  Where possible, filter source data early to minimize data scanned in joins.

---

# 6. API Cost Consumption

```
apiCost: 0.0070 USD