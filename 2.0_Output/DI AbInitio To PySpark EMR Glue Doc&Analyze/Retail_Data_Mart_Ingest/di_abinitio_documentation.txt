```markdown
====================================================
Author:        AAVA
Date:          2024-06-13
Description:   Documentation for Ab Initio Graph: Retail_Data_Mart_Ingest
====================================================

# 1. Overview of Graph/Component

The `Retail_Data_Mart_Ingest` Ab Initio graph orchestrates the end-to-end ingestion, cleansing, enrichment, and aggregation of retail transaction data. The primary business requirement is to process large volumes of raw transaction logs, validate and enrich them with product dimension data, apply pricing rules, and produce both summary and error datasets for downstream analytics and reporting. The graph ensures high data quality, traceability, and robust error handling for a retail data mart.

# 2. Component Structure and Design

**Component Layout & Logical Grouping:**
- **Input Components:**
  - `Read_AWS_S3`: Reads raw transaction data from S3.
  - `Read_Product_Dim`: Reads product dimension lookup file.
- **Transformation Components:**
  - `Cleanse_Data` (Reformat): Applies cleansing and validation logic.
  - `Dedup_Transactions` (Dedup Sort): Removes duplicate transactions based on `txn_id`.
  - `Enrichment_Join` (Join): Enriches transactions with product info via inner join on `product_sku`.
  - `Apply_Pricing` (Reformat): Applies pricing rules and calculations.
  - `Sort_for_Rollup` (Sort): Sorts data by `store_id` and `txn_date` for aggregation.
  - `Store_Aggregation` (Rollup): Aggregates data at the store and date level.
- **Output Components:**
  - `Write_Summary`: Writes the final daily summary.
  - `Write_Cleanse_Rejects`: Writes records rejected during cleansing.
  - `Write_Product_Misses`: Writes records missing product lookup.

**Connection Flow & Parameters:**
- Data flows sequentially from raw input through cleansing, deduplication, enrichment, pricing, sorting, and aggregation to final outputs.
- Parameters and variables are used for file paths, DML/XFR references, and error log locations.

# 3. Data Flow and Processing Logic

**Key Data Sources:**
- `transactions_raw.dat` (Input, S3)
- `product_dim.dat` (Input, local/project directory)

**Intermediate Files:**
- Cleaned and enriched transaction datasets (in-memory or temp files)
- Reject/error logs

**Final Outputs:**
- `daily_summary.dat` (Aggregated output)
- `cleanse_rejects.dat` (Error log)
- `product_misses.dat` (Product lookup misses)

**Logical Steps:**
- **Read_AWS_S3:** Ingests raw transactions using `retail_txn_raw.dml`.
- **Read_Product_Dim:** Ingests product dimension using `retail_product_dim.dml`.
- **Cleanse_Data:** Applies `cleanse_validate.xfr` for field validation, outputs enriched records (`retail_txn_enriched.dml`) and rejects.
- **Dedup_Transactions:** Deduplicates on `txn_id`.
- **Enrichment_Join:** Inner join on `product_sku`, adds `category` and `standard_cost` from dimension.
- **Apply_Pricing:** Applies `pricing_rules.xfr` for pricing logic.
- **Sort_for_Rollup:** Sorts by `store_id`, `txn_date`.
- **Store_Aggregation:** Aggregates using `store_rollup.xfr`, outputs `retail_store_summary.dml`.
- **Write_Summary:** Outputs final summary.
- **Write_Cleanse_Rejects:** Outputs cleansing rejects.
- **Write_Product_Misses:** Outputs product lookup misses.

# 4. Data Mapping (Lineage)

| Target Table           | Target Column   | Source Table        | Source Column    | Remarks                                      |
|------------------------|-----------------|---------------------|------------------|----------------------------------------------|
| daily_summary.dat      | store_id        | transactions_raw    | store_id         | 1:1 Mapping                                 |
| daily_summary.dat      | txn_date        | transactions_raw    | txn_date         | 1:1 Mapping                                 |
| daily_summary.dat      | total_sales     | transactions_raw    | amount           | Transformation (aggregation: SUM(amount))    |
| daily_summary.dat      | txn_count       | transactions_raw    | txn_id           | Transformation (aggregation: COUNT(txn_id))  |
| daily_summary.dat      | category        | product_dim         | category         | Enrichment via Join                          |
| daily_summary.dat      | standard_cost   | product_dim         | standard_cost    | Enrichment via Join                          |
| cleanse_rejects.dat    | *               | transactions_raw    | *                | Validation (rejected by cleanse_validate.xfr)|
| product_misses.dat     | *               | transactions_raw    | *                | Validation (no matching product_sku)         |

# 5. Transformation Logic

- **cleanse_validate.xfr:** Validates required fields, checks data types, and applies business validation rules. Fields involved: all transaction fields.
- **pricing_rules.xfr:** Applies pricing logic, discounts, and computes final sales amounts. Fields involved: amount, product/category, etc.
- **store_rollup.xfr:** Aggregates transactions by `store_id` and `txn_date`, computes total sales and transaction counts.

# 6. Complexity Analysis

- **Number of Graph Components:** 11
- **Number of Lines of Code (.mp):** ~700+ (as per verbose mapping and structure)
- **Transform Functions Used:** 3 (.xfr files: cleanse_validate, pricing_rules, store_rollup)
- **Joins Used:** 1 (Inner Join on product_sku)
- **Lookup Files or Datasets:** 1 (product_dim)
- **Parameter Sets (.pset) or Plan Files Used:** 1 (parameter set in graph)
- **Number of Output Datasets:** 3 (summary, cleanse rejects, product misses)
- **Conditional Logic or if-else flows:** Present in .xfr (at least 2: cleanse, pricing)
- **External Dependencies:** S3 (AWS), DML/XFR files, shell variables for paths
- **Overall Complexity Score:** 70 (on a scale of 0â€“100; moderately complex ETL with robust error handling and multiple transformations)

# 7. Key Outputs

- **daily_summary.dat:** Delimited file, aggregated summary for each store and date, used for reporting and downstream analytics.
- **cleanse_rejects.dat:** Delimited file, contains transactions rejected during cleansing/validation, used for error analysis.
- **product_misses.dat:** Delimited file, transactions with missing product dimension, used for data quality monitoring.

# 8. Error Handling and Logging

- **Reject Components:** `Write_Cleanse_Rejects` captures records rejected during cleansing.
- **Error Tagging:** .xfr-based error tagging in cleanse_validate.xfr.
- **Reject Thresholds:** Not explicitly defined, but all rejects are logged.
- **Control File Usage:** Error log path parameterized for traceability.
- **Handling:** Errors are written to reject files; no auto-abort, but all rejected records are available for review.

# 9. API Cost (LLM Cost ONLY)

- **Tokens Used (Prompt + Completion):** _[To be filled by LLM system, e.g., 9500]_
- **Cost per 1K tokens:** $0.003 (example for GPT-3.5-turbo)
- **Final Cost in USD for this single documentation run:** _[Tokens/1000 * Rate, e.g., $0.0285]_

---

> **Note:** This documentation is generated based on the actual Ab Initio .mp graph structure and parameters provided. For full traceability, please refer to the original .xfr and .dml files for transformation and schema details.
```