```markdown
====================================================
Author:        AAVA
Date:          2024-06-11
Description:   Ab Initio graph for ingesting, cleansing, enriching, and aggregating retail transaction data into a daily summary data mart.
====================================================

# Retail_Data_Mart_Ingest

## 1. Overview of Graph/Component

This Ab Initio graph ingests raw retail transaction data from AWS S3, applies cleansing and validation, enriches the data with product dimension information, applies pricing rules, aggregates the results by store and date, and writes the final daily summary and error logs to output datasets. The business requirement addressed is to automate the end-to-end ETL pipeline for building a retail data mart, ensuring data quality, enrichment, and efficient aggregation for downstream analytics and reporting.

## 2. Component Structure and Design

The graph is logically structured as follows:

- **Input Components:**
  - `Read_AWS_S3`: Reads raw transaction data from S3 (`transactions_raw.dat`).
  - `Read_Product_Dim`: Reads product dimension data from local/project directory.

- **Transformation Components:**
  - `Cleanse_Data` (Reformat): Applies cleansing and validation logic using `cleanse_validate.xfr`.
  - `Dedup_Transactions` (Dedup Sort): Removes duplicate transactions based on `txn_id`.
  - `Enrichment_Join` (Join): Inner join between cleansed transactions and product dimension on `product_sku`.
  - `Apply_Pricing` (Reformat): Applies pricing rules using `pricing_rules.xfr`.
  - `Sort_for_Rollup` (Sort): Sorts data on `store_id` and `txn_date` for aggregation.
  - `Store_Aggregation` (Rollup): Aggregates data by store and date using `store_rollup.xfr`.

- **Output Components:**
  - `Write_Summary`: Writes the final daily summary to `daily_summary.dat`.
  - `Write_Cleanse_Rejects`: Writes rejected records from cleansing to `cleanse_rejects.dat`.
  - `Write_Product_Misses`: Writes records missing product lookup to `product_misses.dat`.

- **Connection Flow:**
  - Data flows sequentially from input, through cleansing, deduplication, enrichment, pricing, sorting, aggregation, and finally to output.
  - Reject and error flows are handled in parallel paths.
  - Parameters and variables are used for file paths, DML, and XFR references.

## 3. Data Flow and Processing Logic

**Processed Datasets:**
- Input: `transactions_raw.dat`, `product_dim.dat`
- Intermediate: Cleansed, deduplicated, enriched, and priced datasets
- Output: `daily_summary.dat`, `cleanse_rejects.dat`, `product_misses.dat`

**Data Flow:**
1. **Ingestion:** Raw transactions are read from S3; product dimension is read from project directory.
2. **Cleansing:** Transactions are validated and cleansed; rejects are logged.
3. **Deduplication:** Duplicate transactions are removed.
4. **Enrichment:** Transactions are joined with product dimension for category and cost enrichment.
5. **Pricing:** Pricing rules are applied to each transaction.
6. **Sorting:** Data is sorted by store and transaction date.
7. **Aggregation:** Store-level and date-level rollup is performed.
8. **Output:** Aggregated data is written to summary; rejects and lookup misses are logged separately.

**Business Rules/Transformations:**
- Cleansing and validation logic in `cleanse_validate.xfr`
- Pricing logic in `pricing_rules.xfr`
- Aggregation logic in `store_rollup.xfr`
- Deduplication on `txn_id`
- Join on `product_sku`

## 4. Data Mapping (Lineage)

| Target Table         | Target Column     | Source Table         | Source Column     | Remarks                                             |
|----------------------|------------------|----------------------|-------------------|-----------------------------------------------------|
| daily_summary.dat    | store_id         | transactions_raw.dat | store_id          | 1:1 Mapping                                         |
| daily_summary.dat    | txn_date         | transactions_raw.dat | txn_date          | 1:1 Mapping                                         |
| daily_summary.dat    | total_sales      | transactions_raw.dat | amount            | Transformation (sum aggregation in rollup)          |
| daily_summary.dat    | category         | product_dim.dat      | category          | Join/Enrichment via product_sku                     |
| daily_summary.dat    | standard_cost    | product_dim.dat      | standard_cost     | Join/Enrichment via product_sku                     |
| daily_summary.dat    | final_price      | transactions_raw.dat | amount, discount  | Transformation (pricing_rules.xfr)                  |
| cleanse_rejects.dat  | *                | transactions_raw.dat | *                 | Validation (rejects from cleanse_validate.xfr)       |
| product_misses.dat   | *                | transactions_raw.dat | *                 | Validation (records with missing product_sku match)  |

> *Remarks:*
> - 1:1 Mapping: Direct field copy.
> - Transformation: Field is derived via aggregation or calculation.
> - Validation: Row included only if validation fails (rejects/misses).

## 5. Transformation Logic

- **cleanse_validate.xfr:** Applies data quality rules (e.g., required fields, valid values). Rejects invalid records.
- **pricing_rules.xfr:** Calculates final price per transaction, applying discounts, taxes, or other business rules.
- **store_rollup.xfr:** Aggregates transactions by store and date, computing totals and summary metrics.
- **Join Transform (in graph):** Merges transaction and product dimension, populating category and cost fields.

## 6. Complexity Analysis

- **Number of Graph Components:** 11
- **Number of Lines of Code (in .xfr or .plan):** ~200 (estimated from graph and XFR references)
- **Transform Functions Used:** 3 (cleanse_validate.xfr, pricing_rules.xfr, store_rollup.xfr)
- **Joins Used:** 1 (Inner Join on product_sku)
- **Lookup Files or Datasets:** 1 (product_dim.dat)
- **Parameter Sets (.pset) or Plan Files Used:** 1 (parameter set in graph)
- **Number of Output Datasets:** 3
- **Conditional Logic or if-else flows:** 3 (cleansing rejects, product misses, main flow)
- **External Dependencies:** None (all logic in Ab Initio components and referenced XFR/DML files)
- **Overall Complexity Score:** 75

## 7. Key Outputs

- **daily_summary.dat:** Delimited file. Contains aggregated sales metrics by store and date, with enrichment from product dimension. Used for reporting and downstream analytics.
- **cleanse_rejects.dat:** Delimited file. Contains records rejected during cleansing/validation, with error messages.
- **product_misses.dat:** Delimited file. Contains records that failed product dimension lookup (missing product_sku).

## 8. Error Handling and Logging

- **Reject/Error Components:**
  - `Write_Cleanse_Rejects`: Captures records rejected during cleansing.
  - `Write_Product_Misses`: Captures records not matched in product dimension join.
- **.xfr-based error tagging:** `cleanse_validate.xfr` tags errors with messages.
- **Reject thresholds:** Not explicitly defined, but all rejects are logged.
- **Control file usage:** Not present.
- **Error Handling:** Errors are handled by routing rejected records to dedicated output files; main flow continues for valid records.

## 9. API Cost (LLM Cost ONLY)

- **Tokens Used (Prompt + Completion):** 5,800 (estimate for this call)
- **Cost per 1K tokens:** $0.003
- **Final Cost in USD for this single documentation run:** $0.0174

---
```