```markdown
====================================================
Author:        AAVA
Date:          2024-06-10
Description:   Documentation for Ab Initio Graph: Retail_Data_Mart_Ingest
====================================================

# Retail_Data_Mart_Ingest Ab Initio Graph Documentation

## 1. Overview of Graph/Component

The `Retail_Data_Mart_Ingest` graph orchestrates the batch ingestion and transformation of retail transaction data from AWS S3, enriching it with product dimension information and producing daily store-level summaries. It applies cleansing, deduplication, enrichment, pricing logic, and aggregation to deliver high-quality, analytics-ready datasets. The business requirement addressed is the reliable, scalable processing of large retail logs for downstream reporting and analytics.

## 2. Component Structure and Design

- **Input Components:**
  - `Read_AWS_S3`: Reads raw transaction logs from S3 (`transactions_raw.dat`).
  - `Read_Product_Dim`: Reads product dimension data (`product_dim.dat`).
- **Transformation Components:**
  - `Cleanse_Data`: Cleanses and validates raw transactions using `cleanse_validate.xfr`.
  - `Dedup_Transactions`: Removes duplicate transactions based on `txn_id`.
  - `Enrichment_Join`: Inner join between cleansed transactions and product dimension on `product_sku`.
  - `Apply_Pricing`: Applies pricing rules via `pricing_rules.xfr`.
  - `Sort_for_Rollup`: Sorts enriched data by `store_id` and `txn_date`.
  - `Store_Aggregation`: Aggregates transactions at store/date level using `store_rollup.xfr`.
- **Output Components:**
  - `Write_Summary`: Outputs daily summary (`daily_summary.dat`).
  - `Write_Cleanse_Rejects`: Outputs rejected records (`cleanse_rejects.dat`).
  - `Write_Product_Misses`: Outputs records missing product lookup (`product_misses.dat`).

**Connection Flow:**  
Raw data flows from S3 input → cleansing → deduplication → enrichment join with product dimension → pricing transformation → sorting → rollup aggregation → output summary. Rejects and lookup misses are written to separate files. Parameters and variables are used for file paths, DML/XFR references, and error log locations.

## 3. Data Flow and Processing Logic

- **Key Data Sources:**
  - `transactions_raw.dat` (Input)
  - `product_dim.dat` (Input)
- **Intermediate Files:**
  - Cleansed transactions
  - Deduplicated transactions
  - Enriched transactions
- **Final Outputs:**
  - `daily_summary.dat` (Store/date summary)
  - `cleanse_rejects.dat` (Rejected records)
  - `product_misses.dat` (Product lookup misses)

**Logical Steps:**
1. **Cleanse_Data**: Filters and validates records, rejects invalid ones.
   - Uses: `cleanse_validate.xfr`, `retail_txn_enriched.dml`
   - Business rules: Data type checks, mandatory field validation.
2. **Dedup_Transactions**: Removes duplicate `txn_id`.
3. **Enrichment_Join**: Inner join on `product_sku`, enriches with category and cost.
   - Uses: `retail_product_dim.dml`
   - Business rules: Only matched records proceed; unmatched go to `product_misses.dat`.
4. **Apply_Pricing**: Applies pricing logic.
   - Uses: `pricing_rules.xfr`
   - Business rules: Discount, markup, or other pricing rules.
5. **Sort_for_Rollup**: Sorts by `store_id`, `txn_date`.
6. **Store_Aggregation**: Aggregates sales by store/date.
   - Uses: `store_rollup.xfr`, `retail_store_summary.dml`
   - Business rules: Summing sales, counting transactions.
7. **Write Outputs**: Final summary, rejects, and lookup misses.

## 4. Data Mapping (Lineage)

| Target Table         | Target Column      | Source Table         | Source Column      | Remarks                           |
|----------------------|-------------------|----------------------|-------------------|------------------------------------|
| daily_summary.dat    | store_id          | transactions_raw.dat | store_id          | 1:1 Mapping                       |
| daily_summary.dat    | txn_date          | transactions_raw.dat | txn_date          | 1:1 Mapping                       |
| daily_summary.dat    | total_sales       | transactions_raw.dat | sales_amount      | Aggregation (SUM)                 |
| daily_summary.dat    | transaction_count | transactions_raw.dat | txn_id            | Aggregation (COUNT)               |
| daily_summary.dat    | category          | product_dim.dat      | category          | Transformation (via join)         |
| daily_summary.dat    | standard_cost     | product_dim.dat      | standard_cost     | Transformation (via join)         |
| cleanse_rejects.dat  | error_message     | transactions_raw.dat | <derived>         | Validation (error tagging logic)   |
| product_misses.dat   | product_sku       | transactions_raw.dat | product_sku       | Validation (lookup miss logic)     |

## 5. Transformation Logic

- **cleanse_validate.xfr**: Validates fields, tags errors, outputs cleansed or rejected records.
- **pricing_rules.xfr**: Applies pricing calculations, discounts, markups based on business logic.
- **store_rollup.xfr**: Aggregates transactions, sums sales, counts records per store/date.
- **Join Transform**: Adds category and standard_cost from product dimension to transactions.

**External Function Calls:**  
- None specified; all transformations are via referenced XFRs.

## 6. Complexity Analysis

- **Number of Graph Components:** 11
- **Number of Lines of Code (.xfr/.plan):** ~700+ (from .mp file verbose mapping)
- **Transform Functions Used:** 3 (cleanse_validate.xfr, pricing_rules.xfr, store_rollup.xfr)
- **Joins Used:** 1 (inner join on product_sku)
- **Lookup Files/Datasets:** 1 (product_dim.dat)
- **Parameter Sets (.pset) or Plan Files Used:** 1 parameter set (from graph header)
- **Number of Output Datasets:** 3
- **Conditional Logic/if-else Flows:** 3 (cleansing, join reject, lookup miss)
- **External Dependencies:** AWS S3, Ab Initio built-in components, referenced XFR/DML files
- **Overall Complexity Score:** 75/100 (moderately complex, multi-step, with error handling and lineage)

## 7. Key Outputs

- **daily_summary.dat**: Delimited file, store/date-level sales summary for reporting.
- **cleanse_rejects.dat**: Delimited file, rejected records with error messages for audit.
- **product_misses.dat**: Delimited file, records missing product lookup for data quality review.

## 8. Error Handling and Logging

- **Reject/Error Components:**  
  - `Write_Cleanse_Rejects` captures records failing cleansing.
  - `Write_Product_Misses` captures records not found in product dimension.
- **.xfr-based Error Tagging:**  
  - `cleanse_validate.xfr` tags errors, outputs to reject port.
- **Reject Thresholds/Control Files:**  
  - Not explicitly defined, but error logs are written to specified paths.
- **Error Handling:**  
  - Auto-abort not specified; errors are logged for review.
  - Separate reject files for cleansing and lookup misses.

## 9. API Cost (LLM Cost ONLY)

- **Tokens Used (Prompt + Completion):** ~4400 tokens (est. prompt: 2800, completion: 1600)
- **Cost per 1K tokens:** $0.003 (OpenAI GPT-4o June 2024 pricing)
- **Final Cost in USD for this single documentation run:** $0.0132

---
```