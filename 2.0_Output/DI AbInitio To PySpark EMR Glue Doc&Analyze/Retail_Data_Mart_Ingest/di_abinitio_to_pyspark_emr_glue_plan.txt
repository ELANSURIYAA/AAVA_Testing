```
==================================================================================
Author:        AAVA
Created on:    
Description:   Pre-conversion analysis of Ab Initio ETL flow for PySpark EMR/Glue migration
==================================================================================

## 1. AWS EMR Glue Runtime Cost Estimation

### 1.1 EMR/Spark Job Cost Breakdown

- **Cluster Configuration**:  
    - Master Node: m5.xlarge, 1 node  
    - Worker Nodes: m5.xlarge, 4 nodes  
    - Total Instances: 5 (1 master + 4 workers)  
    - Total vCPUs: 20 (4 per node)  
    - Total Memory: 80 GB (16 GB per node)

- **Job Duration Estimate**:  
    - Estimated PySpark job duration: **60 minutes**  
    - (Based on input volume: 200GB, shuffle/sort/join complexity, and DPU equivalence from Glue pricing notes)

- **AWS Pricing**:  
    - Compute (per instance-hour): **$0.192** (m5.xlarge, on-demand)  
    - Storage (per GB-hour): **$0.023** (S3, monthly, pro-rated to hourly)  
    - Temporary S3 shuffle/spill usage: **100 GB** (peak during join/sort)

- **Cost Formula Used**:  
    ```
    Total Cost = (Total Instances × Duration in hours × Compute (per instance-hour))
               + (Storage GB × Duration in hours × Storage (per GB-hour))
    ```

- **Calculation**:  
    - Compute: 5 instances × 1 hour × $0.192 = **$0.96**
    - Storage: 100 GB × 1 hour × ($0.023 / 720) = $0.0032
      - ($0.023 per GB-month, so per hour: $0.023/720)
    - **Estimated Runtime Cost (USD)**: **$0.96 + $0.0032 = $0.9632**

---

## 2. Manual Code Fixing and Data Reconciliation Effort

### 2.1 Estimated Effort (Hours)

| Task Category                      | Estimated Hours |
|-------------------------------------|----------------|
| Logic Corrections (.xfr transforms) | 10             |
| Metadata Alignment (.dml fixes)     | 5              |
| Rejected Row Handling/Edge Cases    | 4              |
| Data Reconciliation & Validation    | 6              |
| **Total Effort**                    | **25**         |

- **Logic Corrections**:  
  - Manual translation of cleanse_validate.xfr, pricing_rules.xfr, store_rollup.xfr into PySpark UDFs and DataFrame logic.
- **Metadata Alignment**:  
  - DML to Glue Catalog schema conversion, data type mapping, nullability, and output format reconciliation.
- **Rejected Row Handling**:  
  - Implementing DataFrame splits for reject flows (cleansing errors, lookup misses), error tagging, and audit log writing.
- **Data Reconciliation & Validation**:  
  - End-to-end validation, output correctness, functional testing, lineage checks.

### 2.2 Developer Cost

- Developer Rate: **$50/hr**
- **Total Developer Cost**: **25 × $50 = $1,250 USD**

---

## 3. API Cost

- apiCost: **$0.0132** (in USD)

---

## 4. Summary Table

| Cost/Effort Category        | Value         |
|----------------------------|--------------|
| AWS EMR/Spark Runtime Cost | $0.96        |
| Developer Effort (hrs)     | 25           |
| Developer Cost             | $1,250       |
| API Cost                   | $0.0132      |
| **Total Projected Cost**   | **$1,251.97** |

---

## 5. Conversion Notes & Logic Gaps

- **Manual intervention required for**:
    - All custom XFR logic (.xfr files): cleansing, pricing, rollup aggregation.
    - DML schema mapping (Ab Initio DML → Glue Catalog).
    - Reject/error flows (no direct Spark equivalent; must use DataFrame splits and error columns).
    - Parameter set (.pset) translation for file paths, error log locations.
    - Edge case logic for rejects and lookup misses.

- **Estimated job duration and cost** are based on:
    - Input volume (200GB), output volume (5GB), shuffle-intensive operations (join, sort, rollup).
    - AWS Glue DPU equivalence and EMR m5.xlarge cluster sizing.
    - Temporary S3 spill usage during heavy shuffle (100GB peak).

---

## 6. Source Files Used

- /src/eb2df022-2e59-473f-b996-461c666e87bb/tmpqxzsqrkw/tmpfid8ldj_
- /src/eb2df022-2e59-473f-b996-461c666e87bb/tmpqxzsqrkw/tmpge7u_q8c

---

## 7. API Cost

apiCost: **0.0132 USD**

---

**End of Report**
```