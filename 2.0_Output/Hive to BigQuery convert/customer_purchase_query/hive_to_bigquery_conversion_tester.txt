1. Comprehensive Test Case Document (TC01-TC17)

| Test Case ID | Description | Preconditions | Test Steps | Expected Result | Actual Result | Pass/Fail Status |
|--------------|-------------|---------------|------------|----------------|---------------|------------------|
| TC01 | Validate basic query execution in BigQuery | Test tables exist with sample data | 1. Run converted BigQuery SQL | Query executes without syntax errors |  |  |
| TC02 | Validate schema equivalence | Test tables exist | 1. Run BigQuery SQL<br>2. Inspect output schema | Output schema matches expected columns/types |  |  |
| TC03 | Validate aggregate logic (SUM, COUNT, MIN, MAX, AVG) | Test tables with known aggregates | 1. Run BigQuery SQL<br>2. Compare aggregates to expected | Aggregates are correct |  |  |
| TC04 | Validate window functions (ROW_NUMBER, NTILE) | Test tables with multiple categories/customers | 1. Run BigQuery SQL<br>2. Check window function outputs | Window function results are correct |  |  |
| TC05 | Validate CASE logic for customer_tier | Test data with various total_spent values | 1. Run BigQuery SQL<br>2. Check customer_tier assignment | customer_tier assigned per logic |  |  |
| TC06 | Validate NULL handling in division and aggregation | Test data with NULLs/zeros | 1. Run BigQuery SQL<br>2. Check for errors or incorrect NULLs | No division by zero, NULLs handled |  |  |
| TC07 | Validate date function conversions (DATEDIFF, EXTRACT) | Test data with various dates | 1. Run BigQuery SQL<br>2. Compare date calculations | Dates calculated correctly |  |  |
| TC08 | Validate JOIN logic (INNER, LEFT) | Test data with missing/extra rows | 1. Run BigQuery SQL<br>2. Check join results | Joins behave as expected |  |  |
| TC09 | Validate HAVING clause filtering | Test data with total_spent above/below threshold | 1. Run BigQuery SQL<br>2. Check output rows | Only rows with total_spent > 500 |  |  |
| TC10 | Validate LIMIT and ORDER BY | Test data with >1000 customers | 1. Run BigQuery SQL<br>2. Check row count/order | Max 1000 rows, ordered by total_spent DESC |  |  |
| TC11 | Validate partitioning and clustering recommendations | BigQuery tables partitioned/clustering enabled | 1. Inspect table metadata | Partitioning/clustering matches recommendations |  |  |
| TC12 | Validate performance on large dataset | Large test dataset loaded | 1. Run BigQuery SQL<br>2. Measure execution time | Query completes in reasonable time |  |  |
| TC13 | Validate manual intervention for unsupported Hive features | Test for LATERAL VIEW, DISTRIBUTE BY, etc. | 1. Inspect converted SQL | Manual interventions correctly applied |  |  |
| TC14 | Validate data type conversions | Test data with edge-case types | 1. Run BigQuery SQL<br>2. Check type correctness | Types converted as per mapping |  |  |
| TC15 | Validate error handling for invalid data | Test data with invalid/edge cases | 1. Run BigQuery SQL<br>2. Check for errors | Query handles errors gracefully |  |  |
| TC16 | Validate logic equivalence with Hive output | Run original Hive query and BigQuery query on same data | 1. Compare result sets | Results are functionally equivalent |  |  |
| TC17 | Validate RFM scoring logic | Test data with known recency/frequency/monetary values | 1. Run BigQuery SQL<br>2. Check RFM scores | RFM scores assigned correctly |  |  |

---

2. Pytest Script for Test Cases (example implementation, can be extended per environment)

```python
import pytest
from google.cloud import bigquery

# Setup/teardown for test dataset
@pytest.fixture(scope="module")
def bq_client():
    return bigquery.Client()

@pytest.fixture(scope="module")
def test_dataset(bq_client):
    dataset_id = "test_customer_purchase"
    dataset_ref = bq_client.dataset(dataset_id)
    bq_client.create_dataset(bigquery.Dataset(dataset_ref), exists_ok=True)
    yield dataset_id
    bq_client.delete_dataset(dataset_ref, delete_contents=True, not_found_ok=True)

@pytest.fixture(scope="module")
def load_test_data(bq_client, test_dataset):
    # Create and load tables: customers, orders, regions, products, product_categories, order_items
    # Insert mock data for positive/negative/edge cases
    # This is a placeholder; actual implementation would use bq_client.load_table_from_dataframe or SQL
    pass

@pytest.fixture(scope="module")
def run_bigquery_query(bq_client, test_dataset):
    def _run_query(query):
        job = bq_client.query(query)
        return list(job.result())
    return _run_query

# Example test cases
def test_basic_query_execution(run_bigquery_query, load_test_data):
    query = """<PASTE CONVERTED BIGQUERY SQL HERE, replacing table names with test_dataset prefix if needed>"""
    try:
        results = run_bigquery_query(query)
        assert results is not None
    except Exception as e:
        pytest.fail(f"Query failed: {e}")

def test_schema_equivalence(run_bigquery_query, load_test_data):
    query = """<PASTE CONVERTED BIGQUERY SQL HERE>"""
    results = run_bigquery_query(query)
    expected_columns = [
        "customer_id", "customer_name", "age_group", "gender", "region_name", "country",
        "total_spent", "order_count", "customer_tenure_days", "customer_tier",
        "monthly_purchase_frequency", "top_category", "top_category_spent",
        "second_category", "second_category_spent", "q1_spent", "q2_spent", "q3_spent",
        "q4_spent", "q4_q3_growth_pct", "weekend_spent", "weekday_spent", "overall_aov",
        "days_since_last_purchase", "recency_score", "frequency_score", "monetary_score"
    ]
    assert set(results[0].keys()) == set(expected_columns)

def test_aggregate_logic(run_bigquery_query, load_test_data):
    query = """<PASTE CONVERTED BIGQUERY SQL HERE>"""
    results = run_bigquery_query(query)
    # Compare aggregates to expected values (use known test data)
    for row in results:
        assert row["total_spent"] >= 0
        assert row["order_count"] >= 0

def test_case_logic_customer_tier(run_bigquery_query, load_test_data):
    query = """<PASTE CONVERTED BIGQUERY SQL HERE>"""
    results = run_bigquery_query(query)
    for row in results:
        if row["total_spent"] > 10000:
            assert row["customer_tier"] == "Platinum"
        elif row["total_spent"] > 5000:
            assert row["customer_tier"] == "Gold"
        elif row["total_spent"] > 1000:
            assert row["customer_tier"] == "Silver"
        else:
            assert row["customer_tier"] == "Bronze"

def test_null_handling(run_bigquery_query, load_test_data):
    query = """<PASTE CONVERTED BIGQUERY SQL HERE>"""
    results = run_bigquery_query(query)
    for row in results:
        assert row["monthly_purchase_frequency"] is None or row["monthly_purchase_frequency"] >= 0

# ... Implement other test cases similarly ...

# Performance test example (pseudo-code, actual implementation may vary)
def test_performance(run_bigquery_query, load_test_data):
    import time
    query = """<PASTE CONVERTED BIGQUERY SQL HERE>"""
    start = time.time()
    run_bigquery_query(query)
    duration = time.time() - start
    assert duration < 60  # e.g., must run in under 1 minute

# Test execution report template (to be filled after running tests)
"""
Test Case ID | Description | Expected Result | Actual Result | Pass/Fail | Notes
-------------|-------------|----------------|--------------|-----------|------
TC01         | ...         | ...            | ...          | ...       | ...
TC02         | ...         | ...            | ...          | ...       | ...
...
"""

```

3. API cost for this call: $0.005

You can copy/paste the test case table and Pytest script above to your documentation and codebase. The test case table covers all required validation areas (syntax, logic, data types, manual interventions, performance, and error handling) and the Pytest script provides a robust framework for automated validation of Hive-to-BigQuery SQL conversion.