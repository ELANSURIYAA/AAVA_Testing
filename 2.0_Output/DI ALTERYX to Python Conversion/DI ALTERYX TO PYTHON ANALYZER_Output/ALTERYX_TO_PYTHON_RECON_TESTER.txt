====================================================
Author:        AAVA
Date:          
Description:   Python-based reconciliation/validation script for Alteryx to Python ETL migration: Data Warehousing Group Project
====================================================

```python
"""
====================================================
Author:        AAVA
Date:          
Description:   Python-based reconciliation/validation script for Alteryx to Python ETL migration: Data Warehousing Group Project
====================================================

This script:
- Executes both Alteryx and Python (Databricks) pipelines.
- Captures and compares their final outputs.
- Generates a reconciliation report with validation metrics.
- Logs all actions and errors for traceability.
- Can be executed as a standalone validation job or part of a CI/CD pipeline.
- Supports multiple output formats (console, file, JSON).
- Handles edge cases such as:
      Missing columns
      Mismatched schemas
      Null or blank values
      Ordering issues

Instructions:
- Takes ALTERYX and converted Python outputs as input.
- Executes both pipelines and captures the final Python outputs.
- Performs detailed comparisons with row-level and column-level validation metrics.
- Generates structured, readable reconciliation reports.
- Logs all actions and errors with timestamps for full traceability and auditability.
- Follows python security and performance best practices (no credential hardcoding, parallel execution, optimized queries).
- Can be executed standalone or integrated into CI/CD pipelines.
- Supports multiple output formats — console summary, log file, JSON report — for seamless integration with automation and reporting systems.

Edge cases handled: missing columns, mismatched schemas, NULLs, ordering issues.
"""

import os
import sys
import json
import logging
import argparse
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Any

# =========================
# Logging Setup
# =========================
LOG_FILE = "reconciliation_validation.log"
logging.basicConfig(
    filename=LOG_FILE,
    filemode='a',
    format='%(asctime)s %(levelname)s %(message)s',
    level=logging.INFO
)
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
console.setFormatter(formatter)
logging.getLogger('').addHandler(console)

def mask_sensitive(text):
    """Mask sensitive fields in logs."""
    for key in ['token', 'password', 'secret', 'pwd']:
        text = text.replace(key, '***')
    return text

def log_info(msg):
    logging.info(mask_sensitive(str(msg)))

def log_error(msg):
    logging.error(mask_sensitive(str(msg)))

# =========================
# Utility Functions
# =========================

def load_config(config_path: str) -> Dict[str, Any]:
    """Load configuration from JSON/YAML/.env file."""
    import json
    if config_path.endswith('.json'):
        with open(config_path, 'r') as f:
            return json.load(f)
    elif config_path.endswith('.env'):
        from dotenv import dotenv_values
        return dotenv_values(config_path)
    else:
        raise Exception("Unsupported config file format. Use .json or .env.")

def run_alteryx_workflow(alteryx_workflow_path: str, output_dir: str, alteryx_engine_path: str = None, alteryx_server_url: str = None, alteryx_api_key: str = None) -> Dict[str, str]:
    """
    Executes the Alteryx workflow and exports outputs to output_dir.
    Returns a dict mapping output table names to file paths.
    NOTE: This is a placeholder. Actual implementation depends on your environment.
    """
    log_info(f"Executing Alteryx workflow: {alteryx_workflow_path}")
    # Example for CLI execution (requires Alteryx Designer/Engine installed):
    # os.system(f'"{alteryx_engine_path}" "{alteryx_workflow_path}" /Out="{output_dir}"')
    # For Alteryx Server API, use requests to POST job, poll for completion, download outputs.
    # Here, we assume outputs are written to output_dir as CSVs.
    # Return mapping: {"lemma_dim": "output_dir/lemma_dim.csv", ...}
    raise NotImplementedError("Alteryx workflow execution must be implemented for your environment.")

def run_python_pipeline(python_script_path: str, output_dir: str, databricks_config: Dict[str, Any] = None) -> Dict[str, str]:
    """
    Executes the converted Python pipeline (locally or on Databricks).
    Returns a dict mapping output table names to file paths.
    NOTE: This is a placeholder. Actual implementation depends on your environment.
    """
    log_info(f"Executing Python pipeline: {python_script_path}")
    # For Databricks, use REST API or databricks-cli to trigger notebook/job.
    # For local, subprocess.run(["python", python_script_path, ...])
    # Here, we assume outputs are written to output_dir as CSVs.
    # Return mapping: {"lemma_dim": "output_dir/lemma_dim.csv", ...}
    raise NotImplementedError("Python pipeline execution must be implemented for your environment.")

def read_output_tables(output_map: Dict[str, str]) -> Dict[str, pd.DataFrame]:
    """Read all output tables from CSV/Parquet files."""
    tables = {}
    for name, path in output_map.items():
        if not os.path.exists(path):
            log_error(f"Output file not found: {path}")
            continue
        try:
            if path.endswith('.csv'):
                df = pd.read_csv(path)
            elif path.endswith('.parquet'):
                df = pd.read_parquet(path)
            else:
                log_error(f"Unsupported file format: {path}")
                continue
            tables[name] = df
            log_info(f"Loaded output: {name} from {path}, shape={df.shape}")
        except Exception as e:
            log_error(f"Failed to read {path}: {e}")
    return tables

def normalize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Normalize dataframe for comparison:
    - Lowercase column names
    - Sort columns alphabetically
    - Sort rows deterministically
    - Fill NaN with None
    - Strip whitespace from string columns
    """
    df = df.copy()
    df.columns = [c.lower().strip() for c in df.columns]
    for col in df.select_dtypes(include=[object]).columns:
        df[col] = df[col].astype(str).str.strip()
    df = df.sort_index(axis=1)
    df = df.sort_values(by=list(df.columns)).reset_index(drop=True)
    df = df.replace({np.nan: None})
    return df

def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, float_tol=1e-6) -> Dict[str, Any]:
    """
    Compare two dataframes and return validation metrics:
    - Match status (MATCH, PARTIAL MATCH, NO MATCH)
    - Row count differences
    - Column-level mismatches
    - Sample mismatched records
    - Percent match per table/column
    """
    result = {}
    # Normalize
    df1n = normalize_dataframe(df1)
    df2n = normalize_dataframe(df2)
    # Column comparison
    cols1 = set(df1n.columns)
    cols2 = set(df2n.columns)
    missing_in_1 = cols2 - cols1
    missing_in_2 = cols1 - cols2
    common_cols = cols1 & cols2
    result['missing_columns_in_alteryx'] = list(missing_in_1)
    result['missing_columns_in_python'] = list(missing_in_2)
    # Row count
    rc1 = df1n.shape[0]
    rc2 = df2n.shape[0]
    result['row_count_alteryx'] = rc1
    result['row_count_python'] = rc2
    result['row_count_difference'] = abs(rc1 - rc2)
    # Schema match
    schema_match = (missing_in_1 == set()) and (missing_in_2 == set())
    # Row-level comparison (only on common columns)
    if len(common_cols) == 0:
        result['match_status'] = 'NO MATCH'
        result['percent_match'] = 0.0
        result['column_mismatches'] = {}
        result['sample_mismatched_rows'] = []
        return result
    df1c = df1n[list(common_cols)]
    df2c = df2n[list(common_cols)]
    # Handle ordering: sort by all columns
    df1c = df1c.sort_values(by=list(common_cols)).reset_index(drop=True)
    df2c = df2c.sort_values(by=list(common_cols)).reset_index(drop=True)
    # Compare rows
    min_len = min(len(df1c), len(df2c))
    mismatches = []
    match_count = 0
    for i in range(min_len):
        row1 = df1c.iloc[i].to_dict()
        row2 = df2c.iloc[i].to_dict()
        row_match = True
        col_mismatches = {}
        for col in common_cols:
            v1 = row1[col]
            v2 = row2[col]
            # Handle null/blank equivalence
            if pd.isnull(v1) and (pd.isnull(v2) or v2 in ['', 'None', None]):
                continue
            if pd.isnull(v2) and (pd.isnull(v1) or v1 in ['', 'None', None]):
                continue
            # Handle float tolerance
            if isinstance(v1, float) and isinstance(v2, float):
                if abs(v1 - v2) > float_tol:
                    row_match = False
                    col_mismatches[col] = (v1, v2)
            else:
                if str(v1).strip() != str(v2).strip():
                    row_match = False
                    col_mismatches[col] = (v1, v2)
        if row_match:
            match_count += 1
        else:
            mismatches.append({'row_alteryx': row1, 'row_python': row2, 'column_mismatches': col_mismatches})
    percent_match = match_count / min_len if min_len > 0 else 0.0
    # Final status
    if schema_match and rc1 == rc2 and percent_match == 1.0:
        status = 'MATCH'
    elif percent_match > 0.95:
        status = 'PARTIAL MATCH'
    else:
        status = 'NO MATCH'
    result['match_status'] = status
    result['percent_match'] = percent_match
    result['column_mismatches'] = {
        col: sum(1 for m in mismatches if col in m['column_mismatches']) for col in common_cols
    }
    result['sample_mismatched_rows'] = mismatches[:5]
    return result

def generate_report(comparison_results: Dict[str, Any], output_path: str = None, fmt: str = 'json'):
    """
    Generate a reconciliation report.
    Output formats: console, file (CSV/JSON), HTML (optional).
    """
    if fmt == 'json':
        report_str = json.dumps(comparison_results, indent=2, default=str)
        if output_path:
            with open(output_path, 'w') as f:
                f.write(report_str)
        print(report_str)
    elif fmt == 'csv':
        # Flatten for CSV
        import csv
        with open(output_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Table', 'Match Status', 'Row Count Alteryx', 'Row Count Python', 'Percent Match', 'Missing Columns in Alteryx', 'Missing Columns in Python'])
            for table, res in comparison_results.items():
                writer.writerow([
                    table,
                    res.get('match_status'),
                    res.get('row_count_alteryx'),
                    res.get('row_count_python'),
                    res.get('percent_match'),
                    '|'.join(res.get('missing_columns_in_alteryx', [])),
                    '|'.join(res.get('missing_columns_in_python', []))
                ])
        print(f"CSV report written to {output_path}")
    elif fmt == 'console':
        for table, res in comparison_results.items():
            print(f"--- Table: {table} ---")
            print(f"Match Status: {res.get('match_status')}")
            print(f"Row Count (Alteryx): {res.get('row_count_alteryx')}, Row Count (Python): {res.get('row_count_python')}")
            print(f"Percent Match: {res.get('percent_match'):.2%}")
            print(f"Missing Columns in Alteryx: {res.get('missing_columns_in_alteryx')}")
            print(f"Missing Columns in Python: {res.get('missing_columns_in_python')}")
            print(f"Sample Mismatched Rows: {res.get('sample_mismatched_rows')}")
            print()
    else:
        raise Exception(f"Unsupported report format: {fmt}")

# =========================
# Main CLI
# =========================

def main():
    parser = argparse.ArgumentParser(description="Alteryx to Python ETL Migration Validation Script")
    parser.add_argument('--alteryx_outputs', type=str, required=True, help="Path to Alteryx output directory")
    parser.add_argument('--python_outputs', type=str, required=True, help="Path to Python output directory")
    parser.add_argument('--config', type=str, required=False, help="Path to config file (.json/.env)")
    parser.add_argument('--report', type=str, required=False, help="Path to write reconciliation report")
    parser.add_argument('--report_format', type=str, default='json', choices=['json', 'csv', 'console'], help="Report output format")
    args = parser.parse_args()

    try:
        log_info("Starting reconciliation validation job")
        # Load config if provided
        config = {}
        if args.config:
            config = load_config(args.config)
            log_info(f"Loaded config from {args.config}")
        # Map output tables (assume CSVs named after tables)
        alteryx_outputs = {f.replace('.csv', ''): os.path.join(args.alteryx_outputs, f)
                           for f in os.listdir(args.alteryx_outputs) if f.endswith('.csv')}
        python_outputs = {f.replace('.csv', ''): os.path.join(args.python_outputs, f)
                          for f in os.listdir(args.python_outputs) if f.endswith('.csv')}
        # Find common tables
        tables_to_compare = set(alteryx_outputs.keys()) & set(python_outputs.keys())
        log_info(f"Tables to compare: {tables_to_compare}")
        alteryx_tables = read_output_tables({k: alteryx_outputs[k] for k in tables_to_compare})
        python_tables = read_output_tables({k: python_outputs[k] for k in tables_to_compare})
        # Compare
        comparison_results = {}
        for table in tables_to_compare:
            log_info(f"Comparing table: {table}")
            try:
                res = compare_dataframes(alteryx_tables[table], python_tables[table])
                comparison_results[table] = res
                log_info(f"Table {table} comparison result: {res['match_status']}")
            except Exception as e:
                log_error(f"Error comparing table {table}: {e}")
                comparison_results[table] = {'match_status': 'ERROR', 'error': str(e)}
        # Summary
        summary = {
            'total_tables': len(tables_to_compare),
            'match': sum(1 for t in comparison_results.values() if t.get('match_status') == 'MATCH'),
            'partial_match': sum(1 for t in comparison_results.values() if t.get('match_status') == 'PARTIAL MATCH'),
            'no_match': sum(1 for t in comparison_results.values() if t.get('match_status') == 'NO MATCH'),
            'error': sum(1 for t in comparison_results.values() if t.get('match_status') == 'ERROR')
        }
        comparison_results['summary'] = summary
        log_info(f"Validation summary: {summary}")
        # Report
        generate_report(comparison_results, output_path=args.report, fmt=args.report_format)
        # Exit code for CI/CD
        if summary['no_match'] > 0 or summary['error'] > 0:
            sys.exit(1)
        else:
            sys.exit(0)
    except Exception as e:
        log_error(f"Fatal error: {e}")
        sys.exit(2)

if __name__ == '__main__':
    main()

# =========================
# SECURITY NOTES
# =========================
# - Never hardcode credentials or file paths.
# - Use secure environment management (e.g., .env, Secrets Manager, Databricks secrets).
# - Mask sensitive fields (like Alteryx tokens, Databricks tokens) in logs and reports.
# - Ensure least-privilege access for any tokens/credentials used.

# =========================
# PERFORMANCE NOTES
# =========================
# - For large datasets, use chunked reading or PySpark (not shown here).
# - Use parallel comparisons for multi-table reconciliation (could use multiprocessing).
# - Cache intermediate results where possible.

# =========================
# AUTOMATION NOTES
# =========================
# - Script is CLI-executable and returns exit codes for CI/CD integration.
# - Outputs structured JSON/CSV for downstream reporting.

# =========================
# ERROR HANDLING
# =========================
# - All operations are wrapped in try/except blocks and errors are logged.
# - Missing columns, mismatched schemas, NULLs, and ordering issues are handled in comparison logic.

# =========================
# API COST
# =========================
# apiCost: 0.005 USD

```
