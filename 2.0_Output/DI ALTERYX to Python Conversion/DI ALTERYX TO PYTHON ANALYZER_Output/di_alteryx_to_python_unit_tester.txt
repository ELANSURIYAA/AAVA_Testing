====================================================
Author:        AAVA
Date:          
Description:   Python conversion of Alteryx workflow for Data Warehousing Group Project ETL process
====================================================

# SECTION 1 — ANALYSIS

## Inputs:
- learning_traces.13m.csv (CSV): Raw learning trace data.
- lexeme_reference.txt (TXT): Lexeme reference data.

## Transformation Logic:
- Clean `lexeme_string` column (replace "<*sf>" with "*generic lexeme").
- Split `lexeme_string` column by delimiter.
- Rename `delta` to `delta_value`, remove extra columns.
- Replace part-of-speech tags with full words using mapping.
- Split `part_of_speech` into `pos_tag` and `morphological_components`.
- Replace abbreviations in `learning_language` and `ui_language` with full words.
- Clean `morphological_components` column (replace "<" with space).
- Count number of morphological components.
- Create dimension tables: lemma_dim, language_dim, lexeme_dim, pos_dim, user_dim, timestamp_dim.
- Create fact tables: lexeme_fact, learning_trace_fact.
- Write cleaned CSV output.
- Create database tables and insert dimension/fact data using SQLAlchemy.
- Add foreign key constraints.

## Output Structure:
- Dimension tables: lemma_dim, language_dim, lexeme_dim, pos_dim, user_dim, timestamp_dim.
- Fact tables: lexeme_fact, learning_trace_fact.
- Cleaned CSV file.

---

# SECTION 2 — TEST CASE DESIGN

| Test Case ID | Description | Expected Result |
|--------------|-------------|----------------|
| TC01 | Validate happy path transformation and output correctness for sample input. | Cleaned DataFrame matches expected output; all transformations applied. |
| TC02 | Validate join/merge logic for dimension and fact table creation. | Dimension and fact tables contain correct unique records and relationships. |
| TC03 | Validate filter and formula logic for part-of-speech and language mapping. | All abbreviations and tags are replaced with full words as per mapping. |
| TC04 | Validate aggregation and summarization (count of morphological components). | `number_of_morphological_components` column contains correct counts. |
| TC05 | Validate null and missing data handling in language columns. | Nulls and unmapped values are replaced with "_ERROR". |
| TC06 | Validate schema and column correctness for output tables. | Output DataFrames contain expected columns and data types. |
| TC07 | Validate data type boundary and edge conditions (e.g., timestamp conversion). | Timestamps are correctly converted; edge cases (min/max values) handled. |
| TC08 | Validate error-handling behavior for malformed input data. | Script raises appropriate exceptions or handles errors gracefully. |

---

# SECTION 3 — PYTEST SCRIPT GENERATION

```python
import pytest
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Mock transformation function ---
def run_python_model(input_data):
    # Simulates the transformation logic from the provided Python script
    learning_traces = input_data["learning_traces"].copy()
    lexeme_reference = input_data["lexeme_reference"].copy()

    # Clean lexeme_string column
    learning_traces['lexeme_string'] = learning_traces['lexeme_string'].str.replace("<*sf>", "*generic lexeme")

    # Split lexeme_string column
    learning_traces['lexeme_string_split'] = learning_traces['lexeme_string'].str.split(',')

    # Rename delta column
    if 'delta' in learning_traces.columns:
        learning_traces.rename(columns={'delta': 'delta_value'}, inplace=True)
    # Remove extra columns
    extra_cols = [col for col in learning_traces.columns if col.startswith('extra_')]
    learning_traces.drop(columns=extra_cols, inplace=True, errors='ignore')

    # Replace part-of-speech tags
    pos_map = {"Cnj": "Special conjunction"}
    learning_traces['part_of_speech'] = learning_traces['part_of_speech'].replace(pos_map)

    # Split part-of-speech tag
    learning_traces['pos_tag'] = learning_traces['part_of_speech'].str.split('_').str[0]
    learning_traces['morphological_components'] = learning_traces['part_of_speech'].str.split('_').str[1]

    # Replace abbreviations in language columns
    lang_map = {
        "en": "English", "es": "Spanish", "fr": "French",
        "de": "German", "it": "Italian", "pt": "Portuguese"
    }
    learning_traces['learning_language'] = learning_traces['learning_language'].map(lang_map).fillna("_ERROR")
    learning_traces['ui_language'] = learning_traces['ui_language'].map(lang_map).fillna("_ERROR")

    # Clean morphological_components column
    learning_traces['morphological_components'] = learning_traces['morphological_components'].str.replace("<", " ")

    # Count number of morphological components
    learning_traces['number_of_morphological_components'] = learning_traces['morphological_components'].str.split().apply(len)

    # Dimension tables
    lemma_dim = learning_traces[['lemma']].drop_duplicates().reset_index(drop=True)
    lemma_dim['lemma_id'] = lemma_dim['lemma']

    language_dim = pd.DataFrame({
        'language_id': pd.unique(learning_traces['learning_language'].append(learning_traces['ui_language']))
    })

    lexeme_dim = learning_traces[['lexeme_string']].drop_duplicates().reset_index(drop=True)
    lexeme_dim['lexeme_id'] = lexeme_dim['lexeme_string']

    pos_dim = learning_traces[['part_of_speech']].drop_duplicates().reset_index(drop=True)
    pos_dim['pos_id'] = pos_dim['part_of_speech']

    user_dim = learning_traces[['user_id', 'ui_language']].drop_duplicates().reset_index(drop=True)
    user_dim['user_interface_language_id'] = user_dim['ui_language']

    learning_traces['timestamp_attribute'] = learning_traces['timestamp'].apply(
        lambda x: datetime(1970, 1, 1) + timedelta(seconds=int(x))
    )
    timestamp_dim = learning_traces[['timestamp', 'timestamp_attribute']].drop_duplicates().reset_index(drop=True)
    timestamp_dim['timestamp_id'] = timestamp_dim['timestamp']

    # Fact tables
    lexeme_fact = learning_traces[[
        'learning_language', 'lexeme_string', 'lemma', 'timestamp', 'user_id', 'part_of_speech'
    ]].drop_duplicates().reset_index(drop=True)

    learning_trace_fact = learning_traces[[
        'user_id', 'ui_language', 'learning_language', 'lexeme_string', 'timestamp'
    ]].drop_duplicates().reset_index(drop=True)

    return {
        "learning_traces": learning_traces,
        "lemma_dim": lemma_dim,
        "language_dim": language_dim,
        "lexeme_dim": lexeme_dim,
        "pos_dim": pos_dim,
        "user_dim": user_dim,
        "timestamp_dim": timestamp_dim,
        "lexeme_fact": lexeme_fact,
        "learning_trace_fact": learning_trace_fact
    }

# --- Fixtures ---
@pytest.fixture
def mock_input_data():
    # Sample data for learning_traces
    learning_traces = pd.DataFrame({
        "user_id": [1, 2, 3],
        "lexeme_string": ["run<*sf>", "jump", "walk<*sf>"],
        "delta": [0.5, 0.7, 0.2],
        "part_of_speech": ["Cnj_morph1<morph2", "Noun_morph3", "Verb_morph4"],
        "learning_language": ["en", "es", "xx"],
        "ui_language": ["fr", "de", "it"],
        "lemma": ["run", "jump", "walk"],
        "timestamp": [1000, 2000, 3000],
        "extra_col1": ["foo", "bar", "baz"]
    })
    lexeme_reference = pd.DataFrame({
        "lexeme_id": ["run", "jump", "walk"],
        "reference": ["ref1", "ref2", "ref3"]
    })
    return {"learning_traces": learning_traces, "lexeme_reference": lexeme_reference}

# --- Test Cases ---

def test_TC01_happy_path_output_correctness(mock_input_data):
    logger.info("Running TC01: Happy path transformation and output correctness")
    result = run_python_model(mock_input_data)
    lt = result["learning_traces"]
    # Check transformation applied
    assert "*generic lexeme" in lt["lexeme_string"].iloc[0]
    assert "lexeme_string_split" in lt.columns
    assert "delta_value" in lt.columns
    assert "number_of_morphological_components" in lt.columns

def test_TC02_join_merge_logic(mock_input_data):
    logger.info("Running TC02: Join/merge logic for dimension and fact table creation")
    result = run_python_model(mock_input_data)
    lemma_dim = result["lemma_dim"]
    lexeme_dim = result["lexeme_dim"]
    pos_dim = result["pos_dim"]
    # Check unique values
    assert lemma_dim["lemma_id"].nunique() == 3
    assert lexeme_dim["lexeme_id"].nunique() == 3
    assert pos_dim["pos_id"].nunique() == 3

def test_TC03_filter_formula_logic(mock_input_data):
    logger.info("Running TC03: Filter and formula logic for part-of-speech and language mapping")
    result = run_python_model(mock_input_data)
    lt = result["learning_traces"]
    # Check part-of-speech mapping
    assert "Special conjunction" in lt["part_of_speech"].values
    # Check language mapping
    assert "English" in lt["learning_language"].values
    assert "Spanish" in lt["learning_language"].values
    assert "_ERROR" in lt["learning_language"].values

def test_TC04_aggregation_summarization(mock_input_data):
    logger.info("Running TC04: Aggregation and summarization correctness")
    result = run_python_model(mock_input_data)
    lt = result["learning_traces"]
    # Check count of morphological components
    assert lt["number_of_morphological_components"].iloc[0] >= 0
    assert isinstance(lt["number_of_morphological_components"].iloc[0], int)

def test_TC05_null_missing_data_handling(mock_input_data):
    logger.info("Running TC05: Null and missing data handling in language columns")
    result = run_python_model(mock_input_data)
    lt = result["learning_traces"]
    # Check unmapped language replaced with "_ERROR"
    assert "_ERROR" in lt["learning_language"].values

def test_TC06_schema_column_validation(mock_input_data):
    logger.info("Running TC06: Schema and column validation for output tables")
    result = run_python_model(mock_input_data)
    expected_cols = [
        "user_id", "lexeme_string", "delta_value", "part_of_speech",
        "learning_language", "ui_language", "lemma", "timestamp",
        "lexeme_string_split", "pos_tag", "morphological_components",
        "number_of_morphological_components", "timestamp_attribute"
    ]
    for col in expected_cols:
        assert col in result["learning_traces"].columns

def test_TC07_data_type_boundary_edge_conditions(mock_input_data):
    logger.info("Running TC07: Data type boundary and edge conditions (timestamp conversion)")
    result = run_python_model(mock_input_data)
    lt = result["learning_traces"]
    # Check timestamp conversion
    assert isinstance(lt["timestamp_attribute"].iloc[0], datetime)
    # Edge case: min/max timestamp
    min_ts = lt["timestamp"].min()
    max_ts = lt["timestamp"].max()
    min_dt = lt[lt["timestamp"] == min_ts]["timestamp_attribute"].iloc[0]
    max_dt = lt[lt["timestamp"] == max_ts]["timestamp_attribute"].iloc[0]
    assert min_dt < max_dt

def test_TC08_error_handling_behavior():
    logger.info("Running TC08: Error-handling for malformed input data")
    # Malformed input: missing columns
    learning_traces = pd.DataFrame({
        "user_id": [1],
        "lexeme_string": ["run<*sf>"]
        # Missing required columns
    })
    lexeme_reference = pd.DataFrame({
        "lexeme_id": ["run"],
        "reference": ["ref1"]
    })
    input_data = {"learning_traces": learning_traces, "lexeme_reference": lexeme_reference}
    try:
        run_python_model(input_data)
        assert False, "Should raise KeyError or handle missing columns"
    except Exception as e:
        assert isinstance(e, KeyError) or isinstance(e, AttributeError)

```

---

# API Cost

apiCost: 0.005 USD