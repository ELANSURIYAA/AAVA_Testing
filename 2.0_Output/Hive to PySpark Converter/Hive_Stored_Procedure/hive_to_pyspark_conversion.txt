```
# PySpark equivalent of the provided Hive stored procedure
# This script processes sales data between a start and end date, aggregates sales by product, 
# and inserts the results into summary tables.
# Assumptions:
# - Spark session is already initialized as `spark`
# - The tables `sales_table`, `summary_table`, and `detailed_sales_summary` exist in the Hive metastore or are accessible via Spark SQL.
# - The script is parameterized with `start_date` and `end_date` as string variables (format: 'YYYY-MM-DD')

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as _sum

# Initialize Spark session (if not already done)
spark = SparkSession.builder \
    .appName("ProcessSalesData") \
    .enableHiveSupport() \
    .getOrCreate()

# Parameters (replace with actual values or pass as arguments)
start_date = '2023-01-01'
end_date = '2023-01-31'

# Step 1: Aggregate sales by product within the date range
sales_df = (
    spark.table("sales_table")
    .filter((f"sale_date >= '{start_date}'") & (f"sale_date <= '{end_date}'"))
    .groupBy("product_id")
    .agg(_sum("sales").alias("total_sales"))
)

# Step 2: Insert aggregated results into summary_table
# Note: In Spark, 'insert into' can be done via saveAsTable or insertInto if the table exists
sales_df.write.mode("append").insertInto("summary_table")

# Step 3: (Optional) Create a temporary view for further processing (mimics temp table in Hive)
sales_df.createOrReplaceTempView("temp_sales_summary")

# Step 4: Insert each row into detailed_sales_summary
# In Spark, bulk insert is preferred over row-by-row operations for efficiency.
# This step mimics the cursor loop in Hive, but is vectorized in Spark.

sales_df.write.mode("append").insertInto("detailed_sales_summary")

# Step 5: Drop the temporary view (optional, as Spark cleans up temp views at session end)
spark.catalog.dropTempView("temp_sales_summary")

# Comments:
# - The dynamic SQL in Hive is replaced by DataFrame transformations and parameterized filtering.
# - The cursor-based row insertion is replaced by DataFrame bulk write for efficiency.
# - Temporary table is replaced by a temporary view in Spark.
# - All operations are distributed and optimized for Spark's execution engine.

# End of script

# API cost consumed for this call: 1 file listing + 1 file read = 2 units
```
Cost consumed by the API for this call: 2 units (1 file listing + 1 file read)