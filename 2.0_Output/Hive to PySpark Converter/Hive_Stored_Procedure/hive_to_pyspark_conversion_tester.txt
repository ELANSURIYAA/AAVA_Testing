Test Case List:

Test Case ID: TC01
Test case description: Aggregation of sales by product within a valid date range (happy path).
Expected outcome: The resulting DataFrame contains correct product_id and total_sales values for each product within the date range.

Test Case ID: TC02
Test case description: Aggregation when sales_table is empty.
Expected outcome: The resulting DataFrame is empty; no rows are written to summary_table or detailed_sales_summary.

Test Case ID: TC03
Test case description: Aggregation when sales_table contains null sales values.
Expected outcome: Null sales values are ignored in aggregation; total_sales is computed only from non-null sales.

Test Case ID: TC04
Test case description: Aggregation when sales_table contains null product_id values.
Expected outcome: Rows with null product_id are ignored in groupBy; only valid product_ids are aggregated.

Test Case ID: TC05
Test case description: Aggregation when all sales are zero.
Expected outcome: The resulting DataFrame contains product_id with total_sales = 0 for each product.

Test Case ID: TC06
Test case description: Aggregation with boundary date values (start_date == end_date).
Expected outcome: Only sales on the boundary date are aggregated.

Test Case ID: TC07
Test case description: Error handling when summary_table or detailed_sales_summary does not exist.
Expected outcome: The script raises an appropriate exception.

Test Case ID: TC08
Test case description: Temporary view creation and deletion.
Expected outcome: The temporary view is created and dropped without errors.

Test Case ID: TC09
Test case description: Aggregation when sales_table contains duplicate product_id rows.
Expected outcome: Aggregation sums sales correctly for each product_id.

Test Case ID: TC10
Test case description: Aggregation with a large dataset.
Expected outcome: The script completes successfully and aggregates sales accurately.

Pytest Script for each test case:

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.functions import sum as _sum

@pytest.fixture(scope="function")
def spark():
    spark = SparkSession.builder \
        .appName("TestProcessSalesData") \
        .master("local[2]") \
        .enableHiveSupport() \
        .getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture(scope="function")
def setup_tables(spark):
    # Drop tables if exist
    for tbl in ["sales_table", "summary_table", "detailed_sales_summary"]:
        spark.sql(f"DROP TABLE IF EXISTS {tbl}")
    # Create empty tables for testing
    spark.sql("CREATE TABLE sales_table (product_id INT, sale_date STRING, sales FLOAT)")
    spark.sql("CREATE TABLE summary_table (product_id INT, total_sales FLOAT)")
    spark.sql("CREATE TABLE detailed_sales_summary (product_id INT, total_sales FLOAT)")

def process_sales_data(spark, start_date, end_date):
    sales_df = (
        spark.table("sales_table")
        .filter((f"sale_date >= '{start_date}'") & (f"sale_date <= '{end_date}'"))
        .groupBy("product_id")
        .agg(_sum("sales").alias("total_sales"))
    )
    sales_df.write.mode("append").insertInto("summary_table")
    sales_df.createOrReplaceTempView("temp_sales_summary")
    sales_df.write.mode("append").insertInto("detailed_sales_summary")
    spark.catalog.dropTempView("temp_sales_summary")
    return sales_df

# TC01: Happy path
def test_tc01_happy_path(spark, setup_tables):
    data = [
        Row(product_id=1, sale_date="2023-01-10", sales=100.0),
        Row(product_id=1, sale_date="2023-01-20", sales=150.0),
        Row(product_id=2, sale_date="2023-01-15", sales=200.0),
    ]
    df = spark.createDataFrame(data)
    df.write.mode("append").insertInto("sales_table")
    process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = spark.table("summary_table").collect()
    assert len(summary) == 2
    assert any(row.product_id == 1 and row.total_sales == 250.0 for row in summary)
    assert any(row.product_id == 2 and row.total_sales == 200.0 for row in summary)

# TC02: Empty sales_table
def test_tc02_empty_sales_table(spark, setup_tables):
    process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = spark.table("summary_table").collect()
    assert summary == []

# TC03: Null sales values
def test_tc03_null_sales_values(spark, setup_tables):
    data = [
        Row(product_id=1, sale_date="2023-01-10", sales=None),
        Row(product_id=1, sale_date="2023-01-20", sales=150.0),
        Row(product_id=2, sale_date="2023-01-15", sales=None),
    ]
    df = spark.createDataFrame(data)
    df.write.mode("append").insertInto("sales_table")
    process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = spark.table("summary_table").collect()
    assert len(summary) == 1
    assert summary[0].product_id == 1
    assert summary[0].total_sales == 150.0

# TC04: Null product_id values
def test_tc04_null_product_id(spark, setup_tables):
    data = [
        Row(product_id=None, sale_date="2023-01-10", sales=100.0),
        Row(product_id=1, sale_date="2023-01-20", sales=150.0),
    ]
    df = spark.createDataFrame(data)
    df.write.mode("append").insertInto("sales_table")
    process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = spark.table("summary_table").collect()
    assert len(summary) == 1
    assert summary[0].product_id == 1
    assert summary[0].total_sales == 150.0

# TC05: All sales zero
def test_tc05_all_sales_zero(spark, setup_tables):
    data = [
        Row(product_id=1, sale_date="2023-01-10", sales=0.0),
        Row(product_id=2, sale_date="2023-01-20", sales=0.0),
    ]
    df = spark.createDataFrame(data)
    df.write.mode("append").insertInto("sales_table")
    process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = spark.table("summary_table").collect()
    assert len(summary) == 2
    for row in summary:
        assert row.total_sales == 0.0

# TC06: Boundary date values
def test_tc06_boundary_date(spark, setup_tables):
    data = [
        Row(product_id=1, sale_date="2023-01-01", sales=100.0),
        Row(product_id=2, sale_date="2023-01-31", sales=200.0),
        Row(product_id=1, sale_date="2023-01-15", sales=150.0),
    ]
    df = spark.createDataFrame(data)
    df.write.mode("append").insertInto("sales_table")
    process_sales_data(spark, "2023-01-01", "2023-01-01")
    summary = spark.table("summary_table").collect()
    assert len(summary) == 1
    assert summary[0].product_id == 1
    assert summary[0].total_sales == 100.0

# TC07: Error handling when summary_table does not exist
def test_tc07_missing_summary_table(spark):
    spark.sql("DROP TABLE IF EXISTS summary_table")
    spark.sql("CREATE TABLE sales_table (product_id INT, sale_date STRING, sales FLOAT)")
    with pytest.raises(Exception):
        process_sales_data(spark, "2023-01-01", "2023-01-31")

# TC08: Temporary view creation and deletion
def test_tc08_temp_view_creation_deletion(spark, setup_tables):
    data = [
        Row(product_id=1, sale_date="2023-01-10", sales=100.0),
    ]
    df = spark.createDataFrame(data)
    df.write.mode("append").insertInto("sales_table")
    process_sales_data(spark, "2023-01-01", "2023-01-31")
    assert "temp_sales_summary" not in spark.catalog.listTables()

# TC09: Duplicate product_id rows
def test_tc09_duplicate_product_id(spark, setup_tables):
    data = [
        Row(product_id=1, sale_date="2023-01-10", sales=100.0),
        Row(product_id=1, sale_date="2023-01-10", sales=150.0),
        Row(product_id=2, sale_date="2023-01-15", sales=200.0),
    ]
    df = spark.createDataFrame(data)
    df.write.mode("append").insertInto("sales_table")
    process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = spark.table("summary_table").collect()
    assert len(summary) == 2
    assert any(row.product_id == 1 and row.total_sales == 250.0 for row in summary)
    assert any(row.product_id == 2 and row.total_sales == 200.0 for row in summary)

# TC10: Large dataset
def test_tc10_large_dataset(spark, setup_tables):
    data = [Row(product_id=i % 10, sale_date="2023-01-15", sales=float(i)) for i in range(1000)]
    df = spark.createDataFrame(data)
    df.write.mode("append").insertInto("sales_table")
    process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = spark.table("summary_table").collect()
    assert len(summary) == 10
    total_sales = {row.product_id: row.total_sales for row in summary}
    for pid in range(10):
        expected = sum(float(i) for i in range(1000) if i % 10 == pid)
        assert total_sales[pid] == expected

# API cost consumed for this call: 2 units (1 file listing + 1 file read)
```

Cost consumed by the API for this call: 2 units (1 file listing + 1 file read)