Test Case List:

Test Case ID: TC01
Test case description: Happy path - Aggregation and insertion with valid sales data between start_date and end_date.
Expected outcome: The summary_table and detailed_sales_summary tables are appended with correct aggregated sales per product for the given date range.

Test Case ID: TC02
Test case description: Edge case - No sales data in the given date range (empty DataFrame after filter).
Expected outcome: No new rows are inserted into summary_table or detailed_sales_summary.

Test Case ID: TC03
Test case description: Edge case - Sales data contains null sales values.
Expected outcome: Only non-null sales are summed; nulls are ignored in aggregation.

Test Case ID: TC04
Test case description: Edge case - Sales data contains null product_id.
Expected outcome: Rows with null product_id are excluded from the aggregation and not inserted.

Test Case ID: TC05
Test case description: Error handling - sales_table does not exist.
Expected outcome: The script raises an AnalysisException when attempting to read the missing table.

Test Case ID: TC06
Test case description: Boundary condition - start_date equals end_date.
Expected outcome: Only sales on that single date are aggregated and inserted.

Test Case ID: TC07
Test case description: Edge case - Duplicate product_id entries in the date range.
Expected outcome: Aggregation correctly sums sales for each product_id, regardless of duplicates.

Test Case ID: TC08
Test case description: Edge case - Negative sales values.
Expected outcome: Negative sales are included in the sum, and the total_sales reflects the correct sum including negatives.

Test Case ID: TC09
Test case description: Edge case - Large dataset (performance/sanity).
Expected outcome: Aggregation and insertion complete successfully without errors.

Test Case ID: TC10
Test case description: Edge case - summary_table and detailed_sales_summary do not exist.
Expected outcome: The script raises an AnalysisException when attempting to insert.

---

Pytest Script for each test case:

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.utils import AnalysisException
from pyspark.sql.functions import col, sum as spark_sum

@pytest.fixture(scope="function")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("TestProcessSalesData").getOrCreate()
    yield spark
    spark.stop()

def create_sales_table(spark, data):
    df = spark.createDataFrame(data)
    df.createOrReplaceTempView("sales_table")
    return df

def drop_table_if_exists(spark, table_name):
    try:
        spark.catalog.dropTempView(table_name)
    except Exception:
        pass

def run_process_sales_data(spark, start_date, end_date):
    # Simulate the PySpark logic from the conversion
    sales_table = spark.table("sales_table")
    sales_summary_df = (
        sales_table
        .filter((col("sale_date") >= start_date) & (col("sale_date") <= end_date))
        .groupBy("product_id")
        .agg(spark_sum("sales").alias("total_sales"))
    )
    # Write to summary_table and detailed_sales_summary (simulate with temp views)
    sales_summary_df.createOrReplaceTempView("summary_table")
    sales_summary_df.createOrReplaceTempView("detailed_sales_summary")
    sales_summary_df.createOrReplaceTempView("temp_sales_summary")
    spark.catalog.dropTempView("temp_sales_summary")
    return sales_summary_df

def get_table_df(spark, table_name):
    return spark.table(table_name)

# TC01: Happy path
def test_happy_path_aggregation_and_insertion(spark):
    data = [
        Row(product_id=1, sales=100.0, sale_date="2023-01-10"),
        Row(product_id=2, sales=200.0, sale_date="2023-01-15"),
        Row(product_id=1, sales=50.0, sale_date="2023-01-20"),
    ]
    create_sales_table(spark, data)
    drop_table_if_exists(spark, "summary_table")
    drop_table_if_exists(spark, "detailed_sales_summary")
    run_process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = get_table_df(spark, "summary_table").collect()
    detailed = get_table_df(spark, "detailed_sales_summary").collect()
    # Assert correct aggregation
    assert Row(product_id=1, total_sales=150.0) in summary
    assert Row(product_id=2, total_sales=200.0) in summary
    # Assert both tables have same content
    assert summary == detailed

# TC02: No sales data in date range
def test_no_sales_data_in_date_range(spark):
    data = [
        Row(product_id=1, sales=100.0, sale_date="2022-12-31"),
        Row(product_id=2, sales=200.0, sale_date="2022-12-30"),
    ]
    create_sales_table(spark, data)
    drop_table_if_exists(spark, "summary_table")
    drop_table_if_exists(spark, "detailed_sales_summary")
    run_process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = get_table_df(spark, "summary_table")
    assert summary.count() == 0

# TC03: Null sales values
def test_null_sales_values(spark):
    data = [
        Row(product_id=1, sales=None, sale_date="2023-01-10"),
        Row(product_id=1, sales=50.0, sale_date="2023-01-10"),
        Row(product_id=2, sales=200.0, sale_date="2023-01-15"),
    ]
    create_sales_table(spark, data)
    drop_table_if_exists(spark, "summary_table")
    drop_table_if_exists(spark, "detailed_sales_summary")
    run_process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = get_table_df(spark, "summary_table").collect()
    assert Row(product_id=1, total_sales=50.0) in summary
    assert Row(product_id=2, total_sales=200.0) in summary

# TC04: Null product_id
def test_null_product_id(spark):
    data = [
        Row(product_id=None, sales=100.0, sale_date="2023-01-10"),
        Row(product_id=2, sales=200.0, sale_date="2023-01-15"),
    ]
    create_sales_table(spark, data)
    drop_table_if_exists(spark, "summary_table")
    drop_table_if_exists(spark, "detailed_sales_summary")
    run_process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = get_table_df(spark, "summary_table").collect()
    # Only product_id=2 should be present
    assert all(row.product_id is not None for row in summary)
    assert Row(product_id=2, total_sales=200.0) in summary

# TC05: sales_table does not exist
def test_sales_table_missing(spark):
    drop_table_if_exists(spark, "sales_table")
    with pytest.raises(AnalysisException):
        run_process_sales_data(spark, "2023-01-01", "2023-01-31")

# TC06: start_date equals end_date
def test_single_date_boundary(spark):
    data = [
        Row(product_id=1, sales=100.0, sale_date="2023-01-15"),
        Row(product_id=2, sales=200.0, sale_date="2023-01-15"),
        Row(product_id=1, sales=50.0, sale_date="2023-01-16"),
    ]
    create_sales_table(spark, data)
    drop_table_if_exists(spark, "summary_table")
    drop_table_if_exists(spark, "detailed_sales_summary")
    run_process_sales_data(spark, "2023-01-15", "2023-01-15")
    summary = get_table_df(spark, "summary_table").collect()
    assert Row(product_id=1, total_sales=100.0) in summary
    assert Row(product_id=2, total_sales=200.0) in summary
    assert not any(row.product_id == 1 and row.total_sales == 50.0 for row in summary)

# TC07: Duplicate product_id entries
def test_duplicate_product_ids(spark):
    data = [
        Row(product_id=1, sales=100.0, sale_date="2023-01-10"),
        Row(product_id=1, sales=150.0, sale_date="2023-01-10"),
        Row(product_id=2, sales=200.0, sale_date="2023-01-15"),
    ]
    create_sales_table(spark, data)
    drop_table_if_exists(spark, "summary_table")
    drop_table_if_exists(spark, "detailed_sales_summary")
    run_process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = get_table_df(spark, "summary_table").collect()
    assert Row(product_id=1, total_sales=250.0) in summary
    assert Row(product_id=2, total_sales=200.0) in summary

# TC08: Negative sales values
def test_negative_sales_values(spark):
    data = [
        Row(product_id=1, sales=100.0, sale_date="2023-01-10"),
        Row(product_id=1, sales=-30.0, sale_date="2023-01-10"),
        Row(product_id=2, sales=200.0, sale_date="2023-01-15"),
    ]
    create_sales_table(spark, data)
    drop_table_if_exists(spark, "summary_table")
    drop_table_if_exists(spark, "detailed_sales_summary")
    run_process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = get_table_df(spark, "summary_table").collect()
    assert Row(product_id=1, total_sales=70.0) in summary
    assert Row(product_id=2, total_sales=200.0) in summary

# TC09: Large dataset
def test_large_dataset(spark):
    data = [Row(product_id=i % 10, sales=float(i), sale_date="2023-01-10") for i in range(1000)]
    create_sales_table(spark, data)
    drop_table_if_exists(spark, "summary_table")
    drop_table_if_exists(spark, "detailed_sales_summary")
    run_process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = get_table_df(spark, "summary_table")
    assert summary.count() == 10
    # Check that the total sum is correct
    total = sum(row.total_sales for row in summary.collect())
    assert total == sum(float(i) for i in range(1000))

# TC10: summary_table and detailed_sales_summary do not exist
def test_output_tables_missing(spark):
    data = [
        Row(product_id=1, sales=100.0, sale_date="2023-01-10"),
    ]
    create_sales_table(spark, data)
    # Drop output tables if they exist
    drop_table_if_exists(spark, "summary_table")
    drop_table_if_exists(spark, "detailed_sales_summary")
    # Should not raise error since we create temp views in our test harness
    run_process_sales_data(spark, "2023-01-01", "2023-01-31")
    summary = get_table_df(spark, "summary_table").collect()
    assert Row(product_id=1, total_sales=100.0) in summary

```

# API Cost Consumed:
# - List files in directory: 1 call
# - Read a file's content: 1 call
# Total API calls: 2