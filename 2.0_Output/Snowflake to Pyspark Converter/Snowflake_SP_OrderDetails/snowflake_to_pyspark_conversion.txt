# PySpark equivalent of the Snowflake stored procedure `sp_Order_Detail`
# This script processes order details for the past month, aggregates total amount and order count per customer,
# and prints a summary for each customer. Temporary tables and cursors are replaced with DataFrame operations.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum, count as _count, current_timestamp, expr, date_add

# Initialize Spark session
spark = SparkSession.builder.appName("OrderDetailSummary").getOrCreate()

# Load Orders data into a DataFrame (replace with actual data source)
# Example: orders_df = spark.read.format("parquet").load("path/to/orders")
# For demonstration, we assume 'orders_df' is already loaded.

# Filter orders from the past month
orders_last_month_df = (
    orders_df
    .filter(
        col("OrderDate") >= expr("date_sub(current_timestamp(), 30)")
    )
)

# Aggregate total amount and order count per customer
customer_summary_df = (
    orders_last_month_df
    .groupBy("CustomerID")
    .agg(
        _sum("Amount").alias("TotalAmount"),
        _count("*").alias("OrderCount")
    )
)

# Print summary for each customer
for row in customer_summary_df.collect():
    print(
        f"CustomerID: {row['CustomerID']} | TotalAmount: {row['TotalAmount']} | OrderCount: {row['OrderCount']}"
    )

# Note: Temporary tables are not needed in PySpark as DataFrames are ephemeral.
# No explicit cleanup is required.

# Stop the Spark session if this is the end of your job
# spark.stop()

# API cost consumed for this call: 
# - List files in directory: 1 unit
# - Read a file's content: 1 unit
# Total cost: 2 units