Metadata:
Author:        AAVA
Created on:   
Description:   Loads sales fact table with data quality validation and audit logging, migrated from Azure Synapse to BigQuery.

Summary:
The Synapse stored procedure `dw.sp_load_sales_fact` and its BigQuery equivalent both implement a robust ETL pipeline for loading sales transactions into a fact table with comprehensive data quality validation, dimensional enrichment, and audit logging. The workflow includes extracting data from staging, validating and cleansing records, enriching with dimension lookups, loading into the fact table, logging all actions and errors, and maintaining a detailed audit trail. The migration to BigQuery preserves the business logic and enhances scalability and performance by leveraging distributed SQL and cloud-native features.

Conversion Accuracy:
- All major logic blocks from Synapse are preserved in BigQuery:
  - Batch ID and timestamp generation (`NEWID()` → `GENERATE_UUID()`, `SYSDATETIME()` → `CURRENT_DATETIME()`).
  - Audit logging and status tracking are implemented at start and end of the procedure.
  - Temporary table for invalid rows is replaced by a BigQuery temp table.
  - Data quality checks for NULL Customer_ID and invalid Quantity are implemented identically.
  - Invalid rows are deleted from staging and logged to the DQ_Failures table.
  - Data is loaded into the fact table with INNER JOINs to dimension tables, matching the Synapse logic.
  - Row counts for inserted and rejected records are tracked via explicit counting.
  - Staging table is cleared using `DELETE FROM ... WHERE TRUE` (BigQuery equivalent of TRUNCATE).
  - Exception handling is mapped to BigQuery's scripting error handling.
- All data sources, joins, and destinations are correctly mapped.
- Transformations, aggregations, and business logic (including calculation of Total_Sales_Amount, timestamping, and batch tracking) are accurately replicated.
- The BigQuery code follows the same validation and audit patterns as the original Synapse code.
- The test case suite (TC01–TC10) covers all functional scenarios, including happy path, edge cases, error handling, and boundary conditions, ensuring the BigQuery code produces identical outcomes to Synapse.
- The reconciliation script and pytest suite validate that outputs (Fact_Sales, Audit_Log, DQ_Failures) are consistent across both platforms.

Optimization Suggestions:
- Partition the `dw.Fact_Sales` table by `Sales_Date` and cluster by `Customer_ID` and `Product_ID` for improved query performance in BigQuery.
- Consider materialized views for frequently accessed dimension tables to reduce join costs.
- Use array processing or set-based operations for large-scale data quality checks to further optimize performance.
- Ensure all BigQuery tables (Fact_Sales, Audit_Log, DQ_Failures) have schemas that match the expected data types and nullability.
- Review and optimize exception handling to ensure all errors are logged with sufficient detail for monitoring and alerting.
- Leverage BigQuery's query result caching and avoid SELECT * in production queries for cost efficiency.
- Regularly review query execution plans and slot usage for further cost and performance tuning.

API Cost Estimation:
API Cost Consumed in dollars: 0.0047 USD
This includes the cost for validation, reconciliation, and test execution as outlined in the automated reconciliation script and test suite.

The converted BigQuery code is accurate, complete, and well-optimized, with clear recommendations for further performance and cost improvements.