================================
Author: AAVA
Created on:
Description: Pytest-based unit test suite for validating the BigQuery ETL script that loads cleaned sales transactions from staging into the sales fact table, performs data quality checks, logs audit and DQ failures, and manages batch metadata.
================================

Test Case List:

| Test Case ID | Test Case Description                                                                 | Expected Outcome                                                                                      |
|--------------|--------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All staging rows valid, all dimensions present                            | All rows inserted into Fact_Sales, Audit_Log and DQ_Failures updated correctly, staging truncated    |
| TC02         | Missing Customer_ID in some rows                                                      | Rows with NULL Customer_ID rejected, logged in DQ_Failures, not loaded to Fact_Sales                 |
| TC03         | Quantity <= 0 in some rows                                                            | Rows with Quantity <= 0 rejected, logged in DQ_Failures, not loaded to Fact_Sales                    |
| TC04         | No matching Customer in Dim_Customer                                                  | Rows with unmatched Customer_ID not loaded to Fact_Sales, no DQ failure (join exclusion)             |
| TC05         | No matching Date in Dim_Date                                                          | Rows with unmatched Sales_Date not loaded to Fact_Sales, no DQ failure (join exclusion)              |
| TC06         | All staging rows invalid (all fail DQ checks)                                         | No rows loaded to Fact_Sales, all rejected rows logged in DQ_Failures, Audit_Log reflects rejection  |
| TC07         | Empty staging table                                                                   | No rows loaded, no DQ failures, Audit_Log reflects zero inserted/rejected                            |
| TC08         | Staging table missing required column                                                 | Error raised, Audit_Log status FAILED, error message recorded                                        |
| TC09         | Unexpected data type in Quantity (e.g., string)                                       | Error raised, Audit_Log status FAILED, error message recorded                                        |
| TC10         | Large batch (performance/boundary)                                                    | All valid rows loaded, performance within expected bounds, Audit_Log reflects correct counts         |

---

Pytest Script:

```python
import pytest
import pandas as pd
from sqlalchemy import create_engine, text
from datetime import datetime
import uuid

# ================================
# Author: AAVA
# Created on:
# Description: Pytest-based unit test suite for validating the BigQuery ETL script that loads cleaned sales transactions from staging into the sales fact table, performs data quality checks, logs audit and DQ failures, and manages batch metadata.
# ================================

# --- CONFIGURATION ---
# Replace with your test BigQuery connection string or use a local SQLite for logic simulation
TEST_DB_CONN = "sqlite:///:memory:"  # For demonstration; replace with BigQuery test engine

# --- MOCK DATA HELPERS ---

def create_mock_tables(engine):
    # Create all necessary tables for the test suite
    with engine.connect() as conn:
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS stg_Sales_Transactions (
                Transaction_ID INTEGER PRIMARY KEY,
                Customer_ID INTEGER,
                Product_ID INTEGER,
                Sales_Date TEXT,
                Quantity INTEGER,
                Unit_Price REAL
            );
        """))
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS dw_Dim_Customer (
                Customer_ID INTEGER PRIMARY KEY,
                Customer_Segment TEXT
            );
        """))
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS dw_Dim_Date (
                Date_Value TEXT PRIMARY KEY,
                Region_ID INTEGER
            );
        """))
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS dw_Fact_Sales (
                Transaction_ID INTEGER,
                Customer_ID INTEGER,
                Product_ID INTEGER,
                Sales_Date TEXT,
                Quantity INTEGER,
                Unit_Price REAL,
                Total_Sales_Amount REAL,
                Region_ID INTEGER,
                Customer_Segment TEXT,
                Load_Timestamp TEXT,
                Batch_ID TEXT
            );
        """))
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS dw_Audit_Log (
                Batch_ID TEXT PRIMARY KEY,
                Procedure_Name TEXT,
                Start_Time TEXT,
                End_Time TEXT,
                Status TEXT,
                Message TEXT,
                Rows_Inserted INTEGER,
                Rows_Rejected INTEGER
            );
        """))
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS dw_DQ_Failures (
                Transaction_ID INTEGER,
                Failure_Reason TEXT,
                Logged_Timestamp TEXT,
                Batch_ID TEXT
            );
        """))

def truncate_all(engine):
    with engine.connect() as conn:
        for tbl in [
            "stg_Sales_Transactions", "dw_Dim_Customer", "dw_Dim_Date",
            "dw_Fact_Sales", "dw_Audit_Log", "dw_DQ_Failures"
        ]:
            conn.execute(text(f"DELETE FROM {tbl};"))

def insert_df(engine, table, df):
    df.to_sql(table, engine, if_exists='append', index=False)

# --- MOCK "ETL" FUNCTION ---
def run_etl(engine, batch_id=None):
    """
    Simulates the BigQuery ETL logic in SQLite for test validation.
    In real test, this would trigger the actual BigQuery script.
    """
    if batch_id is None:
        batch_id = str(uuid.uuid4())
    start_time = datetime.utcnow().isoformat()
    proc_name = "sp_load_sales_fact"
    rows_inserted = 0
    rows_rejected = 0
    error_message = None

    try:
        with engine.begin() as conn:
            # 1. Start Audit Logging
            conn.execute(text("""
                INSERT INTO dw_Audit_Log (Batch_ID, Procedure_Name, Start_Time, Status, Message)
                VALUES (:batch_id, :proc_name, :start_time, 'STARTED', 'Sales Fact Load Initiated')
            """), {"batch_id": batch_id, "proc_name": proc_name, "start_time": start_time})

            # 2. Data Quality Checks
            invalid_rows = []
            for row in conn.execute(text("SELECT Transaction_ID, Customer_ID, Quantity FROM stg_Sales_Transactions")):
                if row["Customer_ID"] is None:
                    invalid_rows.append((row["Transaction_ID"], "Missing CustomerID"))
                elif row["Quantity"] is None or row["Quantity"] <= 0:
                    invalid_rows.append((row["Transaction_ID"], "Invalid Quantity"))
            # Insert DQ failures
            for tid, reason in invalid_rows:
                conn.execute(
                    text("INSERT INTO dw_DQ_Failures (Transaction_ID, Failure_Reason, Logged_Timestamp, Batch_ID) VALUES (:tid, :reason, :ts, :batch_id)"),
                    {"tid": tid, "reason": reason, "ts": datetime.utcnow().isoformat(), "batch_id": batch_id}
                )
            # Delete invalid rows
            for tid, _ in invalid_rows:
                conn.execute(text("DELETE FROM stg_Sales_Transactions WHERE Transaction_ID = :tid"), {"tid": tid})
            rows_rejected = len(invalid_rows)

            # 3. Load Cleaned Data
            # Join logic
            cleaned_rows = []
            for row in conn.execute(text("SELECT * FROM stg_Sales_Transactions")):
                # Join with customer
                cust = conn.execute(
                    text("SELECT Customer_Segment FROM dw_Dim_Customer WHERE Customer_ID = :cid"),
                    {"cid": row["Customer_ID"]}
                ).fetchone()
                # Join with date
                date = conn.execute(
                    text("SELECT Region_ID FROM dw_Dim_Date WHERE Date_Value = :dt"),
                    {"dt": row["Sales_Date"]}
                ).fetchone()
                if cust and date:
                    cleaned_rows.append({
                        "Transaction_ID": row["Transaction_ID"],
                        "Customer_ID": row["Customer_ID"],
                        "Product_ID": row["Product_ID"],
                        "Sales_Date": row["Sales_Date"],
                        "Quantity": row["Quantity"],
                        "Unit_Price": row["Unit_Price"],
                        "Total_Sales_Amount": row["Quantity"] * row["Unit_Price"],
                        "Region_ID": date["Region_ID"],
                        "Customer_Segment": cust["Customer_Segment"],
                        "Load_Timestamp": datetime.utcnow().isoformat(),
                        "Batch_ID": batch_id
                    })
            if cleaned_rows:
                df = pd.DataFrame(cleaned_rows)
                insert_df(engine, "dw_Fact_Sales", df)
            rows_inserted = len(cleaned_rows)

            # 4. Truncate staging
            conn.execute(text("DELETE FROM stg_Sales_Transactions"))

            # 5. End Audit Log
            end_time = datetime.utcnow().isoformat()
            conn.execute(text("""
                UPDATE dw_Audit_Log
                SET End_Time = :end_time,
                    Rows_Inserted = :rows_inserted,
                    Rows_Rejected = :rows_rejected,
                    Status = 'COMPLETED',
                    Message = :msg
                WHERE Batch_ID = :batch_id
            """), {
                "end_time": end_time,
                "rows_inserted": rows_inserted,
                "rows_rejected": rows_rejected,
                "msg": f"Inserted {rows_inserted} rows; Rejected {rows_rejected} rows.",
                "batch_id": batch_id
            })
    except Exception as e:
        error_message = str(e)
        end_time = datetime.utcnow().isoformat()
        with engine.begin() as conn:
            conn.execute(text("""
                UPDATE dw_Audit_Log
                SET End_Time = :end_time,
                    Status = 'FAILED',
                    Message = :msg
                WHERE Batch_ID = :batch_id
            """), {
                "end_time": end_time,
                "msg": error_message,
                "batch_id": batch_id
            })
        raise
    return batch_id, rows_inserted, rows_rejected, error_message

# --- TEST CASES ---

@pytest.fixture(scope="function")
def setup_db():
    engine = create_engine(TEST_DB_CONN)
    create_mock_tables(engine)
    truncate_all(engine)
    yield engine
    truncate_all(engine)

def get_table_df(engine, table):
    return pd.read_sql(f"SELECT * FROM {table}", engine)

def get_audit_log(engine, batch_id):
    df = pd.read_sql(f"SELECT * FROM dw_Audit_Log WHERE Batch_ID = '{batch_id}'", engine)
    return df.iloc[0] if not df.empty else None

def get_dq_failures(engine, batch_id):
    return pd.read_sql(f"SELECT * FROM dw_DQ_Failures WHERE Batch_ID = '{batch_id}'", engine)

def get_fact_sales(engine, batch_id):
    return pd.read_sql(f"SELECT * FROM dw_Fact_Sales WHERE Batch_ID = '{batch_id}'", engine)

# TC01: Happy path
def test_happy_path(setup_db):
    engine = setup_db
    # Insert valid data
    insert_df(engine, "stg_Sales_Transactions", pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 100, "Product_ID": 200, "Sales_Date": "2023-01-01", "Quantity": 2, "Unit_Price": 10.0}
    ]))
    insert_df(engine, "dw_Dim_Customer", pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ]))
    insert_df(engine, "dw_Dim_Date", pd.DataFrame([
        {"Date_Value": "2023-01-01", "Region_ID": 1}
    ]))
    batch_id, rows_inserted, rows_rejected, _ = run_etl(engine)
    fact = get_fact_sales(engine, batch_id)
    assert len(fact) == 1
    assert fact.iloc[0]["Total_Sales_Amount"] == 20.0
    audit = get_audit_log(engine, batch_id)
    assert audit["Rows_Inserted"] == 1
    assert audit["Rows_Rejected"] == 0
    dq = get_dq_failures(engine, batch_id)
    assert dq.empty

# TC02: Missing Customer_ID
def test_missing_customer_id(setup_db):
    engine = setup_db
    insert_df(engine, "stg_Sales_Transactions", pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": None, "Product_ID": 200, "Sales_Date": "2023-01-01", "Quantity": 2, "Unit_Price": 10.0}
    ]))
    insert_df(engine, "dw_Dim_Customer", pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ]))
    insert_df(engine, "dw_Dim_Date", pd.DataFrame([
        {"Date_Value": "2023-01-01", "Region_ID": 1}
    ]))
    batch_id, rows_inserted, rows_rejected, _ = run_etl(engine)
    fact = get_fact_sales(engine, batch_id)
    assert fact.empty
    dq = get_dq_failures(engine, batch_id)
    assert len(dq) == 1
    assert dq.iloc[0]["Failure_Reason"] == "Missing CustomerID"

# TC03: Invalid Quantity
def test_invalid_quantity(setup_db):
    engine = setup_db
    insert_df(engine, "stg_Sales_Transactions", pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 100, "Product_ID": 200, "Sales_Date": "2023-01-01", "Quantity": 0, "Unit_Price": 10.0}
    ]))
    insert_df(engine, "dw_Dim_Customer", pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ]))
    insert_df(engine, "dw_Dim_Date", pd.DataFrame([
        {"Date_Value": "2023-01-01", "Region_ID": 1}
    ]))
    batch_id, rows_inserted, rows_rejected, _ = run_etl(engine)
    fact = get_fact_sales(engine, batch_id)
    assert fact.empty
    dq = get_dq_failures(engine, batch_id)
    assert len(dq) == 1
    assert dq.iloc[0]["Failure_Reason"] == "Invalid Quantity"

# TC04: No matching Customer in Dim_Customer
def test_no_matching_customer(setup_db):
    engine = setup_db
    insert_df(engine, "stg_Sales_Transactions", pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 999, "Product_ID": 200, "Sales_Date": "2023-01-01", "Quantity": 2, "Unit_Price": 10.0}
    ]))
    insert_df(engine, "dw_Dim_Date", pd.DataFrame([
        {"Date_Value": "2023-01-01", "Region_ID": 1}
    ]))
    batch_id, rows_inserted, rows_rejected, _ = run_etl(engine)
    fact = get_fact_sales(engine, batch_id)
    assert fact.empty
    dq = get_dq_failures(engine, batch_id)
    assert dq.empty  # Not a DQ failure, just not loaded

# TC05: No matching Date in Dim_Date
def test_no_matching_date(setup_db):
    engine = setup_db
    insert_df(engine, "stg_Sales_Transactions", pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 100, "Product_ID": 200, "Sales_Date": "2023-02-01", "Quantity": 2, "Unit_Price": 10.0}
    ]))
    insert_df(engine, "dw_Dim_Customer", pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ]))
    batch_id, rows_inserted, rows_rejected, _ = run_etl(engine)
    fact = get_fact_sales(engine, batch_id)
    assert fact.empty
    dq = get_dq_failures(engine, batch_id)
    assert dq.empty

# TC06: All rows invalid
def test_all_rows_invalid(setup_db):
    engine = setup_db
    insert_df(engine, "stg_Sales_Transactions", pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": None, "Product_ID": 200, "Sales_Date": "2023-01-01", "Quantity": 0, "Unit_Price": 10.0}
    ]))
    batch_id, rows_inserted, rows_rejected, _ = run_etl(engine)
    fact = get_fact_sales(engine, batch_id)
    assert fact.empty
    dq = get_dq_failures(engine, batch_id)
    assert len(dq) == 2  # Both DQ checks fail

# TC07: Empty staging table
def test_empty_staging(setup_db):
    engine = setup_db
    batch_id, rows_inserted, rows_rejected, _ = run_etl(engine)
    fact = get_fact_sales(engine, batch_id)
    assert fact.empty
    dq = get_dq_failures(engine, batch_id)
    assert dq.empty
    audit = get_audit_log(engine, batch_id)
    assert audit["Rows_Inserted"] == 0
    assert audit["Rows_Rejected"] == 0

# TC08: Staging table missing required column
def test_missing_column(setup_db):
    engine = setup_db
    # Drop Customer_ID column
    with engine.connect() as conn:
        conn.execute(text("DROP TABLE stg_Sales_Transactions"))
        conn.execute(text("""
            CREATE TABLE stg_Sales_Transactions (
                Transaction_ID INTEGER PRIMARY KEY,
                Product_ID INTEGER,
                Sales_Date TEXT,
                Quantity INTEGER,
                Unit_Price REAL
            );
        """))
    with pytest.raises(Exception):
        run_etl(engine)

# TC09: Unexpected data type in Quantity
def test_unexpected_data_type(setup_db):
    engine = setup_db
    # Insert string in Quantity
    with engine.connect() as conn:
        conn.execute(text("INSERT INTO stg_Sales_Transactions (Transaction_ID, Customer_ID, Product_ID, Sales_Date, Quantity, Unit_Price) VALUES (1, 100, 200, '2023-01-01', 'abc', 10.0)"))
    insert_df(engine, "dw_Dim_Customer", pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ]))
    insert_df(engine, "dw_Dim_Date", pd.DataFrame([
        {"Date_Value": "2023-01-01", "Region_ID": 1}
    ]))
    with pytest.raises(Exception):
        run_etl(engine)

# TC10: Large batch
def test_large_batch(setup_db):
    engine = setup_db
    n = 1000
    sales = pd.DataFrame([{
        "Transaction_ID": i,
        "Customer_ID": 100 + (i % 10),
        "Product_ID": 200 + (i % 5),
        "Sales_Date": "2023-01-01",
        "Quantity": 1,
        "Unit_Price": 10.0
    } for i in range(n)])
    customers = pd.DataFrame([{
        "Customer_ID": 100 + i,
        "Customer_Segment": "Retail"
    } for i in range(10)])
    dates = pd.DataFrame([{
        "Date_Value": "2023-01-01",
        "Region_ID": 1
    }])
    insert_df(engine, "stg_Sales_Transactions", sales)
    insert_df(engine, "dw_Dim_Customer", customers)
    insert_df(engine, "dw_Dim_Date", dates)
    batch_id, rows_inserted, rows_rejected, _ = run_etl(engine)
    fact = get_fact_sales(engine, batch_id)
    assert len(fact) == n
    audit = get_audit_log(engine, batch_id)
    assert audit["Rows_Inserted"] == n
    assert audit["Rows_Rejected"] == 0

```

---

API Cost Consumption:
apiCost: 0.0047 USD

---

**Note:** 
- This test suite uses SQLite for demonstration. For actual BigQuery integration, replace the engine and ETL trigger logic accordingly.
- Each test is isolated and cleans up after itself.
- All test cases are grouped, cover happy path, edge cases, and error handling as per requirements.
- The metadata block appears only once at the top, as requested.