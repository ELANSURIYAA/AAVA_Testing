================================
Author: AAVA
Created on: 
Description: Unit tests and Pytest script for validating the BigQuery migration of the Sales Fact Load procedure, covering data quality checks, transformations, audit logging, and error handling.
================================

Test Case List:

| Test Case ID | Test Case Description                                                                 | Expected Outcome                                                                                  |
|--------------|--------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All staging records are valid and join to dimension tables                | All records inserted into dw.Fact_Sales; Audit_Log and DQ_Failures updated correctly              |
| TC02         | Missing Customer_ID in some staging records                                          | Records with NULL Customer_ID are rejected, logged in DQ_Failures, not inserted into Fact_Sales   |
| TC03         | Invalid Quantity (<=0) in some staging records                                       | Records with Quantity <=0 are rejected, logged in DQ_Failures, not inserted into Fact_Sales       |
| TC04         | Staging records with no matching Customer in Dim_Customer                            | Records with unmatched Customer_ID are not loaded into Fact_Sales                                 |
| TC05         | Staging records with no matching Date in Dim_Date                                    | Records with unmatched Sales_Date are not loaded into Fact_Sales                                  |
| TC06         | All staging records invalid (all fail DQ checks)                                     | No records inserted into Fact_Sales; all rejected and logged in DQ_Failures                       |
| TC07         | Empty staging table                                                                  | No records inserted or rejected; Audit_Log reflects zero processed                                |
| TC08         | Staging table missing required columns                                               | Error is raised, Audit_Log status set to FAILED                                                   |
| TC09         | Unexpected data types in staging (e.g., string in Quantity)                          | Error is raised, Audit_Log status set to FAILED                                                   |
| TC10         | Large batch: High volume of valid and invalid records                                | All valid records inserted, invalids rejected and logged; Audit_Log accurate                      |
| TC11         | NULL values in non-key, non-validated columns (e.g., Product_ID)                     | Records inserted if DQ checks pass; NULLs allowed in Product_ID                                   |
| TC12         | Multiple DQ failures for the same Transaction_ID                                     | All DQ failures for a record are logged; record is not loaded into Fact_Sales                     |

---

Pytest Script:

```python
================================
Author: AAVA
Created on: 
Description: Pytest script for validating BigQuery migration of Sales Fact Load procedure, covering DQ, transformations, and logging.
================================

import pytest
import pandas as pd
from sqlalchemy import create_engine, text

# ---- CONFIGURATION ----
BQ_CONN_STRING = "bigquery://project/dataset"  # Replace with actual connection string for test env

# ---- HELPER FUNCTIONS ----

def setup_test_tables(engine, sales_data, customer_data, date_data):
    # Create and populate test tables
    with engine.begin() as conn:
        # Clean up
        conn.execute(text("DROP TABLE IF EXISTS stg.Sales_Transactions"))
        conn.execute(text("DROP TABLE IF EXISTS dw.Dim_Customer"))
        conn.execute(text("DROP TABLE IF EXISTS dw.Dim_Date"))
        conn.execute(text("DROP TABLE IF EXISTS dw.Fact_Sales"))
        conn.execute(text("DROP TABLE IF EXISTS dw.Audit_Log"))
        conn.execute(text("DROP TABLE IF EXISTS dw.DQ_Failures"))
        # Create tables
        conn.execute(text("""
            CREATE TABLE stg.Sales_Transactions (
                Transaction_ID INT64,
                Customer_ID INT64,
                Product_ID INT64,
                Sales_Date DATE,
                Quantity INT64,
                Unit_Price FLOAT64
            )
        """))
        conn.execute(text("""
            CREATE TABLE dw.Dim_Customer (
                Customer_ID INT64,
                Customer_Segment STRING
            )
        """))
        conn.execute(text("""
            CREATE TABLE dw.Dim_Date (
                Date_Value DATE,
                Region_ID STRING
            )
        """))
        conn.execute(text("""
            CREATE TABLE dw.Fact_Sales AS SELECT * FROM UNNEST([])  -- Empty table with correct schema
        """))
        conn.execute(text("""
            CREATE TABLE dw.Audit_Log (
                Batch_ID STRING,
                Procedure_Name STRING,
                Start_Time TIMESTAMP,
                End_Time TIMESTAMP,
                Status STRING,
                Message STRING,
                Rows_Inserted INT64,
                Rows_Rejected INT64
            )
        """))
        conn.execute(text("""
            CREATE TABLE dw.DQ_Failures (
                Transaction_ID INT64,
                Failure_Reason STRING,
                Logged_Timestamp TIMESTAMP,
                Batch_ID STRING
            )
        """))
        # Insert data
        sales_data.to_sql('Sales_Transactions', con=engine, schema='stg', if_exists='append', index=False)
        customer_data.to_sql('Dim_Customer', con=engine, schema='dw', if_exists='append', index=False)
        date_data.to_sql('Dim_Date', con=engine, schema='dw', if_exists='append', index=False)

def teardown_test_tables(engine):
    with engine.begin() as conn:
        conn.execute(text("DROP TABLE IF EXISTS stg.Sales_Transactions"))
        conn.execute(text("DROP TABLE IF EXISTS dw.Dim_Customer"))
        conn.execute(text("DROP TABLE IF EXISTS dw.Dim_Date"))
        conn.execute(text("DROP TABLE IF EXISTS dw.Fact_Sales"))
        conn.execute(text("DROP TABLE IF EXISTS dw.Audit_Log"))
        conn.execute(text("DROP TABLE IF EXISTS dw.DQ_Failures"))

def run_bigquery_procedure(engine):
    # This function should execute the BigQuery SQL migration script.
    # For testing, you may wrap the script in a stored procedure and call it here.
    with engine.begin() as conn:
        conn.execute(text("CALL dw.sp_load_sales_fact()"))

def fetch_table(engine, table):
    return pd.read_sql(f"SELECT * FROM {table}", engine)

# ---- TEST CASES ----

@pytest.fixture(scope="function")
def bq_engine():
    engine = create_engine(BQ_CONN_STRING)
    yield engine
    engine.dispose()

@pytest.fixture(autouse=True)
def clean_tables(bq_engine):
    teardown_test_tables(bq_engine)
    yield
    teardown_test_tables(bq_engine)

def test_TC01_happy_path(bq_engine):
    # All valid records
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 100, "Product_ID": 10, "Sales_Date": "2024-01-01", "Quantity": 2, "Unit_Price": 50.0},
        {"Transaction_ID": 2, "Customer_ID": 101, "Product_ID": 11, "Sales_Date": "2024-01-02", "Quantity": 1, "Unit_Price": 30.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"},
        {"Customer_ID": 101, "Customer_Segment": "Wholesale"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"},
        {"Date_Value": "2024-01-02", "Region_ID": "EU"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    fact = fetch_table(bq_engine, "dw.Fact_Sales")
    assert len(fact) == 2
    assert fact['Total_Sales_Amount'].tolist() == [100.0, 30.0]
    dq = fetch_table(bq_engine, "dw.DQ_Failures")
    assert dq.empty
    audit = fetch_table(bq_engine, "dw.Audit_Log")
    assert audit.iloc[-1]['Rows_Inserted'] == 2
    assert audit.iloc[-1]['Rows_Rejected'] == 0

def test_TC02_missing_customer_id(bq_engine):
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": None, "Product_ID": 10, "Sales_Date": "2024-01-01", "Quantity": 2, "Unit_Price": 50.0},
        {"Transaction_ID": 2, "Customer_ID": 101, "Product_ID": 11, "Sales_Date": "2024-01-02", "Quantity": 1, "Unit_Price": 30.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 101, "Customer_Segment": "Wholesale"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"},
        {"Date_Value": "2024-01-02", "Region_ID": "EU"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    fact = fetch_table(bq_engine, "dw.Fact_Sales")
    assert len(fact) == 1
    dq = fetch_table(bq_engine, "dw.DQ_Failures")
    assert dq['Transaction_ID'].tolist() == [1]
    assert dq['Failure_Reason'].iloc[0] == 'Missing CustomerID'
    audit = fetch_table(bq_engine, "dw.Audit_Log")
    assert audit.iloc[-1]['Rows_Inserted'] == 1
    assert audit.iloc[-1]['Rows_Rejected'] == 1

def test_TC03_invalid_quantity(bq_engine):
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 100, "Product_ID": 10, "Sales_Date": "2024-01-01", "Quantity": 0, "Unit_Price": 50.0},
        {"Transaction_ID": 2, "Customer_ID": 101, "Product_ID": 11, "Sales_Date": "2024-01-02", "Quantity": -1, "Unit_Price": 30.0},
        {"Transaction_ID": 3, "Customer_ID": 102, "Product_ID": 12, "Sales_Date": "2024-01-03", "Quantity": 2, "Unit_Price": 10.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"},
        {"Customer_ID": 101, "Customer_Segment": "Wholesale"},
        {"Customer_ID": 102, "Customer_Segment": "Retail"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"},
        {"Date_Value": "2024-01-02", "Region_ID": "EU"},
        {"Date_Value": "2024-01-03", "Region_ID": "APAC"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    fact = fetch_table(bq_engine, "dw.Fact_Sales")
    assert len(fact) == 1
    assert fact['Transaction_ID'].iloc[0] == 3
    dq = fetch_table(bq_engine, "dw.DQ_Failures")
    assert set(dq['Transaction_ID']) == {1, 2}
    assert set(dq['Failure_Reason']) == {'Invalid Quantity'}
    audit = fetch_table(bq_engine, "dw.Audit_Log")
    assert audit.iloc[-1]['Rows_Inserted'] == 1
    assert audit.iloc[-1]['Rows_Rejected'] == 2

def test_TC04_no_matching_customer(bq_engine):
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 999, "Product_ID": 10, "Sales_Date": "2024-01-01", "Quantity": 2, "Unit_Price": 50.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    fact = fetch_table(bq_engine, "dw.Fact_Sales")
    assert fact.empty
    dq = fetch_table(bq_engine, "dw.DQ_Failures")
    assert dq.empty

def test_TC05_no_matching_date(bq_engine):
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 100, "Product_ID": 10, "Sales_Date": "2024-01-05", "Quantity": 2, "Unit_Price": 50.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    fact = fetch_table(bq_engine, "dw.Fact_Sales")
    assert fact.empty
    dq = fetch_table(bq_engine, "dw.DQ_Failures")
    assert dq.empty

def test_TC06_all_invalid(bq_engine):
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": None, "Product_ID": 10, "Sales_Date": "2024-01-01", "Quantity": 0, "Unit_Price": 50.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    fact = fetch_table(bq_engine, "dw.Fact_Sales")
    assert fact.empty
    dq = fetch_table(bq_engine, "dw.DQ_Failures")
    assert dq['Transaction_ID'].iloc[0] == 1
    audit = fetch_table(bq_engine, "dw.Audit_Log")
    assert audit.iloc[-1]['Rows_Inserted'] == 0
    assert audit.iloc[-1]['Rows_Rejected'] == 1

def test_TC07_empty_staging(bq_engine):
    sales = pd.DataFrame(columns=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    fact = fetch_table(bq_engine, "dw.Fact_Sales")
    assert fact.empty
    dq = fetch_table(bq_engine, "dw.DQ_Failures")
    assert dq.empty
    audit = fetch_table(bq_engine, "dw.Audit_Log")
    assert audit.iloc[-1]['Rows_Inserted'] == 0
    assert audit.iloc[-1]['Rows_Rejected'] == 0

def test_TC08_missing_columns(bq_engine):
    # Missing Quantity column
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 100, "Product_ID": 10, "Sales_Date": "2024-01-01", "Unit_Price": 50.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    with pytest.raises(Exception):
        run_bigquery_procedure(bq_engine)
    audit = fetch_table(bq_engine, "dw.Audit_Log")
    assert audit.iloc[-1]['Status'] == 'FAILED'

def test_TC09_unexpected_datatype(bq_engine):
    # Quantity as string
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 100, "Product_ID": 10, "Sales_Date": "2024-01-01", "Quantity": "abc", "Unit_Price": 50.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    with pytest.raises(Exception):
        run_bigquery_procedure(bq_engine)
    audit = fetch_table(bq_engine, "dw.Audit_Log")
    assert audit.iloc[-1]['Status'] == 'FAILED'

def test_TC10_large_batch(bq_engine):
    sales = pd.DataFrame([
        {"Transaction_ID": i, "Customer_ID": 100 + (i % 2), "Product_ID": 10, "Sales_Date": "2024-01-01", "Quantity": 1 if i % 10 else 0, "Unit_Price": 10.0}
        for i in range(1, 101)
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"},
        {"Customer_ID": 101, "Customer_Segment": "Wholesale"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    fact = fetch_table(bq_engine, "dw.Fact_Sales")
    # Every 10th record has Quantity=0 (invalid)
    assert len(fact) == 90
    dq = fetch_table(bq_engine, "dw.DQ_Failures")
    assert len(dq) == 10
    audit = fetch_table(bq_engine, "dw.Audit_Log")
    assert audit.iloc[-1]['Rows_Inserted'] == 90
    assert audit.iloc[-1]['Rows_Rejected'] == 10

def test_TC11_null_product_id(bq_engine):
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": 100, "Product_ID": None, "Sales_Date": "2024-01-01", "Quantity": 2, "Unit_Price": 50.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    fact = fetch_table(bq_engine, "dw.Fact_Sales")
    assert len(fact) == 1
    assert pd.isnull(fact['Product_ID'].iloc[0])

def test_TC12_multiple_dq_failures(bq_engine):
    sales = pd.DataFrame([
        {"Transaction_ID": 1, "Customer_ID": None, "Product_ID": 10, "Sales_Date": "2024-01-01", "Quantity": 0, "Unit_Price": 50.0}
    ])
    customers = pd.DataFrame([
        {"Customer_ID": 100, "Customer_Segment": "Retail"}
    ])
    dates = pd.DataFrame([
        {"Date_Value": "2024-01-01", "Region_ID": "US"}
    ])
    setup_test_tables(bq_engine, sales, customers, dates)
    run_bigquery_procedure(bq_engine)
    dq = fetch_table(bq_engine, "dw.DQ_Failures")
    # Both DQ checks should be logged for the same Transaction_ID
    assert set(dq['Failure_Reason']) == {'Missing CustomerID', 'Invalid Quantity'}
    assert (dq['Transaction_ID'] == 1).all()
```

---

API Cost Consumption:
apiCost: 0.0040 USD