================================
Author: AAVA
Created on:
Description: Pytest-based unit tests for BigQuery sales fact table ETL procedure, covering data quality validation, transformation logic, audit logging, and error handling.
================================

Test Case List:

| Test Case ID | Description                                                                                       | Expected Outcome                                                                                          |
|--------------|--------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All sales transactions valid, all dimension lookups succeed                           | All rows loaded to dw_Fact_Sales; audit log shows correct counts; no DQ failures logged                  |
| TC02         | Data quality: Transactions with NULL Customer_ID                                                  | Invalid rows rejected, logged in dw_DQ_Failures; not loaded to dw_Fact_Sales; audit log reflects counts  |
| TC03         | Data quality: Transactions with Quantity <= 0                                                     | Invalid rows rejected, logged in dw_DQ_Failures; not loaded to dw_Fact_Sales; audit log reflects counts  |
| TC04         | Edge case: Empty stg_Sales_Transactions                                                           | No rows loaded or rejected; audit log shows zero counts; no DQ failures                                  |
| TC05         | Edge case: All rows invalid (all fail validation)                                                 | All rows rejected, logged in dw_DQ_Failures; dw_Fact_Sales remains empty; audit log reflects counts      |
| TC06         | Edge case: Missing columns in stg_Sales_Transactions                                              | Procedure fails; audit log status set to FAILED; error message captured                                  |
| TC07         | Edge case: Unexpected data types in Quantity (e.g., string instead of INT64)                      | Procedure fails; audit log status set to FAILED; error message captured                                  |
| TC08         | Edge case: Sales_Date outside range of dw_Dim_Date                                                | Rows not loaded (no matching join); not counted as rejected; audit log reflects inserted count           |
| TC09         | Error handling: Simulate runtime error (e.g., missing target table dw_Fact_Sales)                 | Procedure fails; audit log status set to FAILED; error message captured                                  |
| TC10         | Audit logging: Verify batch_id uniqueness and correct timestamps                                  | Audit log entries have unique batch_id and correct start/end times                                       |

Pytest Script:

```python
================================
Author: AAVA
Created on:
Description: Pytest-based unit tests for BigQuery sales fact table ETL procedure, covering data quality validation, transformation logic, audit logging, and error handling.
================================

import pytest
import pandas as pd
from sqlalchemy import create_engine
from uuid import uuid4
from datetime import datetime, timedelta

# Helper functions and mock data setup

def generate_mock_data(valid=True, null_customer=False, invalid_quantity=False, empty=False, all_invalid=False, bad_types=False, date_out_of_range=False):
    """Generate mock sales transactions DataFrame for testing."""
    if empty:
        return pd.DataFrame(columns=[
            'Transaction_ID', 'Customer_ID', 'Product_ID', 'Sales_Date', 'Quantity', 'Unit_Price'
        ])
    if all_invalid:
        data = [
            {'Transaction_ID': 1, 'Customer_ID': None, 'Product_ID': 101, 'Sales_Date': '2024-06-01', 'Quantity': -5, 'Unit_Price': 10.0},
            {'Transaction_ID': 2, 'Customer_ID': None, 'Product_ID': 102, 'Sales_Date': '2024-06-02', 'Quantity': 0, 'Unit_Price': 20.0},
        ]
        return pd.DataFrame(data)
    if bad_types:
        data = [
            {'Transaction_ID': 1, 'Customer_ID': 1001, 'Product_ID': 101, 'Sales_Date': '2024-06-01', 'Quantity': 'ten', 'Unit_Price': 10.0},
        ]
        return pd.DataFrame(data)
    if date_out_of_range:
        data = [
            {'Transaction_ID': 1, 'Customer_ID': 1001, 'Product_ID': 101, 'Sales_Date': '1900-01-01', 'Quantity': 10, 'Unit_Price': 10.0},
        ]
        return pd.DataFrame(data)
    data = [
        {'Transaction_ID': 1, 'Customer_ID': None if null_customer else 1001, 'Product_ID': 101, 'Sales_Date': '2024-06-01', 'Quantity': -5 if invalid_quantity else 10, 'Unit_Price': 10.0},
        {'Transaction_ID': 2, 'Customer_ID': 1002, 'Product_ID': 102, 'Sales_Date': '2024-06-02', 'Quantity': 20, 'Unit_Price': 20.0},
    ]
    return pd.DataFrame(data)

def generate_dim_customer():
    return pd.DataFrame([
        {'Customer_ID': 1001, 'Customer_Segment': 'Retail'},
        {'Customer_ID': 1002, 'Customer_Segment': 'Wholesale'},
    ])

def generate_dim_date():
    return pd.DataFrame([
        {'Date_Value': pd.Timestamp('2024-06-01'), 'Region_ID': 'North'},
        {'Date_Value': pd.Timestamp('2024-06-02'), 'Region_ID': 'South'},
    ])

def run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures, simulate_error=None):
    """
    Simulate the ETL procedure logic in Python, mimicking the BigQuery procedure.
    All tables are pandas DataFrames.
    """
    batch_id = str(uuid4())
    start_time = datetime.now()
    status = 'STARTED'
    message = 'Sales Fact Load Initiated'
    rows_inserted = 0
    rows_rejected = 0
    error_message = None

    # Start Audit Log
    audit_log.append({
        'Batch_ID': batch_id,
        'Procedure_Name': 'sp_load_sales_fact',
        'Start_Time': start_time,
        'Status': status,
        'Message': message,
        'End_Time': None,
        'Rows_Inserted': None,
        'Rows_Rejected': None
    })

    try:
        if simulate_error == 'missing_fact_table':
            fact_sales = None  # Simulate missing table

        # Data Quality Validation
        invalid_rows = []
        for idx, row in stg_sales.iterrows():
            if pd.isnull(row['Customer_ID']):
                invalid_rows.append({'Transaction_ID': row['Transaction_ID'], 'Reason': 'Missing CustomerID'})
            elif not isinstance(row['Quantity'], (int, float)) or row['Quantity'] <= 0:
                invalid_rows.append({'Transaction_ID': row['Transaction_ID'], 'Reason': 'Invalid Quantity'})

        deleted_ids = [r['Transaction_ID'] for r in invalid_rows]
        clean_sales = stg_sales[~stg_sales['Transaction_ID'].isin(deleted_ids)]

        # Transformation and Join
        transformed = pd.merge(clean_sales, dim_customer, on='Customer_ID', how='inner')
        transformed = pd.merge(transformed, dim_date, left_on=clean_sales['Sales_Date'].apply(pd.Timestamp), right_on='Date_Value', how='inner')
        if not transformed.empty:
            transformed['Total_Sales_Amount'] = transformed['Quantity'] * transformed['Unit_Price']
            transformed['Load_Timestamp'] = datetime.now()
            transformed['Batch_ID'] = batch_id

        # Insert into Fact Table
        if fact_sales is None:
            raise Exception("Target table dw_Fact_Sales does not exist")
        fact_sales = pd.concat([fact_sales, transformed], ignore_index=True)
        rows_inserted = len(transformed)

        # Archive Staging Table (simulate by clearing DataFrame)
        stg_sales.drop(stg_sales.index, inplace=True)

        # Log Validation Failures
        dq_failures.extend([
            {
                'Transaction_ID': r['Transaction_ID'],
                'Failure_Reason': r['Reason'],
                'Logged_Timestamp': datetime.now(),
                'Batch_ID': batch_id
            }
            for r in invalid_rows
        ])
        rows_rejected = len(invalid_rows)

        # End Audit Log
        end_time = datetime.now()
        audit_log[-1].update({
            'End_Time': end_time,
            'Rows_Inserted': rows_inserted,
            'Rows_Rejected': rows_rejected,
            'Status': 'COMPLETED',
            'Message': f'Inserted {rows_inserted} rows; Rejected {rows_rejected} rows.'
        })

    except Exception as e:
        end_time = datetime.now()
        error_message = str(e)
        audit_log[-1].update({
            'End_Time': end_time,
            'Status': 'FAILED',
            'Message': error_message
        })
        # Optionally re-raise for pipeline monitoring
        # raise
    return fact_sales, audit_log, dq_failures

# Pytest Fixtures

@pytest.fixture
def setup_tables():
    """Setup empty target tables and logs for each test."""
    fact_sales = pd.DataFrame(columns=[
        'Transaction_ID', 'Customer_ID', 'Product_ID', 'Sales_Date', 'Quantity', 'Unit_Price',
        'Total_Sales_Amount', 'Region_ID', 'Customer_Segment', 'Load_Timestamp', 'Batch_ID'
    ])
    audit_log = []
    dq_failures = []
    return fact_sales, audit_log, dq_failures

# Test Cases

def test_TC01_happy_path(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = generate_mock_data(valid=True)
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures)
    assert len(fact_sales) == 2
    assert audit_log[-1]['Rows_Inserted'] == 2
    assert audit_log[-1]['Rows_Rejected'] == 0
    assert len(dq_failures) == 0
    assert audit_log[-1]['Status'] == 'COMPLETED'

def test_TC02_null_customer_id(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = generate_mock_data(null_customer=True)
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures)
    # Only one row should be loaded
    assert len(fact_sales) == 1
    assert audit_log[-1]['Rows_Inserted'] == 1
    assert audit_log[-1]['Rows_Rejected'] == 1
    assert len(dq_failures) == 1
    assert dq_failures[0]['Failure_Reason'] == 'Missing CustomerID'
    assert audit_log[-1]['Status'] == 'COMPLETED'

def test_TC03_invalid_quantity(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = generate_mock_data(invalid_quantity=True)
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures)
    # Only one row should be loaded
    assert len(fact_sales) == 1
    assert audit_log[-1]['Rows_Inserted'] == 1
    assert audit_log[-1]['Rows_Rejected'] == 1
    assert dq_failures[0]['Failure_Reason'] == 'Invalid Quantity'
    assert audit_log[-1]['Status'] == 'COMPLETED'

def test_TC04_empty_staging(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = generate_mock_data(empty=True)
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures)
    assert len(fact_sales) == 0
    assert audit_log[-1]['Rows_Inserted'] == 0
    assert audit_log[-1]['Rows_Rejected'] == 0
    assert len(dq_failures) == 0
    assert audit_log[-1]['Status'] == 'COMPLETED'

def test_TC05_all_rows_invalid(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = generate_mock_data(all_invalid=True)
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures)
    assert len(fact_sales) == 0
    assert audit_log[-1]['Rows_Inserted'] == 0
    assert audit_log[-1]['Rows_Rejected'] == 2
    assert len(dq_failures) == 2
    assert audit_log[-1]['Status'] == 'COMPLETED'

def test_TC06_missing_columns(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = pd.DataFrame([{'Transaction_ID': 1, 'Product_ID': 101, 'Sales_Date': '2024-06-01', 'Quantity': 10, 'Unit_Price': 10.0}])  # Missing Customer_ID
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures)
    assert audit_log[-1]['Status'] == 'FAILED'
    assert 'Customer_ID' in audit_log[-1]['Message']

def test_TC07_bad_quantity_type(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = generate_mock_data(bad_types=True)
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures)
    assert audit_log[-1]['Status'] == 'FAILED'
    assert 'Target table' not in audit_log[-1]['Message']  # Should be a type error

def test_TC08_sales_date_out_of_range(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = generate_mock_data(date_out_of_range=True)
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures)
    # No matching join, so no rows loaded
    assert len(fact_sales) == 0
    assert audit_log[-1]['Rows_Inserted'] == 0
    assert audit_log[-1]['Rows_Rejected'] == 0
    assert audit_log[-1]['Status'] == 'COMPLETED'

def test_TC09_missing_fact_table(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = generate_mock_data(valid=True)
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures, simulate_error='missing_fact_table')
    assert audit_log[-1]['Status'] == 'FAILED'
    assert 'does not exist' in audit_log[-1]['Message']

def test_TC10_audit_logging_batch_id_and_timestamps(setup_tables):
    fact_sales, audit_log, dq_failures = setup_tables
    stg_sales = generate_mock_data(valid=True)
    dim_customer = generate_dim_customer()
    dim_date = generate_dim_date()
    fact_sales, audit_log, dq_failures = run_etl(stg_sales, dim_customer, dim_date, fact_sales, audit_log, dq_failures)
    entry = audit_log[-1]
    assert isinstance(entry['Batch_ID'], str)
    assert entry['Start_Time'] is not None
    assert entry['End_Time'] is not None
    assert entry['End_Time'] >= entry['Start_Time']
    assert entry['Status'] == 'COMPLETED'
```

API Cost Consumed in dollars: 0.0040 USD