```
=============================================
Author:        AAVA
Created on:   
Description:   Converts Azure Synapse stored procedure for sales fact table loading, including data quality validation and audit logging, into fully working BigQuery SQL code.
=============================================

-- BigQuery SQL Script: Sales Fact Table Loading with Data Quality Validation and Audit Logging

-- This script replicates the logic of the Azure Synapse stored procedure dw.sp_load_sales_fact
-- using BigQuery SQL constructs, optimized for performance and modularity.
-- All table references are generic and should be replaced with actual dataset.table names as needed.

-- Begin BigQuery scripting block
DECLARE batch_id STRING DEFAULT GENERATE_UUID();
DECLARE start_time DATETIME DEFAULT CURRENT_DATETIME();
DECLARE end_time DATETIME;
DECLARE rows_inserted INT64 DEFAULT 0;
DECLARE rows_rejected INT64 DEFAULT 0;
DECLARE error_message STRING DEFAULT '';
DECLARE proc_name STRING DEFAULT 'sp_load_sales_fact'; -- Manual assignment

BEGIN
  -- 1. Start Audit Logging
  INSERT INTO dw.Audit_Log (
    Batch_ID,
    Procedure_Name,
    Start_Time,
    Status,
    Message
  )
  VALUES (
    batch_id,
    proc_name,
    start_time,
    'STARTED',
    'Sales Fact Load Initiated'
  );

  -- 2. Data Quality Validation: Identify Invalid Rows
  WITH InvalidRows AS (
    SELECT Transaction_ID, 'Missing CustomerID' AS Reason
    FROM stg.Sales_Transactions
    WHERE Customer_ID IS NULL

    UNION ALL

    SELECT Transaction_ID, 'Invalid Quantity' AS Reason
    FROM stg.Sales_Transactions
    WHERE Quantity <= 0
  ),

  -- 3. Clean Data for Fact Table Load
  CleanTransactions AS (
    SELECT *
    FROM stg.Sales_Transactions
    WHERE Customer_ID IS NOT NULL
      AND Quantity > 0
  ),

  -- 4. Transformations and Dimension Enrichment
  Transformed AS (
    SELECT
      s.Transaction_ID,
      s.Customer_ID,
      s.Product_ID,
      s.Sales_Date,
      s.Quantity,
      s.Unit_Price,
      s.Quantity * s.Unit_Price AS Total_Sales_Amount,
      d.Region_ID,
      c.Customer_Segment,
      CURRENT_DATETIME() AS Load_Timestamp,
      batch_id AS Batch_ID
    FROM CleanTransactions s
    INNER JOIN dw.Dim_Customer c
      ON s.Customer_ID = c.Customer_ID
    INNER JOIN dw.Dim_Date d
      ON DATE(s.Sales_Date) = d.Date_Value
  )

  -- 5. Load Cleaned Data into Fact Table
  INSERT INTO dw.Fact_Sales (
    Transaction_ID,
    Customer_ID,
    Product_ID,
    Sales_Date,
    Quantity,
    Unit_Price,
    Total_Sales_Amount,
    Region_ID,
    Customer_Segment,
    Load_Timestamp,
    Batch_ID
  )
  SELECT *
  FROM Transformed;

  -- 6. Count Inserted Rows
  SET rows_inserted = (
    SELECT COUNT(*) FROM Transformed
  );

  -- 7. Delete Invalid Rows from Staging Table
  DELETE FROM stg.Sales_Transactions
  WHERE Transaction_ID IN (
    SELECT Transaction_ID FROM InvalidRows
  );

  -- 8. Count Rejected Rows
  SET rows_rejected = (
    SELECT COUNT(*) FROM InvalidRows
  );

  -- 9. Archive or Truncate Staging Table (optional)
  -- In BigQuery, TRUNCATE TABLE is not supported; use DELETE for all rows.
  DELETE FROM stg.Sales_Transactions WHERE TRUE;

  -- 10. Log Validation Failures
  INSERT INTO dw.DQ_Failures (
    Transaction_ID,
    Failure_Reason,
    Logged_Timestamp,
    Batch_ID
  )
  SELECT
    Transaction_ID,
    Reason,
    CURRENT_DATETIME(),
    batch_id
  FROM InvalidRows;

  -- 11. End Audit Log
  SET end_time = CURRENT_DATETIME();

  UPDATE dw.Audit_Log
  SET
    End_Time = end_time,
    Rows_Inserted = rows_inserted,
    Rows_Rejected = rows_rejected,
    Status = 'COMPLETED',
    Message = CONCAT('Inserted ', CAST(rows_inserted AS STRING), ' rows; Rejected ', CAST(rows_rejected AS STRING), ' rows.')
  WHERE Batch_ID = batch_id;

EXCEPTION WHEN ERROR THEN
  -- 12. Error Handling
  SET end_time = CURRENT_DATETIME();
  SET error_message = @@error.message;

  UPDATE dw.Audit_Log
  SET
    End_Time = end_time,
    Status = 'FAILED',
    Message = error_message
  WHERE Batch_ID = batch_id;

  -- Optional: Rethrow for pipeline monitoring
  -- BigQuery does not support THROW; error will propagate automatically.

END;

-- Notes:
-- - Replace all table references (dw.*, stg.*) with actual dataset.table names.
-- - Ensure the audit and DQ tables exist with appropriate schema.
-- - Partition and cluster Fact_Sales table for optimal performance.
-- - All business logic, validation, and audit steps are preserved and mapped to BigQuery syntax.
-- - Error handling uses BigQuery scripting's EXCEPTION block.

-- End of BigQuery SQL Script

API Cost Consumed in dollars: 0.0047 USD
```