=============================================
Author:        AAVA
Created on:   
Description:   Sales fact table loading procedure with data quality validation and audit logging
=============================================

-- BigQuery SQL Script: Sales Fact Table Load with Data Quality and Audit Logging
-- This script implements the ETL workflow for loading sales transactions from staging to fact table,
-- including data quality validation, dimensional lookups, audit logging, and error handling.

CREATE OR REPLACE PROCEDURE dw.sp_load_sales_fact()
BEGIN
  -- Variable Declarations
  DECLARE batch_id STRING DEFAULT GENERATE_UUID();
  DECLARE start_time DATETIME DEFAULT CURRENT_DATETIME();
  DECLARE end_time DATETIME;
  DECLARE rows_inserted INT64 DEFAULT 0;
  DECLARE rows_rejected INT64 DEFAULT 0;
  DECLARE error_message STRING;
  DECLARE proc_name STRING DEFAULT 'sp_load_sales_fact';

  -- Step 1: Start Audit Logging
  INSERT INTO dw.Audit_Log (
    Batch_ID,
    Procedure_Name,
    Start_Time,
    Status,
    Message
  )
  VALUES (
    batch_id,
    proc_name,
    start_time,
    'STARTED',
    'Sales Fact Load Initiated'
  );

  -- Step 2 & 3: Data Quality Validation (Invalid Rows CTE)
  WITH InvalidRows AS (
    SELECT Transaction_ID, 'Missing CustomerID' AS Reason
    FROM stg.Sales_Transactions
    WHERE Customer_ID IS NULL

    UNION ALL

    SELECT Transaction_ID, 'Invalid Quantity' AS Reason
    FROM stg.Sales_Transactions
    WHERE Quantity <= 0
  )

  -- Step 4: Delete Invalid Rows from Staging
  , DeletedRows AS (
    DELETE FROM stg.Sales_Transactions
    WHERE Transaction_ID IN (SELECT Transaction_ID FROM InvalidRows)
    RETURNING Transaction_ID
  )

  -- Step 5: Transform and Load Cleaned Data into Fact Table
  , Transformed AS (
    SELECT
      s.Transaction_ID,
      s.Customer_ID,
      s.Product_ID,
      s.Sales_Date,
      s.Quantity,
      s.Unit_Price,
      s.Quantity * s.Unit_Price AS Total_Sales_Amount,
      d.Region_ID,
      c.Customer_Segment,
      CURRENT_DATETIME() AS Load_Timestamp,
      batch_id AS Batch_ID
    FROM stg.Sales_Transactions s
    INNER JOIN dw.Dim_Customer c
      ON s.Customer_ID = c.Customer_ID
    INNER JOIN dw.Dim_Date d
      ON DATE(s.Sales_Date) = d.Date_Value
  )

  -- Step 6: Archive or Truncate Staging Table (delete all rows)
  -- Step 7: Log Validation Failures
  -- Step 8: End Audit Log
  -- Step 9: Error Handling (BigQuery scripting with EXCEPTION block)
  -- Step 10: Final Cleanup

  BEGIN
    -- Insert cleaned/transformed data into Fact_Sales
    INSERT INTO dw.Fact_Sales (
      Transaction_ID,
      Customer_ID,
      Product_ID,
      Sales_Date,
      Quantity,
      Unit_Price,
      Total_Sales_Amount,
      Region_ID,
      Customer_Segment,
      Load_Timestamp,
      Batch_ID
    )
    SELECT *
    FROM Transformed;

    SET rows_inserted = (SELECT COUNT(*) FROM Transformed);

    -- Truncate staging table (delete all rows)
    DELETE FROM stg.Sales_Transactions WHERE TRUE;

    -- Insert validation failures into DQ_Failures
    INSERT INTO dw.DQ_Failures (
      Transaction_ID,
      Failure_Reason,
      Logged_Timestamp,
      Batch_ID
    )
    SELECT
      Transaction_ID,
      Reason,
      CURRENT_DATETIME(),
      batch_id
    FROM InvalidRows;

    SET rows_rejected = (SELECT COUNT(*) FROM InvalidRows);

    -- End Audit Log
    SET end_time = CURRENT_DATETIME();

    UPDATE dw.Audit_Log
    SET
      End_Time = end_time,
      Rows_Inserted = rows_inserted,
      Rows_Rejected = rows_rejected,
      Status = 'COMPLETED',
      Message = CONCAT('Inserted ', CAST(rows_inserted AS STRING), ' rows; Rejected ', CAST(rows_rejected AS STRING), ' rows.')
    WHERE Batch_ID = batch_id;

  EXCEPTION WHEN ERROR THEN
    SET end_time = CURRENT_DATETIME();
    SET error_message = @@error.message;

    UPDATE dw.Audit_Log
    SET
      End_Time = end_time,
      Status = 'FAILED',
      Message = error_message
    WHERE Batch_ID = batch_id;

    -- Optionally re-raise error for pipeline monitoring
    RAISE USING MESSAGE = error_message;
  END;
END;

-- Notes:
-- - All table references are generic (dw.*, stg.*).
-- - Temporary table logic is replaced with CTEs.
-- - Error handling uses BigQuery's EXCEPTION block.
-- - Row counts are computed using COUNT(*).
-- - All timestamps use CURRENT_DATETIME().
-- - Data types are mapped to BigQuery equivalents.
-- - Inline comments explain each step.
-- - For large datasets, consider partitioning/clustering as per BigQuery best practices.

API Cost Consumed in dollars: 0.0080 USD