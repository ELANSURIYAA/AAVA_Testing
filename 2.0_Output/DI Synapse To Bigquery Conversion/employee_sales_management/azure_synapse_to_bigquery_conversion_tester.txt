=============================================
Author:    AAVA
Created on:    
Description:   Loads sales fact table from staging, performs data quality validation, logs audits, and handles errors.
=============================================

Test Case List:

| Test Case ID | Test Case Description                                                        | Expected Outcome                                                                                   |
|--------------|------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All sales transactions valid, all dimension lookups succeed      | All rows loaded to Fact_Sales, Audit_Log shows correct counts, no DQ_Failures logged               |
| TC02         | Edge case: Transactions with NULL Customer_ID                               | Those rows rejected, logged in DQ_Failures, not loaded to Fact_Sales, Audit_Log counts accurate    |
| TC03         | Edge case: Transactions with Quantity <= 0                                  | Those rows rejected, logged in DQ_Failures, not loaded to Fact_Sales, Audit_Log counts accurate    |
| TC04         | Edge case: Empty staging table                                              | No rows loaded/rejected, Audit_Log reflects zero counts, no DQ_Failures                            |
| TC05         | Edge case: Missing dimension lookup (Customer_ID not in Dim_Customer)       | Row not loaded to Fact_Sales, not rejected by DQ, Audit_Log counts accurate                        |
| TC06         | Edge case: Missing dimension lookup (Sales_Date not in Dim_Date)            | Row not loaded to Fact_Sales, not rejected by DQ, Audit_Log counts accurate                        |
| TC07         | Error handling: Simulate SQL error during insert                            | Audit_Log status set to FAILED, error message logged                                               |
| TC08         | Boundary: Maximum allowed Quantity and Unit_Price                           | Row loaded with correct Total_Sales_Amount, Audit_Log counts accurate                              |
| TC09         | Data type: Non-numeric Quantity/Unit_Price                                  | Row rejected, error logged, Audit_Log counts accurate                                              |
| TC10         | Audit: Verify batch_id uniqueness and correct timestamps                    | Each run has unique batch_id, timestamps in Audit_Log and Fact_Sales are correct                   |

Pytest Script for Each Test Case:

```python
================================
Author: AAVA
Created on: 
Description: Pytest script for validating BigQuery ETL logic for sales fact table loading, including data quality, enrichment, audit logging, and error handling.
================================

import pytest
import pandas as pd
from uuid import uuid4
from datetime import datetime

# Helper functions and mock data setup

def mock_staging_data(case):
    """Returns a DataFrame for stg.Sales_Transactions based on the test case."""
    if case == 'TC01':
        return pd.DataFrame([
            {'Transaction_ID': 1, 'Customer_ID': 101, 'Product_ID': 501, 'Sales_Date': '2023-01-01', 'Quantity': 2, 'Unit_Price': 10.0},
            {'Transaction_ID': 2, 'Customer_ID': 102, 'Product_ID': 502, 'Sales_Date': '2023-01-02', 'Quantity': 5, 'Unit_Price': 20.0}
        ])
    elif case == 'TC02':
        return pd.DataFrame([
            {'Transaction_ID': 3, 'Customer_ID': None, 'Product_ID': 503, 'Sales_Date': '2023-01-03', 'Quantity': 3, 'Unit_Price': 15.0}
        ])
    elif case == 'TC03':
        return pd.DataFrame([
            {'Transaction_ID': 4, 'Customer_ID': 104, 'Product_ID': 504, 'Sales_Date': '2023-01-04', 'Quantity': 0, 'Unit_Price': 25.0}
        ])
    elif case == 'TC04':
        return pd.DataFrame([], columns=['Transaction_ID', 'Customer_ID', 'Product_ID', 'Sales_Date', 'Quantity', 'Unit_Price'])
    elif case == 'TC05':
        return pd.DataFrame([
            {'Transaction_ID': 5, 'Customer_ID': 999, 'Product_ID': 505, 'Sales_Date': '2023-01-05', 'Quantity': 1, 'Unit_Price': 30.0}
        ])
    elif case == 'TC06':
        return pd.DataFrame([
            {'Transaction_ID': 6, 'Customer_ID': 106, 'Product_ID': 506, 'Sales_Date': '2099-12-31', 'Quantity': 2, 'Unit_Price': 40.0}
        ])
    elif case == 'TC08':
        return pd.DataFrame([
            {'Transaction_ID': 8, 'Customer_ID': 108, 'Product_ID': 508, 'Sales_Date': '2023-01-08', 'Quantity': 2147483647, 'Unit_Price': 999999.99}
        ])
    elif case == 'TC09':
        return pd.DataFrame([
            {'Transaction_ID': 9, 'Customer_ID': 109, 'Product_ID': 509, 'Sales_Date': '2023-01-09', 'Quantity': 'abc', 'Unit_Price': 'xyz'}
        ])
    else:
        return pd.DataFrame([])

def mock_dim_customer():
    return pd.DataFrame([
        {'Customer_ID': 101, 'Customer_Segment': 'Retail'},
        {'Customer_ID': 102, 'Customer_Segment': 'Wholesale'},
        {'Customer_ID': 104, 'Customer_Segment': 'Online'},
        {'Customer_ID': 106, 'Customer_Segment': 'Retail'},
        {'Customer_ID': 108, 'Customer_Segment': 'Enterprise'},
        {'Customer_ID': 109, 'Customer_Segment': 'Retail'}
    ])

def mock_dim_date():
    return pd.DataFrame([
        {'Date_Value': pd.to_datetime('2023-01-01'), 'Region_ID': 'East'},
        {'Date_Value': pd.to_datetime('2023-01-02'), 'Region_ID': 'West'},
        {'Date_Value': pd.to_datetime('2023-01-04'), 'Region_ID': 'North'},
        {'Date_Value': pd.to_datetime('2023-01-08'), 'Region_ID': 'South'},
        {'Date_Value': pd.to_datetime('2023-01-09'), 'Region_ID': 'Central'}
    ])

# Mock Fact_Sales, Audit_Log, DQ_Failures tables
def empty_fact_sales():
    return pd.DataFrame(columns=[
        'Transaction_ID', 'Customer_ID', 'Product_ID', 'Sales_Date', 'Quantity', 'Unit_Price',
        'Total_Sales_Amount', 'Region_ID', 'Customer_Segment', 'Load_Timestamp', 'Batch_ID'
    ])

def empty_audit_log():
    return pd.DataFrame(columns=[
        'Batch_ID', 'Procedure_Name', 'Start_Time', 'End_Time', 'Rows_Inserted', 'Rows_Rejected', 'Status', 'Message'
    ])

def empty_dq_failures():
    return pd.DataFrame(columns=[
        'Transaction_ID', 'Failure_Reason', 'Logged_Timestamp', 'Batch_ID'
    ])

def run_etl(staging_df, dim_customer_df, dim_date_df, fact_sales_df, audit_log_df, dq_failures_df, simulate_error=False):
    """Simulate the ETL logic of the BigQuery procedure."""
    batch_id = str(uuid4())
    start_time = datetime.now()
    proc_name = 'dw.sp_load_sales_fact'
    rows_inserted = 0
    rows_rejected = 0
    error_message = None

    # Start audit log
    audit_log_df = audit_log_df.append({
        'Batch_ID': batch_id,
        'Procedure_Name': proc_name,
        'Start_Time': start_time,
        'Status': 'STARTED',
        'Message': 'Sales Fact Load Initiated'
    }, ignore_index=True)

    # Data quality validation
    invalid_rows = []
    for idx, row in staging_df.iterrows():
        if pd.isnull(row['Customer_ID']):
            invalid_rows.append({'Transaction_ID': row['Transaction_ID'], 'Reason': 'Missing CustomerID'})
        elif not isinstance(row['Quantity'], (int, float)) or row['Quantity'] <= 0:
            invalid_rows.append({'Transaction_ID': row['Transaction_ID'], 'Reason': 'Invalid Quantity'})
    invalid_df = pd.DataFrame(invalid_rows)
    rows_rejected = len(invalid_df)

    # Remove invalid rows
    valid_transactions = staging_df.copy()
    if not invalid_df.empty:
        valid_transactions = valid_transactions[~valid_transactions['Transaction_ID'].isin(invalid_df['Transaction_ID'])]

    # Transform and enrich
    transformed = []
    for idx, row in valid_transactions.iterrows():
        # Dimension lookups
        cust_row = dim_customer_df[dim_customer_df['Customer_ID'] == row['Customer_ID']]
        date_row = dim_date_df[dim_date_df['Date_Value'] == pd.to_datetime(row['Sales_Date'])]
        if cust_row.empty or date_row.empty:
            continue  # skip if dimension lookup fails
        try:
            total_sales_amount = float(row['Quantity']) * float(row['Unit_Price'])
        except Exception:
            continue  # skip if calculation fails
        transformed.append({
            'Transaction_ID': row['Transaction_ID'],
            'Customer_ID': row['Customer_ID'],
            'Product_ID': row['Product_ID'],
            'Sales_Date': row['Sales_Date'],
            'Quantity': row['Quantity'],
            'Unit_Price': row['Unit_Price'],
            'Total_Sales_Amount': total_sales_amount,
            'Region_ID': date_row.iloc[0]['Region_ID'],
            'Customer_Segment': cust_row.iloc[0]['Customer_Segment'],
            'Load_Timestamp': datetime.now(),
            'Batch_ID': batch_id
        })
    transformed_df = pd.DataFrame(transformed)
    rows_inserted = len(transformed_df)

    # Insert into Fact_Sales
    fact_sales_df = pd.concat([fact_sales_df, transformed_df], ignore_index=True)

    # Archive staging (simulate by clearing DataFrame)
    staging_df = staging_df.iloc[0:0]

    # Log validation failures
    if not invalid_df.empty:
        invalid_df['Logged_Timestamp'] = datetime.now()
        invalid_df['Batch_ID'] = batch_id
        dq_failures_df = pd.concat([dq_failures_df, invalid_df.rename(columns={'Reason': 'Failure_Reason'})], ignore_index=True)

    # End audit log
    end_time = datetime.now()
    status = 'COMPLETED'
    message = f'Inserted {rows_inserted} rows; Rejected {rows_rejected} rows.'
    if simulate_error:
        status = 'FAILED'
        message = 'Simulated SQL error during insert'
        error_message = message
    audit_log_df.loc[audit_log_df['Batch_ID'] == batch_id, ['End_Time', 'Rows_Inserted', 'Rows_Rejected', 'Status', 'Message']] = [
        end_time, rows_inserted, rows_rejected, status, message
    ]

    return fact_sales_df, audit_log_df, dq_failures_df, batch_id, error_message

# Pytest test cases

@pytest.mark.parametrize("case,expected_inserted,expected_rejected", [
    ('TC01', 2, 0),
    ('TC02', 0, 1),
    ('TC03', 0, 1),
    ('TC04', 0, 0),
    ('TC05', 0, 0),
    ('TC06', 0, 0),
    ('TC08', 1, 0),
])
def test_etl_cases(case, expected_inserted, expected_rejected):
    staging_df = mock_staging_data(case)
    dim_customer_df = mock_dim_customer()
    dim_date_df = mock_dim_date()
    fact_sales_df = empty_fact_sales()
    audit_log_df = empty_audit_log()
    dq_failures_df = empty_dq_failures()

    fact_sales_df, audit_log_df, dq_failures_df, batch_id, error_message = run_etl(
        staging_df, dim_customer_df, dim_date_df, fact_sales_df, audit_log_df, dq_failures_df
    )

    # Check Fact_Sales row count
    assert len(fact_sales_df) == expected_inserted

    # Check DQ_Failures row count
    assert len(dq_failures_df) == expected_rejected

    # Check Audit_Log for correct counts
    audit_row = audit_log_df[audit_log_df['Batch_ID'] == batch_id].iloc[0]
    assert audit_row['Rows_Inserted'] == expected_inserted
    assert audit_row['Rows_Rejected'] == expected_rejected
    assert audit_row['Status'] == 'COMPLETED'

@pytest.mark.parametrize("case", ['TC05', 'TC06'])
def test_etl_dimension_lookup_failure(case):
    staging_df = mock_staging_data(case)
    dim_customer_df = mock_dim_customer()
    dim_date_df = mock_dim_date()
    fact_sales_df = empty_fact_sales()
    audit_log_df = empty_audit_log()
    dq_failures_df = empty_dq_failures()

    fact_sales_df, audit_log_df, dq_failures_df, batch_id, error_message = run_etl(
        staging_df, dim_customer_df, dim_date_df, fact_sales_df, audit_log_df, dq_failures_df
    )

    # Should not load row if dimension lookup fails
    assert len(fact_sales_df) == 0
    # Should not log as DQ failure (not a validation error)
    assert len(dq_failures_df) == 0

def test_etl_error_handling():
    staging_df = mock_staging_data('TC01')
    dim_customer_df = mock_dim_customer()
    dim_date_df = mock_dim_date()
    fact_sales_df = empty_fact_sales()
    audit_log_df = empty_audit_log()
    dq_failures_df = empty_dq_failures()

    fact_sales_df, audit_log_df, dq_failures_df, batch_id, error_message = run_etl(
        staging_df, dim_customer_df, dim_date_df, fact_sales_df, audit_log_df, dq_failures_df, simulate_error=True
    )

    audit_row = audit_log_df[audit_log_df['Batch_ID'] == batch_id].iloc[0]
    assert audit_row['Status'] == 'FAILED'
    assert 'Simulated SQL error' in audit_row['Message']

def test_etl_boundary_max_values():
    staging_df = mock_staging_data('TC08')
    dim_customer_df = mock_dim_customer()
    dim_date_df = mock_dim_date()
    fact_sales_df = empty_fact_sales()
    audit_log_df = empty_audit_log()
    dq_failures_df = empty_dq_failures()

    fact_sales_df, audit_log_df, dq_failures_df, batch_id, error_message = run_etl(
        staging_df, dim_customer_df, dim_date_df, fact_sales_df, audit_log_df, dq_failures_df
    )

    assert len(fact_sales_df) == 1
    row = fact_sales_df.iloc[0]
    assert row['Quantity'] == 2147483647
    assert row['Unit_Price'] == 999999.99
    assert row['Total_Sales_Amount'] == 2147483647 * 999999.99

def test_etl_non_numeric_quantity_unit_price():
    staging_df = mock_staging_data('TC09')
    dim_customer_df = mock_dim_customer()
    dim_date_df = mock_dim_date()
    fact_sales_df = empty_fact_sales()
    audit_log_df = empty_audit_log()
    dq_failures_df = empty_dq_failures()

    fact_sales_df, audit_log_df, dq_failures_df, batch_id, error_message = run_etl(
        staging_df, dim_customer_df, dim_date_df, fact_sales_df, audit_log_df, dq_failures_df
    )

    # Should not load row, should log as DQ failure
    assert len(fact_sales_df) == 0
    # No DQ failure since it's not caught by validation, but calculation fails and row is skipped

def test_audit_batch_id_and_timestamps():
    staging_df = mock_staging_data('TC01')
    dim_customer_df = mock_dim_customer()
    dim_date_df = mock_dim_date()
    fact_sales_df = empty_fact_sales()
    audit_log_df = empty_audit_log()
    dq_failures_df = empty_dq_failures()

    fact_sales_df, audit_log_df, dq_failures_df, batch_id, error_message = run_etl(
        staging_df, dim_customer_df, dim_date_df, fact_sales_df, audit_log_df, dq_failures_df
    )

    audit_row = audit_log_df[audit_log_df['Batch_ID'] == batch_id].iloc[0]
    assert audit_row['Batch_ID'] == batch_id
    assert pd.notnull(audit_row['Start_Time'])
    assert pd.notnull(audit_row['End_Time'])
    assert audit_row['Procedure_Name'] == 'dw.sp_load_sales_fact'
    # Check timestamps in Fact_Sales
    for _, row in fact_sales_df.iterrows():
        assert pd.notnull(row['Load_Timestamp'])
        assert row['Batch_ID'] == batch_id

# End of Pytest script

```

API Cost Estimation:
apiCost: 0.0047 USD