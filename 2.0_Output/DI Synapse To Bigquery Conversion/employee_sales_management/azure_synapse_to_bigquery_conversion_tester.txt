=============================================
Author:    AAVA
Created on:    
Description:   Converts Azure Synapse stored procedure for sales fact table loading, including data quality validation and audit logging, into fully working BigQuery SQL code.
=============================================

Test Case List:

| Test Case ID | Test Case Description                                                                                          | Expected Outcome                                                                                   |
|--------------|---------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All rows valid, all dimensions exist, all columns present                                          | All rows loaded into dw.Fact_Sales, audit log status COMPLETED, no DQ failures, correct counts    |
| TC02         | Edge case: Some rows with NULL Customer_ID                                                                    | Those rows rejected, logged in dw.DQ_Failures, not loaded into dw.Fact_Sales, audit log correct   |
| TC03         | Edge case: Some rows with Quantity <= 0                                                                       | Those rows rejected, logged in dw.DQ_Failures, not loaded into dw.Fact_Sales, audit log correct   |
| TC04         | Edge case: Empty stg.Sales_Transactions table                                                                 | No rows loaded, audit log shows 0 inserted/rejected, no DQ failures                               |
| TC05         | Edge case: Missing columns in stg.Sales_Transactions                                                          | Error raised, audit log status FAILED, error message logged                                       |
| TC06         | Edge case: No matching dimension rows (Dim_Customer or Dim_Date)                                              | Rows not loaded into dw.Fact_Sales, not rejected, audit log correct, no DQ failures               |
| TC07         | Error handling: Unexpected data type in Quantity (e.g., string instead of int)                                | Error raised, audit log status FAILED, error message logged                                       |
| TC08         | Boundary: Quantity = 1 (minimum valid), Quantity = 0 (invalid)                                                | Quantity=1 row loaded, Quantity=0 row rejected, DQ failure logged                                 |
| TC09         | Edge case: Duplicate Transaction_IDs in stg.Sales_Transactions                                                | Both rows processed, possible error or both loaded/rejected as per logic, audit log correct        |
| TC10         | Edge case: Large batch (performance)                                                                         | All valid rows loaded, audit log correct, performance within reasonable bounds                    |

Pytest Script for Each Test Case

```python
================================
Author: AAVA
Created on: 
Description: Pytest script for validating BigQuery sales fact table loading logic, including data quality, transformation, and audit/error handling.
================================

import pytest
import pandas as pd
from datetime import datetime

# Helper functions to simulate BigQuery logic in-memory using Pandas

def run_sales_fact_load(
    sales_transactions, dim_customer, dim_date,
    audit_log, dq_failures, fact_sales
):
    batch_id = "test-batch-uuid"
    start_time = datetime.now()
    proc_name = "sp_load_sales_fact"
    rows_inserted = 0
    rows_rejected = 0
    error_message = ""
    status = "STARTED"
    message = "Sales Fact Load Initiated"

    # 1. Start Audit Logging
    audit_log.append({
        "Batch_ID": batch_id,
        "Procedure_Name": proc_name,
        "Start_Time": start_time,
        "Status": status,
        "Message": message,
        "End_Time": None,
        "Rows_Inserted": None,
        "Rows_Rejected": None
    })

    try:
        # 2. Data Quality Validation
        invalid_rows = pd.DataFrame(columns=["Transaction_ID", "Reason"])
        if "Customer_ID" not in sales_transactions.columns or "Quantity" not in sales_transactions.columns:
            raise Exception("Missing required columns in Sales_Transactions")

        missing_customer = sales_transactions[sales_transactions["Customer_ID"].isnull()]
        if not missing_customer.empty:
            invalid_rows = pd.concat([
                invalid_rows,
                pd.DataFrame({
                    "Transaction_ID": missing_customer["Transaction_ID"],
                    "Reason": ["Missing CustomerID"] * len(missing_customer)
                })
            ], ignore_index=True)

        invalid_quantity = sales_transactions[sales_transactions["Quantity"] <= 0]
        if not invalid_quantity.empty:
            invalid_rows = pd.concat([
                invalid_rows,
                pd.DataFrame({
                    "Transaction_ID": invalid_quantity["Transaction_ID"],
                    "Reason": ["Invalid Quantity"] * len(invalid_quantity)
                })
            ], ignore_index=True)

        # 3. Clean Data
        clean_transactions = sales_transactions[
            sales_transactions["Customer_ID"].notnull() &
            (sales_transactions["Quantity"] > 0)
        ]

        # 4. Transformations and Dimension Enrichment
        # Join with customer and date dimensions
        transformed = pd.merge(
            clean_transactions, dim_customer, on="Customer_ID", how="inner"
        )
        transformed = pd.merge(
            transformed, dim_date, left_on=transformed["Sales_Date"].dt.date if pd.api.types.is_datetime64_any_dtype(transformed["Sales_Date"]) else transformed["Sales_Date"], right_on="Date_Value", how="inner"
        )
        transformed["Total_Sales_Amount"] = transformed["Quantity"] * transformed["Unit_Price"]
        transformed["Load_Timestamp"] = datetime.now()
        transformed["Batch_ID"] = batch_id

        # 5. Load Cleaned Data into Fact Table
        fact_sales.extend(transformed[[
            "Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity",
            "Unit_Price", "Total_Sales_Amount", "Region_ID", "Customer_Segment",
            "Load_Timestamp", "Batch_ID"
        ]].to_dict(orient="records"))

        rows_inserted = len(transformed)
        rows_rejected = len(invalid_rows)

        # 6. Delete Invalid Rows from Staging Table (simulated by not including them in clean_transactions)
        # 7. Archive or Truncate Staging Table (simulated by clearing the input DataFrame)
        sales_transactions.drop(sales_transactions.index, inplace=True)

        # 8. Log Validation Failures
        for _, row in invalid_rows.iterrows():
            dq_failures.append({
                "Transaction_ID": row["Transaction_ID"],
                "Failure_Reason": row["Reason"],
                "Logged_Timestamp": datetime.now(),
                "Batch_ID": batch_id
            })

        # 9. End Audit Log
        end_time = datetime.now()
        status = "COMPLETED"
        message = f"Inserted {rows_inserted} rows; Rejected {rows_rejected} rows."
        audit_log[-1].update({
            "End_Time": end_time,
            "Rows_Inserted": rows_inserted,
            "Rows_Rejected": rows_rejected,
            "Status": status,
            "Message": message
        })

    except Exception as e:
        end_time = datetime.now()
        error_message = str(e)
        status = "FAILED"
        audit_log[-1].update({
            "End_Time": end_time,
            "Status": status,
            "Message": error_message
        })
        # Rethrow for pipeline monitoring (simulated)
        raise

# Fixtures for mock datasets
@pytest.fixture
def dim_customer():
    return pd.DataFrame({
        "Customer_ID": [1, 2],
        "Customer_Segment": ["Retail", "Wholesale"]
    })

@pytest.fixture
def dim_date():
    return pd.DataFrame({
        "Date_Value": [pd.Timestamp("2023-01-01").date(), pd.Timestamp("2023-01-02").date()],
        "Region_ID": [101, 102]
    })

@pytest.fixture
def audit_log():
    return []

@pytest.fixture
def dq_failures():
    return []

@pytest.fixture
def fact_sales():
    return []

# Test Cases

def test_TC01_happy_path(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame({
        "Transaction_ID": [100, 101],
        "Customer_ID": [1, 2],
        "Product_ID": [200, 201],
        "Sales_Date": [pd.Timestamp("2023-01-01"), pd.Timestamp("2023-01-02")],
        "Quantity": [10, 5],
        "Unit_Price": [20.0, 30.0]
    })
    run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    assert len(fact_sales) == 2
    assert all(row["Total_Sales_Amount"] == row["Quantity"] * row["Unit_Price"] for row in fact_sales)
    assert audit_log[-1]["Status"] == "COMPLETED"
    assert audit_log[-1]["Rows_Inserted"] == 2
    assert audit_log[-1]["Rows_Rejected"] == 0
    assert len(dq_failures) == 0

def test_TC02_null_customer_id(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame({
        "Transaction_ID": [100, 101],
        "Customer_ID": [None, 2],
        "Product_ID": [200, 201],
        "Sales_Date": [pd.Timestamp("2023-01-01"), pd.Timestamp("2023-01-02")],
        "Quantity": [10, 5],
        "Unit_Price": [20.0, 30.0]
    })
    run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    assert len(fact_sales) == 1
    assert fact_sales[0]["Customer_ID"] == 2
    assert audit_log[-1]["Rows_Inserted"] == 1
    assert audit_log[-1]["Rows_Rejected"] == 1
    assert len(dq_failures) == 1
    assert dq_failures[0]["Failure_Reason"] == "Missing CustomerID"

def test_TC03_invalid_quantity(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame({
        "Transaction_ID": [100, 101],
        "Customer_ID": [1, 2],
        "Product_ID": [200, 201],
        "Sales_Date": [pd.Timestamp("2023-01-01"), pd.Timestamp("2023-01-02")],
        "Quantity": [0, 5],
        "Unit_Price": [20.0, 30.0]
    })
    run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    assert len(fact_sales) == 1
    assert fact_sales[0]["Quantity"] == 5
    assert audit_log[-1]["Rows_Inserted"] == 1
    assert audit_log[-1]["Rows_Rejected"] == 1
    assert len(dq_failures) == 1
    assert dq_failures[0]["Failure_Reason"] == "Invalid Quantity"

def test_TC04_empty_staging(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame(columns=[
        "Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"
    ])
    run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    assert len(fact_sales) == 0
    assert audit_log[-1]["Rows_Inserted"] == 0
    assert audit_log[-1]["Rows_Rejected"] == 0
    assert len(dq_failures) == 0

def test_TC05_missing_columns(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame({
        "Transaction_ID": [100, 101],
        "Product_ID": [200, 201],
        "Sales_Date": [pd.Timestamp("2023-01-01"), pd.Timestamp("2023-01-02")],
        "Quantity": [10, 5]
        # Missing Customer_ID
    })
    with pytest.raises(Exception) as excinfo:
        run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    assert "Missing required columns" in str(excinfo.value)
    assert audit_log[-1]["Status"] == "FAILED"

def test_TC06_no_matching_dimensions(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame({
        "Transaction_ID": [100, 101],
        "Customer_ID": [3, 4],  # Not in dim_customer
        "Product_ID": [200, 201],
        "Sales_Date": [pd.Timestamp("2023-01-01"), pd.Timestamp("2023-01-02")],
        "Quantity": [10, 5],
        "Unit_Price": [20.0, 30.0]
    })
    run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    assert len(fact_sales) == 0
    assert audit_log[-1]["Rows_Inserted"] == 0
    assert audit_log[-1]["Rows_Rejected"] == 0
    assert len(dq_failures) == 0

def test_TC07_invalid_quantity_type(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame({
        "Transaction_ID": [100],
        "Customer_ID": [1],
        "Product_ID": [200],
        "Sales_Date": [pd.Timestamp("2023-01-01")],
        "Quantity": ["ten"],  # Invalid type
        "Unit_Price": [20.0]
    })
    with pytest.raises(Exception) as excinfo:
        run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    assert audit_log[-1]["Status"] == "FAILED"

def test_TC08_quantity_boundary(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame({
        "Transaction_ID": [100, 101],
        "Customer_ID": [1, 2],
        "Product_ID": [200, 201],
        "Sales_Date": [pd.Timestamp("2023-01-01"), pd.Timestamp("2023-01-02")],
        "Quantity": [1, 0],
        "Unit_Price": [20.0, 30.0]
    })
    run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    assert len(fact_sales) == 1
    assert fact_sales[0]["Quantity"] == 1
    assert audit_log[-1]["Rows_Inserted"] == 1
    assert audit_log[-1]["Rows_Rejected"] == 1
    assert len(dq_failures) == 1
    assert dq_failures[0]["Failure_Reason"] == "Invalid Quantity"

def test_TC09_duplicate_transaction_ids(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame({
        "Transaction_ID": [100, 100],
        "Customer_ID": [1, 1],
        "Product_ID": [200, 200],
        "Sales_Date": [pd.Timestamp("2023-01-01"), pd.Timestamp("2023-01-01")],
        "Quantity": [10, 10],
        "Unit_Price": [20.0, 20.0]
    })
    run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    # Both rows loaded, as no explicit duplicate check in logic
    assert len(fact_sales) == 2
    assert audit_log[-1]["Rows_Inserted"] == 2
    assert audit_log[-1]["Rows_Rejected"] == 0

def test_TC10_large_batch(dim_customer, dim_date, audit_log, dq_failures, fact_sales):
    sales_transactions = pd.DataFrame({
        "Transaction_ID": range(1000),
        "Customer_ID": [1] * 1000,
        "Product_ID": [200] * 1000,
        "Sales_Date": [pd.Timestamp("2023-01-01")] * 1000,
        "Quantity": [10] * 1000,
        "Unit_Price": [20.0] * 1000
    })
    run_sales_fact_load(sales_transactions, dim_customer, dim_date, audit_log, dq_failures, fact_sales)
    assert len(fact_sales) == 1000
    assert audit_log[-1]["Rows_Inserted"] == 1000
    assert audit_log[-1]["Rows_Rejected"] == 0
    assert len(dq_failures) == 0

```

API Cost Consumed in dollars: 0.0047 USD