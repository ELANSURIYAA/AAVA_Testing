=============================================
Author:    AAVA
Created on:    
Description:   Loads sales transaction data from staging to fact table, performs data quality checks, logs audit and validation failures, and handles error reporting.
=============================================

Transformation Change Detection

**Expression Transformation Mapping:**
- Azure Synapse uses `NEWID()` for batch ID; BigQuery uses `GENERATE_UUID()`.
- `SYSDATETIME()` in Synapse is mapped to `CURRENT_TIMESTAMP()` in BigQuery.
- `OBJECT_NAME(@@PROCID)` for procedure name is replaced by manual assignment in BigQuery.
- `@@ROWCOUNT` is replaced by explicit counting in BigQuery.
- Data quality checks (`Customer_ID IS NULL`, `Quantity <= 0`) are mapped directly.
- Calculation of `Total_Sales_Amount` (`Quantity * Unit_Price`) is preserved.
- Audit log message concatenation uses `CONCAT()` in both, but BigQuery requires explicit casting for numbers.

**Aggregator Transformations:**
- No GROUP BY or analytic functions; row counts are tracked via temp tables and explicit counts.

**Join Strategies:**
- INNER JOINs for dimension lookups (`Dim_Customer`, `Dim_Date`) are preserved.
- Deletion of invalid rows in Synapse uses JOIN; in BigQuery, uses `WHERE IN (SELECT ...)`.

**Data Type Transformations:**
- `UNIQUEIDENTIFIER` → `STRING`
- `DATETIME` → `TIMESTAMP`
- `NVARCHAR` → `STRING`
- `BIGINT`/`INT` → `INT64`

**Null Handling and Case Sensitivity Adjustments:**
- Explicit null checks for `Customer_ID`.
- BigQuery is case-sensitive for column names; ensure consistency.

Recommended Manual Interventions

- **Performance optimizations:** Partition `Fact_Sales` by `Sales_Date`, cluster by `Customer_ID`/`Product_ID`.
- **Edge case handling:** Ensure all validation logic matches business rules; handle NULLs and unexpected formats.
- **Complex transformations:** If additional business logic is needed, implement as BigQuery UDFs.
- **String manipulations:** Ensure all string concatenations and format conversions are compatible.
- **Audit and DQ tables:** Confirm schema compatibility and adjust for BigQuery data types.
- **Error handling:** Refactor TRY-CATCH to BigQuery's `EXCEPTION WHEN ERROR THEN` blocks.

Test Case List:

| Test Case ID | Test Case Description                                                                 | Expected Outcome                                                                                                 |
|--------------|--------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: Valid sales transactions, all required fields present, positive quantity | All rows inserted into dw.Fact_Sales; audit log status 'COMPLETED'; no rows in dw.DQ_Failures; staging emptied.  |
| TC02         | Edge case: Transaction with NULL Customer_ID                                         | Row rejected, logged in dw.DQ_Failures with 'Missing CustomerID'; not inserted into dw.Fact_Sales; staging emptied. |
| TC03         | Edge case: Transaction with Quantity <= 0                                            | Row rejected, logged in dw.DQ_Failures with 'Invalid Quantity'; not inserted into dw.Fact_Sales; staging emptied. |
| TC04         | Edge case: Empty staging table                                                       | No rows inserted or rejected; audit log reflects zero inserted/rejected; staging remains empty.                  |
| TC05         | Error handling: Missing required column (e.g., Product_ID) in staging                | Procedure fails, audit log status 'FAILED', error message logged.                                                |
| TC06         | Edge case: Transaction with missing dimension lookup (Customer_ID not in Dim_Customer)| Row not inserted into dw.Fact_Sales; not logged as DQ failure; audit log reflects correct counts.                |
| TC07         | Edge case: Transaction with Sales_Date not in Dim_Date                               | Row not inserted into dw.Fact_Sales; not logged as DQ failure; audit log reflects correct counts.                |
| TC08         | Error handling: Unexpected data format in Quantity (e.g., string instead of integer) | Procedure fails, audit log status 'FAILED', error message logged.                                                |
| TC09         | Happy path: Multiple batches, audit log batch tracking                               | Each batch has unique Batch_ID, correct audit log entries, and correct row counts per batch.                     |
| TC10         | Edge case: All rows invalid (all fail validation)                                    | All rows rejected, all logged in dw.DQ_Failures, none inserted into dw.Fact_Sales; audit log reflects counts.    |

---

Pytest Script for Each Test Case

```python
================================
Author: AAVA
Created on:
Description: Pytest script for validating BigQuery sales fact load procedure, covering data quality, audit, and error handling.
================================

import pytest
import pandas as pd

# Helper function to simulate BigQuery SQL transformations using Pandas
def run_sales_fact_load(staging_df, dim_customer_df, dim_date_df):
    # Data Quality Checks
    invalid_rows = []
    valid_rows = []

    for idx, row in staging_df.iterrows():
        if pd.isnull(row['Customer_ID']):
            invalid_rows.append({'Transaction_ID': row['Transaction_ID'], 'Reason': 'Missing CustomerID'})
            continue
        if not isinstance(row['Quantity'], (int, float)) or row['Quantity'] <= 0:
            invalid_rows.append({'Transaction_ID': row['Transaction_ID'], 'Reason': 'Invalid Quantity'})
            continue
        valid_rows.append(row)

    # Remove invalid rows from staging
    cleaned_df = pd.DataFrame(valid_rows) if valid_rows else pd.DataFrame(columns=staging_df.columns)
    invalid_rows_df = pd.DataFrame(invalid_rows)

    # Dimensional lookups
    if not cleaned_df.empty:
        merged_df = cleaned_df.merge(dim_customer_df, on='Customer_ID', how='inner')
        merged_df = merged_df.merge(dim_date_df, left_on='Sales_Date', right_on='Date_Value', how='inner')
        merged_df['Total_Sales_Amount'] = merged_df['Quantity'] * merged_df['Unit_Price']
        merged_df['Load_Timestamp'] = pd.Timestamp.now()
        merged_df['Batch_ID'] = 'test-batch'
        fact_sales_df = merged_df[[
            'Transaction_ID', 'Customer_ID', 'Product_ID', 'Sales_Date', 'Quantity', 'Unit_Price',
            'Total_Sales_Amount', 'Region_ID', 'Customer_Segment', 'Load_Timestamp', 'Batch_ID'
        ]]
    else:
        fact_sales_df = pd.DataFrame(columns=[
            'Transaction_ID', 'Customer_ID', 'Product_ID', 'Sales_Date', 'Quantity', 'Unit_Price',
            'Total_Sales_Amount', 'Region_ID', 'Customer_Segment', 'Load_Timestamp', 'Batch_ID'
        ])

    # Audit log simulation
    audit_log = {
        'Batch_ID': 'test-batch',
        'Rows_Inserted': len(fact_sales_df),
        'Rows_Rejected': len(invalid_rows_df),
        'Status': 'COMPLETED',
        'Message': f"Inserted {len(fact_sales_df)} rows; Rejected {len(invalid_rows_df)} rows."
    }

    # DQ Failures
    dq_failures_df = invalid_rows_df.copy()
    if not dq_failures_df.empty:
        dq_failures_df['Logged_Timestamp'] = pd.Timestamp.now()
        dq_failures_df['Batch_ID'] = 'test-batch'

    return fact_sales_df, dq_failures_df, audit_log

# Fixtures for test datasets
@pytest.fixture
def dim_customer_df():
    return pd.DataFrame({
        'Customer_ID': [1, 2],
        'Customer_Segment': ['Retail', 'Wholesale']
    })

@pytest.fixture
def dim_date_df():
    return pd.DataFrame({
        'Date_Value': [pd.Timestamp('2023-01-01'), pd.Timestamp('2023-01-02')],
        'Region_ID': [101, 102]
    })

# TC01: Happy path
def test_happy_path(dim_customer_df, dim_date_df):
    staging_df = pd.DataFrame({
        'Transaction_ID': [1001],
        'Customer_ID': [1],
        'Product_ID': [501],
        'Sales_Date': [pd.Timestamp('2023-01-01')],
        'Quantity': [10],
        'Unit_Price': [20.0]
    })
    fact_sales_df, dq_failures_df, audit_log = run_sales_fact_load(staging_df, dim_customer_df, dim_date_df)
    assert len(fact_sales_df) == 1
    assert len(dq_failures_df) == 0
    assert audit_log['Status'] == 'COMPLETED'
    assert audit_log['Rows_Inserted'] == 1
    assert audit_log['Rows_Rejected'] == 0

# TC02: NULL Customer_ID
def test_missing_customer_id(dim_customer_df, dim_date_df):
    staging_df = pd.DataFrame({
        'Transaction_ID': [1002],
        'Customer_ID': [None],
        'Product_ID': [502],
        'Sales_Date': [pd.Timestamp('2023-01-01')],
        'Quantity': [5],
        'Unit_Price': [15.0]
    })
    fact_sales_df, dq_failures_df, audit_log = run_sales_fact_load(staging_df, dim_customer_df, dim_date_df)
    assert len(fact_sales_df) == 0
    assert len(dq_failures_df) == 1
    assert dq_failures_df.iloc[0]['Reason'] == 'Missing CustomerID'
    assert audit_log['Rows_Inserted'] == 0
    assert audit_log['Rows_Rejected'] == 1

# TC03: Invalid Quantity
def test_invalid_quantity(dim_customer_df, dim_date_df):
    staging_df = pd.DataFrame({
        'Transaction_ID': [1003],
        'Customer_ID': [1],
        'Product_ID': [503],
        'Sales_Date': [pd.Timestamp('2023-01-01')],
        'Quantity': [0],
        'Unit_Price': [25.0]
    })
    fact_sales_df, dq_failures_df, audit_log = run_sales_fact_load(staging_df, dim_customer_df, dim_date_df)
    assert len(fact_sales_df) == 0
    assert len(dq_failures_df) == 1
    assert dq_failures_df.iloc[0]['Reason'] == 'Invalid Quantity'
    assert audit_log['Rows_Inserted'] == 0
    assert audit_log['Rows_Rejected'] == 1

# TC04: Empty staging table
def test_empty_staging(dim_customer_df, dim_date_df):
    staging_df = pd.DataFrame(columns=['Transaction_ID', 'Customer_ID', 'Product_ID', 'Sales_Date', 'Quantity', 'Unit_Price'])
    fact_sales_df, dq_failures_df, audit_log = run_sales_fact_load(staging_df, dim_customer_df, dim_date_df)
    assert len(fact_sales_df) == 0
    assert len(dq_failures_df) == 0
    assert audit_log['Rows_Inserted'] == 0
    assert audit_log['Rows_Rejected'] == 0

# TC05: Missing required column (Product_ID)
def test_missing_column(dim_customer_df, dim_date_df):
    staging_df = pd.DataFrame({
        'Transaction_ID': [1004],
        'Customer_ID': [1],
        # 'Product_ID' column missing
        'Sales_Date': [pd.Timestamp('2023-01-01')],
        'Quantity': [10],
        'Unit_Price': [30.0]
    })
    with pytest.raises(KeyError):
        run_sales_fact_load(staging_df, dim_customer_df, dim_date_df)

# TC06: Missing dimension lookup (Customer_ID not in Dim_Customer)
def test_missing_dim_customer(dim_customer_df, dim_date_df):
    staging_df = pd.DataFrame({
        'Transaction_ID': [1005],
        'Customer_ID': [999],  # Not in dim_customer_df
        'Product_ID': [504],
        'Sales_Date': [pd.Timestamp('2023-01-01')],
        'Quantity': [10],
        'Unit_Price': [40.0]
    })
    fact_sales_df, dq_failures_df, audit_log = run_sales_fact_load(staging_df, dim_customer_df, dim_date_df)
    assert len(fact_sales_df) == 0
    assert audit_log['Rows_Inserted'] == 0

# TC07: Missing dimension lookup (Sales_Date not in Dim_Date)
def test_missing_dim_date(dim_customer_df, dim_date_df):
    staging_df = pd.DataFrame({
        'Transaction_ID': [1006],
        'Customer_ID': [1],
        'Product_ID': [505],
        'Sales_Date': [pd.Timestamp('2023-02-01')],  # Not in dim_date_df
        'Quantity': [10],
        'Unit_Price': [50.0]
    })
    fact_sales_df, dq_failures_df, audit_log = run_sales_fact_load(staging_df, dim_customer_df, dim_date_df)
    assert len(fact_sales_df) == 0
    assert audit_log['Rows_Inserted'] == 0

# TC08: Unexpected data format in Quantity
def test_unexpected_quantity_format(dim_customer_df, dim_date_df):
    staging_df = pd.DataFrame({
        'Transaction_ID': [1007],
        'Customer_ID': [1],
        'Product_ID': [506],
        'Sales_Date': [pd.Timestamp('2023-01-01')],
        'Quantity': ['ten'],  # Invalid format
        'Unit_Price': [60.0]
    })
    fact_sales_df, dq_failures_df, audit_log = run_sales_fact_load(staging_df, dim_customer_df, dim_date_df)
    assert len(fact_sales_df) == 0
    assert len(dq_failures_df) == 1
    assert dq_failures_df.iloc[0]['Reason'] == 'Invalid Quantity'

# TC09: Multiple batches, audit log batch tracking
def test_multiple_batches(dim_customer_df, dim_date_df):
    staging_df1 = pd.DataFrame({
        'Transaction_ID': [1008],
        'Customer_ID': [1],
        'Product_ID': [507],
        'Sales_Date': [pd.Timestamp('2023-01-01')],
        'Quantity': [10],
        'Unit_Price': [70.0]
    })
    staging_df2 = pd.DataFrame({
        'Transaction_ID': [1009],
        'Customer_ID': [2],
        'Product_ID': [508],
        'Sales_Date': [pd.Timestamp('2023-01-02')],
        'Quantity': [20],
        'Unit_Price': [80.0]
    })
    fact_sales_df1, dq_failures_df1, audit_log1 = run_sales_fact_load(staging_df1, dim_customer_df, dim_date_df)
    fact_sales_df2, dq_failures_df2, audit_log2 = run_sales_fact_load(staging_df2, dim_customer_df, dim_date_df)
    assert audit_log1['Batch_ID'] == 'test-batch'
    assert audit_log2['Batch_ID'] == 'test-batch'
    assert audit_log1['Rows_Inserted'] == 1
    assert audit_log2['Rows_Inserted'] == 1

# TC10: All rows invalid
def test_all_rows_invalid(dim_customer_df, dim_date_df):
    staging_df = pd.DataFrame({
        'Transaction_ID': [1010, 1011],
        'Customer_ID': [None, 2],
        'Product_ID': [509, 510],
        'Sales_Date': [pd.Timestamp('2023-01-01'), pd.Timestamp('2023-01-02')],
        'Quantity': [10, 0],  # Second row invalid quantity
        'Unit_Price': [90.0, 100.0]
    })
    fact_sales_df, dq_failures_df, audit_log = run_sales_fact_load(staging_df, dim_customer_df, dim_date_df)
    assert len(fact_sales_df) == 0
    assert len(dq_failures_df) == 2
    assert audit_log['Rows_Inserted'] == 0
    assert audit_log['Rows_Rejected'] == 2

```

API Cost Consumption:
apiCost: 0.0047 USD