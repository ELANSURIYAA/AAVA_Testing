Author:        AAVA
Created on:   
Description:   BigQuery SQL script for loading sales fact table with data quality validation and audit logging, converted from Azure Synapse stored procedure.
=============================================

Test Case List:
| Test Case ID | Test Case Description                                                                 | Expected Outcome                                                                                                    |
|--------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| TC01         | Insert valid sales transactions                                                      | Only valid rows are inserted into dw.Fact_Sales; Audit_Log and DQ_Failures updated accordingly                     |
| TC02         | Handle missing Customer_ID in sales transactions                                     | Rows with NULL Customer_ID are excluded from Fact_Sales and logged in DQ_Failures                                  |
| TC03         | Handle invalid Quantity (<=0) in sales transactions                                  | Rows with Quantity <= 0 are excluded from Fact_Sales and logged in DQ_Failures                                     |
| TC04         | Data enrichment with dimension tables (Dim_Customer, Dim_Date)                        | Only rows with matching Customer_ID and Sales_Date in dimension tables are inserted; others are excluded            |
| TC05         | Audit log records correct start/end time, status, and row counts                      | Audit_Log contains accurate metadata for the batch                                                                  |
| TC06         | All staging data is deleted after load                                                | stg.Sales_Transactions is empty after successful execution                                                          |
| TC07         | Error handling: simulate failure (e.g., missing dimension table)                      | Audit_Log status is 'FAILED' and error message is logged                                                            |
| TC08         | Null and edge case handling in string and numeric fields                              | Nulls and edge cases are handled without causing load failure or incorrect data                                     |
| TC09         | Correct calculation of Total_Sales_Amount                                             | Total_Sales_Amount = Quantity * Unit_Price for all inserted rows                                                    |
| TC10         | Multiple invalid reasons for a single transaction (should be logged once per reason)  | Each invalid reason for a transaction is logged as a separate row in DQ_Failures                                    |

Pytest Script for Each Test Case

```python
import pytest
from google.cloud import bigquery
from datetime import datetime, timedelta

client = bigquery.Client()

# Helper functions
def query_table(table, where_clause=""):
    query = f"SELECT * FROM {table} {where_clause}"
    return [dict(row) for row in client.query(query).result()]

def insert_rows(table, rows):
    errors = client.insert_rows_json(table, rows)
    assert errors == [], f"Insert errors: {errors}"

def clear_table(table):
    client.query(f"DELETE FROM {table} WHERE TRUE").result()

def run_sales_fact_load_procedure():
    # This should trigger the BigQuery script (e.g., via a stored procedure or script execution)
    # For testing, assume it's a stored procedure named `dw.sp_load_sales_fact`
    client.query("CALL dw.sp_load_sales_fact()").result()

def get_latest_batch_id():
    rows = query_table("dw.Audit_Log", "ORDER BY Start_Time DESC LIMIT 1")
    return rows[0]["Batch_ID"] if rows else None

@pytest.fixture(autouse=True)
def setup_and_teardown():
    # Clear all relevant tables before each test
    for table in [
        "stg.Sales_Transactions",
        "dw.Fact_Sales",
        "dw.DQ_Failures",
        "dw.Audit_Log"
    ]:
        clear_table(table)
    yield
    # Optionally clear again after test

def test_TC01_insert_valid_sales_transactions():
    # Insert valid data
    insert_rows("stg.Sales_Transactions", [
        {
            "Transaction_ID": "T1001",
            "Customer_ID": "C001",
            "Product_ID": "P001",
            "Sales_Date": "2024-06-01",
            "Quantity": 10,
            "Unit_Price": 20.0
        }
    ])
    # Ensure dimension tables have matching data
    insert_rows("dw.Dim_Customer", [{"Customer_ID": "C001", "Customer_Segment": "Retail"}])
    insert_rows("dw.Dim_Date", [{"Date_Value": "2024-06-01", "Region_ID": "R1"}])
    run_sales_fact_load_procedure()
    # Validate Fact_Sales
    fact_rows = query_table("dw.Fact_Sales")
    assert len(fact_rows) == 1
    assert fact_rows[0]["Transaction_ID"] == "T1001"
    # Validate Audit_Log
    batch_id = get_latest_batch_id()
    audit_rows = query_table("dw.Audit_Log", f"WHERE Batch_ID = '{batch_id}'")
    assert audit_rows[0]["Rows_Inserted"] == 1
    assert audit_rows[0]["Rows_Rejected"] == 0
    # DQ_Failures should be empty
    dq_rows = query_table("dw.DQ_Failures")
    assert len(dq_rows) == 0

def test_TC02_handle_missing_customer_id():
    insert_rows("stg.Sales_Transactions", [
        {
            "Transaction_ID": "T1002",
            "Customer_ID": None,
            "Product_ID": "P002",
            "Sales_Date": "2024-06-01",
            "Quantity": 5,
            "Unit_Price": 15.0
        }
    ])
    run_sales_fact_load_procedure()
    fact_rows = query_table("dw.Fact_Sales")
    assert len(fact_rows) == 0
    dq_rows = query_table("dw.DQ_Failures")
    assert any(row["Transaction_ID"] == "T1002" and "Missing CustomerID" in row["Failure_Reason"] for row in dq_rows)

def test_TC03_handle_invalid_quantity():
    insert_rows("stg.Sales_Transactions", [
        {
            "Transaction_ID": "T1003",
            "Customer_ID": "C002",
            "Product_ID":