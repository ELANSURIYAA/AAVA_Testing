Author:        AAVA
Created on:   
Description:   BigQuery SQL script for loading sales fact table with data quality validation and audit logging, converted from Azure Synapse stored procedure.

================================================================================

Summary
The provided document details the conversion of an Azure Synapse stored procedure (`dw.sp_load_sales_fact`) into a BigQuery SQL script. The procedure orchestrates the ETL process for loading sales transactions into a fact table, with embedded data quality validation, audit logging, error handling, and reporting. The BigQuery implementation leverages scripting constructs, CTEs, and native SQL features to replicate the original logic, ensuring robust data processing and operational traceability.

Conversion Accuracy
- Data Flow & Structure: The BigQuery code accurately mirrors the Synapse procedure’s stepwise logic: audit log initiation, data quality validation (missing Customer_ID, invalid Quantity), enrichment via joins to dimension tables, fact table loading, staging cleanup, logging of validation failures, and audit log completion.
- Table Mapping: All source, dimension, and target tables (`stg.Sales_Transactions`, `dw.Dim_Customer`, `dw.Dim_Date`, `dw.Fact_Sales`, `dw.Audit_Log`, `dw.DQ_Failures`) are correctly referenced and mapped.
- Transformations: Calculations (e.g., `Total_Sales_Amount = Quantity * Unit_Price`), enrichment, and filtering logic are preserved using CTEs and joins.
- Error Handling: The BigQuery script uses `EXCEPTION WHEN ERROR THEN` for error capture, updating the audit log and optionally propagating errors, analogous to Synapse’s TRY/CATCH.
- Audit & Logging: Audit log entries and updates are performed at start and end, with batch IDs, timestamps, status, and row counts, matching the original procedure.
- Data Quality: Invalid rows are detected and logged in a dedicated table (`DQ_Failures`), with reasons, timestamps, and batch IDs.
- Staging Management: Staging table cleanup is handled via `DELETE FROM ... WHERE TRUE`, equivalent to Synapse’s `TRUNCATE TABLE`.
- Completeness: All business logic, including edge case handling and multi-reason validation, is present and correctly implemented.
- Differences: Temporary tables in Synapse are replaced with CTEs in BigQuery, which is the recommended approach for ephemeral data sets in BigQuery.

Optimization Suggestions
- Partitioning & Clustering: For large `Fact_Sales` and `Sales_Transactions` tables, implement partitioning (e.g., by `Sales_Date`) and clustering (e.g., by `Customer_ID`, `Product_ID`) to optimize query performance and cost.
- Materialized Views: Consider materialized views for frequently accessed aggregates or reporting queries to reduce compute cost.
- Table References: Use fully qualified table names (`project.dataset.table`) for clarity and cross-project compatibility.
- Audit Log Indexing: Index or cluster the `Audit_Log` table on `Batch_ID` and `Start_Time` for faster retrieval and monitoring.
- UDFs: If business logic becomes more complex, encapsulate reusable logic in BigQuery UDFs for maintainability.
- Error Propagation: Integrate with orchestration tools (e.g., Cloud Composer, Airflow) for automated error handling and notification.
- Staging Table Management: For extremely high-frequency loads, consider partitioned staging tables to avoid contention and improve cleanup efficiency.
- Cost Controls: Monitor query costs and optimize SQL to minimize full table scans, especially in audit and failure logging.
- Logging Granularity: Enhance logging with additional metadata (e.g., source system, load type) for improved traceability.

API Cost Estimation
API Cost Consumed in dollars: 0.0047 USD

================================================================================

Original Synapse Stored Procedure (dw.sp_load_sales_fact):

CREATE OR ALTER PROCEDURE dw.sp_load_sales_fact
AS
BEGIN
    SET NOCOUNT ON;

    DECLARE 
        @batch_id UNIQUEIDENTIFIER = NEWID(),
        @start_time DATETIME = SYSDATETIME(),
        @end_time DATETIME,
        @rows_inserted INT = 0,
        @rows_rejected INT = 0,
        @error_message NVARCHAR(4000),
        @proc_name NVARCHAR(128) = OBJECT_NAME(@@PROCID);

    BEGIN TRY
        -------------------------------------------------------------------------
        -- 1. Start Audit Logging
        -------------------------------------------------------------------------
        INSERT INTO dw.Audit_Log
        (
            Batch_ID,
            Procedure_Name,
            Start_Time,
            Status,
            Message
        )
        VALUES
        (
            @batch_id,
            @proc_name,
            @start_time,
            'STARTED',
            'Sales Fact Load Initiated'
        );

        -------------------------------------------------------------------------
        -- 2. Temporary Table for Validation Failures
        -------------------------------------------------------------------------
        IF OBJECT_ID('tempdb..#InvalidRows') IS NOT NULL DROP TABLE #InvalidRows;

        CREATE TABLE #InvalidRows
        (
            Transaction_ID BIGINT,
            Reason NVARCHAR(255)
        );

        -------------------------------------------------------------------------
        -- 3. Basic Data Quality Checks
        -------------------------------------------------------------------------
        INSERT INTO #InvalidRows (Transaction_ID, Reason)
        SELECT Transaction_ID, 'Missing CustomerID'
        FROM stg.Sales_Transactions
        WHERE Customer_ID IS NULL;

        INSERT INTO #InvalidRows (Transaction_ID, Reason)
        SELECT Transaction_ID, 'Invalid Quantity'
        FROM stg.Sales_Transactions
        WHERE Quantity <= 0;

        -------------------------------------------------------------------------
        -- 4. Delete Invalid Rows from Staging for This Batch
        -------------------------------------------------------------------------
        DELETE s
        FROM stg.Sales_Transactions s
        INNER JOIN #InvalidRows i
            ON s.Transaction_ID = i.Transaction_ID;

        SET @rows_rejected = @@ROWCOUNT;

        -------------------------------------------------------------------------
        -- 5. Load Cleaned Data into Fact Table
        -------------------------------------------------------------------------
        ;WITH transformed AS (
            SELECT
                s.Transaction_ID,
                s.Customer_ID,
                s.Product_ID,
                s.Sales_Date,
                s.Quantity,
                s.Unit_Price,
                s.Quantity * s.Unit_Price AS Total_Sales_Amount,
                d.Region_ID,
                c.Customer_Segment,
                SYSDATETIME() AS Load_Timestamp,
                @batch_id AS Batch_ID
            FROM stg.Sales_Transactions s
            INNER JOIN dw.Dim_Customer c
                ON s.Customer_ID = c.Customer_ID
            INNER JOIN dw.Dim_Date d
                ON CAST(s.Sales_Date AS DATE) = d.Date_Value
        )
        INSERT INTO dw.Fact_Sales
        (
            Transaction_ID,
            Customer_ID,
            Product_ID,
            Sales_Date,
            Quantity,
            Unit_Price,
            Total_Sales_Amount,
            Region_ID,
            Customer_Segment,
            Load_Timestamp,
            Batch_ID
        )
        SELECT *
        FROM transformed;

        SET @rows_inserted = @@ROWCOUNT;

        -------------------------------------------------------------------------
        -- 6. Archive or Truncate Staging Table (optional)
        -------------------------------------------------------------------------
        TRUNCATE TABLE stg.Sales_Transactions;

        -------------------------------------------------------------------------
        -- 7. Log Validation Failures
        -------------------------------------------------------------------------
        INSERT INTO dw.DQ_Failures
        (
            Transaction_ID,
            Failure_Reason,
            Logged_Timestamp,
            Batch_ID
        )
        SELECT 
            Transaction_ID,
            Reason,
            SYSDATETIME(),
            @batch_id
        FROM #InvalidRows;

        -------------------------------------------------------------------------
        -- 8. End Audit Log
        -------------------------------------------------------------------------
        SET @end_time = SYSDATETIME();

        UPDATE dw.Audit_Log
        SET 
            End_Time = @end_time,
            Rows_Inserted = @rows_inserted,
            Rows_Rejected = @rows_rejected,
            Status = 'COMPLETED',
            Message = CONCAT('Inserted ', @rows_inserted, ' rows; Rejected ', @rows_rejected, ' rows.')
        WHERE Batch_ID = @batch_id;

    END TRY
    BEGIN CATCH
        -------------------------------------------------------------------------
        -- 9. Error Handling
        -------------------------------------------------------------------------
        SET @end_time = SYSDATETIME();
        SET @error_message = ERROR_MESSAGE();

        UPDATE dw.Audit_Log
        SET 
            End_Time = @end_time,
            Status = 'FAILED',
            Message = @error_message
        WHERE Batch_ID = @batch_id;

        -- Optional: Rethrow for pipeline monitoring
        THROW;
    END CATCH;

    -------------------------------------------------------------------------
    -- 10. Final Cleanup
    -------------------------------------------------------------------------
    IF OBJECT_ID('tempdb..#InvalidRows') IS NOT NULL DROP TABLE #InvalidRows;

END;
GO

================================================================================

Converted BigQuery SQL Script:

-- BigQuery SQL Script: Sales Fact Table Loading with Data Quality Validation and Audit Logging
-- Converted from Azure Synapse stored procedure `dw.sp_load_sales_fact`
-- All table references are generic; adjust dataset names as needed.

DECLARE batch_id STRING DEFAULT GENERATE_UUID();
DECLARE start_time DATETIME DEFAULT CURRENT_DATETIME();
DECLARE end_time DATETIME;
DECLARE rows_inserted INT64 DEFAULT 0;
DECLARE rows_rejected INT64 DEFAULT 0;
DECLARE error_message STRING;
DECLARE proc_name STRING DEFAULT 'sp_load_sales_fact'; -- Manual assignment, no direct equivalent in BigQuery

BEGIN

  -- 1. Start Audit Logging
  INSERT INTO dw.Audit_Log (
    Batch_ID,
    Procedure_Name,
    Start_Time,
    Status,
    Message
  )
  VALUES (
    batch_id,
    proc_name,
    start_time,
    'STARTED',
    'Sales Fact Load Initiated'
  );

  -- 2. Data Quality Validation: Identify Invalid Rows
  WITH InvalidRows AS (
    SELECT Transaction_ID, 'Missing CustomerID' AS Reason
    FROM stg.Sales_Transactions
    WHERE Customer_ID IS NULL

    UNION ALL

    SELECT Transaction_ID, 'Invalid Quantity' AS Reason
    FROM stg.Sales_Transactions
    WHERE Quantity <= 0
  ),

  -- 3. Cleaned Transactions: Remove Invalid Rows
  Cleaned_Transactions AS (
    SELECT s.*
    FROM stg.Sales_Transactions s
    LEFT JOIN InvalidRows i
      ON s.Transaction_ID = i.Transaction_ID
    WHERE i.Transaction_ID IS NULL
  ),

  -- 4. Transformed Data: Enrich with Dimension Data
  transformed AS (
    SELECT
      s.Transaction_ID,
      s.Customer_ID,
      s.Product_ID,
      s.Sales_Date,
      s.Quantity,
      s.Unit_Price,
      s.Quantity * s.Unit_Price AS Total_Sales_Amount,
      d.Region_ID,
      c.Customer_Segment,
      CURRENT_DATETIME() AS Load_Timestamp,
      batch_id AS Batch_ID
    FROM Cleaned_Transactions s
    INNER JOIN dw.Dim_Customer c
      ON s.Customer_ID = c.Customer_ID
    INNER JOIN dw.Dim_Date d
      ON DATE(s.Sales_Date) = d.Date_Value
  )

  -- 5. Load Cleaned Data into Fact Table
  INSERT INTO dw.Fact_Sales (
    Transaction_ID,
    Customer_ID,
    Product_ID,
    Sales_Date,
    Quantity,
    Unit_Price,
    Total_Sales_Amount,
    Region_ID,
    Customer_Segment,
    Load_Timestamp,
    Batch_ID
  )
  SELECT *
  FROM transformed;

  -- 6. Count Inserted Rows
  SET rows_inserted = (SELECT COUNT(*) FROM transformed);

  -- 7. Archive or Truncate Staging Table (BigQuery: DELETE all rows)
  DELETE FROM stg.Sales_Transactions WHERE TRUE;

  -- 8. Log Validation Failures
  INSERT INTO dw.DQ_Failures (
    Transaction_ID,
    Failure_Reason,
    Logged_Timestamp,
    Batch_ID
  )
  SELECT
    Transaction_ID,
    Reason,
    CURRENT_DATETIME(),
    batch_id
  FROM InvalidRows;

  -- 9. Count Rejected Rows
  SET rows_rejected = (SELECT COUNT(*) FROM InvalidRows);

  -- 10. End Audit Log
  SET end_time = CURRENT_DATETIME();

  UPDATE dw.Audit_Log
  SET
    End_Time = end_time,
    Rows_Inserted = rows_inserted,
    Rows_Rejected = rows_rejected,
    Status = 'COMPLETED',
    Message = CONCAT('Inserted ', CAST(rows_inserted AS STRING), ' rows; Rejected ', CAST(rows_rejected AS STRING), ' rows.')
  WHERE Batch_ID = batch_id;

EXCEPTION WHEN ERROR THEN
  -- 11. Error Handling
  SET end_time = CURRENT_DATETIME();
  SET error_message = ERROR_MESSAGE();

  UPDATE dw.Audit_Log
  SET
    End_Time = end_time,
    Status = 'FAILED',
    Message = error_message
  WHERE Batch_ID = batch_id;

  -- Optionally: RAISE ERROR to propagate to orchestration layer
  RAISE USING MESSAGE = error_message;

END;

-- Notes:
-- - All temporary table logic is implemented via CTEs.
-- - Use appropriate dataset prefixes (e.g., `project.dataset.table`) as needed.
-- - Ensure all referenced tables exist in BigQuery with compatible schemas.
-- - For large tables, consider partitioning and clustering as per BigQuery best practices.

API Cost Consumed in dollars: 0.0047 USD

================================================================================