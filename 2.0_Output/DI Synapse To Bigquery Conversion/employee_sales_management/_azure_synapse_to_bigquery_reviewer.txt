=============================================
Author:        Ascendion AVA
Created on:   
Description:   Sales fact table loading procedure with data quality validation and audit logging
=============================================

Summary
The original Synapse stored procedure (`dw.sp_load_sales_fact`) is a batch ETL process that loads sales data from a staging table, performs data quality validation (e.g., missing Customer_ID, invalid Quantity), enriches data with dimension lookups, loads cleaned data into the fact table, logs audit events, and records data quality failures. It features robust error handling, audit logging, and cleanup logic. The BigQuery conversion requires adapting T-SQL constructs (temp tables, TRY-CATCH, variable handling) to BigQuery scripting and SQL idioms.

Conversion Accuracy
- **Data Sources and Destinations:** All source tables (`stg.Sales_Transactions`, `dw.Dim_Customer`, `dw.Dim_Date`) and target tables (`dw.Fact_Sales`, `dw.Audit_Log`, `dw.DQ_Failures`) are mapped directly in the converted BigQuery code.
- **Data Quality Validation:** The logic for identifying invalid rows (missing Customer_ID, Quantity <= 0) is preserved using CTEs in BigQuery, replacing temp tables in Synapse.
- **Transformations and Enrichment:** The calculation of `Total_Sales_Amount` and enrichment with `Region_ID` and `Customer_Segment` via joins are accurately implemented.
- **Audit Logging:** Start and end audit events, row counts, and messages are logged in both implementations, with BigQuery using `INSERT` and `UPDATE` statements and manual variable tracking.
- **Error Handling:** Synapse TRY-CATCH is mapped to BigQuery's `EXCEPTION WHEN ERROR THEN` block, ensuring failed runs are logged with error messages.
- **Row Count Tracking:** Synapse's `@@ROWCOUNT` is replaced by explicit `SELECT COUNT(*)` statements in BigQuery.
- **Cleanup:** Staging table truncation (`TRUNCATE TABLE`) is replaced by `DELETE FROM ... WHERE TRUE` in BigQuery.
- **Business Logic:** All core business logic, including batch ID generation, timestamping, and audit trail, is preserved.
- **Testing:** The provided Pytest suite validates all business rules, edge cases, error handling, and audit logic, confirming functional equivalence.

Optimization Suggestions
- **Partitioning:** Partition `dw.Fact_Sales` by `Sales_Date` and cluster by `Customer_ID`, `Product_ID` to optimize query performance and reduce scan costs.
- **Materialized Views:** Use materialized views for frequently accessed dimension data to speed up lookups.
- **Query Optimization:** Avoid unnecessary `CAST` operations and use explicit column lists instead of `SELECT *` for clarity and performance.
- **Batch Processing:** Consider array-based or set-based processing for large data quality checks to leverage BigQueryâ€™s distributed engine.
- **Error Logging:** Enhance error logging with more granular codes or error types for operational monitoring.
- **Resource Management:** Monitor slot usage and leverage reservation assignments for predictable performance in production.
- **Cost Optimization:** Enable query result caching and avoid full table scans on large tables when possible.

API Cost Estimation
API Cost Consumed in dollars: 0.0047 USD