=============================================
Author:        AAVA
Created on:   
Description:   Sales fact table loading procedure with data quality validation and audit logging
=============================================

Summary:
The document details the conversion of a high-complexity Azure Synapse stored procedure (`dw.sp_load_sales_fact`) into a modern BigQuery SQL implementation. The procedure orchestrates the ETL workflow for loading sales transactions from a staging table into a fact table, incorporating data quality validation, dimensional lookups, audit logging, and robust error handling. The conversion aims to preserve original business logic while leveraging BigQuery’s distributed processing, scalability, and integration capabilities.

Conversion Accuracy:
- **Data Flow and Structure:** The converted BigQuery code maintains the original ETL flow: audit logging initiation, data quality validation, deletion of invalid rows, transformation and enrichment via dimensional lookups, fact table loading, staging table truncation, logging of validation failures, audit completion, and error handling.
- **Data Sources, Joins, and Destinations:** All source tables (`stg.Sales_Transactions`, `dw.Dim_Customer`, `dw.Dim_Date`) and target tables (`dw.Fact_Sales`, `dw.Audit_Log`, `dw.DQ_Failures`) are correctly mapped in BigQuery. JOIN logic for dimension enrichment is preserved using INNER JOINs.
- **SQL Transformations and Business Logic:** Data quality rules (NULL Customer_ID, Quantity <= 0) are implemented using CTEs in BigQuery, replacing Synapse’s temporary table logic. Calculation of `Total_Sales_Amount`, timestamp generation, and batch ID assignment mirror the original logic, with appropriate BigQuery functions (`GENERATE_UUID()`, `CURRENT_DATETIME()`).
- **Error Handling and Logging:** BigQuery’s EXCEPTION block replaces Synapse’s TRY-CATCH, ensuring errors are logged in the audit table and optionally re-raised for pipeline monitoring. Audit logging is comprehensive, tracking batch IDs, procedure names, timestamps, row counts, status, and messages.
- **Query Optimization and Performance:** The code comments recommend partitioning and clustering strategies for large datasets, and materialized views for frequently accessed data. Temporary tables are replaced with CTEs for scalability.
- **Validation and Testing:** The provided Pytest scripts rigorously test all ETL scenarios, including happy path, edge cases (invalid data, missing dimensions), error handling, audit logging, and staging truncation. The reconciliation Python script automates output comparison between Synapse and BigQuery, ensuring functional parity.

Optimization Suggestions:
- **Partitioning:** Implement partitioning on `dw.Fact_Sales` by `Sales_Date` to enhance query performance and reduce scan costs.
- **Clustering:** Cluster the fact table on `Customer_ID` and `Product_ID` for efficient filtering and aggregation.
- **Materialized Views:** Use materialized views for dimension tables if they are frequently joined, to speed up lookups.
- **Array-Based Validation:** For very large batches, consider array-based data quality checks instead of row-by-row validation for improved efficiency.
- **Window Functions:** Where applicable, use window functions to replace complex subqueries for aggregations or ranking.
- **Resource Management:** Monitor slot allocation and leverage BigQuery’s parallelization for large data loads.
- **Cost Optimization:** Avoid `SELECT *` in production queries, use query result caching, and regularly review table statistics for optimal JOIN performance.
- **Error Recovery:** For large-scale pipelines, implement checkpointing or idempotent batch processing to recover from failures without data duplication.
- **Audit Table Indexing:** Ensure audit and DQ failure tables are indexed or clustered on batch ID and timestamp for fast retrieval and monitoring.

API Cost Estimation:
API Cost Consumed in dollars: 0.0080 USD

The conversion is accurate, complete, and efficient, with all business logic, data flow, and error handling preserved. The BigQuery implementation is well-optimized for distributed processing and future scalability, with clear recommendations for further performance and cost improvements. The comprehensive unit and reconciliation tests ensure correctness and business continuity.