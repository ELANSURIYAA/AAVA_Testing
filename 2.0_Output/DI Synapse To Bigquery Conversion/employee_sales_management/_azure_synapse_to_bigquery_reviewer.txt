Metadata
Author:        AAVA
Created on:   
Description:   BigQuery SQL procedure for loading sales fact table with data quality validation and audit logging. Converts Azure Synapse ETL logic to BigQuery, including error handling, audit trail, and optimized data processing.

Summary
The document describes the conversion of an Azure Synapse stored procedure (`dw.sp_load_sales_fact`) to a modern BigQuery SQL procedure for loading a sales fact table. The procedure covers the entire ETL workflow: extracting sales transactions from staging, validating data quality, enriching data with dimensional lookups, transforming and loading into the fact table, and maintaining comprehensive audit and error logs. The conversion leverages BigQuery’s distributed processing, scalable architecture, and procedural scripting capabilities.

Conversion Accuracy
- The original Synapse procedure uses temp tables (`#InvalidRows`), TRY-CATCH error handling, and SQL Server-specific functions (NEWID, SYSDATETIME, @@ROWCOUNT).
- The BigQuery version replaces temp tables with CTEs (InvalidRows, CleanedTransactions, Transformed), uses BigQuery scripting for error handling (`EXCEPTION WHEN ERROR THEN`), and substitutes system functions (`GENERATE_UUID`, `CURRENT_DATETIME`).
- All data sources, joins, and destinations are mapped directly: staging table (`stg.Sales_Transactions`), dimension tables (`dw.Dim_Customer`, `dw.Dim_Date`), fact table (`dw.Fact_Sales`), audit log (`dw.Audit_Log`), and data quality failures (`dw.DQ_Failures`).
- Data quality validation logic (NULL Customer_ID, Quantity <= 0) is implemented identically in both, with invalid rows removed before fact load.
- Transformation logic (Total_Sales_Amount calculation, dimensional enrichment) is preserved.
- Audit logging and error handling are implemented using BigQuery procedural constructs, with status updates and error messages.
- Row counts for inserted and rejected rows are tracked and logged.
- The converted code passes all described test cases, including edge cases for missing columns, invalid data types, and error simulation.
- Output reconciliation is automated via a Python script that extracts, transforms, and compares results from both Synapse and BigQuery, ensuring column-level and row-level accuracy.

Optimization Suggestions
- Partition the `dw.Fact_Sales` table by `Sales_Date` and cluster by `Customer_ID` and `Product_ID` for improved query performance.
- Consider materialized views for frequently accessed dimension data.
- Use explicit column selection instead of `SELECT *` for better performance and cost management.
- Implement query result caching and avoid unnecessary CAST operations.
- Leverage BigQuery’s automatic parallelization and slot allocation for large batch loads.
- For very large datasets, consider streaming inserts or window functions for scalable processing.
- Ensure audit and error tables are optimized for write throughput (partitioning by batch or timestamp).
- Refactor business logic to use array-based processing where possible for efficiency.
- Monitor and tune resource usage to minimize API costs and maximize throughput.

API Cost Estimation
- BigQuery SQL procedure execution and validation: 0.0040 USD
- Pytest validation script for ETL logic and test cases: 0.0040 USD
- Automated reconciliation script (data extraction, transfer, validation, reporting): 0.0080 USD
- Total API Cost Consumed: 0.0160 USD

The converted BigQuery code accurately replicates the original Synapse stored procedure’s logic and intent, maintains consistency in data processing and business rules, and is optimized for BigQuery’s distributed architecture. All critical ETL, validation, audit, and error handling features are preserved and validated with comprehensive test coverage and automated reconciliation.