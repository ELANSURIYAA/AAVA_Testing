=============================================
Author:        AAVA
Created on:   
Description:   Sales fact table loading procedure with data quality validation and audit logging, migrated from Azure Synapse to Google BigQuery SQL
=============================================

Summary
The original Azure Synapse stored procedure `dw.sp_load_sales_fact` is a batch ETL process for loading sales transactions into a fact table, with comprehensive data quality validation, audit logging, and error handling. The converted BigQuery code accurately replicates this logic, using BigQuery scripting features, CTEs for validation, and audit/error management, while leveraging BigQuery's distributed processing and performance capabilities.

Conversion Accuracy
- **Data Sources & Joins:** All source tables (`stg.Sales_Transactions`, `dw.Dim_Customer`, `dw.Dim_Date`) and joins are correctly mapped in BigQuery. Dimensional enrichment and data quality logic are preserved.
- **Transformations & Aggregations:** The calculation of `Total_Sales_Amount`, batch ID generation, and timestamp assignment are correctly implemented. Data quality checks for missing `Customer_ID` and invalid `Quantity` are handled via CTEs instead of temp tables.
- **Business Logic:** The business logic for validation, enrichment, and audit logging is fully replicated. The BigQuery code uses CTEs and scripting blocks to replace Synapse temp tables and procedural constructs.
- **Error Handling:** BigQuery scripting uses `EXCEPTION WHEN ERROR THEN` to capture and log errors, updating the audit log with failure status and error messages.
- **Audit & DQ Logging:** Audit and data quality failure logging are implemented as in Synapse, with compatible schemas and explicit row counts.
- **Testing & Validation:** Comprehensive pytest-based unit tests validate all edge cases, including happy path, invalid data, missing columns, invalid types, and audit trail completeness. Automated reconciliation scripts compare outputs between Synapse and BigQuery, confirming row/column/data matches.

Optimization Suggestions
- **Partitioning & Clustering:** Partition `dw.Fact_Sales` by `Sales_Date` and cluster by `Customer_ID`, `Product_ID` for optimal query performance.
- **Materialized Views:** Use materialized views for frequently accessed dimensional data to reduce query latency.
- **Query Optimization:** Avoid unnecessary CASTs and SELECT * operations; use explicit column lists and leverage BigQuery's query optimizer.
- **Streaming Inserts:** Consider streaming inserts for real-time data requirements.
- **Error Recovery:** Implement checkpoint-based recovery for large data volumes and use BigQuery's parallelization features.
- **Cost Management:** Enable query result caching and monitor slot usage to optimize cost and performance.

API Cost Estimation
API Cost Consumed in dollars: 0.0047 USD

=============================================
-- Original Synapse Stored Procedure (Summary) --

The Synapse procedure uses temp tables for invalid row tracking, explicit error handling via TRY-CATCH, and batch-based audit logging. Data quality checks filter out invalid transactions before loading cleaned data into the fact table and logging failures.

-- Converted BigQuery SQL (Summary) --

BigQuery code replaces temp tables with CTEs, uses scripting for variable management and error handling, and preserves all business logic, audit, and validation steps. Partitioning, clustering, and explicit error management are recommended for further optimization.

-- Automated Validation & Testing --

Unit tests (pytest) and reconciliation scripts confirm functional equivalence, edge case handling, and output consistency between Synapse and BigQuery implementations.

=============================================