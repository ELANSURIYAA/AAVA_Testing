========================================
Author:        Ascendion AVA
Created on:   
Description:   Sales fact table loading procedure with data quality validation and audit logging

Summary
The document describes the conversion and validation of an Azure Synapse stored procedure (`dw.sp_load_sales_fact`) to BigQuery SQL. The procedure is responsible for loading sales transaction data from a staging table to a fact table, applying data quality checks, performing dimensional lookups, calculating business metrics, and maintaining audit and error logs. The conversion aims to preserve all business logic and data processing steps while leveraging BigQuery’s distributed architecture and performance features.

Conversion Accuracy
- **Data Flow & Structure:**  
  The BigQuery code accurately replicates the original Synapse procedure’s workflow:
  - Variable declarations for batch tracking, timestamps, and row counts.
  - Audit logging at the start and end of the process.
  - Data quality validation for missing `Customer_ID` and invalid `Quantity` using CTEs (in BigQuery) and temporary tables (in Synapse).
  - Removal of invalid rows from the staging table.
  - Dimensional lookups using INNER JOINs to enrich sales data with customer and date dimensions.
  - Calculation of `Total_Sales_Amount`.
  - Insertion of cleaned and transformed data into the fact table.
  - Logging of validation failures into a dedicated table.
  - Comprehensive error handling and audit updates on failure.
- **Mapping of Data Sources, Joins, and Destinations:**  
  All source tables (`stg.Sales_Transactions`, `dw.Dim_Customer`, `dw.Dim_Date`) and target tables (`dw.Fact_Sales`, `dw.Audit_Log`, `dw.DQ_Failures`) are correctly mapped.
- **SQL Transformations & Business Logic:**  
  All transformations, aggregations, and business logic (including calculation of totals, enrichment, and validation) are implemented as per the original intent.  
  - UDFs and scripting logic are not required for this conversion; all logic is handled via SQL and procedural constructs.
- **Error Handling & Logging:**  
  BigQuery’s scripting block uses `EXCEPTION WHEN ERROR THEN` for error handling, updating the audit log and optionally raising errors for external monitoring, matching Synapse’s TRY-CATCH behavior.
- **Best Practices & Optimization:**  
  The conversion notes recommend partitioning and clustering for large tables, use of CTEs for validation, and explicit variable assignments for row counts.
- **Manual Adjustments:**  
  - Temporary tables in Synapse are replaced by CTEs in BigQuery.
  - System functions (`NEWID()`, `SYSDATETIME()`, `OBJECT_NAME(@@PROCID)`) are mapped to BigQuery equivalents (`GENERATE_UUID()`, `CURRENT_DATETIME()`, manual procedure name assignment).
  - Data type conversions are handled appropriately (e.g., `UNIQUEIDENTIFIER` to `STRING`, `DATETIME` to `DATETIME`).
  - Row count tracking uses explicit COUNT queries in BigQuery.
- **Validation & Testing:**  
  The provided Pytest suite covers all major scenarios: happy path, edge cases (invalid data, empty staging), error handling, transformation accuracy, audit log correctness, and large batch processing. This ensures the converted logic is fully validated against representative data.

Optimization Suggestions
- **Partitioning & Clustering:**  
  Partition the `dw.Fact_Sales` table by `Sales_Date` and cluster by `Customer_ID` and `Product_ID` for improved query performance and cost efficiency.
- **Materialized Views:**  
  Consider materialized views for dimension tables (`Dim_Customer`, `Dim_Date`) if they are frequently joined, to optimize lookup performance.
- **Batch Processing:**  
  For very large datasets, use array-based or batch processing for data quality checks to minimize query costs.
- **Error Recovery:**  
  Implement checkpoint-based recovery for large data volumes to improve resilience.
- **Resource Management:**  
  Monitor and adjust BigQuery slot allocation for consistent performance in production workloads.
- **Cost Optimization:**  
  Use query result caching and avoid unnecessary SELECT * operations.  
  Ensure only required columns are selected and processed.
- **Monitoring & Scheduling:**  
  Integrate with BigQuery’s monitoring and job scheduling tools for automated pipeline execution and alerting.
- **Audit & Logging Enhancements:**  
  Expand audit logging to include query execution metrics, resource usage, and error details for enhanced observability.

API Cost Estimation
apiCost: 0.0040 USD

========================================
Original Synapse Procedure Complexity & Mapping

## 1. Procedure Overview

The Azure Synapse stored procedure `dw.sp_load_sales_fact` implements a comprehensive ETL workflow for sales transaction data processing. This procedure serves as a critical component in the organization's data warehouse pipeline, responsible for extracting sales data from staging tables, performing data quality validations, transforming the data with business logic, and loading it into the fact table while maintaining complete audit trails.

**Key Business Objectives:**
- Data integration from staging to fact tables
- Data quality validation and cleansing
- Business logic enrichment through dimensional lookups
- Comprehensive audit logging and error tracking
- Automated data pipeline processing

**Workflow Structure:**
- **Number of Mappings per Workflow:** 1 main data flow with 8 logical processing steps
- **Session Count:** Single session execution with comprehensive error handling
- **Data Processing Pattern:** Batch-based ETL with validation framework

## 2. Complexity Metrics

| Metric | Count | Type/Details |
|--------|-------|--------------|
| Number of Source Qualifiers | 3 | stg.Sales_Transactions (SQL Server), dw.Dim_Customer (SQL Server), dw.Dim_Date (SQL Server) |
| Number of Transformations | 6 | Data validation, calculation, lookup enrichment, audit logging, error handling, cleanup |
| Lookup Usage | 2 | Connected lookups (Dim_Customer, Dim_Date) |
| Expression Logic | 4 | Total_Sales_Amount calculation, timestamp generation, batch ID assignment, validation conditions |
| Join Conditions | 3 | 2 INNER JOINs (dimension lookups), 1 INNER JOIN (invalid row removal) |
| Conditional Logic | 2 | Data quality validation filters (NULL Customer_ID, Invalid Quantity) |
| Reusable Components | 0 | No reusable transformations or mapplets |
| Data Sources | 3 | SQL Server staging table, SQL Server dimension tables |
| Data Targets | 3 | SQL Server fact table, SQL Server audit table, SQL Server DQ failures table |
| Pre/Post SQL Logic | 4 | Temp table creation/cleanup, staging truncation, audit initialization/completion |
| Session/Workflow Controls | 1 | TRY-CATCH error handling with transaction control |
| DML Logic | 6 | INSERT (4), DELETE (1), UPDATE (1), TRUNCATE (1) |
| **Complexity Score (0–100)** | **75** | High complexity due to multiple data quality checks, dimensional lookups, audit framework, and comprehensive error handling |

**High-Complexity Areas:**
- Multiple data quality validation rules with temporary table management
- Complex CTE with multiple INNER JOINs for dimensional enrichment
- Comprehensive audit logging framework with batch tracking
- Error handling with rollback and cleanup mechanisms

## 3. Syntax Differences

**Azure Synapse Functions Requiring BigQuery Conversion:**
- `NEWID()` → `GENERATE_UUID()` for unique identifier generation
- `SYSDATETIME()` → `CURRENT_DATETIME()` for timestamp functions
- `OBJECT_NAME(@@PROCID)` → Manual procedure name assignment (no direct equivalent)
- `@@ROWCOUNT` → `ROW_COUNT()` or explicit counting in BigQuery
- `CAST(s.Sales_Date AS DATE)` → `DATE(s.Sales_Date)` for date casting
- `CONCAT()` → `CONCAT()` (similar but may need adjustment for NULL handling)

**Data Type Conversions:**
- `UNIQUEIDENTIFIER` → `STRING` (UUID format)
- `DATETIME` → `DATETIME` (compatible)
- `NVARCHAR` → `STRING`
- `BIGINT` → `INT64`
- `INT` → `INT64`

**Control Flow Restructuring:**
- `TRY-CATCH` blocks → BigQuery exception handling with `BEGIN EXCEPTION` blocks
- `SET NOCOUNT ON` → Not applicable in BigQuery
- Temporary tables (`#InvalidRows`) → Common Table Expressions (CTEs) or temporary datasets
- `TRUNCATE TABLE` → `DELETE FROM table WHERE TRUE` in BigQuery

## 4. Manual Adjustments

**Components Requiring Manual Implementation:**
- **Procedure Structure:** BigQuery stored procedures use different syntax (`CREATE OR REPLACE PROCEDURE`)
- **Variable Declarations:** BigQuery uses `DECLARE` with different syntax patterns
- **Temporary Table Logic:** Convert `#InvalidRows` temporary table to CTE or array-based logic
- **Error Handling:** Restructure TRY-CATCH to BigQuery's exception handling model
- **Row Count Tracking:** Replace `@@ROWCOUNT` with explicit counting or `ROW_COUNT()` function calls
- **System Functions:** Manual implementation of procedure name tracking (no `@@PROCID` equivalent)

**External Dependencies:**
- **Audit Framework:** Verify audit table schemas are compatible with BigQuery
- **Data Quality Tables:** Ensure DQ_Failures table structure supports BigQuery data types
- **Scheduling Integration:** Update job scheduling to work with BigQuery's execution environment
- **Monitoring Systems:** Adjust monitoring tools to capture BigQuery procedure execution metrics

**Business Logic Validation Areas:**
- **Data Quality Rules:** Validate that validation logic produces same results in BigQuery
- **Calculation Accuracy:** Verify Total_Sales_Amount calculations maintain precision
- **Dimensional Lookup Logic:** Ensure JOIN conditions work correctly with BigQuery's execution engine
- **Audit Trail Completeness:** Confirm all audit information is captured properly

## 5. Optimization Techniques

**BigQuery Best Practices:**
- **Partitioning Strategy:** Partition `dw.Fact_Sales` table by `Sales_Date` for improved query performance
- **Clustering Implementation:** Cluster fact table on `Customer_ID` and `Product_ID` for efficient filtering
- **Materialized Views:** Consider materialized views for frequently accessed dimensional data
- **Query Optimization:** Use BigQuery's query optimizer by avoiding unnecessary CAST operations

**Data Processing Optimizations:**
- **Batch Processing:** Implement array-based processing for data quality checks instead of row-by-row validation
- **Streaming Inserts:** Consider BigQuery streaming inserts for real-time data processing requirements
- **Window Functions:** Replace complex subqueries with window functions for better performance
- **JOIN Optimization:** Use BigQuery's automatic JOIN optimization by ensuring proper table statistics

**Pipeline Improvements:**
- **Error Recovery:** Implement checkpoint-based recovery for large data volumes
- **Parallel Processing:** Leverage BigQuery's automatic parallelization for large datasets
- **Resource Management:** Use appropriate slot allocation for consistent performance
- **Cost Optimization:** Implement query result caching and avoid SELECT * operations

**Recommendation: REFACTOR**
The existing logic is well-structured and follows good ETL practices. The recommendation is to **REFACTOR** the current implementation to BigQuery syntax while maintaining the core business logic, audit framework, and data quality validation patterns. The procedure's modular design and comprehensive error handling make it suitable for adaptation rather than complete rebuilding.

**Migration Priority:**
1. Convert basic syntax and data types
2. Implement BigQuery-compatible error handling
3. Optimize dimensional lookups with BigQuery best practices
4. Enhance with BigQuery-specific performance optimizations
5. Validate data quality and audit functionality

API Cost Estimation
apiCost: 0.0040 USD