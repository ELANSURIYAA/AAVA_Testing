Author:        AAVA
Created on:   
Description:   Loads sales fact data from staging to the fact table, performs data quality checks, logs audit and DQ failures, and handles error/audit logging.

Summary
The document describes the migration of the Azure Synapse stored procedure `dw.sp_load_sales_fact` to BigQuery SQL. The procedure is a batch ETL pipeline that loads sales transactions from a staging table into a fact table, performs data quality validations (such as checking for missing Customer_ID and invalid Quantity), enriches data via dimensional lookups, and logs audit and data quality failures. The migration process involves converting procedural T-SQL logic to BigQuery scripting, adapting control flow, temporary table usage, and error handling to BigQuery’s environment, while ensuring business logic and audit frameworks are preserved.

Conversion Accuracy
- The conversion accurately replicates the original Synapse logic:
    - **Audit Logging:** Both start and end audit logs are implemented, capturing batch ID, procedure name, timestamps, status, and row counts.
    - **Temporary Table/Validation:** Synapse uses a temp table (#InvalidRows); BigQuery uses a scripting temp table or CTE for invalid rows.
    - **Data Quality Checks:** Both implementations check for missing Customer_ID and invalid Quantity, logging failures and removing invalid records before loading to the fact table.
    - **Transformation & Enrichment:** The join logic for dimensional enrichment (Dim_Customer, Dim_Date) is preserved, and calculations (Total_Sales_Amount) are correctly mapped.
    - **Fact Table Load:** Only validated and enriched records are inserted into the fact table.
    - **Staging Cleanup:** Staging table is truncated post-load in both implementations.
    - **DQ Failure Logging:** All validation failures are logged to the DQ_Failures table.
    - **Error Handling:** Synapse uses TRY-CATCH; BigQuery uses scripting blocks and recommends wrapping in a stored procedure for production error management.
    - **Audit Update:** Final audit log update includes inserted/rejected row counts and status.
- **Manual Adjustments:** System functions (NEWID, SYSDATETIME, @@ROWCOUNT) are replaced with BigQuery equivalents (GENERATE_UUID, CURRENT_TIMESTAMP, explicit counting).
- **Data Types:** All data types are mapped appropriately (UNIQUEIDENTIFIER→STRING, BIGINT→INT64, NVARCHAR→STRING).
- **Test Coverage:** The migration is validated with comprehensive pytest scripts covering all major scenarios, including happy path, invalid data, missing columns, datatype errors, large batches, and multiple DQ failures.

Optimization Suggestions
- **Partitioning:** Partition `dw.Fact_Sales` by `Sales_Date` for query performance.
- **Clustering:** Cluster on `Customer_ID` and `Product_ID` for efficient filtering.
- **Materialized Views:** Use for frequently accessed dimensional data.
- **Query Optimization:** Avoid unnecessary CASTs; use BigQuery’s optimizer.
- **Batch Processing:** Prefer array-based DQ checks over row-by-row validation.
- **Streaming Inserts:** For real-time needs, consider BigQuery streaming.
- **Window Functions:** Replace complex subqueries with window functions where possible.
- **JOIN Optimization:** Ensure proper table statistics for automatic join optimization.
- **Error Recovery:** Implement checkpoint-based recovery for large volumes.
- **Parallel Processing:** Leverage BigQuery’s parallelization.
- **Resource Management:** Use appropriate slot allocation.
- **Cost Optimization:** Use query result caching and avoid SELECT *.

API Cost Estimation
apiCost: 0.0040 USD

---

Original Synapse Stored Procedure:

CREATE OR ALTER PROCEDURE dw.sp_load_sales_fact
AS
BEGIN
    SET NOCOUNT ON;

    DECLARE 
        @batch_id UNIQUEIDENTIFIER = NEWID(),
        @start_time DATETIME = SYSDATETIME(),
        @end_time DATETIME,
        @rows_inserted INT = 0,
        @rows_rejected INT = 0,
        @error_message NVARCHAR(4000),
        @proc_name NVARCHAR(128) = OBJECT_NAME(@@PROCID);

    BEGIN TRY
        -------------------------------------------------------------------------
        -- 1. Start Audit Logging
        -------------------------------------------------------------------------
        INSERT INTO dw.Audit_Log
        (
            Batch_ID,
            Procedure_Name,
            Start_Time,
            Status,
            Message
        )
        VALUES
        (
            @batch_id,
            @proc_name,
            @start_time,
            'STARTED',
            'Sales Fact Load Initiated'
        );

        -------------------------------------------------------------------------
        -- 2. Temporary Table for Validation Failures
        -------------------------------------------------------------------------
        IF OBJECT_ID('tempdb..#InvalidRows') IS NOT NULL DROP TABLE #InvalidRows;

        CREATE TABLE #InvalidRows
        (
            Transaction_ID BIGINT,
            Reason NVARCHAR(255)
        );

        -------------------------------------------------------------------------
        -- 3. Basic Data Quality Checks
        -------------------------------------------------------------------------
        INSERT INTO #InvalidRows (Transaction_ID, Reason)
        SELECT Transaction_ID, 'Missing CustomerID'
        FROM stg.Sales_Transactions
        WHERE Customer_ID IS NULL;

        INSERT INTO #InvalidRows (Transaction_ID, Reason)
        SELECT Transaction_ID, 'Invalid Quantity'
        FROM stg.Sales_Transactions
        WHERE Quantity <= 0;

        -------------------------------------------------------------------------
        -- 4. Delete Invalid Rows from Staging for This Batch
        -------------------------------------------------------------------------
        DELETE s
        FROM stg.Sales_Transactions s
        INNER JOIN #InvalidRows i
            ON s.Transaction_ID = i.Transaction_ID;

        SET @rows_rejected = @@ROWCOUNT;

        -------------------------------------------------------------------------
        -- 5. Load Cleaned Data into Fact Table
        -------------------------------------------------------------------------
        ;WITH transformed AS (
            SELECT
                s.Transaction_ID,
                s.Customer_ID,
                s.Product_ID,
                s.Sales_Date,
                s.Quantity,
                s.Unit_Price,
                s.Quantity * s.Unit_Price AS Total_Sales_Amount,
                d.Region_ID,
                c.Customer_Segment,
                SYSDATETIME() AS Load_Timestamp,
                @batch_id AS Batch_ID
            FROM stg.Sales_Transactions s
            INNER JOIN dw.Dim_Customer c
                ON s.Customer_ID = c.Customer_ID
            INNER JOIN dw.Dim_Date d
                ON CAST(s.Sales_Date AS DATE) = d.Date_Value
        )
        INSERT INTO dw.Fact_Sales
        (
            Transaction_ID,
            Customer_ID,
            Product_ID,
            Sales_Date,
            Quantity,
            Unit_Price,
            Total_Sales_Amount,
            Region_ID,
            Customer_Segment,
            Load_Timestamp,
            Batch_ID
        )
        SELECT *
        FROM transformed;

        SET @rows_inserted = @@ROWCOUNT;

        -------------------------------------------------------------------------
        -- 6. Archive or Truncate Staging Table (optional)
        -------------------------------------------------------------------------
        TRUNCATE TABLE stg.Sales_Transactions;

        -------------------------------------------------------------------------
        -- 7. Log Validation Failures
        -------------------------------------------------------------------------
        INSERT INTO dw.DQ_Failures
        (
            Transaction_ID,
            Failure_Reason,
            Logged_Timestamp,
            Batch_ID
        )
        SELECT 
            Transaction_ID,
            Reason,
            SYSDATETIME(),
            @batch_id
        FROM #InvalidRows;

        -------------------------------------------------------------------------
        -- 8. End Audit Log
        -------------------------------------------------------------------------
        SET @end_time = SYSDATETIME();

        UPDATE dw.Audit_Log
        SET 
            End_Time = @end_time,
            Rows_Inserted = @rows_inserted,
            Rows_Rejected = @rows_rejected,
            Status = 'COMPLETED',
            Message = CONCAT('Inserted ', @rows_inserted, ' rows; Rejected ', @rows_rejected, ' rows.')
        WHERE Batch_ID = @batch_id;

    END TRY
    BEGIN CATCH
        -------------------------------------------------------------------------
        -- 9. Error Handling
        -------------------------------------------------------------------------
        SET @end_time = SYSDATETIME();
        SET @error_message = ERROR_MESSAGE();

        UPDATE dw.Audit_Log
        SET 
            End_Time = @end_time,
            Status = 'FAILED',
            Message = @error_message
        WHERE Batch_ID = @batch_id;

        -- Optional: Rethrow for pipeline monitoring
        THROW;
    END CATCH;

    -------------------------------------------------------------------------
    -- 10. Final Cleanup
    -------------------------------------------------------------------------
    IF OBJECT_ID('tempdb..#InvalidRows') IS NOT NULL DROP TABLE #InvalidRows;

END;
GO

---

BigQuery Conversion Analysis:

=============================================
Author:        Ascendion AVA
Created on:   
Description:   Sales fact table loading procedure with data quality validation and audit logging
=============================================

## 1. Procedure Overview

The Azure Synapse stored procedure `dw.sp_load_sales_fact` implements a comprehensive ETL workflow for sales transaction data processing. This procedure serves as a critical component in the organization's data warehouse pipeline, responsible for extracting sales data from staging tables, performing data quality validations, transforming the data with business logic, and loading it into the fact table while maintaining complete audit trails.

**Key Business Objectives:**
- Data integration from staging to fact tables
- Data quality validation and cleansing
- Business logic enrichment through dimensional lookups
- Comprehensive audit logging and error tracking
- Automated data pipeline processing

**Workflow Structure:**
- **Number of Mappings per Workflow:** 1 main data flow with 8 logical processing steps
- **Session Count:** Single session execution with comprehensive error handling
- **Data Processing Pattern:** Batch-based ETL with validation framework

## 2. Complexity Metrics

| Metric | Count | Type/Details |
|--------|-------|--------------|
| Number of Source Qualifiers | 3 | stg.Sales_Transactions (SQL Server), dw.Dim_Customer (SQL Server), dw.Dim_Date (SQL Server) |
| Number of Transformations | 6 | Data validation, calculation, lookup enrichment, audit logging, error handling, cleanup |
| Lookup Usage | 2 | Connected lookups (Dim_Customer, Dim_Date) |
| Expression Logic | 4 | Total_Sales_Amount calculation, timestamp generation, batch ID assignment, validation conditions |
| Join Conditions | 3 | 2 INNER JOINs (dimension lookups), 1 INNER JOIN (invalid row removal) |
| Conditional Logic | 2 | Data quality validation filters (NULL Customer_ID, Invalid Quantity) |
| Reusable Components | 0 | No reusable transformations or mapplets |
| Data Sources | 3 | SQL Server staging table, SQL Server dimension tables |
| Data Targets | 3 | SQL Server fact table, SQL Server audit table, SQL Server DQ failures table |
| Pre/Post SQL Logic | 4 | Temp table creation/cleanup, staging truncation, audit initialization/completion |
| Session/Workflow Controls | 1 | TRY-CATCH error handling with transaction control |
| DML Logic | 6 | INSERT (4), DELETE (1), UPDATE (1), TRUNCATE (1) |
| **Complexity Score (0–100)** | **75** | High complexity due to multiple data quality checks, dimensional lookups, audit framework, and comprehensive error handling |

**High-Complexity Areas:**
- Multiple data quality validation rules with temporary table management
- Complex CTE with multiple INNER JOINs for dimensional enrichment
- Comprehensive audit logging framework with batch tracking
- Error handling with rollback and cleanup mechanisms

## 3. Syntax Differences

**Azure Synapse Functions Requiring BigQuery Conversion:**
- `NEWID()` → `GENERATE_UUID()` for unique identifier generation
- `SYSDATETIME()` → `CURRENT_DATETIME()` for timestamp functions
- `OBJECT_NAME(@@PROCID)` → Manual procedure name assignment (no direct equivalent)
- `@@ROWCOUNT` → `ROW_COUNT()` or explicit counting in BigQuery
- `CAST(s.Sales_Date AS DATE)` → `DATE(s.Sales_Date)` for date casting
- `CONCAT()` → `CONCAT()` (similar but may need adjustment for NULL handling)

**Data Type Conversions:**
- `UNIQUEIDENTIFIER` → `STRING` (UUID format)
- `DATETIME` → `DATETIME` (compatible)
- `NVARCHAR` → `STRING`
- `BIGINT` → `INT64`
- `INT` → `INT64`

**Control Flow Restructuring:**
- `TRY-CATCH` blocks → BigQuery exception handling with `BEGIN EXCEPTION` blocks
- `SET NOCOUNT ON` → Not applicable in BigQuery
- Temporary tables (`#InvalidRows`) → Common Table Expressions (CTEs) or temporary datasets
- `TRUNCATE TABLE` → `DELETE FROM table WHERE TRUE` in BigQuery

## 4. Manual Adjustments

**Components Requiring Manual Implementation:**
- **Procedure Structure:** BigQuery stored procedures use different syntax (`CREATE OR REPLACE PROCEDURE`)
- **Variable Declarations:** BigQuery uses `DECLARE` with different syntax patterns
- **Temporary Table Logic:** Convert `#InvalidRows` temporary table to CTE or array-based logic
- **Error Handling:** Restructure TRY-CATCH to BigQuery's exception handling model
- **Row Count Tracking:** Replace `@@ROWCOUNT` with explicit counting or `ROW_COUNT()` function calls
- **System Functions:** Manual implementation of procedure name tracking (no `@@PROCID` equivalent)

**External Dependencies:**
- **Audit Framework:** Verify audit table schemas are compatible with BigQuery
- **Data Quality Tables:** Ensure DQ_Failures table structure supports BigQuery data types
- **Scheduling Integration:** Update job scheduling to work with BigQuery's execution environment
- **Monitoring Systems:** Adjust monitoring tools to capture BigQuery procedure execution metrics

**Business Logic Validation Areas:**
- **Data Quality Rules:** Validate that validation logic produces same results in BigQuery
- **Calculation Accuracy:** Verify Total_Sales_Amount calculations maintain precision
- **Dimensional Lookup Logic:** Ensure JOIN conditions work correctly with BigQuery's execution engine
- **Audit Trail Completeness:** Confirm all audit information is captured properly

## 5. Optimization Techniques

**BigQuery Best Practices:**
- **Partitioning Strategy:** Partition `dw.Fact_Sales` table by `Sales_Date` for improved query performance
- **Clustering Implementation:** Cluster fact table on `Customer_ID` and `Product_ID` for efficient filtering
- **Materialized Views:** Consider materialized views for frequently accessed dimensional data
- **Query Optimization:** Use BigQuery's query optimizer by avoiding unnecessary CAST operations

**Data Processing Optimizations:**
- **Batch Processing:** Implement array-based processing for data quality checks instead of row-by-row validation
- **Streaming Inserts:** Consider BigQuery streaming inserts for real-time data processing requirements
- **Window Functions:** Replace complex subqueries with window functions for better performance
- **JOIN Optimization:** Use BigQuery's automatic JOIN optimization by ensuring proper table statistics

**Pipeline Improvements:**
- **Error Recovery:** Implement checkpoint-based recovery for large data volumes
- **Parallel Processing:** Leverage BigQuery's automatic parallelization for large datasets
- **Resource Management:** Use appropriate slot allocation for consistent performance
- **Cost Optimization:** Implement query result caching and avoid SELECT * operations

**Recommendation: REFACTOR**
The existing logic is well-structured and follows good ETL practices. The recommendation is to **REFACTOR** the current implementation to BigQuery syntax while maintaining the core business logic, audit framework, and data quality validation patterns. The procedure's modular design and comprehensive error handling make it suitable for adaptation rather than complete rebuilding.

**Migration Priority:**
1. Convert basic syntax and data types
2. Implement BigQuery-compatible error handling
3. Optimize dimensional lookups with BigQuery best practices
4. Enhance with BigQuery-specific performance optimizations
5. Validate data quality and audit functionality

API Cost Consumption:
apiCost: 0.0040 USD