Author:        Ascendion AVA
Created on:   
Description:   Sales fact table loading procedure with data quality validation and audit logging

Summary
This document reviews the conversion of the Azure Synapse stored procedure for sales fact table loading to its BigQuery equivalent. The original Synapse procedure, `dw.sp_load_sales_fact`, is a high-complexity batch ETL process responsible for extracting sales data from staging, validating and cleansing it, enriching with dimensional lookups, and loading into the fact table with comprehensive audit and error logging. The conversion to BigQuery leverages scripting, CTEs, and native error handling, and is validated by a robust suite of Pytest cases and a Python-based reconciliation framework.

Conversion Accuracy
- The BigQuery procedure accurately replicates all core logic from Synapse:
  - Data is extracted from staging (`stg.Sales_Transactions`), validated for missing `Customer_ID` and invalid `Quantity`, and invalid rows are excluded using CTEs.
  - Dimensional enrichment is performed via INNER JOINs with `dw.Dim_Customer` and `dw.Dim_Date`.
  - Cleaned and transformed data is loaded into `dw.Fact_Sales`.
  - Data quality failures are logged in `dw.DQ_Failures`.
  - Audit logging is implemented in `dw.Audit_Log`, capturing batch ID, timestamps, row counts, status, and error messages.
  - The procedure uses BigQuery scripting for variable declaration, error handling (EXCEPTION block), and batch control.
  - All Synapse-specific functions and control flow (e.g., `NEWID()`, `SYSDATETIME()`, `TRY-CATCH`, temp tables) are properly mapped to BigQuery equivalents (`GENERATE_UUID()`, `CURRENT_DATETIME()`, scripting EXCEPTION, CTEs).
- The Pytest test suite covers all critical scenarios: happy path, all DQ rules, edge cases (missing dimensions, empty staging), error handling, and schema mismatches.
- The reconciliation Python script provides automated, column-level validation between Synapse and BigQuery outputs, ensuring functional equivalence.

Optimization Suggestions
- Partition `dw.Fact_Sales` by `Sales_Date` and cluster on `Customer_ID`, `Product_ID` for improved query performance.
- Consider materialized views for dimensional tables if lookup performance becomes a bottleneck.
- For very large data volumes, leverage BigQuery's parallelization and avoid SELECT * in downstream queries.
- Use query result caching and avoid unnecessary CAST operations.
- For real-time or micro-batch requirements, evaluate streaming inserts.
- Monitor slot usage and audit logs for ongoing performance tuning.
- Refactor any remaining procedural logic into set-based operations where possible to maximize BigQuery's distributed engine.

API Cost Estimation
apiCost: 0.0040 USD