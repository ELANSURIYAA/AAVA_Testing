=============================================
Author:        Ascendion AVA+
Date:   
Description:   Effort and cost estimation plan for PySpark migration and testing of TAMBR_RINGS.txt SAS code, which generates branch-customer distance rings for priority and most-used branches in support of TAMBr process reporting and analytics.
=============================================

1.1 Cost Estimation

   1.1.1 PySpark Runtime Cost 
         - The PySpark runtime cost estimation cannot be completed because the Azure Databricks environment details (cluster type, node specs, pricing, and data volume) are not available in the provided files or directory. 
         - Please provide the Env_File or specify the environment configuration and pricing to enable a precise runtime cost calculation.
         - **API cost for this call:** 0.01 USD

2. Code Fixing and Testing Effort Estimation

   2.1 PySpark identified manual code fixes and unit testing effort in hours covering the various temp tables, calculations

**Manual Adjustments Required (from Analyzer Output and Script Review):**
- Replace `geodist` with a PySpark UDF for Haversine or Vincenty distance calculation.
- Replace PROC UNIVARIATE percentile logic with PySpark's approx_percentile or window functions.
- Replace SAS macro variable logic with Python variables and functions.
- Replace DATA step IF/OUTPUT with DataFrame filtering and conditional writes.
- Replace PROC SQL joins with DataFrame joins (inner, left, full).
- Replace PROC FREQ with DataFrame groupBy/count operations.
- Remove SAS-specific options (bulkload, bl_method, noprint).
- Replace macro calls with Python functions.
- Replace table existence checks and drops with Spark catalog commands.
- Replace SAS library references with Spark database/table paths.
- Rewrite dynamic macro variable creation and inline macro logic as Python functions.
- Rewrite Data step output to multiple tables as DataFrame partitioning.
- Modularize ETL steps as PySpark functions/classes.
- Implement spatial calculations as UDFs.
- Use window functions for percentile calculations.
- Replace reporting logic with DataFrame aggregations and exports.

**Effort Estimation for Manual Code Fixes and Testing:**

- **Number of Temporary/Intermediate Tables:** 10+
- **Complex Joins and Aggregations:** 8+ (including percentile, rolling sums, groupings)
- **Custom Function Replacements:** 2 (geodist, percentile)
- **Macro to Python Conversion:** 2 major macros, multiple variable replacements
- **Error Handling/Data Quality:** 2+ (invalid geocodes, conditional outputs)
- **Reporting Logic:** 2 (frequency tables, summary outputs)

| Task Area                              | Estimated Hours |
|---------------------------------------- |----------------|
| Geodist UDF implementation & testing    | 6              |
| Percentile calculation migration        | 4              |
| Macro logic to Python                   | 4              |
| Data step logic to DataFrame migration  | 6              |
| SQL joins/aggregations to DataFrames    | 6              |
| Table existence/drop logic migration    | 2              |
| Error handling/data quality logic       | 2              |
| Reporting logic (freq, summary)         | 2              |
| Unit testing for each transformation    | 8              |
| Data reconciliation and validation      | 8              |
| Integration/system testing              | 6              |
| **Total Estimated Effort**              | **54 hours**   |

- This estimate covers all manual code fixes, UDF implementations, macro rewrites, and unit/data reconciliation testing for all temp tables and calculations.
- The estimate assumes high complexity due to spatial logic, percentile calculations, and SAS-to-PySpark migration challenges.
- The estimate does not include effort for pure syntax differences that are automatically converted.

**API Cost: 0.01 USD**

**Note:** Please provide the Azure Databricks environment details (Env_File) or equivalent configuration for a complete and accurate PySpark runtime cost calculation.