=============================================
Author:        Ascendion AVA+
Date:   
Description:   Technical documentation for TAMBR_RINGS.txt SAS code, which generates branch-customer distance rings for priority and most-used branches in support of TAMBr process reporting and analytics.
============================================

## Script Overview

The TAMBR_RINGS.txt SAS program is a monthly batch process that computes spatial "rings" (distance bands) around bank branches, identifying both the priority and most-used branches for each customer. This supports the TAMBr process, enabling branch network optimization, customer segmentation, and targeted marketing. The code integrates customer, account, branch, and geolocation data to calculate proximity and percentile-based distance thresholds for each branch. Key functional sections include:

- **DATA Steps**: Used for data cleansing, filtering, and creating intermediate datasets (e.g., rings_branch_data, rings_cust_data).
- **PROC SQL**: Orchestrates table creation, joins, aggregations, and merges across customer, branch, and geocode tables. Implements complex joins and subqueries for business logic.
- **PROC SORT/PROC FREQ**: Sorting for deduplication, frequency analysis for reporting.
- **PROC UNIVARIATE**: Computes 80th percentile distance thresholds for rings.
- **Macros**: `%masofdt`, `%mdrop_mac` automate date and table management.
- **Conditional Processing**: Handles missing geocodes, bad lat/longs, and business rules (branch types, account age).
- **Error Handling**: Captures invalid geocodes in dedicated tables for quality review.
- **Temporary Dataset Usage**: Stages intermediate results for ring calculations and merges.

The workflow is ETL-centric, with a linear flow but includes conditional logic for data quality and business rules.

## Complexity Metrics

| Metric                | Count / Type                                                                 |
|-----------------------|------------------------------------------------------------------------------|
| Number of Lines       | 420                                                                          |
| Tables Used           | 19 (e.g., tambr.cust_state_match, macommon.dbm_cust_state, work.geo_customer_mth, branches, rings_branch_data, tambr.bad_latlong_branch, branch_active, most_used, macommon.&SYSUSERID._mu_br, customers, customers1, rings_cust_data, tambr.bad_latlong_cust, rings_priority_cust, tambr.ring_priority, rings_most_used_cust, tambr.ring_most_used, ring_priority2, ring_most_used2, branch_data2, tambr.tambr_rings_&cust_occr.) |
| Joins                 | INNER JOIN, LEFT JOIN, FULL JOIN (across customer, account, branch, geocode tables) |
| Temporary Tables      | 10 (volatile/intermediate: e.g., rings_branch_data, rings_cust_data, ring_priority2, ring_most_used2, branch_data2, most_used, dupes, join, branch_active, customers1) |
| Aggregate Functions   | 8 (SUM, CASE, PERCENTILE, COUNT, OUTPUT, GROUP BY, PROC UNIVARIATE, PROC FREQ) |
| DML Statements        | SELECT (extensively), INSERT (via CREATE TABLE AS SELECT), UPDATE (via DATA steps), DELETE (via DROP TABLE), CALL (macros), LOCK (implicit via bulkload), Export/Import (via DB2 CONNECT/DISCONNECT) |
| Conditional Logic     | 14 (IF-THEN-ELSE, CASE, macro IF, data step IF, WHERE clause conditions)      |
| Complexity Score      | 82 (High complexity due to multi-step ETL, spatial calculations, percentile logic, macro usage, and SAS-specific syntax) |
| High-Complexity Areas | PROC SQL joins/subqueries, PROC UNIVARIATE percentile calculations, macro-driven dynamic date/table logic, geodist function for spatial calculations |

- Indexing: Minimal explicit indexing; sorting for deduplication.
- Efficient Sorting: PROC SORT used for deduplication and ordering.
- Dataset Partitioning: Not explicit; handled via temporary tables.
- Modular Macros: `%masofdt`, `%mdrop_mac` for dynamic date/table management.
- Semi-Structured Data Handling: Not present (no PROC JSON/XML Mapper).
- Recommendation: **Rebuild** is preferred due to high complexity, spatial logic, and SAS-specific features. Refactoring would require substantial manual adjustments for PySpark compatibility, especially for geodist, percentile calculations, and macro-driven dynamic logic.

## Syntax Differences

- Extensive syntax differences between SAS SQL/Data Step and PySpark:
    - SAS Data Step logic (e.g., IF, OUTPUT, SET) must be rewritten as PySpark DataFrame operations.
    - PROC SQL syntax differs from PySpark SQL (e.g., CREATE TABLE AS SELECT, macro variables, bulkload options).
    - Macros and dynamic variable management are not directly supported in PySpark.
    - PROC UNIVARIATE percentile calculation must be replaced with PySpark's approx_percentile or equivalent.
    - Geodist function must be implemented as a custom UDF in PySpark.
    - Error handling and conditional output (e.g., output to different tables based on IF) require DataFrame filtering and partitioning.
    - Table dropping and existence checks must be rewritten using Spark catalog commands.

Estimated syntax differences: 30+ distinct incompatibilities.

## Manual Adjustments

- **Function Replacements**:
    - Replace `geodist` with a PySpark UDF for Haversine or Vincenty distance calculation.
    - Replace PROC UNIVARIATE percentile logic with PySpark's approx_percentile or window functions.
    - Replace SAS macro variable logic with Python variables and functions.
    - Replace DATA step IF/OUTPUT with DataFrame filtering and conditional writes.
    - Replace PROC SQL joins with DataFrame joins (inner, left, full).
    - Replace PROC FREQ with DataFrame groupBy/count operations.

- **Syntax Adjustments**:
    - Remove SAS-specific options (bulkload, bl_method, noprint).
    - Replace macro calls with Python functions.
    - Replace table existence checks and drops with Spark catalog commands.
    - Replace SAS library references with Spark database/table paths.

- **Unsupported Features**:
    - Dynamic macro variable creation (e.g., %masofdt) must be rewritten as Python functions.
    - Inline macro logic (e.g., conditional table drops) must be handled in Python.
    - Data step output to multiple tables based on IF must be rewritten as DataFrame partitioning.

- **Strategies for Rewriting**:
    - Modularize ETL steps as PySpark functions/classes.
    - Use PySpark DataFrame APIs for all data transformations.
    - Implement spatial calculations as UDFs.
    - Use window functions for percentile calculations.
    - Replace reporting logic with DataFrame aggregations and exports.

## Optimization Techniques

- **Clustering**: Use Spark's partitionBy on branch or customer IDs to optimize joins and aggregations.
- **Partitioning**: Partition intermediate tables by branch or customer for efficient processing.
- **Query Design**: Minimize shuffles by joining on partitioned columns; cache frequently used DataFrames.
- **UDFs**: Implement geodist as a vectorized UDF for performance.
- **Window Functions**: Use Spark window functions for rolling aggregations and percentile calculations.
- **Broadcast Joins**: Use broadcast joins for small dimension tables (e.g., branch metadata).
- **Error Handling**: Use DataFrame filters to capture and log invalid geocodes.

## apiCost: 0.01 USD