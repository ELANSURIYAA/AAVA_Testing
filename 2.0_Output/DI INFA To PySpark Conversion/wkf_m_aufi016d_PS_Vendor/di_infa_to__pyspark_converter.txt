```
=============================================
Author:        Ascendion AVA+
Date:   
Description:   PySpark script to read PS_VENDOR_S4 vendor flat file, apply mapping logic, and write to Delta Lake table PS_VENDOR for GCP Dataproc. Replicates Informatica workflow m_aufi016d_PS_Vendor.
=============================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, to_date, col
from pyspark.sql.types import IntegerType, StringType, DateType

# Initialize Spark session for Dataproc
spark = SparkSession.builder \
    .appName("PS_VENDOR_ETL") \
    .getOrCreate()

# Path to input flat file in Google Cloud Storage (GCS)
input_path = "gs://<your-bucket>/corp/SrcFiles/profrec_vendor.txt"

# Define schema based on Informatica source definition
from pyspark.sql.types import StructType, StructField

schema = StructType([
    StructField("AUD_YR_NBR", IntegerType(), False),
    StructField("SET_ID", StringType(), True),
    StructField("VENDOR_NBR_ID", StringType(), True),
    StructField("VENDOR_SHORT_NAME", StringType(), True),
    StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
    StructField("NAME_1", StringType(), True),
    StructField("NAME_2", StringType(), True),
    StructField("VENDOR_STAT_CD", StringType(), True),
    StructField("VENDOR_CLASS_CD", StringType(), True),
    StructField("REMIT_SET_ID", StringType(), True),
    StructField("REMIT_VENDOR_ID", StringType(), True),
    StructField("CORP_SET_ID", StringType(), True),
    StructField("CORP_VENDOR_ID", StringType(), True),
    StructField("ENTERED_BY_ID", StringType(), True),
    StructField("WTHD_SW_CD", StringType(), True),
    StructField("PRIM_VENDOR_ID", StringType(), True),
    StructField("LOAD_DT", StringType(), True),
    StructField("ACCOUNT_GROUP", StringType(), True)
])

# Read the flat file using the correct delimiter (||)
df = spark.read.format("csv") \
    .option("delimiter", "||") \
    .option("header", "false") \
    .option("nullValue", "*") \
    .schema(schema) \
    .load(input_path)

# Apply transformation logic from Informatica mapping
# AUD_YR_NBR: Use mapping variable $$Audit_Year, default 2019
audit_year = 2019
df = df.withColumn("AUD_YR_NBR", lit(audit_year).cast(IntegerType()))

# LOAD_DT: Use session start time, here set to current date
from pyspark.sql.functions import current_date
df = df.withColumn("LOAD_DT", current_date().cast(DateType()))

# All other columns are direct mappings

# Type casting and null handling (example for demonstration)
for col_name, dtype in [
    ("AUD_YR_NBR", IntegerType()),
    ("SET_ID", StringType()),
    ("VENDOR_NBR_ID", StringType()),
    ("VENDOR_SHORT_NAME", StringType()),
    ("VENDOR_SHORT_USR_NAME", StringType()),
    ("NAME_1", StringType()),
    ("NAME_2", StringType()),
    ("VENDOR_STAT_CD", StringType()),
    ("VENDOR_CLASS_CD", StringType()),
    ("REMIT_SET_ID", StringType()),
    ("REMIT_VENDOR_ID", StringType()),
    ("CORP_SET_ID", StringType()),
    ("CORP_VENDOR_ID", StringType()),
    ("ENTERED_BY_ID", StringType()),
    ("WTHD_SW_CD", StringType()),
    ("PRIM_VENDOR_ID", StringType()),
    ("ACCOUNT_GROUP", StringType())
]:
    df = df.withColumn(col_name, col(col_name).cast(dtype))

# Performance optimizations
df = df.repartition(8)  # Example: repartition for parallelism
df.cache()              # Cache for repeated access

# Write to Delta Lake table in GCS
output_path = "gs://<your-bucket>/corp/TargetFiles/PS_VENDOR"
df.write.format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .save(output_path)

# Validate row counts (optional)
src_count = df.count()
print(f"Rows loaded to Delta table: {src_count}")

# API Cost Consumed in dollars: $X.XX
```