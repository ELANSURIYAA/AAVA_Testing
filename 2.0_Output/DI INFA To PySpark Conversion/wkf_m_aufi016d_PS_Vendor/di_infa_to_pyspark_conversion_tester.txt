=============================================
Author:        Ascendion AVA+
Date:   
Description:   PySpark script to read PS_VENDOR_S4 vendor flat file, apply mapping logic, and write to Delta Lake table PS_VENDOR for GCP Dataproc. Replicates Informatica workflow m_aufi016d_PS_Vendor.
=============================================

1. Transformation Change Detection

- Expression Transformation Mapping:
  - Informatica: `AUD_YR_NBR = $$Audit_Year` (mapping variable, default 2019); `LOAD_DT = SESSSTARTTIME` (session start time).
  - PySpark: `df.withColumn("AUD_YR_NBR", lit(2019).cast(IntegerType()))`; `df.withColumn("LOAD_DT", current_date().cast(DateType()))`.
  - All other columns are direct mappings.
- Aggregator Transformations: None present in either Informatica or PySpark.
- Join Strategies: No Joiner transformation or JOIN operations present.
- Data Type Transformations:
  - Informatica: `number`, `string`, `date/time`.
  - PySpark: `IntegerType`, `StringType`, `DateType`.
  - Mapping: DECIMAL → IntegerType, VARCHAR2 → StringType, DATE → DateType.
- Null Handling and Case Sensitivity Adjustments:
  - Informatica: NULL represented as `*` in flat file.
  - PySpark: `.option("nullValue", "*")` in read logic; all nullable fields handled as `None`.

2. Recommended Manual Interventions

- Performance optimizations:
  - Use `df.repartition(8)` for parallelism.
  - Use `df.cache()` if repeated access is needed.
- Edge case handling:
  - Ensure input delimiter (`||`) and null value (`*`) are correctly set.
  - Validate schema matches target.
- Complex transformations:
  - None required; all mappings are direct or simple assignments.
- String manipulations and format conversions:
  - Ensure string lengths match target schema (e.g., max length for `NAME_1`, `NAME_2`).
  - Date conversion for `LOAD_DT` to match Oracle target format.
- Target table truncation:
  - Use `.mode("overwrite")` in PySpark to mimic Informatica truncate behavior.

3. Test Case Generation

Test Case List:

Test Case ID: TC01
Description: Happy path - Valid input data is transformed and loaded as expected.
Expected outcome: All columns are mapped correctly, AUD_YR_NBR is set to 2019, LOAD_DT is set to current date, and data types match target schema.

Test Case ID: TC02
Description: Edge case - Input contains NULL values (represented as '*') in nullable fields.
Expected outcome: Corresponding fields in the output should be null.

Test Case ID: TC03
Description: Edge case - Input dataset is empty.
Expected outcome: Output Delta table should be empty, no errors should occur.

Test Case ID: TC04
Description: Edge case - Input contains boundary values (e.g., max string lengths, min/max integers).
Expected outcome: Output should retain all values correctly without truncation or overflow.

Test Case ID: TC05
Description: Error handling - Input contains invalid data types (e.g., non-integer in AUD_YR_NBR).
Expected outcome: Rows with invalid data types should be either dropped or raise a clear error.

Test Case ID: TC06
Description: Error handling - Input is missing required columns (e.g., AUD_YR_NBR).
Expected outcome: The process should raise a schema mismatch error.

Test Case ID: TC07
Description: Edge case - Input contains duplicate rows.
Expected outcome: All rows should be loaded as-is (since no deduplication logic is present).

Test Case ID: TC08
Description: Edge case - Input contains only nulls or '*' for all columns.
Expected outcome: All columns in output should be null, except AUD_YR_NBR (set to 2019) and LOAD_DT (current date).

---

2. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import lit, current_date
import pandas as pd

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("pytest_PS_VENDOR").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def schema():
    return StructType([
        StructField("AUD_YR_NBR", IntegerType(), False),
        StructField("SET_ID", StringType(), True),
        StructField("VENDOR_NBR_ID", StringType(), True),
        StructField("VENDOR_SHORT_NAME", StringType(), True),
        StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
        StructField("NAME_1", StringType(), True),
        StructField("NAME_2", StringType(), True),
        StructField("VENDOR_STAT_CD", StringType(), True),
        StructField("VENDOR_CLASS_CD", StringType(), True),
        StructField("REMIT_SET_ID", StringType(), True),
        StructField("REMIT_VENDOR_ID", StringType(), True),
        StructField("CORP_SET_ID", StringType(), True),
        StructField("CORP_VENDOR_ID", StringType(), True),
        StructField("ENTERED_BY_ID", StringType(), True),
        StructField("WTHD_SW_CD", StringType(), True),
        StructField("PRIM_VENDOR_ID", StringType(), True),
        StructField("LOAD_DT", StringType(), True),
        StructField("ACCOUNT_GROUP", StringType(), True)
    ])

def apply_transformations(df):
    # Apply mapping logic as in the ETL script
    df = df.withColumn("AUD_YR_NBR", lit(2019).cast(IntegerType()))
    df = df.withColumn("LOAD_DT", current_date().cast(DateType()))
    # Cast all columns to correct types
    for col_name, dtype in [
        ("AUD_YR_NBR", IntegerType()),
        ("SET_ID", StringType()),
        ("VENDOR_NBR_ID", StringType()),
        ("VENDOR_SHORT_NAME", StringType()),
        ("VENDOR_SHORT_USR_NAME", StringType()),
        ("NAME_1", StringType()),
        ("NAME_2", StringType()),
        ("VENDOR_STAT_CD", StringType()),
        ("VENDOR_CLASS_CD", StringType()),
        ("REMIT_SET_ID", StringType()),
        ("REMIT_VENDOR_ID", StringType()),
        ("CORP_SET_ID", StringType()),
        ("CORP_VENDOR_ID", StringType()),
        ("ENTERED_BY_ID", StringType()),
        ("WTHD_SW_CD", StringType()),
        ("PRIM_VENDOR_ID", StringType()),
        ("ACCOUNT_GROUP", StringType())
    ]:
        df = df.withColumn(col_name, df[col_name].cast(dtype))
    return df

def make_input_df(spark, schema, data):
    return spark.createDataFrame(data, schema=schema)

def to_pandas(df):
    return df.toPandas()

def test_TC01_happy_path(spark, schema):
    data = [
        (2020, "S01", "V001", "VendorA", "VSA", "Name1", "Name2", "A", "C", "RS1", "RV1", "CS1", "CV1", "EID1", "W", "PV1", "2022-01-01", "AG1")
    ]
    df = make_input_df(spark, schema, data)
    df = apply_transformations(df)
    pdf = to_pandas(df)
    assert pdf.loc[0, "AUD_YR_NBR"] == 2019
    assert pd.notnull(pdf.loc[0, "LOAD_DT"])
    assert pdf.loc[0, "SET_ID"] == "S01"
    assert pdf.loc[0, "VENDOR_NBR_ID"] == "V001"

def test_TC02_null_values(spark, schema):
    data = [
        (2020, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)
    ]
    df = make_input_df(spark, schema, data)
    df = apply_transformations(df)
    pdf = to_pandas(df)
    assert pdf.loc[0, "SET_ID"] is None
    assert pdf.loc[0, "VENDOR_NBR_ID"] is None

def test_TC03_empty_dataset(spark, schema):
    data = []
    df = make_input_df(spark, schema, data)
    df = apply_transformations(df)
    assert df.count() == 0

def test_TC04_boundary_values(spark, schema):
    data = [
        (2020, "S"*5, "V"*10, "N"*14, "U"*10, "A"*40, "B"*40, "Z", "Y", "R"*5, "T"*10, "C"*5, "D"*10, "E"*8, "X", "P"*10, "2022-12-31", "G"*10)
    ]
    df = make_input_df(spark, schema, data)
    df = apply_transformations(df)
    pdf = to_pandas(df)
    assert len(pdf.loc[0, "SET_ID"]) == 5
    assert len(pdf.loc[0, "VENDOR_NBR_ID"]) == 10
    assert len(pdf.loc[0, "NAME_1"]) == 40

def test_TC05_invalid_datatype(spark, schema):
    data = [
        ("not_an_int", "S01", "V001", "VendorA", "VSA", "Name1", "Name2", "A", "C", "RS1", "RV1", "CS1", "CV1", "EID1", "W", "PV1", "2022-01-01", "AG1")
    ]
    with pytest.raises(Exception):
        df = make_input_df(spark, schema, data)
        df = apply_transformations(df)
        df.collect()

def test_TC06_missing_column(spark):
    # Remove AUD_YR_NBR from schema
    bad_schema = StructType([
        StructField("SET_ID", StringType(), True),
        StructField("VENDOR_NBR_ID", StringType(), True),
        # ... all other fields except AUD_YR_NBR
    ])
    data = [("S01", "V001")]
    with pytest.raises(Exception):
        df = make_input_df(spark, bad_schema, data)
        df = apply_transformations(df)
        df.collect()

def test_TC07_duplicate_rows(spark, schema):
    data = [
        (2020, "S01", "V001", "VendorA", "VSA", "Name1", "Name2", "A", "C", "RS1", "RV1", "CS1", "CV1", "EID1", "W", "PV1", "2022-01-01", "AG1"),
        (2020, "S01", "V001", "VendorA", "VSA", "Name1", "Name2", "A", "C", "RS1", "RV1", "CS1", "CV1", "EID1", "W", "PV1", "2022-01-01", "AG1")
    ]
    df = make_input_df(spark, schema, data)
    df = apply_transformations(df)
    assert df.count() == 2

def test_TC08_all_nulls(spark, schema):
    data = [
        (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)
    ]
    df = make_input_df(spark, schema, data)
    df = apply_transformations(df)
    pdf = to_pandas(df)
    assert pdf.loc[0, "AUD_YR_NBR"] == 2019
    assert pd.notnull(pdf.loc[0, "LOAD_DT"])
    for col in [
        "SET_ID", "VENDOR_NBR_ID", "VENDOR_SHORT_NAME", "VENDOR_SHORT_USR_NAME", "NAME_1", "NAME_2",
        "VENDOR_STAT_CD", "VENDOR_CLASS_CD", "REMIT_SET_ID", "REMIT_VENDOR_ID", "CORP_SET_ID",
        "CORP_VENDOR_ID", "ENTERED_BY_ID", "WTHD_SW_CD", "PRIM_VENDOR_ID", "ACCOUNT_GROUP"
    ]:
        assert pd.isnull(pdf.loc[0, col])
```

3. API Cost Estimation

apiCost: 0.012 USD

(Explanation: The cost is based on two file tool invocations and the size of the content processed.)