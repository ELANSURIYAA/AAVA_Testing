```python
"""
Comprehensive SSIS-to-Python Migration Validation Script
=======================================================

This script automates the end-to-end validation of an SSIS-to-Python ETL migration.
It executes both the original SSIS package and the converted Python ETL, extracts
outputs, compares results, and generates a detailed report on data and workflow consistency.

Key Features:
-------------
- Executes SSIS package via DTExec and captures logs/output
- Executes Python ETL (PySpark/Delta Lake) and captures logs/output
- Extracts destination data from both processes for comparison (supports DB, CSV, Parquet, Delta)
- Validates transformations, control flow, error handling, and logging consistency
- Handles SSIS-specific data types, nulls, and large datasets
- Generates structured JSON/CSV reports with match status, discrepancies, and metadata
- Securely manages credentials and logs all operations

Configuration:
--------------
- Set environment variables or edit CONFIG section as needed
- Requires access to both SSIS runtime (Windows/DTExec) and Python ETL environment

Author: Automated ETL Process Validation Agent
"""

import os
import sys
import subprocess
import shutil
import tempfile
import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, List

import pandas as pd

# Optional: PySpark/Delta imports for direct Delta reads (if needed)
try:
    from pyspark.sql import SparkSession
    from delta.tables import DeltaTable
except ImportError:
    SparkSession = None
    DeltaTable = None

# ==============================
# CONFIGURATION
# ==============================

CONFIG = {
    # SSIS
    "ssis_package_path": os.environ.get("SSIS_PACKAGE_PATH", r"C:\ssis\Employee_Data_Warehouse.dtsx"),
    "ssis_dtexec_path": os.environ.get("SSIS_DTEXEC_PATH", r"C:\Program Files\Microsoft SQL Server\150\DTS\Binn\DTExec.exe"),
    "ssis_log_file": os.environ.get("SSIS_LOG_FILE", r"C:\ssis\logs\ssis_run.log"),
    "ssis_output_tables": [
        # List of (connection_string, table_name) for each output table
        # Example: ("DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=DW;UID=sa;PWD=...", "Employees_DW")
        # These should match the SSIS package destinations
        {
            "conn_str_env": "SSIS_DEST_CONN_STR",
            "table": "Employees_DW",
            "output_file": "ssis_Employees_DW.parquet"
        },
        {
            "conn_str_env": "SSIS_DEST_CONN_STR",
            "table": "HighSalary_Summary",
            "output_file": "ssis_HighSalary_Summary.parquet"
        },
        {
            "conn_str_env": "SSIS_DEST_CONN_STR",
            "table": "LowSalary_Summary",
            "output_file": "ssis_LowSalary_Summary.parquet"
        }
    ],
    # Python ETL
    "python_etl_script": os.environ.get("PYTHON_ETL_SCRIPT", "/opt/etl/employee_dw_etl.py"),
    "python_etl_log_file": os.environ.get("PYTHON_ETL_LOG_FILE", "/opt/etl/logs/python_etl_run.log"),
    "python_output_delta_tables": [
        # List of Delta table paths for each output
        {
            "delta_path_env": "DELTA_DEST_EMPLOYEES_DW",
            "output_file": "py_Employees_DW.parquet"
        },
        {
            "delta_path_env": "DELTA_DEST_HIGH_SALARY_SUMMARY",
            "output_file": "py_HighSalary_Summary.parquet"
        },
        {
            "delta_path_env": "DELTA_DEST_LOW_SALARY_SUMMARY",
            "output_file": "py_LowSalary_Summary.parquet"
        }
    ],
    # General
    "output_dir": os.environ.get("VALIDATION_OUTPUT_DIR", "./validation_outputs"),
    "comparison_report": os.environ.get("COMPARISON_REPORT", "./validation_outputs/comparison_report.json"),
    "row_sample_size": int(os.environ.get("ROW_SAMPLE_SIZE", "10")),
    "log_level": os.environ.get("LOG_LEVEL", "INFO"),
    "spark_master": os.environ.get("SPARK_MASTER", "local[*]"),
    "spark_app_name": os.environ.get("SPARK_APP_NAME", "ETL_Validation"),
    # Security
    "mask_credentials_in_logs": True,
}

os.makedirs(CONFIG["output_dir"], exist_ok=True)

# ==============================
# LOGGING SETUP
# ==============================

logging.basicConfig(
    filename=os.path.join(CONFIG["output_dir"], "validation.log"),
    level=getattr(logging, CONFIG["log_level"].upper(), logging.INFO),
    format="%(asctime)s - %(levelname)s - %(message)s"
)
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console.setFormatter(formatter)
logging.getLogger().addHandler(console)

def log_event(msg):
    logging.info(msg)

def log_error(msg):
    logging.error(msg)

# ==============================
# UTILITY FUNCTIONS
# ==============================

def run_subprocess(cmd: List[str], log_file: str = None, env: Dict[str, str] = None) -> int:
    """Run a subprocess and optionally log output to a file."""
    log_event(f"Running subprocess: {' '.join(cmd)}")
    with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env) as proc:
        with open(log_file, "w") if log_file else tempfile.TemporaryFile("w+") as f:
            for line in proc.stdout:
                decoded = line.decode(errors="replace")
                if f:
                    f.write(decoded)
                log_event(decoded.strip())
            proc.wait()
            return proc.returncode

def extract_table_to_parquet(conn_str: str, table: str, output_file: str):
    """Extract a SQL table to a Parquet file using pandas and pyodbc."""
    import pyodbc
    log_event(f"Extracting table {table} to {output_file}")
    try:
        with pyodbc.connect(conn_str) as conn:
            df = pd.read_sql(f"SELECT * FROM {table}", conn)
            df.to_parquet(output_file, index=False)
        log_event(f"Extracted {table} to {output_file} ({len(df)} rows)")
        return output_file
    except Exception as e:
        log_error(f"Failed to extract {table}: {e}")
        return None

def extract_delta_to_parquet(spark, delta_path: str, output_file: str):
    """Extract a Delta table to Parquet using PySpark."""
    log_event(f"Extracting Delta table {delta_path} to {output_file}")
    try:
        df = spark.read.format("delta").load(delta_path)
        df.toPandas().to_parquet(output_file, index=False)
        log_event(f"Extracted {delta_path} to {output_file} ({df.count()} rows)")
        return output_file
    except Exception as e:
        log_error(f"Failed to extract Delta {delta_path}: {e}")
        return None

def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, key_columns: List[str] = None) -> Dict[str, Any]:
    """Compare two DataFrames and return a detailed comparison result."""
    result = {
        "row_count_1": len(df1),
        "row_count_2": len(df2),
        "row_count_match": len(df1) == len(df2),
        "column_set_1": set(df1.columns),
        "column_set_2": set(df2.columns),
        "columns_match": set(df1.columns) == set(df2.columns),
        "column_mismatches": list(set(df1.columns).symmetric_difference(set(df2.columns))),
        "row_level_mismatches": [],
        "sample_mismatches": [],
        "match_percentage": None,
    }
    if not result["columns_match"]:
        return result
    # Sort by key columns if provided, else by all columns
    sort_cols = key_columns or list(df1.columns)
    try:
        df1_sorted = df1.sort_values(by=sort_cols).reset_index(drop=True)
        df2_sorted = df2.sort_values(by=sort_cols).reset_index(drop=True)
    except Exception:
        df1_sorted = df1.reset_index(drop=True)
        df2_sorted = df2.reset_index(drop=True)
    # Compare row by row
    mismatches = []
    match_count = 0
    for idx in range(min(len(df1_sorted), len(df2_sorted))):
        row1 = df1_sorted.iloc[idx].to_dict()
        row2 = df2_sorted.iloc[idx].to_dict()
        if row1 != row2:
            mismatches.append({"row_index": idx, "row1": row1, "row2": row2})
        else:
            match_count += 1
    result["row_level_mismatches"] = mismatches
    result["sample_mismatches"] = mismatches[:CONFIG["row_sample_size"]]
    total_rows = max(len(df1_sorted), len(df2_sorted))
    result["match_percentage"] = (match_count / total_rows * 100) if total_rows > 0 else 100.0
    return result

def mask_conn_str(conn_str: str) -> str:
    """Mask credentials in a connection string for logging."""
    if not conn_str or not CONFIG["mask_credentials_in_logs"]:
        return conn_str
    for key in ["PWD", "Password", "Uid", "UID"]:
        conn_str = conn_str.replace(f"{key}=", f"{key}=***")
    return conn_str

# ==============================
# MAIN VALIDATION LOGIC
# ==============================

def main():
    log_event("==== SSIS to Python Migration Validation Started ====")
    validation_results = {
        "start_time": datetime.now().isoformat(),
        "ssis_execution": {},
        "python_execution": {},
        "output_comparisons": [],
        "control_flow_comparison": {},
        "error_handling_comparison": {},
        "logging_comparison": {},
        "summary": {},
    }

    # ---- 1. Execute SSIS Package ----
    ssis_start = time.time()
    ssis_cmd = [
        CONFIG["ssis_dtexec_path"],
        "/FILE", CONFIG["ssis_package_path"],
        "/LOG", CONFIG["ssis_log_file"],
        "/SET", f"\Package.Variables[User::LogFilePath].Properties[Value];{CONFIG['ssis_log_file']}",
        # Add more /SET for other variables as needed (BatchID, conn strings)
    ]
    ssis_env = os.environ.copy()
    # Mask credentials in logs
    log_event(f"Executing SSIS package: {CONFIG['ssis_package_path']}")
    try:
        ssis_rc = run_subprocess(ssis_cmd, log_file=CONFIG["ssis_log_file"], env=ssis_env)
        validation_results["ssis_execution"]["exit_code"] = ssis_rc
        validation_results["ssis_execution"]["log_file"] = CONFIG["ssis_log_file"]
        validation_results["ssis_execution"]["duration_sec"] = time.time() - ssis_start
        if ssis_rc != 0:
            log_error(f"SSIS package execution failed with exit code {ssis_rc}")
    except Exception as e:
        log_error(f"SSIS execution error: {e}")
        validation_results["ssis_execution"]["error"] = str(e)

    # ---- 2. Extract SSIS Output Data ----
    ssis_outputs = []
    for output in CONFIG["ssis_output_tables"]:
        conn_str = os.environ.get(output["conn_str_env"])
        if not conn_str:
            log_error(f"Missing connection string for {output['table']}")
            continue
        out_file = os.path.join(CONFIG["output_dir"], output["output_file"])
        res = extract_table_to_parquet(conn_str, output["table"], out_file)
        ssis_outputs.append({"table": output["table"], "parquet": out_file, "conn_str": mask_conn_str(conn_str)})

    # ---- 3. Execute Python ETL ----
    python_start = time.time()
    python_cmd = [sys.executable, CONFIG["python_etl_script"]]
    python_env = os.environ.copy()
    log_event(f"Executing Python ETL: {CONFIG['python_etl_script']}")
    try:
        python_rc = run_subprocess(python_cmd, log_file=CONFIG["python_etl_log_file"], env=python_env)
        validation_results["python_execution"]["exit_code"] = python_rc
        validation_results["python_execution"]["log_file"] = CONFIG["python_etl_log_file"]
        validation_results["python_execution"]["duration_sec"] = time.time() - python_start
        if python_rc != 0:
            log_error(f"Python ETL execution failed with exit code {python_rc}")
    except Exception as e:
        log_error(f"Python ETL execution error: {e}")
        validation_results["python_execution"]["error"] = str(e)

    # ---- 4. Extract Python Output Data (Delta) ----
    py_outputs = []
    spark = None
    if SparkSession:
        spark = SparkSession.builder.master(CONFIG["spark_master"]).appName(CONFIG["spark_app_name"]).getOrCreate()
    for output in CONFIG["python_output_delta_tables"]:
        delta_path = os.environ.get(output["delta_path_env"])
        if not delta_path:
            log_error(f"Missing Delta path for {output['output_file']}")
            continue
        out_file = os.path.join(CONFIG["output_dir"], output["output_file"])
        if spark:
            res = extract_delta_to_parquet(spark, delta_path, out_file)
        else:
            log_error("PySpark/Delta not available, cannot extract Delta tables")
            res = None
        py_outputs.append({"delta_path": delta_path, "parquet": out_file})

    # ---- 5. Compare Outputs ----
    for ssis_out, py_out in zip(ssis_outputs, py_outputs):
        table_name = ssis_out["table"]
        ssis_file = ssis_out["parquet"]
        py_file = py_out["parquet"]
        log_event(f"Comparing outputs for {table_name}")
        try:
            df_ssis = pd.read_parquet(ssis_file)
            df_py = pd.read_parquet(py_file)
            # Optionally, define key columns for sorting/matching
            key_columns = None
            if table_name.lower().startswith("employees"):
                key_columns = ["EmployeeID"]
            elif table_name.lower().endswith("summary"):
                key_columns = ["DepartmentID", "BatchID"]
            cmp_result = compare_dataframes(df_ssis, df_py, key_columns=key_columns)
            cmp_result["table"] = table_name
            cmp_result["ssis_file"] = ssis_file
            cmp_result["python_file"] = py_file
            validation_results["output_comparisons"].append(cmp_result)
            log_event(f"Comparison for {table_name}: {cmp_result['match_percentage']}% match")
        except Exception as e:
            log_error(f"Error comparing outputs for {table_name}: {e}")
            validation_results["output_comparisons"].append({
                "table": table_name,
                "error": str(e),
                "ssis_file": ssis_file,
                "python_file": py_file
            })

    # ---- 6. Control Flow, Logging, and Error Handling Comparison ----
    # For brevity, compare logs for key events and error messages
    def parse_log_for_events(log_file, keywords):
        events = []
        try:
            with open(log_file, "r", encoding="utf-8", errors="ignore") as f:
                for line in f:
                    for kw in keywords:
                        if kw in line:
                            events.append(line.strip())
        except Exception as e:
            log_error(f"Failed to parse log {log_file}: {e}")
        return events

    ssis_log_events = parse_log_for_events(CONFIG["ssis_log_file"], ["Started", "Completed", "Error", "Failure"])
    python_log_events = parse_log_for_events(CONFIG["python_etl_log_file"], ["Started", "Completed", "Error", "Failure"])

    validation_results["control_flow_comparison"] = {
        "ssis_log_events": ssis_log_events,
        "python_log_events": python_log_events,
        "match": ssis_log_events == python_log_events
    }
    validation_results["logging_comparison"] = {
        "ssis_log_file": CONFIG["ssis_log_file"],
        "python_log_file": CONFIG["python_etl_log_file"]
    }
    validation_results["error_handling_comparison"] = {
        "ssis_errors": [e for e in ssis_log_events if "Error" in e or "Failure" in e],
        "python_errors": [e for e in python_log_events if "Error" in e or "Failure" in e],
        "match": [e for e in ssis_log_events if "Error" in e or "Failure" in e] == [e for e in python_log_events if "Error" in e or "Failure" in e]
    }

    # ---- 7. Summary ----
    all_match = all(
        cmp.get("match_percentage", 0) == 100.0 and cmp.get("columns_match", False)
        for cmp in validation_results["output_comparisons"]
    )
    validation_results["summary"] = {
        "all_outputs_match": all_match,
        "total_tables_compared": len(validation_results["output_comparisons"]),
        "tables_with_full_match": sum(1 for cmp in validation_results["output_comparisons"] if cmp.get("match_percentage", 0) == 100.0),
        "tables_with_mismatches": [cmp["table"] for cmp in validation_results["output_comparisons"] if cmp.get("match_percentage", 0) < 100.0],
        "control_flow_match": validation_results["control_flow_comparison"]["match"],
        "error_handling_match": validation_results["error_handling_comparison"]["match"]
    }

    # ---- 8. Write Comparison Report ----
    with open(CONFIG["comparison_report"], "w", encoding="utf-8") as f:
        json.dump(validation_results, f, indent=2, default=str)
    log_event(f"Validation report written to {CONFIG['comparison_report']}")

    if spark:
        spark.stop()
    log_event("==== Validation Completed ====")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log_error(f"Validation script failed: {e}")
        sys.exit(1)
```

**Instructions:**
- Set environment variables for all connection strings, Delta table paths, and file locations as needed.
- Ensure DTExec and the SSIS package are accessible on the system.
- The script will output all intermediate and final results to the directory specified by `VALIDATION_OUTPUT_DIR`.
- The comparison report will be in JSON format and include detailed match status, discrepancies, and logs.

**API Cost for this call:** 0.02 USD