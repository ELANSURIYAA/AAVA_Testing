1. **Test Case Document**

| Test Case ID | Description | Preconditions | Test Steps | Expected Result | Actual Result | Pass/Fail Status |
|--------------|-------------|---------------|------------|----------------|--------------|------------------|
| TC01 | Successful ETL run with valid, non-empty source and lookup tables | All source and lookup tables are populated with valid data | 1. Run the ETL script<br>2. Check destination tables<br>3. Check logs | All destination tables updated correctly, logs show success, no errors logged | | |
| TC02 | Source employees table is empty | Source employees table is empty, lookups populated | 1. Run ETL<br>2. Check destination tables<br>3. Check logs | Dest tables unchanged/empty, logs show success, no errors | | |
| TC03 | Department lookup table is empty | Department lookup empty, other tables populated | 1. Run ETL<br>2. Check employees DW table | Employees have null department fields, no join errors, logs show success | | |
| TC04 | Location lookup table is empty | Location lookup empty, other tables populated | 1. Run ETL<br>2. Check employees DW table | Employees have null location fields, no join errors, logs show success | | |
| TC05 | Employee with NULL/None HireDate | At least one employee with NULL HireDate | 1. Run ETL<br>2. Check derived columns | HireYear/HireMonth null for that employee, no crash | | |
| TC06 | Employee with NULL/None Salary | At least one employee with NULL Salary | 1. Run ETL<br>2. Check high/low salary splits | Employee excluded from splits, no crash | | |
| TC07 | Employee with salary exactly at HIGH_SALARY_THRESHOLD | At least one employee at threshold | 1. Run ETL<br>2. Check high salary group | Employee in high salary group, not low salary | | |
| TC08 | All employees below HIGH_SALARY_THRESHOLD | All salaries below threshold | 1. Run ETL<br>2. Check summaries | High salary summary empty, low salary summary has all | | |
| TC09 | All employees above HIGH_SALARY_THRESHOLD | All salaries above threshold | 1. Run ETL<br>2. Check summaries | Low salary summary empty, high salary summary has all | | |
| TC10 | Duplicate EmployeeID in source | Source has duplicate EmployeeID | 1. Run ETL<br>2. Check destination | Only one record per EmployeeID in dest | | |
| TC11 | Error during data load (simulate exception) | Simulate ETL error | 1. Run ETL with error<br>2. Check error log | Error logged to Delta error log, process exits with failure | | |
| TC12 | Invalid data types in source (e.g., string in Salary) | Source has invalid data type | 1. Run ETL<br>2. Check error log | Error logged, process fails gracefully | | |
| TC13 | Logging file path is unwritable | Set log path unwritable | 1. Run ETL<br>2. Check error log | Error logged, process fails gracefully | | |
| TC14 | Delta destination table path does not exist | Set dest path invalid | 1. Run ETL<br>2. Check error log | Error logged, process fails gracefully | | |
| TC15 | OPTIMIZE/VACUUM SQL fails | Simulate SQL failure | 1. Run ETL<br>2. Check error log | Error logged, process fails gracefully | | |

---

2. **Pytest Script**

```python
import os
import pytest
from unittest import mock
from datetime import datetime
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from delta.tables import DeltaTable

# Fixtures for Spark session and test data
@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder \
        .master("local[1]") \
        .appName("pytest-employee-dw") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def sample_employees(spark):
    data = [
        (1, "Alice", 101, 201, datetime(2020, 1, 15), 120000),
        (2, "Bob", 102, 202, datetime(2019, 6, 1), 90000),
        (3, "Charlie", 103, 203, None, 110000),
        (4, "David", 104, 204, datetime(2021, 3, 10), None),
        (5, "Eve", 101, 201, datetime(2018, 7, 20), 100000),
    ]
    columns = ["EmployeeID", "Name", "DepartmentID", "LocationID", "HireDate", "Salary"]
    return spark.createDataFrame(data, columns)

@pytest.fixture
def sample_departments(spark):
    data = [
        (101, "HR"),
        (102, "Finance"),
        (103, "IT"),
        (104, "Marketing"),
    ]
    columns = ["DepartmentID", "DepartmentName"]
    return spark.createDataFrame(data, columns)

@pytest.fixture
def sample_locations(spark):
    data = [
        (201, "New York"),
        (202, "London"),
        (203, "Berlin"),
        (204, "Tokyo"),
    ]
    columns = ["LocationID", "LocationName"]
    return spark.createDataFrame(data, columns)

@pytest.fixture
def temp_delta_path(tmp_path):
    path = str(tmp_path / "delta")
    os.makedirs(path, exist_ok=True)
    return path

# Mock DeltaTable for upsert operations
class MockDeltaTable:
    def __init__(self, df):
        self.df = df
        self.upserted = False

    def alias(self, name):
        return self

    def merge(self, source, condition):
        self.upserted = True
        return self

    def whenMatchedUpdateAll(self):
        return self

    def whenNotMatchedInsertAll(self):
        return self

    def execute(self):
        self.upserted = True

    @staticmethod
    def forPath(spark, path):
        # Return a mock table
        return MockDeltaTable(None)

# Patch DeltaTable.forPath to use the mock
@pytest.fixture(autouse=True)
def patch_deltatable(monkeypatch):
    monkeypatch.setattr("delta.tables.DeltaTable.forPath", MockDeltaTable.forPath)

# Patch logging to avoid file IO
@pytest.fixture(autouse=True)
def patch_logging(monkeypatch):
    import logging
    monkeypatch.setattr(logging, "info", lambda msg: None)
    monkeypatch.setattr(logging, "error", lambda msg: None)
    monkeypatch.setattr(logging, "basicConfig", lambda **kwargs: None)

# Test cases

def test_successful_etl_run(spark, sample_employees, sample_departments, sample_locations, temp_delta_path):
    # Simulate the ETL logic up to writing to destination
    df_employees = sample_employees.join(sample_departments, on="DepartmentID", how="left")
    df_employees = df_employees.join(sample_locations, on="LocationID", how="left")
    df_employees = df_employees \
        .withColumn("HireYear", F.year("HireDate")) \
        .withColumn("HireMonth", F.month("HireDate")) \
        .withColumn("LoadDate", F.lit("2023-01-01 00:00:00")) \
        .withColumn("BatchID", F.lit("batch-1"))
    # High/Low salary split
    HIGH_SALARY_THRESHOLD = 100000
    df_high_salary = df_employees.filter(F.col("Salary") >= HIGH_SALARY_THRESHOLD)
    df_low_salary = df_employees.filter(F.col("Salary") < HIGH_SALARY_THRESHOLD)
    # Aggregates
    df_high_salary_summary = df_high_salary.groupBy("DepartmentID").agg(
        F.count("*").alias("HighSalaryCount"),
        F.avg("Salary").alias("AvgHighSalary"),
        F.lit("2023-01-01 00:00:00").alias("LoadDate"),
        F.lit("batch-1").alias("BatchID")
    )
    df_low_salary_summary = df_low_salary.groupBy("DepartmentID").agg(
        F.count("*").alias("LowSalaryCount"),
        F.avg("Salary").alias("AvgLowSalary"),
        F.lit("2023-01-01 00:00:00").alias("LoadDate"),
        F.lit("batch-1").alias("BatchID")
    )
    # Assert results
    assert df_high_salary.count() == 3
    assert df_low_salary.count() == 2
    assert df_high_salary_summary.count() > 0
    assert df_low_salary_summary.count() > 0

def test_empty_source_table(spark, sample_departments, sample_locations):
    empty_employees = spark.createDataFrame([], "EmployeeID INT, Name STRING, DepartmentID INT, LocationID INT, HireDate TIMESTAMP, Salary DOUBLE")
    df_employees = empty_employees.join(sample_departments, on="DepartmentID", how="left")
    df_employees = df_employees.join(sample_locations, on="LocationID", how="left")
    assert df_employees.count() == 0

def test_empty_departments_lookup(spark, sample_employees, sample_locations):
    empty_departments = spark.createDataFrame([], "DepartmentID INT, DepartmentName STRING")
    df_employees = sample_employees.join(empty_departments, on="DepartmentID", how="left")
    df_employees = df_employees.join(sample_locations, on="LocationID", how="left")
    assert df_employees.filter(F.col("DepartmentName").isNull()).count() == df_employees.count()

def test_empty_locations_lookup(spark, sample_employees, sample_departments):
    empty_locations = spark.createDataFrame([], "LocationID INT, LocationName STRING")
    df_employees = sample_employees.join(sample_departments, on="DepartmentID", how="left")
    df_employees = df_employees.join(empty_locations, on="LocationID", how="left")
    assert df_employees.filter(F.col("LocationName").isNull()).count() == df_employees.count()

def test_null_hiredate(spark, sample_employees, sample_departments, sample_locations):
    df_employees = sample_employees.join(sample_departments, on="DepartmentID", how="left")
    df_employees = df_employees.join(sample_locations, on="LocationID", how="left")
    df_employees = df_employees.withColumn("HireYear", F.year("HireDate"))
    null_hireyear_count = df_employees.filter(F.col("HireDate").isNull() & F.col("HireYear").isNull()).count()
    assert null_hireyear_count > 0

def test_null_salary(spark, sample_employees, sample_departments, sample_locations):
    df_employees = sample_employees.join(sample_departments, on="DepartmentID", how="left")
    df_employees = df_employees.join(sample_locations, on="LocationID", how="left")
    df_high_salary = df_employees.filter(F.col("Salary") >= 100000)
    df_low_salary = df_employees.filter(F.col("Salary") < 100000)
    # Employee with None salary should not appear in either group
    assert df_high_salary.filter(F.col("Salary").isNull()).count() == 0
    assert df_low_salary.filter(F.col("Salary").isNull()).count() == 0

def test_salary_at_threshold(spark, sample_employees, sample_departments, sample_locations):
    df_employees = sample_employees.join(sample_departments, on="DepartmentID", how="left")
    df_employees = df_employees.join(sample_locations, on="LocationID", how="left")
    df_at_threshold = df_employees.filter(F.col("Salary") == 100000)
    df_high_salary = df_employees.filter(F.col("Salary") >= 100000)
    assert df_at_threshold.count() == 1
    assert df_high_salary.filter(F.col("EmployeeID") == 5).count() == 1

def test_all_low_salary(spark, sample_employees, sample_departments, sample_locations):
    df = sample_employees.withColumn("Salary", F.lit(50000))
    df = df.join(sample_departments, on="DepartmentID", how="left")
    df = df.join(sample_locations, on="LocationID", how="left")
    df_high_salary = df.filter(F.col("Salary") >= 100000)
    df_low_salary = df.filter(F.col("Salary") < 100000)
    assert df_high_salary.count() == 0
    assert df_low_salary.count() == df.count()

def test_all_high_salary(spark, sample_employees, sample_departments, sample_locations):
    df = sample_employees.withColumn("Salary", F.lit(200000))
    df = df.join(sample_departments, on="DepartmentID", how="left")
    df = df.join(sample_locations, on="LocationID", how="left")
    df_high_salary = df.filter(F.col("Salary") >= 100000)
    df_low_salary = df.filter(F.col("Salary") < 100000)
    assert df_low_salary.count() == 0
    assert df_high_salary.count() == df.count()

def test_duplicate_employeeid_upsert(monkeypatch, spark, sample_employees, sample_departments, sample_locations):
    # Duplicate EmployeeID
    data = [
        (1, "Alice", 101, 201, datetime(2020, 1, 15), 120000),
        (1, "Alicia", 101, 201, datetime(2021, 1, 15), 130000),
    ]
    columns = ["EmployeeID", "Name", "DepartmentID", "LocationID", "HireDate", "Salary"]
    df = spark.createDataFrame(data, columns)
    df = df.join(sample_departments, on="DepartmentID", how="left")
    df = df.join(sample_locations, on="LocationID", how="left")
    # Simulate upsert: only one record per EmployeeID should exist after upsert
    df_upserted = df.groupBy("EmployeeID").agg(F.max("HireDate").alias("LatestHireDate"))
    assert df_upserted.count() == 1

def test_error_during_data_load(monkeypatch):
    # Simulate error in ETL
    def raise_exception(*args, **kwargs):
        raise Exception("Simulated ETL error")
    monkeypatch.setattr("delta.tables.DeltaTable.forPath", lambda *a, **k: (_ for _ in ()).throw(Exception("Simulated ETL error")))
    with pytest.raises(Exception, match="Simulated ETL error"):
        DeltaTable.forPath(None, "/fake/path")

def test_invalid_salary_type(spark, sample_employees, sample_departments, sample_locations):
    # Insert a string in Salary
    data = [
        (1, "Alice", 101, 201, datetime(2020, 1, 15), "not_a_number"),
    ]
    columns = ["EmployeeID", "Name", "DepartmentID", "LocationID", "HireDate", "Salary"]
    df = spark.createDataFrame(data, columns)
    df = df.join(sample_departments, on="DepartmentID", how="left")
    df = df.join(sample_locations, on="LocationID", how="left")
    with pytest.raises(Exception):
        df.filter(F.col("Salary") >= 100000).count()

def test_logging_path_unwritable(monkeypatch):
    # Simulate logging failure
    def raise_ioerror(*args, **kwargs):
        raise IOError("Cannot write log file")
    import logging
    monkeypatch.setattr(logging, "info", raise_ioerror)
    with pytest.raises(IOError, match="Cannot write log file"):
        logging.info("test")

def test_missing_delta_table_path(monkeypatch):
    # Simulate missing path
    def raise_exception(*args, **kwargs):
        raise Exception("Delta table path not found")
    monkeypatch.setattr("delta.tables.DeltaTable.forPath", lambda *a, **k: (_ for _ in ()).throw(Exception("Delta table path not found")))
    with pytest.raises(Exception, match="Delta table path not found"):
        DeltaTable.forPath(None, "/missing/path")

def test_optimize_vacuum_failure(monkeypatch, spark):
    # Simulate SQL failure
    def raise_exception(sql):
        raise Exception("SQL failed")
    monkeypatch.setattr(spark, "sql", raise_exception)
    with pytest.raises(Exception, match="SQL failed"):
        spark.sql("OPTIMIZE delta.`/some/path`")

```

---

3. **Test Execution Report Template**

| Test Case ID | Description | Expected Result | Actual Result | Pass/Fail | Comments |
|--------------|-------------|----------------|--------------|-----------|----------|
|              |             |                |              |           |          |

---

**API Cost:** 0.02 USD