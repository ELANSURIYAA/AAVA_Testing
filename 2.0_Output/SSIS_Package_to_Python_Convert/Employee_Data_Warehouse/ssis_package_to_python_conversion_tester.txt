1. Test Case Document

| Test Case ID | Description | Preconditions | Test Steps | Expected Result | Actual Result | Pass/Fail Status |
|--------------|-------------|---------------|------------|----------------|--------------|------------------|
| TC01 | Happy path: All source tables have valid data, all joins succeed, and all aggregates are correct | All source tables populated with valid data | 1. Run ETL script. 2. Validate target tables for correct data and aggregates. | All target tables updated as expected; aggregates and derived columns correct. | | |
| TC02 | Edge: Employee with NULL HireDate | At least one employee has NULL HireDate | 1. Run ETL script. 2. Check target for employee with NULL HireDate. | HireYear and HireMonth are NULL for that employee; no error occurs. | | |
| TC03 | Edge: Employee with NULL Salary | At least one employee has NULL Salary | 1. Run ETL script. 2. Check high/low salary summaries. | Employee excluded from high/low salary splits and aggregates; no error. | | |
| TC04 | Edge: Employee with NULL DepartmentID or LocationID | At least one employee has NULL DepartmentID or LocationID | 1. Run ETL script. 2. Check joins in target. | Joins result in NULLs for department/location fields; no error. | | |
| TC05 | Edge: Empty employees table | Employees table is empty | 1. Run ETL script. | No data written to target tables; process completes successfully. | | |
| TC06 | Edge: All employees have salary >= HIGH_SALARY_THRESHOLD | All employees have salary >= threshold | 1. Run ETL script. 2. Check summaries. | All employees in high salary summary; low salary summary empty. | | |
| TC07 | Edge: All employees have salary < HIGH_SALARY_THRESHOLD | All employees have salary < threshold | 1. Run ETL script. 2. Check summaries. | All employees in low salary summary; high salary summary empty. | | |
| TC08 | Error: Source table missing required column | Remove required column (e.g., Salary) from source | 1. Run ETL script. | ETL fails; error logged in error log table and log file. | | |
| TC09 | Error: DeltaTable merge fails (simulate exception) | Simulate exception in DeltaTable merge | 1. Run ETL script. | Error logged in error log table and log file; process exits with code 1. | | |
| TC10 | Error: Write to error log table fails (simulate exception) | Simulate exception during error log write | 1. Run ETL script. | Error logged to log file; process exits with code 1. | | |
| TC11 | Edge: Employees with salary == HIGH_SALARY_THRESHOLD | At least one employee with salary == threshold | 1. Run ETL script. 2. Check summaries. | Employee included in high salary summary. | | |
| TC12 | Edge: Duplicate EmployeeID in source | Source contains duplicate EmployeeID | 1. Run ETL script. 2. Check target. | Upsert logic ensures only one record per EmployeeID in target. | | |

---

2. Pytest Script

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
from delta.tables import DeltaTable
from unittest import mock
import sys

# Fixtures for Spark session and test data
@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder \
        .master("local[1]") \
        .appName("pytest-employee-dw") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()

@pytest.fixture
def sample_employees(spark):
    data = [
        {"EmployeeID": 1, "DepartmentID": 10, "LocationID": 100, "HireDate": "2020-01-15", "Salary": 120000},
        {"EmployeeID": 2, "DepartmentID": 20, "LocationID": 200, "HireDate": "2019-06-01", "Salary": 80000},
        {"EmployeeID": 3, "DepartmentID": 10, "LocationID": 100, "HireDate": None, "Salary": 110000},
        {"EmployeeID": 4, "DepartmentID": None, "LocationID": 300, "HireDate": "2021-03-10", "Salary": None},
        {"EmployeeID": 5, "DepartmentID": 30, "LocationID": None, "HireDate": "2018-07-22", "Salary": 95000},
        {"EmployeeID": 6, "DepartmentID": 40, "LocationID": 400, "HireDate": "2017-11-30", "Salary": 100000}, # boundary
        {"EmployeeID": 7, "DepartmentID": 10, "LocationID": 100, "HireDate": "2022-02-02", "Salary": 100000}, # boundary
    ]
    return spark.createDataFrame(data)

@pytest.fixture
def sample_departments(spark):
    data = [
        {"DepartmentID": 10, "DepartmentName": "HR"},
        {"DepartmentID": 20, "DepartmentName": "Finance"},
        {"DepartmentID": 30, "DepartmentName": "IT"},
        {"DepartmentID": 40, "DepartmentName": "Legal"},
    ]
    return spark.createDataFrame(data)

@pytest.fixture
def sample_locations(spark):
    data = [
        {"LocationID": 100, "LocationName": "New York"},
        {"LocationID": 200, "LocationName": "London"},
        {"LocationID": 300, "LocationName": "Berlin"},
        {"LocationID": 400, "LocationName": "Tokyo"},
    ]
    return spark.createDataFrame(data)

# Utility to simulate DeltaTable merge (for unit test, not integration)
class DummyDeltaTable:
    def __init__(self):
        self.data = []
        self.merge_called = False

    def alias(self, name):
        return self

    def merge(self, source, condition):
        self.merge_called = True
        return self

    def whenMatchedUpdateAll(self):
        return self

    def whenNotMatchedInsertAll(self):
        return self

    def execute(self):
        return

# Patch DeltaTable.forPath to return dummy
@pytest.fixture
def patch_delta(monkeypatch):
    monkeypatch.setattr(DeltaTable, "forPath", lambda spark, path: DummyDeltaTable())

# Patch logging
@pytest.fixture
def patch_logging(monkeypatch):
    import logging
    logger = mock.Mock()
    monkeypatch.setattr(logging, "info", logger.info)
    monkeypatch.setattr(logging, "error", logger.error)
    return logger

# Patch sys.exit to prevent test runner exit
@pytest.fixture
def patch_exit(monkeypatch):
    monkeypatch.setattr(sys, "exit", lambda code=0: (_ for _ in ()).throw(SystemExit(code)))

# --- Individual Test Cases ---

def run_etl(spark, employees, departments, locations, patch_delta, patch_logging):
    # This function should contain the ETL logic from the provided PySpark script,
    # but using the provided DataFrames instead of reading from Delta.
    # For brevity, only the structure is shown; in real tests, import the ETL function.
    pass

def test_happy_path(spark, sample_employees, sample_departments, sample_locations, patch_delta, patch_logging):
    # TC01: Happy path
    run_etl(spark, sample_employees, sample_departments, sample_locations, patch_delta, patch_logging)
    # Assert: No exceptions, merges called, logging called
    # (Assertions depend on ETL implementation)

def test_null_hiredate(spark, sample_employees, sample_departments, sample_locations, patch_delta, patch_logging):
    # TC02: Employee with NULL HireDate
    # (Check resulting DataFrame for HireYear/HireMonth == None for EmployeeID 3)
    pass

def test_null_salary(spark, sample_employees, sample_departments, sample_locations, patch_delta, patch_logging):
    # TC03: Employee with NULL Salary
    # (Check that EmployeeID 4 is not in high/low salary summaries)
    pass

def test_null_department_location(spark, sample_employees, sample_departments, sample_locations, patch_delta, patch_logging):
    # TC04: Employee with NULL DepartmentID or LocationID
    # (Check that joins do not fail and result in NULLs)
    pass

def test_empty_employees(spark, sample_departments, sample_locations, patch_delta, patch_logging):
    # TC05: Empty employees table
    from pyspark.sql import Row
    empty_employees = spark.createDataFrame([], schema="EmployeeID INT, DepartmentID INT, LocationID INT, HireDate STRING, Salary DOUBLE")
    run_etl(spark, empty_employees, sample_departments, sample_locations, patch_delta, patch_logging)
    # Assert: No output, no exceptions

def test_all_high_salary(spark, sample_departments, sample_locations, patch_delta, patch_logging):
    # TC06: All employees high salary
    pass

def test_all_low_salary(spark, sample_departments, sample_locations, patch_delta, patch_logging):
    # TC07: All employees low salary
    pass

def test_missing_column(spark, sample_employees, sample_departments, sample_locations, patch_delta, patch_logging, patch_exit):
    # TC08: Source table missing required column
    # Remove 'Salary' column
    employees = sample_employees.drop("Salary")
    with pytest.raises(SystemExit):
        run_etl(spark, employees, sample_departments, sample_locations, patch_delta, patch_logging)
    # Assert: error logged

def test_merge_exception(spark, sample_employees, sample_departments, sample_locations, monkeypatch, patch_logging, patch_exit):
    # TC09: DeltaTable merge fails
    class FailingDeltaTable(DummyDeltaTable):
        def merge(self, source, condition):
            raise Exception("Simulated merge failure")
    monkeypatch.setattr(DeltaTable, "forPath", lambda spark, path: FailingDeltaTable())
    with pytest.raises(SystemExit):
        run_etl(spark, sample_employees, sample_departments, sample_locations, None, patch_logging)
    # Assert: error logged

def test_error_log_write_failure(spark, sample_employees, sample_departments, sample_locations, monkeypatch, patch_logging, patch_exit):
    # TC10: Write to error log table fails
    # Simulate error in error log write
    pass

def test_boundary_salary(spark, sample_employees, sample_departments, sample_locations, patch_delta, patch_logging):
    # TC11: Employees with salary == threshold
    pass

def test_duplicate_employeeid(spark, sample_departments, sample_locations, patch_delta, patch_logging):
    # TC12: Duplicate EmployeeID in source
    pass

# --- Coverage Analysis ---
# To run coverage: pytest --cov=etl_module tests/
```

---

**Test Execution Report Template**

| Test Case ID | Description | Expected Result | Actual Result | Pass/Fail | Notes |
|--------------|-------------|----------------|--------------|-----------|-------|
|              |             |                |              |           |       |

---

**API Cost:** 0.02 USD