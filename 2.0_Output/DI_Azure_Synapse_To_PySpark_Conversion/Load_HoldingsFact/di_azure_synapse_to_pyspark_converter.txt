=============================================
Author:        AAVA
Created on:   
Description:   Convert Azure Synapse stored procedure for loading summarized holding metrics into FACT_EXECUTIVE_SUMMARY from staging, with validation and dimensional joins, into equivalent Databricks PySpark code.
=============================================

# Databricks PySpark Script: Load FACT_EXECUTIVE_SUMMARY from Staging

# Import required libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col
from pyspark.sql.utils import AnalysisException

# Initialize Spark session (Databricks automatically provides 'spark')
# spark = SparkSession.builder.getOrCreate() # Uncomment if running outside Databricks

# Step 1: Prepare staging data (Load STG_HOLDING_METRICS into DataFrame)
stg_holding_metrics_df = spark.table("STG_HOLDING_METRICS")

# Optional: Cache staging data if reused multiple times
stg_holding_metrics_df.cache()

# Step 2: Load dimension tables
dim_date_df = spark.table("DIM_DATE")
dim_institution_df = spark.table("DIM_INSTITUTION")
dim_corporation_df = spark.table("DIM_CORPORATION")
dim_product_df = spark.table("DIM_PRODUCT")

# Optional: Broadcast smaller dimension tables for join optimization
from pyspark.sql.functions import broadcast
dim_date_df = broadcast(dim_date_df)
dim_institution_df = broadcast(dim_institution_df)
dim_corporation_df = broadcast(dim_corporation_df)
dim_product_df = broadcast(dim_product_df)

# Step 3: Join staging data with dimension tables and apply business rules
fact_exec_summary_df = (
    stg_holding_metrics_df
    .join(dim_date_df, dim_date_df.date_key == stg_holding_metrics_df.date_value, "inner")
    .join(dim_institution_df, dim_institution_df.institution_id == stg_holding_metrics_df.institution_id, "inner")
    .join(dim_corporation_df, dim_corporation_df.corporation_id == stg_holding_metrics_df.corporation_id, "inner")
    .join(dim_product_df, dim_product_df.product_id == stg_holding_metrics_df.product_id, "inner")
    .select(
        dim_date_df.date_key.alias("date_key"),
        dim_institution_df.institution_id.alias("institution_id"),
        dim_corporation_df.corporation_id.alias("corporation_id"),
        dim_product_df.product_id.alias("product_id"),
        stg_holding_metrics_df.a120_amount,
        stg_holding_metrics_df.a120_count,
        stg_holding_metrics_df.a30_to_59_amount,
        stg_holding_metrics_df.a30_to_59_count,
        stg_holding_metrics_df.a60_to_89_amount,
        stg_holding_metrics_df.a60_to_89_count,
        stg_holding_metrics_df.a90_to_119_amount,
        stg_holding_metrics_df.a90_to_119_count,
        stg_holding_metrics_df.charge_off_amount,
        stg_holding_metrics_df.charge_off_count,
        stg_holding_metrics_df.fraud_amount,
        stg_holding_metrics_df.fraud_count,
        # Business rule for income_amount: if NULL or < 0, set to 0
        when(
            (col("income_amount").isNull()) | (col("income_amount") < 0),
            0
        ).otherwise(col("income_amount")).alias("income_amount"),
        stg_holding_metrics_df.number_of_accounts,
        stg_holding_metrics_df.purchases_amount,
        stg_holding_metrics_df.purchases_count
    )
)

# Step 4: Insert records into FACT_EXECUTIVE_SUMMARY Delta table
# Replace with your target database/schema if needed
fact_table_name = "FACT_EXECUTIVE_SUMMARY"

# Write to Delta table (append mode)
fact_exec_summary_df.write.format("delta").mode("append").saveAsTable(fact_table_name)

# Step 5: Audit logging (Row count)
v_row_count = fact_exec_summary_df.count()
print(f"{v_row_count} records inserted into {fact_table_name}.")

# Step 6: Cleanup (Unpersist cached DataFrames if necessary)
stg_holding_metrics_df.unpersist()

print("*** LOAD_FACT_EXECUTIVE_SUMMARY completed successfully ***")

# Notes:
# - All table references (STG_HOLDING_METRICS, DIM_DATE, etc.) must exist in the Databricks workspace/catalog.
# - Delta Lake is used for efficient insert/update/merge operations.
# - Business logic for income_amount is implemented using PySpark's when().otherwise().
# - Joins are performed using DataFrame API for scalability.
# - Caching and broadcasting are used for performance optimization.
# - NULL handling and data types are managed via PySpark expressions.

# End of script

API Cost Consumed in dollars:
apiCost: 0.016 USD