=============================================
Author:    AAVA
Created on:   
Description:   Convert Azure Synapse stored procedures into equivalent Databricks PySpark code
=============================================

### Test Case List

| Test Case ID | Test Case Description                                                                 | Expected Outcome                                                                                                   |
|--------------|--------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All staging rows valid, all dimension lookups succeed                    | All rows inserted into Fact_Sales, Audit_Log and DQ_Failures updated correctly, staging table truncated           |
| TC02         | Edge: Staging has rows with NULL Customer_ID                                         | Rows with NULL Customer_ID rejected, logged in DQ_Failures, not loaded to Fact_Sales, Audit_Log updated           |
| TC03         | Edge: Staging has rows with Quantity <= 0                                            | Rows with invalid Quantity rejected, logged in DQ_Failures, not loaded to Fact_Sales, Audit_Log updated           |
| TC04         | Edge: Staging table is empty                                                         | No rows inserted or rejected, Audit_Log reflects zero rows, no DQ_Failures entry                                  |
| TC05         | Edge: All staging rows invalid (all fail DQ checks)                                  | No rows inserted, all rejected, DQ_Failures contains all, Audit_Log updated, staging table truncated              |
| TC06         | Edge: Some rows fail dimension (Customer or Date) join                               | Only rows with matching dimension keys loaded, others dropped, Audit_Log reflects correct counts                  |
| TC07         | Error: Missing required column in staging (e.g., Quantity missing)                   | Pipeline fails, Audit_Log status set to FAILED, error message logged                                              |
| TC08         | Error: Invalid data type in staging (e.g., Quantity is string)                       | Pipeline fails, Audit_Log status set to FAILED, error message logged                                              |
| TC09         | Edge: Duplicate Transaction_ID in staging                                            | Both rows processed (since not deduped in logic), both loaded if valid, Audit_Log counts both                     |
| TC10         | Edge: Boundary values (Quantity=1, Unit_Price=0, extreme dates)                      | Rows processed correctly, Total_Sales_Amount calculated, dates handled, Audit_Log updated                         |

---

### Pytest Script for Each Test Case


# =============================================================================
# Author: AAVA
# Created on: 
# Description: Pytest unit tests for Databricks PySpark sales fact load pipeline
# =============================================================================

import pytest
from pyspark.sql import SparkSession, Row
from pyspark.sql import functions as F
from pyspark.sql.types import *
import pandas as pd

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("unit-test").getOrCreate()

@pytest.fixture(autouse=True)
def clean_tables(spark):
    for tbl in [
        "stg.Sales_Transactions", "dw.Dim_Customer", "dw.Dim_Date", 
        "dw.Fact_Sales", "dw.Audit_Log", "dw.DQ_Failures"
    ]:
        spark.sql(f"DROP TABLE IF EXISTS {tbl}")
    yield
    for tbl in [
        "stg.Sales_Transactions", "dw.Dim_Customer", "dw.Dim_Date", 
        "dw.Fact_Sales", "dw.Audit_Log", "dw.DQ_Failures"
    ]:
        spark.sql(f"DROP TABLE IF EXISTS {tbl}")

def run_pipeline(spark):
    from sales_fact_pipeline import run_sales_fact_load
    run_sales_fact_load(spark)

def create_table(spark, table_name, df):
    df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(table_name)

def test_happy_path(spark):
    stg = spark.createDataFrame([
        (1, 100, 200, "2023-01-01", 5, 10.0),
        (2, 101, 201, "2023-01-02", 2, 20.0)
    ], schema="Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Quantity int, Unit_Price double")
    dim_customer = spark.createDataFrame([
        (100, "Retail"),
        (101, "Wholesale")
    ], schema="Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([
        ("2023-01-01", 1),
        ("2023-01-02", 2)
    ], schema="Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", spark.createDataFrame([], stg.schema))
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    run_pipeline(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 2
    assert set(fact["Transaction_ID"]) == {1,2}
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 2
    assert audit["Rows_Rejected"] == 0
    assert audit["Status"] == "COMPLETED"
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert dq.empty

def test_null_customer_id(spark):
    stg = spark.createDataFrame([
        (1, None, 200, "2023-01-01", 5, 10.0),
        (2, 101, 201, "2023-01-02", 2, 20.0)
    ], schema="Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Quantity int, Unit_Price double")
    dim_customer = spark.createDataFrame([(101, "Wholesale")], "Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([("2023-01-02", 2)], "Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", spark.createDataFrame([], stg.schema))
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    run_pipeline(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert fact["Transaction_ID"].iloc[0] == 2
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert len(dq) == 1
    assert dq["Transaction_ID"].iloc[0] == 1
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 1
    assert audit["Rows_Rejected"] == 1

def test_invalid_quantity(spark):
    stg = spark.createDataFrame([
        (1, 100, 200, "2023-01-01", -1, 10.0),
        (2, 101, 201, "2023-01-02", 0, 20.0),
        (3, 102, 202, "2023-01-03", 5, 30.0)
    ], schema="Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Quantity int, Unit_Price double")
    dim_customer = spark.createDataFrame([(100, "Retail"), (101, "Wholesale"), (102, "Retail")], "Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([
        ("2023-01-01", 1), ("2023-01-02", 2), ("2023-01-03", 3)
    ], "Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", spark.createDataFrame([], stg.schema))
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    run_pipeline(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert fact["Transaction_ID"].iloc[0] == 3
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert set(dq["Transaction_ID"]) == {1,2}
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 1
    assert audit["Rows_Rejected"] == 2

def test_empty_staging(spark):
    stg = spark.createDataFrame([], "Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Quantity int, Unit_Price double")
    dim_customer = spark.createDataFrame([(100, "Retail")], "Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([("2023-01-01", 1)], "Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", stg)
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    run_pipeline(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert fact.empty
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 0
    assert audit["Rows_Rejected"] == 0
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert dq.empty

def test_all_invalid_rows(spark):
    stg = spark.createDataFrame([
        (1, None, 200, "2023-01-01", 0, 10.0),
        (2, None, 201, "2023-01-02", -1, 20.0)
    ], schema="Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Quantity int, Unit_Price double")
    dim_customer = spark.createDataFrame([], "Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([], "Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", stg)
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    run_pipeline(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert fact.empty
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert set(dq["Transaction_ID"]) == {1,2}
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 0
    assert audit["Rows_Rejected"] == 2

def test_dimension_join_failure(spark):
    stg = spark.createDataFrame([
        (1, 100, 200, "2023-01-01", 5, 10.0),
        (2, 999, 201, "2023-01-02", 2, 20.0)
    ], schema="Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Quantity int, Unit_Price double")
    dim_customer = spark.createDataFrame([(100, "Retail")], "Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([("2023-01-01", 1), ("2023-01-02", 2)], "Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", stg)
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    run_pipeline(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert fact["Transaction_ID"].iloc[0] == 1
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 1

def test_missing_column_in_staging(spark):
    stg = spark.createDataFrame([
        (1, 100, 200, "2023-01-01", 10.0)
    ], schema="Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Unit_Price double")
    dim_customer = spark.createDataFrame([(100, "Retail")], "Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([("2023-01-01", 1)], "Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", stg)
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    with pytest.raises(Exception):
        run_pipeline(spark)
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Status"] == "FAILED"
    assert "Quantity" in audit["Message"]

def test_invalid_data_type_in_staging(spark):
    stg = spark.createDataFrame([
        (1, 100, 200, "2023-01-01", "five", 10.0)
    ], schema="Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Quantity string, Unit_Price double")
    dim_customer = spark.createDataFrame([(100, "Retail")], "Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([("2023-01-01", 1)], "Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", stg)
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    with pytest.raises(Exception):
        run_pipeline(spark)
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Status"] == "FAILED"
    assert "Quantity" in audit["Message"]

def test_duplicate_transaction_id(spark):
    stg = spark.createDataFrame([
        (1, 100, 200, "2023-01-01", 5, 10.0),
        (1, 100, 200, "2023-01-01", 3, 10.0)
    ], schema="Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Quantity int, Unit_Price double")
    dim_customer = spark.createDataFrame([(100, "Retail")], "Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([("2023-01-01", 1)], "Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", stg)
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    run_pipeline(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 2
    assert all(fact["Transaction_ID"] == 1)
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 2

def test_boundary_values(spark):
    stg = spark.createDataFrame([
        (1, 100, 200, "1900-01-01", 1, 0.0),
        (2, 100, 200, "2099-12-31", 1000000, 99999.99)
    ], schema="Transaction_ID long, Customer_ID long, Product_ID long, Sales_Date string, Quantity int, Unit_Price double")
    dim_customer = spark.createDataFrame([(100, "Retail")], "Customer_ID long, Customer_Segment string")
    dim_date = spark.createDataFrame([("1900-01-01", 1), ("2099-12-31", 2)], "Date_Value string, Region_ID int")
    create_table(spark, "stg.Sales_Transactions", stg)
    create_table(spark, "dw.Dim_Customer", dim_customer)
    create_table(spark, "dw.Dim_Date", dim_date)
    create_table(spark, "dw.Fact_Sales", stg)
    create_table(spark, "dw.Audit_Log", spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType()), StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()), StructField("End_Time", TimestampType()),
        StructField("Status", StringType()), StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()), StructField("Rows_Rejected", IntegerType()),
        StructField("Audit_ID", LongType())
    ])))
    create_table(spark, "dw.DQ_Failures", spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType()), StructField("Failure_Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()), StructField("Batch_ID", StringType())
    ])))
    run_pipeline(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 2
    assert fact.loc[fact["Transaction_ID"]==1, "Total_Sales_Amount"].iloc[0] == 0.0
    assert fact.loc[fact["Transaction_ID"]==2, "Total_Sales_Amount"].iloc[0] == 1000000 * 99999.99
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 2


---

**API Cost Consumption:**

apiCost: 0.0047 USD