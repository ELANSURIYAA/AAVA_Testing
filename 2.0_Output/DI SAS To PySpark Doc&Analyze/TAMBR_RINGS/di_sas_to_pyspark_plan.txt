=============================================
Author:        Ascendion AVA+
Date:   
Description:   Effort and cost estimation for PySpark conversion and testing of TAMBR_RINGS.txt SAS code on Azure Databricks
=============================================

1.1 Cost Estimation

   1.1.1 PySpark Runtime Cost 

   **Calculation Breakup:**
   - **Databricks DBU Cost Range:** $0.15 - $0.75 per DBU-hour (Enterprise)
   - **Data Processed (10% of each table):**
     - CUSTOMER_MTH: 2 TB × 10% = 200 GB
     - BRANCH_HRCY_MTH: 500 GB × 10% = 50 GB
     - GEO_CUSTOMER_MTH: 500 GB × 10% = 50 GB
     - MACOMMON._GEOCODE_WEEKLY: 100 GB × 10% = 10 GB
     - TAMBR_RINGS (output): 200 GB × 10% = 20 GB
     - **Total Data Processed:** 330 GB

   - **Assumed Cluster:** 8 DBU/hour (typical for ETL workloads with geospatial and percentile aggregations)
   - **Estimated Processing Time:** 2 hours (based on complexity: 490 lines, 22 datasets, multiple joins, percentile, geospatial UDFs, and temp tables)
   - **DBU Consumption:** 8 DBU/hour × 2 hours = 16 DBU

   - **Cost Calculation:**
     - **Low Estimate:** 16 DBU × $0.15 = $2.40
     - **High Estimate:** 16 DBU × $0.75 = $12.00

   - **Recommended Budget:** $10 (rounded, allowing for retries and cluster overhead)

   **Reasons:**
   - The process involves heavy joins, window functions, geospatial UDFs, and percentile aggregations, which are compute-intensive.
   - Data volume is moderate (330 GB processed), but distributed compute is required due to the number of joins and aggregations.
   - 2 hours is a conservative estimate for a monthly batch ETL of this complexity on a mid-sized Databricks cluster.

   **apiCost: 0.01 USD**

2. Code Fixing and Testing Effort Estimation

   2.1 PySpark identified manual code fixes and unit testing effort in hours covering the various temp tables, calculations

   **Manual Adjustments (from Analyzer output):**
   - Replace SAS macros (`%macro`, `%mdrop_mac`, `%masofdt`) with Python functions or parameterized notebook cells.
   - Replace `geodist` with a PySpark UDF or use the haversine formula (e.g., via `geopy` or custom code).
   - Replace PROC UNIVARIATE percentile with PySpark `approxQuantile` or `percentile_approx`.
   - Replace Data Step IF-THEN-ELSE with DataFrame `when`/`otherwise` or `withColumn`.
   - Replace PROC SQL joins with DataFrame joins.
   - Replace temporary table management with DataFrame caching and checkpointing.
   - Replace SAS-style error handling with Python exception handling.
   - Replace DB2 SQL with Spark JDBC reads/writes.
   - Adjust for case sensitivity and column naming conventions.
   - Review all hard-coded values (e.g., max_dist = 40) for business rule alignment.

   **Effort Estimation:**
   - **Manual Code Fixes (macros, geodist, percentile, error handling, JDBC):** 12 hours
   - **Unit Testing (temp tables, joins, geospatial, percentile, edge cases):** 10 hours
   - **Data Reconciliation Testing (output validation, data quality, ring calculations):** 8 hours
   - **Integration/Regression Testing (end-to-end, performance, cluster tuning):** 6 hours

   **Total Effort Estimate:** 36 hours

   **Breakdown:**
   - Manual code refactoring (macros, UDFs, error handling): 12h
   - Unit tests for temp tables, joins, calculations: 10h
   - Data recon and output validation: 8h
   - Integration/performance testing: 6h

   **Manual Adjustments (replicated from Analyzer output):**
   - Replace SAS macros with Python functions or parameterized notebook cells.
   - Replace `geodist` with a PySpark UDF or use haversine formula.
   - Replace PROC UNIVARIATE percentile with `approxQuantile`.
   - Replace Data Step IF-THEN-ELSE with DataFrame `when`/`otherwise`.
   - Replace PROC SQL joins with DataFrame joins.
   - Replace temp table management with DataFrame caching/checkpointing.
   - Replace SAS error handling with Python exception handling.
   - Replace DB2 SQL with Spark JDBC reads/writes.
   - Adjust for case sensitivity and column naming.
   - Review all hard-coded values for business rule alignment.

apiCost: 0.01 USD