1. **Test Case Document:**

| Test Case ID | Test Case Description | Expected Outcome |
|--------------|----------------------|------------------|
| TC01 | Happy path: All required fields present, all joins succeed, all extension fields populated, valid email addresses, valid dates, LoopInstance = 0 | Output DataFrame contains correct JSON_Message with all fields, all extension fields present, UnderwriterId included, all values correctly mapped |
| TC02 | NationalAccountFlag = 1, ExpirationDate > cutoff, UnderwriterId should be NULL | UnderwriterId in JSON is null, rest of the fields as in happy path |
| TC03 | AccountID missing, IdOverride.RCT_ID present | AccountID is taken from IdOverride, not AccountID table |
| TC04 | AccountID and IdOverride.RCT_ID both missing | AccountID is null in output |
| TC05 | Some extension fields are null | Those extension fields are omitted from JSON_Message |
| TC06 | Email fields do not contain '@' | Email fields in JSON are null or omitted |
| TC07 | Empty Account DataFrame | Output DataFrame is empty |
| TC08 | Account row fails filter (PostPatch != 'Patch', Validated not null, DateSent not null, SubmissionFlag != 0) | Output DataFrame is empty |
| TC09 | Brand/Status not in BrandCode/INForceListCode | Extension fields for Brand/Status are null/omitted |
| TC10 | Account row with boundary values (e.g., ExternalUniqueId length = 15, NewBusinessPolicyFlag = 1/0) | [167] and [120] fields are correctly calculated in JSON |
| TC11 | Invalid schema: missing required columns | Exception is raised, handled gracefully |
| TC12 | Account row with NULLs in join keys | Joins result in nulls, output fields reflect nulls as per logic |
| TC13 | Multiple rows, LoopInstance partitioning | Only rows with correct LoopInstance are output, partitioning is correct |
| TC14 | Large dataset (performance) | Output is correct, no performance errors, setup/teardown works |
| TC15 | Special characters in string fields | JSON_Message is correctly escaped and valid JSON |

---

2. **Pytest Script for each test case:**

```python
import pytest
from pyspark.sql import SparkSession, Row, functions as F, types as T
from pyspark.sql.utils import AnalysisException

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("uspAPIPatchAccountTest").getOrCreate()

def build_json_message(row):
    # Minimal version for test, expand as needed
    json_parts = []
    if row.UnderwriterId is not None:
        json_parts.append(f'''{{
  "op": "replace",
  "path": "/UnderwriterId",
  "value": "{row.UnderwriterId}"
}}''')
    extension_fields = []
    for ext_id in [11, 37, 40, 67, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 160, 161, 167, 168]:
        val = getattr(row, str(ext_id), None)
        if val is not None:
            extension_fields.append(f'''{{
  "Id":{ext_id},
  "value": "{val}"
}}''')
    extension_fields_json = ',\n'.join(extension_fields)
    json_message = '[\n' + ',\n'.join(json_parts) + f''',
{{
      "op": "replace",
      "path": "/ExtensionFields",
      "value": [
{extension_fields_json}
      ]
  }}
]'''
    return json_message

build_json_message_udf = F.udf(build_json_message, T.StringType())

def make_account_df(spark, data, schema=None):
    return spark.createDataFrame(data, schema=schema)

def make_policy_descriptors_df(spark, data, schema=None):
    return spark.createDataFrame(data, schema=schema)

def make_accountid_df(spark, data, schema=None):
    return spark.createDataFrame(data, schema=schema)

def make_idoverride_df(spark, data, schema=None):
    return spark.createDataFrame(data, schema=schema)

def run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, loop_instance):
    from datetime import datetime, timedelta
    import pyspark.sql.functions as F
    from pyspark.sql import Window

    date_cutoff = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    NationalAccts = PolicyDescriptors.filter(
        (F.col("NationalAccountFlag") == 1) &
        (F.col("ExpirationDate") > F.lit(date_cutoff))
    ).select("AccountNumber").distinct()

    inforce_list = [
        ("2443", "inforce"),
        ("2341", "propsect"),
        ("2325", "inactive")
    ]
    INForceListCode = spark.createDataFrame(inforce_list, ["ItemID", "ItemCode"])

    brand_list = [
        ("2533", "Accident Fund"),
        ("2536", "Third Coast Underwriters"),
        ("2534", "AF Specialty"),
        ("2535", "United Heartland"),
        ("2537", "CompWest")
    ]
    BrandCode = spark.createDataFrame(brand_list, ["ItemID", "ItemCode"])

    w_loop = Window.orderBy("ContactNumber")
    w_rown = Window.orderBy(F.col("Premium1").desc())

    Final = (
        Account
        .join(NationalAccts, Account.ContactNumber == NationalAccts.AccountNumber, "left")
        .join(INForceListCode, Account.AccountStatus == INForceListCode.ItemCode, "left")
        .join(BrandCode, Account.Brand == BrandCode.ItemCode, "left")
        .join(AccountID, Account.ContactNumber == AccountID.ContactNumber, "left")
        .join(IdOverride, (IdOverride.ExternalUniqueID == Account.ExternalUniqueId) & (IdOverride.ObjectType == F.lit("Account")), "left")
        .filter(
            (Account.PostPatch == "Patch") &
            (Account.Validated.isNull()) &
            (Account.DateSent.isNull()) &
            (Account.SubmissionFlag == 0)
        )
        .withColumn("LoopInstance", ((F.row_number().over(w_loop) - 1) / 250).cast("int"))
        .withColumn("UnderwriterId", F.when(NationalAccts.AccountNumber.isNull(), Account.UnderwriterId).otherwise(F.lit(None)))
        .withColumn("AccountID", F.coalesce(AccountID.AccountID, IdOverride.RCT_ID))
        .withColumn("RowN", F.row_number().over(w_rown))
    )

    # Add only a subset of columns for test brevity
    Final = Final.select(
        "AccountID",
        "ContactNumber",
        "SubmissionFlag",
        "LoopInstance",
        "RowN",
        "UnderwriterId",
        "ExternalUniqueId",
        F.col("Name"),
        F.col("BusinessPhone").alias("Phone"),
        F.col("FaxPhone").alias("Fax"),
        F.when(F.col("EmailAddress").contains("@"), F.col("EmailAddress")).otherwise(F.lit(None)).alias("Email"),
        F.col("Notes").cast("string").alias("Notes"),
        "ContactNumber",
        "SubmissionFlag",
        # Extension fields for test
        F.col("Premium1").alias("73"),
        F.col("UnderwriterEmail").alias("40"),
    )

    Final_filtered = Final.filter(F.col("LoopInstance") == loop_instance)
    Final_with_json = Final_filtered.withColumn("JSON_Message", build_json_message_udf(F.struct([Final_filtered[x] for x in Final_filtered.columns])))
    result = Final_with_json.select("AccountID", "ContactNumber", "SubmissionFlag", "LoopInstance", "JSON_Message")
    return result

# --- Test Cases ---

def test_TC01_happy_path(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C1", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW1", ExternalUniqueId="EU1", Name="Acme Inc", BusinessPhone="555-1234", FaxPhone="555-5678",
            EmailAddress="user@acme.com", Notes="Test", Premium1="1000", UnderwriterEmail="uw@acme.com", AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [
        Row(AccountNumber="C1", NationalAccountFlag=0, ExpirationDate="2099-01-01")
    ])
    AccountID = make_accountid_df(spark, [
        Row(ContactNumber="C1", AccountID="AID1")
    ])
    IdOverride = make_idoverride_df(spark, [
        Row(ExternalUniqueID="EU1", ObjectType="Account", RCT_ID=None)
    ])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    assert rows[0]["AccountID"] == "AID1"
    assert '"UnderwriterId"' in rows[0]["JSON_Message"]
    assert '"value": "UW1"' in rows[0]["JSON_Message"]
    assert '"Id":73' in rows[0]["JSON_Message"]
    assert '"value": "1000"' in rows[0]["JSON_Message"]

def test_TC02_national_account_nulls_underwriter(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C2", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW2", ExternalUniqueId="EU2", Name="Beta Inc", BusinessPhone="555-2222", FaxPhone="555-3333",
            EmailAddress="beta@acme.com", Notes="Test2", Premium1="2000", UnderwriterEmail="uw2@acme.com", AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [
        Row(AccountNumber="C2", NationalAccountFlag=1, ExpirationDate="2099-01-01")
    ])
    AccountID = make_accountid_df(spark, [
        Row(ContactNumber="C2", AccountID="AID2")
    ])
    IdOverride = make_idoverride_df(spark, [
        Row(ExternalUniqueID="EU2", ObjectType="Account", RCT_ID=None)
    ])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    assert '"UnderwriterId"' not in rows[0]["JSON_Message"]

def test_TC03_accountid_missing_idoverride_used(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C3", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW3", ExternalUniqueId="EU3", Name="Gamma Inc", BusinessPhone="555-4444", FaxPhone="555-5555",
            EmailAddress="gamma@acme.com", Notes="Test3", Premium1="3000", UnderwriterEmail="uw3@acme.com", AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [])
    IdOverride = make_idoverride_df(spark, [
        Row(ExternalUniqueID="EU3", ObjectType="Account", RCT_ID="IDO3")
    ])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    assert rows[0]["AccountID"] == "IDO3"

def test_TC04_both_accountid_and_idoverride_missing(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C4", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW4", ExternalUniqueId="EU4", Name="Delta Inc", BusinessPhone="555-6666", FaxPhone="555-7777",
            EmailAddress="delta@acme.com", Notes="Test4", Premium1="4000", UnderwriterEmail="uw4@acme.com", AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    assert rows[0]["AccountID"] is None

def test_TC05_some_extension_fields_null(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C5", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW5", ExternalUniqueId="EU5", Name="Epsilon Inc", BusinessPhone="555-8888", FaxPhone="555-9999",
            EmailAddress="epsilon@acme.com", Notes="Test5", Premium1=None, UnderwriterEmail=None, AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [
        Row(ContactNumber="C5", AccountID="AID5")
    ])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    assert '"Id":73' not in rows[0]["JSON_Message"]  # Premium1 missing, so [73] not in JSON
    assert '"Id":40' not in rows[0]["JSON_Message"]  # UnderwriterEmail missing

def test_TC06_email_fields_invalid(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C6", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW6", ExternalUniqueId="EU6", Name="Zeta Inc", BusinessPhone="555-0000", FaxPhone="555-1111",
            EmailAddress="notanemail", Notes="Test6", Premium1="6000", UnderwriterEmail="notanemail", AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [
        Row(ContactNumber="C6", AccountID="AID6")
    ])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    assert '"Email"' in rows[0]["JSON_Message"]
    assert '"value": "None"' in rows[0]["JSON_Message"] or '"value": null' in rows[0]["JSON_Message"]

def test_TC07_empty_account_df(spark):
    Account = make_account_df(spark, [])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    assert result.count() == 0

def test_TC08_account_row_fails_filter(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C8", PostPatch="NoPatch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW8", ExternalUniqueId="EU8", Name="Eta Inc", BusinessPhone="555-2222", FaxPhone="555-3333",
            EmailAddress="eta@acme.com", Notes="Test8", Premium1="8000", UnderwriterEmail="uw8@acme.com", AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    assert result.count() == 0

def test_TC09_brand_status_not_in_code_tables(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C9", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW9", ExternalUniqueId="EU9", Name="Theta Inc", BusinessPhone="555-4444", FaxPhone="555-5555",
            EmailAddress="theta@acme.com", Notes="Test9", Premium1="9000", UnderwriterEmail="uw9@acme.com", AccountStatus="unknown", Brand="unknown")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [
        Row(ContactNumber="C9", AccountID="AID9")
    ])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    # Brand/Status not in BrandCode/INForceListCode, so extension fields for those are null/omitted

def test_TC10_boundary_values(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C10", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW10", ExternalUniqueId="AF1234567890123", Name="Iota Inc", BusinessPhone="555-6666", FaxPhone="555-7777",
            EmailAddress="iota@acme.com", Notes="Test10", Premium1="10000", UnderwriterEmail="uw10@acme.com", AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [
        Row(ContactNumber="C10", AccountID="AID10")
    ])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    # [167] field logic can be checked if implemented in the pipeline

def test_TC11_invalid_schema(spark):
    # Missing required columns
    Account = spark.createDataFrame([Row(Foo="bar")])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [])
    IdOverride = make_idoverride_df(spark, [])
    with pytest.raises(AnalysisException):
        run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)

def test_TC12_nulls_in_join_keys(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber=None, PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW12", ExternalUniqueId=None, Name="Lambda Inc", BusinessPhone="555-8888", FaxPhone="555-9999",
            EmailAddress="lambda@acme.com", Notes="Test12", Premium1="12000", UnderwriterEmail="uw12@acme.com", AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    assert rows[0]["AccountID"] is None

def test_TC13_loopinstance_partitioning(spark):
    data = []
    for i in range(300):
        data.append(Row(ContactNumber=f"C{i}", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
                        UnderwriterId=f"UW{i}", ExternalUniqueId=f"EU{i}", Name=f"Name{i}", BusinessPhone="555-0000", FaxPhone="555-1111",
                        EmailAddress=f"user{i}@acme.com", Notes=f"Note{i}", Premium1=str(i), UnderwriterEmail=f"uw{i}@acme.com", AccountStatus="inforce", Brand="Accident Fund"))
    Account = make_account_df(spark, data)
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [])
    IdOverride = make_idoverride_df(spark, [])
    # LoopInstance 0: first 250, LoopInstance 1: next 50
    result0 = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    result1 = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 1)
    assert result0.count() == 250
    assert result1.count() == 50

def test_TC14_large_dataset_performance(spark):
    # Just ensure no crash, not timing
    data = []
    for i in range(1000):
        data.append(Row(ContactNumber=f"C{i}", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
                        UnderwriterId=f"UW{i}", ExternalUniqueId=f"EU{i}", Name=f"Name{i}", BusinessPhone="555-0000", FaxPhone="555-1111",
                        EmailAddress=f"user{i}@acme.com", Notes=f"Note{i}", Premium1=str(i), UnderwriterEmail=f"uw{i}@acme.com", AccountStatus="inforce", Brand="Accident Fund"))
    Account = make_account_df(spark, data)
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    assert result.count() == 250  # Only first 250 for LoopInstance 0

def test_TC15_special_characters_in_strings(spark):
    Account = make_account_df(spark, [
        Row(ContactNumber="C15", PostPatch="Patch", Validated=None, DateSent=None, SubmissionFlag=0,
            UnderwriterId="UW15", ExternalUniqueId="EU15", Name='Name"15', BusinessPhone="555-0000", FaxPhone="555-1111",
            EmailAddress='user15"@acme.com', Notes='Note\n15', Premium1='15000', UnderwriterEmail='uw15@acme.com', AccountStatus="inforce", Brand="Accident Fund")
    ])
    PolicyDescriptors = make_policy_descriptors_df(spark, [])
    AccountID = make_accountid_df(spark, [
        Row(ContactNumber="C15", AccountID="AID15")
    ])
    IdOverride = make_idoverride_df(spark, [])
    result = run_patch_account_pipeline(spark, Account, PolicyDescriptors, AccountID, IdOverride, 0)
    rows = result.collect()
    assert len(rows) == 1
    # Ensure JSON is still valid (no crash, quotes escaped)
    assert 'Name"15' in rows[0]["JSON_Message"]

```

3. **API Cost:**
- apiCost: 0.0080