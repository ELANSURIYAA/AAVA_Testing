1. **Test Case List:**

| Test Case ID | Test Case Description | Expected Outcome |
|--------------|----------------------|------------------|
| TC01 | Happy path: All required fields present, all joins succeed, valid data for JSON construction | Output DataFrame contains correct AccountID, ContactNumber, SubmissionFlag, LoopInstance, and a well-formed JSON_Message with all fields and extension fields populated as per input |
| TC02 | Edge: NationalAccountFlag=1, ExpirationDate > yesterday, triggers National Account logic (UnderwriterId is NULL) | UnderwriterId in JSON is null/empty string, Account is included in output |
| TC03 | Edge: NationalAccountFlag=0, ExpirationDate < yesterday, not a National Account | UnderwriterId is preserved in JSON, Account is included in output |
| TC04 | Edge: AccountStatus not matching any INForceListCode | Extension field [11] is null/empty string in JSON, but record is included |
| TC05 | Edge: Brand not matching any BrandCode | Extension field [116] is null/empty string in JSON, but record is included |
| TC06 | Edge: EmailAddress does not contain '@' | Email field in JSON is null/empty string |
| TC07 | Edge: PrimaryContactEmail does not contain '@' | Extension field [124] is null/empty string |
| TC08 | Edge: UnderwriterEmail does not contain '@' | Extension field [40] is null/empty string |
| TC09 | Edge: All fields for extension fields are NULL | All extension fields in JSON are empty strings |
| TC10 | Edge: Empty input DataFrames | Output DataFrame is empty |
| TC11 | Error: Missing required columns in input DataFrames | Function raises AnalysisException or similar error |
| TC12 | Error: Invalid data types (e.g., string in numeric field) | Function raises AnalysisException or similar error |
| TC13 | Edge: Multiple rows, test LoopInstance partitioning and row_number logic | Only records with correct LoopInstance are included, LoopInstance is set correctly |
| TC14 | Edge: a.PostPatch != 'Patch', a.Validated IS NOT NULL, a.DateSent IS NOT NULL, a.SubmissionFlag != 0 | No records are returned (filtered out) |
| TC15 | Edge: Complex string/numeric values (special chars, long strings, boundary numeric values) | Output JSON_Message correctly escapes/handles values |

---

2. **Pytest Script for each test case**

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
import json

# Assume the uspAPIPatchAccount function is imported and available

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("unit-tests").getOrCreate()

def make_account_df(spark, rows):
    schema = StructType([
        StructField("AccountID", StringType(), True),
        StructField("ContactNumber", StringType(), True),
        StructField("SubmissionFlag", IntegerType(), True),
        StructField("LoopInstance", IntegerType(), True),
        StructField("Name", StringType(), True),
        StructField("BusinessPhone", StringType(), True),
        StructField("FaxPhone", StringType(), True),
        StructField("EmailAddress", StringType(), True),
        StructField("Notes", StringType(), True),
        StructField("AccountStatus", StringType(), True),
        StructField("Brand", StringType(), True),
        StructField("ExternalUniqueId", StringType(), True),
        StructField("UnderwriterId", StringType(), True),
        StructField("PostPatch", StringType(), True),
        StructField("Validated", StringType(), True),
        StructField("DateSent", StringType(), True),
        StructField("PrimaryContactEmail", StringType(), True),
        StructField("UnderwriterEmail", StringType(), True),
        StructField("Premium1", DoubleType(), True),
        # ... add all other required columns for extension fields
    ])
    return spark.createDataFrame(rows, schema=schema)

def make_policydescriptors_df(spark, rows):
    schema = StructType([
        StructField("AccountNumber", StringType(), True),
        StructField("NationalAccountFlag", IntegerType(), True),
        StructField("ExpirationDate", StringType(), True),
    ])
    return spark.createDataFrame(rows, schema=schema)

def make_accountid_df(spark, rows):
    schema = StructType([
        StructField("ContactNumber", StringType(), True),
        StructField("RCT_ID", StringType(), True),
        StructField("AccountID", StringType(), True),
    ])
    return spark.createDataFrame(rows, schema=schema)

def make_idoverride_df(spark, rows):
    schema = StructType([
        StructField("ExternalUniqueID", StringType(), True),
        StructField("ObjectType", StringType(), True),
    ])
    return spark.createDataFrame(rows, schema=schema)

def register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df):
    account_df.createOrReplaceTempView("RCT.Account")
    policydescriptors_df.createOrReplaceTempView("EDSMART.Semantic.PolicyDescriptors")
    accountid_df.createOrReplaceTempView("RCT.AccountID")
    idoverride_df.createOrReplaceTempView("RCT.IdOverride")

def parse_json_message(json_str):
    # Helper to parse JSON_Message column
    return json.loads(json_str)

# TC01: Happy path
def test_happy_path(spark):
    account_df = make_account_df(spark, [
        ("A1", "C1", 0, 0, "Test Name", "555-1234", "555-4321", "test@email.com", "note", "inforce", "Accident Fund", "EU1", "UW1", "Patch", None, None, "pc@email.com", "uw@email.com", 1000.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [
        ("C1", 0, "2099-12-31")
    ])
    accountid_df = make_accountid_df(spark, [
        ("C1", "RCT1", "A1")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU1", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    row = result[0]
    assert row.AccountID == "A1"
    assert row.ContactNumber == "C1"
    assert row.SubmissionFlag == 0
    assert row.LoopInstance == 0
    json_msg = parse_json_message(row.JSON_Message)
    assert any(j['path'] == "/UnderwriterId" and j['value'] == "UW1" for j in json_msg)

# TC02: National Account logic
def test_national_account_logic(spark):
    account_df = make_account_df(spark, [
        ("A2", "C2", 0, 0, "Nat Acct", "555-0000", "555-1111", "nat@email.com", "note", "inforce", "Accident Fund", "EU2", "UW2", "Patch", None, None, "pc@email.com", "uw@email.com", 2000.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [
        ("C2", 1, "2099-12-31")
    ])
    accountid_df = make_accountid_df(spark, [
        ("C2", "RCT2", "A2")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU2", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    json_msg = parse_json_message(result[0].JSON_Message)
    # UnderwriterId should be empty string for National Account
    assert any(j['path'] == "/UnderwriterId" and j['value'] == "" for j in json_msg)

# TC03: Not a National Account
def test_not_national_account(spark):
    account_df = make_account_df(spark, [
        ("A3", "C3", 0, 0, "Non Nat Acct", "555-2222", "555-3333", "nonat@email.com", "note", "inforce", "Accident Fund", "EU3", "UW3", "Patch", None, None, "pc@email.com", "uw@email.com", 3000.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [
        ("C3", 0, "2000-01-01")
    ])
    accountid_df = make_accountid_df(spark, [
        ("C3", "RCT3", "A3")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU3", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    json_msg = parse_json_message(result[0].JSON_Message)
    # UnderwriterId should be preserved
    assert any(j['path'] == "/UnderwriterId" and j['value'] == "UW3" for j in json_msg)

# TC04: AccountStatus not matching INForceListCode
def test_accountstatus_not_matching_inforcelistcode(spark):
    account_df = make_account_df(spark, [
        ("A4", "C4", 0, 0, "No Inforce", "555-4444", "555-5555", "noinforce@email.com", "note", "not_inforce", "Accident Fund", "EU4", "UW4", "Patch", None, None, "pc@email.com", "uw@email.com", 4000.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [
        ("C4", "RCT4", "A4")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU4", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    json_msg = parse_json_message(result[0].JSON_Message)
    # Extension field [11] should be empty string
    ext_fields = [j for j in json_msg if j['path'] == "/ExtensionFields"][0]['value']
    ext_11 = next((e for e in ext_fields if e['Id'] == 11), None)
    assert ext_11 is not None and ext_11['value'] == ""

# TC05: Brand not matching BrandCode
def test_brand_not_matching_brandcode(spark):
    account_df = make_account_df(spark, [
        ("A5", "C5", 0, 0, "No Brand", "555-6666", "555-7777", "nobrand@email.com", "note", "inforce", "UnknownBrand", "EU5", "UW5", "Patch", None, None, "pc@email.com", "uw@email.com", 5000.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [
        ("C5", "RCT5", "A5")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU5", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    json_msg = parse_json_message(result[0].JSON_Message)
    ext_fields = [j for j in json_msg if j['path'] == "/ExtensionFields"][0]['value']
    ext_116 = next((e for e in ext_fields if e['Id'] == 116), None)
    assert ext_116 is not None and ext_116['value'] == ""

# TC06: EmailAddress does not contain '@'
def test_emailaddress_invalid(spark):
    account_df = make_account_df(spark, [
        ("A6", "C6", 0, 0, "No Email", "555-8888", "555-9999", "invalidemail", "note", "inforce", "Accident Fund", "EU6", "UW6", "Patch", None, None, "pc@email.com", "uw@email.com", 6000.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [
        ("C6", "RCT6", "A6")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU6", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    json_msg = parse_json_message(result[0].JSON_Message)
    assert any(j['path'] == "/Email" and j['value'] == "" for j in json_msg)

# TC07: PrimaryContactEmail does not contain '@'
def test_primarycontactemail_invalid(spark):
    account_df = make_account_df(spark, [
        ("A7", "C7", 0, 0, "No PC Email", "555-0001", "555-0002", "valid@email.com", "note", "inforce", "Accident Fund", "EU7", "UW7", "Patch", None, None, "invalidemail", "uw@email.com", 7000.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [
        ("C7", "RCT7", "A7")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU7", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    json_msg = parse_json_message(result[0].JSON_Message)
    ext_fields = [j for j in json_msg if j['path'] == "/ExtensionFields"][0]['value']
    ext_124 = next((e for e in ext_fields if e['Id'] == 124), None)
    assert ext_124 is not None and ext_124['value'] == ""

# TC08: UnderwriterEmail does not contain '@'
def test_underwriteremail_invalid(spark):
    account_df = make_account_df(spark, [
        ("A8", "C8", 0, 0, "No UW Email", "555-0003", "555-0004", "valid@email.com", "note", "inforce", "Accident Fund", "EU8", "UW8", "Patch", None, None, "pc@email.com", "invalidemail", 8000.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [
        ("C8", "RCT8", "A8")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU8", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    json_msg = parse_json_message(result[0].JSON_Message)
    ext_fields = [j for j in json_msg if j['path'] == "/ExtensionFields"][0]['value']
    ext_40 = next((e for e in ext_fields if e['Id'] == 40), None)
    assert ext_40 is not None and ext_40['value'] == ""

# TC09: All extension fields NULL
def test_all_extension_fields_null(spark):
    # Only required columns, all extension fields missing/None
    account_df = make_account_df(spark, [
        ("A9", "C9", 0, 0, "All Ext Null", "555-0005", "555-0006", "valid@email.com", "note", "inforce", "Accident Fund", "EU9", "UW9", "Patch", None, None, "pc@email.com", "uw@email.com", 9000.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [
        ("C9", "RCT9", "A9")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU9", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    json_msg = parse_json_message(result[0].JSON_Message)
    ext_fields = [j for j in json_msg if j['path'] == "/ExtensionFields"][0]['value']
    for ext in ext_fields:
        assert ext['value'] == "" or ext['value'] is None

# TC10: Empty input DataFrames
def test_empty_input(spark):
    account_df = make_account_df(spark, [])
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [])
    idoverride_df = make_idoverride_df(spark, [])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    assert result_df.count() == 0

# TC11: Missing required columns
def test_missing_required_columns(spark):
    # Remove required column
    schema = StructType([
        StructField("AccountID", StringType(), True),
        # Missing ContactNumber
        StructField("SubmissionFlag", IntegerType(), True),
        StructField("LoopInstance", IntegerType(), True),
        StructField("Name", StringType(), True),
        StructField("BusinessPhone", StringType(), True),
        StructField("FaxPhone", StringType(), True),
        StructField("EmailAddress", StringType(), True),
        StructField("Notes", StringType(), True),
        StructField("AccountStatus", StringType(), True),
        StructField("Brand", StringType(), True),
        StructField("ExternalUniqueId", StringType(), True),
        StructField("UnderwriterId", StringType(), True),
        StructField("PostPatch", StringType(), True),
        StructField("Validated", StringType(), True),
        StructField("DateSent", StringType(), True),
        StructField("PrimaryContactEmail", StringType(), True),
        StructField("UnderwriterEmail", StringType(), True),
        StructField("Premium1", DoubleType(), True),
    ])
    account_df = spark.createDataFrame([], schema=schema)
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [])
    idoverride_df = make_idoverride_df(spark, [])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    with pytest.raises(Exception):
        uspAPIPatchAccount(loop_instance=0, spark=spark)

# TC12: Invalid data types
def test_invalid_data_types(spark):
    # Premium1 as string instead of double
    schema = StructType([
        StructField("AccountID", StringType(), True),
        StructField("ContactNumber", StringType(), True),
        StructField("SubmissionFlag", IntegerType(), True),
        StructField("LoopInstance", IntegerType(), True),
        StructField("Name", StringType(), True),
        StructField("BusinessPhone", StringType(), True),
        StructField("FaxPhone", StringType(), True),
        StructField("EmailAddress", StringType(), True),
        StructField("Notes", StringType(), True),
        StructField("AccountStatus", StringType(), True),
        StructField("Brand", StringType(), True),
        StructField("ExternalUniqueId", StringType(), True),
        StructField("UnderwriterId", StringType(), True),
        StructField("PostPatch", StringType(), True),
        StructField("Validated", StringType(), True),
        StructField("DateSent", StringType(), True),
        StructField("PrimaryContactEmail", StringType(), True),
        StructField("UnderwriterEmail", StringType(), True),
        StructField("Premium1", StringType(), True),  # Should be DoubleType
    ])
    account_df = spark.createDataFrame([], schema=schema)
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [])
    idoverride_df = make_idoverride_df(spark, [])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    with pytest.raises(Exception):
        uspAPIPatchAccount(loop_instance=0, spark=spark)

# TC13: Multiple rows, LoopInstance partitioning
def test_loopinstance_partitioning(spark):
    rows = []
    for i in range(500):
        rows.append((
            f"A{i}", f"C{i}", 0, 0, f"Name{i}", "555-0000", "555-0000", f"user{i}@email.com", "note", "inforce", "Accident Fund", f"EU{i}", f"UW{i}", "Patch", None, None, "pc@email.com", "uw@email.com", float(i)
        ))
    account_df = make_account_df(spark, rows)
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [(f"C{i}", f"RCT{i}", f"A{i}") for i in range(500)])
    idoverride_df = make_idoverride_df(spark, [(f"EU{i}", "Account") for i in range(500)])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df_0 = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result_df_1 = uspAPIPatchAccount(loop_instance=1, spark=spark)
    assert result_df_0.count() == 250
    assert result_df_1.count() == 250

# TC14: Filtering out by PostPatch, Validated, DateSent, SubmissionFlag
def test_filtering_logic(spark):
    # Only one row matches all filters
    rows = [
        ("A1", "C1", 0, 0, "Name1", "555-0000", "555-0000", "user1@email.com", "note", "inforce", "Accident Fund", "EU1", "UW1", "Patch", None, None, "pc@email.com", "uw@email.com", 1.0),
        ("A2", "C2", 0, 0, "Name2", "555-0000", "555-0000", "user2@email.com", "note", "inforce", "Accident Fund", "EU2", "UW2", "NotPatch", None, None, "pc@email.com", "uw@email.com", 2.0),
        ("A3", "C3", 0, 0, "Name3", "555-0000", "555-0000", "user3@email.com", "note", "inforce", "Accident Fund", "EU3", "UW3", "Patch", "Yes", None, "pc@email.com", "uw@email.com", 3.0),
        ("A4", "C4", 0, 0, "Name4", "555-0000", "555-0000", "user4@email.com", "note", "inforce", "Accident Fund", "EU4", "UW4", "Patch", None, "2022-01-01", "pc@email.com", "uw@email.com", 4.0),
        ("A5", "C5", 1, 0, "Name5", "555-0000", "555-0000", "user5@email.com", "note", "inforce", "Accident Fund", "EU5", "UW5", "Patch", None, None, "pc@email.com", "uw@email.com", 5.0),
    ]
    account_df = make_account_df(spark, rows)
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [(f"C{i}", f"RCT{i}", f"A{i}") for i in range(1, 6)])
    idoverride_df = make_idoverride_df(spark, [(f"EU{i}", "Account") for i in range(1, 6)])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    assert result[0].AccountID == "A1"

# TC15: Complex string/numeric values
def test_complex_values(spark):
    account_df = make_account_df(spark, [
        ("A10", "C10", 0, 0, "Name\nWith\"Special\\Chars", "555-0000", "555-0000", "user10@email.com", "note\nwith special chars", "inforce", "Accident Fund", "EU10", "UW10", "Patch", None, None, "pc@email.com", "uw@email.com", 10.0)
    ])
    policydescriptors_df = make_policydescriptors_df(spark, [])
    accountid_df = make_accountid_df(spark, [
        ("C10", "RCT10", "A10")
    ])
    idoverride_df = make_idoverride_df(spark, [
        ("EU10", "Account")
    ])
    register_temp_tables(spark, account_df, policydescriptors_df, accountid_df, idoverride_df)
    result_df = uspAPIPatchAccount(loop_instance=0, spark=spark)
    result = result_df.collect()
    assert len(result) == 1
    json_msg = parse_json_message(result[0].JSON_Message)
    # Should not throw, and JSON should be valid
    assert any(j['path'] == "/Name" and "Special\\Chars" in j['value'] for j in json_msg)
```

---

3. **API Cost**

- apiCost: 0.016

---
**Note:**  
- The test script above assumes the `uspAPIPatchAccount` function is imported and available in the test environment.
- The helper functions generate mock DataFrames with required schemas.
- The tests cover all described scenarios, including happy path, edge cases, and error handling.
- For brevity, only a subset of extension fields is checked in some tests; expand as needed for full coverage.
- The cost is calculated based on two tool calls (directory + file read) at $0.008 each.