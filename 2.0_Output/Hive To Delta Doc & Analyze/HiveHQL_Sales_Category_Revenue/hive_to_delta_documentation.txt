1. Script Overview

Purpose:
This Hive script calculates the top 3 product categories by total revenue for each region over the past 12 months. The objective is to provide business insights into which product categories are performing best in each region, supporting sales strategy and resource allocation.

Business Problem Addressed:
The script helps business and technical teams identify high-performing product categories by region, enabling targeted marketing, inventory planning, and revenue optimization.

Data Sources:
- sales: Contains transactional sales data (order_id, product_id, region_id, order_date, quantity, price).
- products: Contains product details (product_id, category_id).
- regions: Contains region details (region_id).

KPI Definitions:
- total_revenue: The sum of revenue (quantity * price) for each category in each region over the past 12 months.
- avg_order_value: The average revenue per order for each category in each region.
- category_rank: The rank of each category within a region based on total revenue (1 = highest).

2. Query Breakdown

Step 1: FilteredSales CTE
- Joins sales with products (on product_id) and regions (on region_id).
- Filters sales to only include orders from the past 12 months using `WHERE s.order_date >= ADD_MONTHS(CAST(CURRENT_TIMESTAMP AS DATE), -12)`.
- Calculates revenue per order line as `quantity * price`.

Step 2: CategoryRevenue CTE
- Aggregates filtered sales by region and category.
- Calculates:
  - total_revenue: SUM of revenue.
  - avg_order_value: AVG of revenue.

Step 3: RankedCategories CTE
- Ranks categories within each region by total_revenue in descending order using the RANK() window function.
- Adds a category_rank column.

Step 4: Final SELECT
- Selects region_id, category_id, total_revenue, avg_order_value, and category_rank for the top 3 categories (category_rank <= 3) in each region.
- Orders the result by region_id and category_rank.

Hive vs. Delta Syntax Differences:
- Hive uses `ADD_MONTHS` and `CAST(CURRENT_TIMESTAMP AS DATE)` for date filtering.
- Window functions and CTEs are supported in both Hive and Delta, but Delta (Databricks SQL) may use slightly different syntax for date functions and window definitions.

3. Data Mapping Details

| Destination Table | Destination Column | Source Table | Source Column | Mapping                                   |
|-------------------|-------------------|-------------|--------------|-------------------------------------------|
| (Result Set)      | region_id         | regions     | region_id    | 1 to 1                                    |
| (Result Set)      | category_id       | products    | category_id  | 1 to 1 (via join on product_id)           |
| (Result Set)      | total_revenue     | sales       | quantity, price | Transformation: SUM(quantity * price)      |
| (Result Set)      | avg_order_value   | sales       | quantity, price | Transformation: AVG(quantity * price)      |
| (Result Set)      | category_rank     | (derived)   | total_revenue | Transformation: RANK() over region_id      |

4. Complexity Metrics

- Number of Lines: 31 (including whitespace and comments)
- Tables Used: 3 (sales, products, regions)
- Joins: 2 (JOIN sales to products on product_id, JOIN sales to regions on region_id; both are INNER JOINs)
- Temporary Tables (CTEs): 3 (FilteredSales, CategoryRevenue, RankedCategories)
- Aggregate Functions: 2 (SUM, AVG), plus 1 window function (RANK)
- DML Statements: 1 (SELECT)
- Conditional Logic: 1 (WHERE clause for date filter), 1 (WHERE clause for category_rank <= 3)

5. Key Outputs

- The script outputs a result set with the following columns: region_id, category_id, total_revenue, avg_order_value, category_rank.
- The output is not stored in a physical table but is likely used for reporting or further analysis.

6. Error Handling and Optimization

Error Handling:
- No explicit error handling (e.g., TRY/CATCH) is present, as is typical in Hive SQL.
- Implicit error handling via filtering (e.g., only valid dates, only joined records).

Optimization:
- Uses CTEs for modular query design, which can help the query planner optimize execution.
- Filters data early (last 12 months only) to reduce processing volume.
- Aggregates before ranking to minimize data processed in window functions.
- No explicit partitioning or statistics collection in the script, but these may be handled at the table level.

Delta Migration Performance Improvements (if applicable):
- Delta Lake can offer faster query performance due to data skipping, caching, and optimized storage.
- Partitioning and ZORDER in Delta can further improve performance for large datasets.
- Delta supports ACID transactions, which can improve reliability for concurrent workloads.

7. API Cost Calculation

API cost for this call (including two file reads):

- Each file read is a basic metadata operation, not a data scan. For documentation purposes, assume negligible DBU usage.
- For completeness, if this script were run on Databricks (Delta), and assuming 10% of 1.5 TB sales data processed (150 GB), with a DBU cost of $0.15 per hour and a typical job duration of 5 minutes (0.083 hours):

  Estimated compute cost = 0.083 hours * $0.15 = $0.01245

  However, for this API documentation call (not a script execution), the cost is only for metadata/file reads, which is negligible.

apiCost: 0.00020 USD

(Assuming $0.00010 per file read, 2 reads performed; if actual API pricing is available, please adjust accordingly.)

---