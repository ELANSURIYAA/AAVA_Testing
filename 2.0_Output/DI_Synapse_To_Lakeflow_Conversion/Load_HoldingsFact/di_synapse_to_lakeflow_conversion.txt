=============================================
Author: Ascendion AAVA
Created on:   
Description:   Convert Azure Synapse stored procedures into equivalent Databricks Lakeflow pipelines
=============================================

Fully orchestrated Databricks Lakeflow pipeline converted from Azure Synapse stored procedures.  
All data read/write operations are implemented using standard table or view references across SQL, PySpark, and Python notebooks.  

---

# Databricks Lakeflow Pipeline: Conversion of Azure Synapse `LOAD_FACT_EXECUTIVE_SUMMARY` Stored Procedure

## Overview

This solution converts the Azure Synapse stored procedure logic from `Load_HoldingsFact.txt` into a modular, production-grade Databricks Lakeflow pipeline. The pipeline is split into three main notebooks:

- **SQL Notebook:** Handles data extraction, joins, and transformation logic.
- **PySpark Notebook:** Performs business rule enforcement and additional transformations.
- **Python Driver Notebook:** Orchestrates the workflow, manages logging, and handles audit metrics.

All data operations use Delta Lake tables/views, and the pipeline is designed for maintainability, performance, and cost efficiency, following Databricks best practices.

---

## 1. SQL Notebook: `01_load_staging_metrics.sql`

```sql
-- Databricks Lakeflow SQL Notebook
-- Step 1: Prepare staging data as a temporary view

CREATE OR REPLACE TEMP VIEW staging_metrics AS
SELECT *
FROM STG_HOLDING_METRICS;
```

---

## 2. SQL Notebook: `02_transform_and_join.sql`

```sql
-- Databricks Lakeflow SQL Notebook
-- Step 2: Join staging with dimension tables and apply business rules

CREATE OR REPLACE TEMP VIEW executive_summary_transformed AS
SELECT 
    dt.date_key,
    inst.institution_id,
    corp.corporation_id,
    prod.product_id,
    stg.a120_amount,
    stg.a120_count,
    stg.a30_to_59_amount,
    stg.a30_to_59_count,
    stg.a60_to_89_amount,
    stg.a60_to_89_count,
    stg.a90_to_119_amount,
    stg.a90_to_119_count,
    stg.charge_off_amount,
    stg.charge_off_count,
    stg.fraud_amount,
    stg.fraud_count,
    CASE 
        WHEN stg.income_amount IS NULL OR stg.income_amount < 0 THEN 0
        ELSE stg.income_amount
    END AS income_amount,
    stg.number_of_accounts,
    stg.purchases_amount,
    stg.purchases_count
FROM staging_metrics stg
INNER JOIN DIM_DATE dt ON dt.date_key = stg.date_value
INNER JOIN DIM_INSTITUTION inst ON inst.institution_id = stg.institution_id
INNER JOIN DIM_CORPORATION corp ON corp.corporation_id = stg.corporation_id
INNER JOIN DIM_PRODUCT prod ON prod.product_id = stg.product_id;
```

---

## 3. PySpark Notebook: `03_load_fact_executive_summary.py`

```python
# Databricks Lakeflow PySpark Notebook
# Step 3: Load transformed data into FACT_EXECUTIVE_SUMMARY Delta table and log audit metrics

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Read the transformed data from the temp view
df = spark.table("executive_summary_transformed")

# Write to FACT_EXECUTIVE_SUMMARY Delta table (append mode)
df.write.format("delta").mode("append").saveAsTable("FACT_EXECUTIVE_SUMMARY")

# Audit logging: Row count
row_count = df.count()
print(f"{row_count} records inserted into FACT_EXECUTIVE_SUMMARY.")

# Optionally: Clean up temp views (handled automatically in Databricks notebook scope)
```

---

## 4. Python Driver Notebook: `00_driver_orchestration.py`

```python
# Databricks Lakeflow Python Driver Notebook
# Orchestrates the pipeline execution and logs progress

from databricks import workflow

print("*** Starting LOAD_FACT_EXECUTIVE_SUMMARY Lakeflow Pipeline ***")

# Run SQL notebook to load staging data
workflow.run("01_load_staging_metrics.sql")

# Run SQL notebook to transform and join data
workflow.run("02_transform_and_join.sql")

# Run PySpark notebook to load fact table and log audit metrics
workflow.run("03_load_fact_executive_summary.py")

print("*** LOAD_FACT_EXECUTIVE_SUMMARY Lakeflow Pipeline completed successfully ***")
```

---

## Notes & Best Practices

- **Temporary Views:** Used instead of temp tables for staging and transformation.
- **Business Rules:** All CASE logic is preserved in SQL transformation.
- **Audit Logging:** Row counts are printed in the PySpark notebook for traceability.
- **Orchestration:** The Python driver notebook ensures modular, maintainable execution.
- **Performance:** Use Delta Lake for efficient writes; joins are performed in SQL for pushdown optimization.
- **Error Handling:** For production, enhance with try/except and Lakeflow error notification features.
- **Resource Cleanup:** Temp views are session-scoped and auto-dropped at notebook end.

---

## Conversion Mapping (Synapse â†’ Lakeflow)

| Synapse Construct            | Lakeflow Equivalent                              |
|-----------------------------|--------------------------------------------------|
| Stored Procedure            | Modular SQL/PySpark/Python Notebooks             |
| Temp Table (#staging)       | Temporary View (`CREATE OR REPLACE TEMP VIEW`)   |
| DECLARE/SET Variables       | Python variables / DataFrame operations          |
| PRINT                       | Python `print()` or notebook output              |
| CASE                        | SQL CASE (compatible)                            |
| Joins                       | SQL JOIN (compatible)                            |
| INSERT INTO                 | DataFrame `.write.saveAsTable()`                 |
| @@ROWCOUNT                  | DataFrame `.count()`                             |

---

## End-to-End Data Flow

1. **Extract:** Read from `STG_HOLDING_METRICS` into a temp view.
2. **Transform:** Join with dimension tables, apply CASE logic, output to another temp view.
3. **Load:** Write results to `FACT_EXECUTIVE_SUMMARY` Delta table.
4. **Audit:** Log row count and execution status.

---

apiCost: 0.0182 USD