============================================= 
Author: Ascendion AAVA 
Created on: 
Description: Load_HoldingsFact.txt to Databricks Lakeflow Conversion Test 
=============================================

**Test Case List:**

| Test Case ID | Test Case Description | Expected Outcome |
|--------------|----------------------|------------------|
| TC01 | Validate column expression mapping from Load_HoldingsFact.txt to Lakeflow notebooks | All column-level transformations match expected logic |
| TC02 | Validate aggregation logic mapping (groupBy/window functions) | Aggregated output consistent with Load_HoldingsFact.txt results |
| TC03 | Validate join strategy equivalence | Join outputs identical to Load_HoldingsFact.txt logic |
| TC04 | Validate data type conversions between Load_HoldingsFact.txt and Lakeflow | Schema matches expected PySpark/SQL types |
| TC05 | Validate null handling and case sensitivity | Nulls handled correctly and column names consistent |
| TC06 | Validate orchestration flow in Lakeflow | Notebooks execute in correct sequence with correct dependencies |
| TC07 | Validate manual interventions and performance optimizations | Adjusted logic produces correct results with improved performance |

---

**Pytest Script for Each Test Case**

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType

@pytest.fixture(scope="module")
def spark():
    return SparkSession.builder.appName("Lakeflow_Test").getOrCreate()

def load_df(spark, path):
    return spark.read.parquet(path)

def compare_dataframes(df1, df2, ignore_order=True):
    if ignore_order:
        return set(df1.collect()) == set(df2.collect())
    else:
        return df1.collect() == df2.collect()

def test_expression_mapping(spark):
    # TC01: Validate column expression mapping
    df_synapse = load_df(spark, "synapse_output.parquet")
    df_lakeflow = load_df(spark, "lakeflow_output.parquet")
    assert compare_dataframes(df_synapse.select(
        "date_key", "institution_id", "corporation_id", "product_id",
        "a120_amount", "a120_count", "a30_to_59_amount", "a30_to_59_count",
        "a60_to_89_amount", "a60_to_89_count", "a90_to_119_amount", "a90_to_119_count",
        "charge_off_amount", "charge_off_count", "fraud_amount", "fraud_count",
        "income_amount", "number_of_accounts", "purchases_amount", "purchases_count"
    ), df_lakeflow.select(
        "date_key", "institution_id", "corporation_id", "product_id",
        "a120_amount", "a120_count", "a30_to_59_amount", "a30_to_59_count",
        "a60_to_89_amount", "a60_to_89_count", "a90_to_119_amount", "a90_to_119_count",
        "charge_off_amount", "charge_off_count", "fraud_amount", "fraud_count",
        "income_amount", "number_of_accounts", "purchases_amount", "purchases_count"
    ))

def test_aggregation_logic(spark):
    # TC02: Validate aggregation logic mapping
    df_synapse_agg = load_df(spark, "synapse_agg_output.parquet")
    df_lakeflow_agg = load_df(spark, "lakeflow_agg_output.parquet")
    assert compare_dataframes(df_synapse_agg, df_lakeflow_agg)

def test_join_strategy_equivalence(spark):
    # TC03: Validate join strategy equivalence
    df_synapse = load_df(spark, "synapse_join_output.parquet")
    df_lakeflow = load_df(spark, "lakeflow_join_output.parquet")
    assert compare_dataframes(df_synapse, df_lakeflow)

def test_data_type_conversions(spark):
    # TC04: Validate data type conversions
    df_lakeflow = load_df(spark, "lakeflow_output.parquet")
    expected_schema = StructType().add("date_key", "integer").add("institution_id", "integer") \
        .add("corporation_id", "integer").add("product_id", "integer") \
        .add("a120_amount", "double").add("a120_count", "integer") \
        .add("a30_to_59_amount", "double").add("a30_to_59_count", "integer") \
        .add("a60_to_89_amount", "double").add("a60_to_89_count", "integer") \
        .add("a90_to_119_amount", "double").add("a90_to_119_count", "integer") \
        .add("charge_off_amount", "double").add("charge_off_count", "integer") \
        .add("fraud_amount", "double").add("fraud_count", "integer") \
        .add("income_amount", "double").add("number_of_accounts", "integer") \
        .add("purchases_amount", "double").add("purchases_count", "integer")
    assert df_lakeflow.schema == expected_schema

def test_null_handling_and_case_sensitivity(spark):
    # TC05: Validate null handling and case sensitivity
    df_lakeflow = load_df(spark, "lakeflow_output.parquet")
    # Check that income_amount is never null or negative
    assert df_lakeflow.filter("income_amount IS NULL OR income_amount < 0").count() == 0
    # Check column names are lower case and as expected
    expected_columns = [
        "date_key", "institution_id", "corporation_id", "product_id",
        "a120_amount", "a120_count", "a30_to_59_amount", "a30_to_59_count",
        "a60_to_89_amount", "a60_to_89_count", "a90_to_119_amount", "a90_to_119_count",
        "charge_off_amount", "charge_off_count", "fraud_amount", "fraud_count",
        "income_amount", "number_of_accounts", "purchases_amount", "purchases_count"
    ]
    assert [c.lower() for c in df_lakeflow.columns] == expected_columns

def test_orchestration_flow():
    # TC06: Validate orchestration flow in Lakeflow
    # This is a placeholder: In practice, you would check notebook execution logs or workflow status
    # For demonstration, we assert the expected execution order
    executed_notebooks = [
        "01_load_staging_metrics.sql",
        "02_transform_and_join.sql",
        "03_load_fact_executive_summary.py"
    ]
    # Simulate fetching executed notebook order from logs
    actual_executed = [
        "01_load_staging_metrics.sql",
        "02_transform_and_join.sql",
        "03_load_fact_executive_summary.py"
    ]
    assert executed_notebooks == actual_executed

def test_manual_interventions_and_performance(spark):
    # TC07: Validate manual interventions and performance optimizations
    df_lakeflow = load_df(spark, "lakeflow_output.parquet")
    # Example: Check that caching or partitioning was applied (mocked, as actual check requires Spark plan inspection)
    # Here, we check row count as a proxy for successful optimized load
    assert df_lakeflow.count() > 0
```

---

**API Cost Estimation**

apiCost: 0.0537 USD