```python
=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Python script to automate Teradata to Snowflake migration validation for employee backup logic, including connection, data transfer, execution, comparison, reporting, error handling, and security.
=============================================

import os
import sys
import logging
import traceback
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import teradatasql
import snowflake.connector
from datetime import datetime

# ------------------- CONFIGURATION -------------------
# Use environment variables for credentials
TD_HOST = os.getenv('TD_HOST')
TD_USER = os.getenv('TD_USER')
TD_PASS = os.getenv('TD_PASS')
TD_DATABASE = os.getenv('TD_DATABASE')

SF_USER = os.getenv('SF_USER')
SF_PASS = os.getenv('SF_PASS')
SF_ACCOUNT = os.getenv('SF_ACCOUNT')
SF_WAREHOUSE = os.getenv('SF_WAREHOUSE')
SF_DATABASE = os.getenv('SF_DATABASE')
SF_SCHEMA = os.getenv('SF_SCHEMA')
SF_STAGE = os.getenv('SF_STAGE', 'EMPLOYEE_STAGE')  # Default stage

LOG_FILE = os.getenv('LOG_FILE', 'migration_validation.log')

# ------------------- LOGGING SETUP -------------------
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s'
)

def log(msg):
    print(msg)
    logging.info(msg)

def log_error(msg):
    print(msg, file=sys.stderr)
    logging.error(msg)

# ------------------- UTILITY FUNCTIONS -------------------
def get_timestamp():
    return datetime.now().strftime('%Y%m%d_%H%M%S')

def safe_execute(cursor, sql, params=None):
    try:
        if params:
            cursor.execute(sql, params)
        else:
            cursor.execute(sql)
    except Exception as e:
        log_error(f"SQL Error: {sql}\n{e}")
        raise

# ------------------- TERADATA FUNCTIONS -------------------
def teradata_connect():
    try:
        conn = teradatasql.connect(
            host=TD_HOST,
            user=TD_USER,
            password=TD_PASS,
            database=TD_DATABASE
        )
        log("Connected to Teradata.")
        return conn
    except Exception as e:
        log_error(f"Teradata connection failed: {e}")
        raise

def get_td_table(cursor, table_name):
    try:
        sql = f"SELECT * FROM {table_name}"
        safe_execute(cursor, sql)
        rows = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        df = pd.DataFrame(rows, columns=columns)
        log(f"Fetched {len(df)} rows from Teradata table {table_name}.")
        return df
    except Exception as e:
        log_error(f"Failed to fetch table {table_name} from Teradata: {e}")
        raise

def export_td_table_to_parquet(cursor, table_name, export_dir):
    df = get_td_table(cursor, table_name)
    parquet_file = os.path.join(
        export_dir, f"{table_name}_{get_timestamp()}.parquet"
    )
    table = pa.Table.from_pandas(df)
    pq.write_table(table, parquet_file)
    log(f"Exported Teradata table {table_name} to {parquet_file}.")
    return parquet_file

# ------------------- SNOWFLAKE FUNCTIONS -------------------
def snowflake_connect():
    try:
        conn = snowflake.connector.connect(
            user=SF_USER,
            password=SF_PASS,
            account=SF_ACCOUNT,
            warehouse=SF_WAREHOUSE,
            database=SF_DATABASE,
            schema=SF_SCHEMA
        )
        log("Connected to Snowflake.")
        return conn
    except Exception as e:
        log_error(f"Snowflake connection failed: {e}")
        raise

def put_file_to_snowflake_stage(cursor, parquet_file, stage):
    try:
        sql = f"PUT file://{parquet_file} @{stage} AUTO_COMPRESS=FALSE"
        safe_execute(cursor, sql)
        log(f"PUT file {parquet_file} to Snowflake stage {stage}.")
    except Exception as e:
        log_error(f"Failed to PUT file to Snowflake: {e}")
        raise

def create_external_table(cursor, ext_table_name, parquet_file, schema):
    try:
        sql = f"""
        CREATE OR REPLACE EXTERNAL TABLE {ext_table_name} (
            {schema}
        )
        LOCATION=@{SF_STAGE}/{os.path.basename(parquet_file)}
        FILE_FORMAT=(TYPE=PARQUET)
        AUTO_REFRESH = FALSE;
        """
        safe_execute(cursor, sql)
        log(f"Created external table {ext_table_name} in Snowflake.")
    except Exception as e:
        log_error(f"Failed to create external table: {e}")
        raise

def run_snowflake_sql(cursor, sql):
    try:
        safe_execute(cursor, sql)
        log(f"Executed Snowflake SQL:\n{sql}")
    except Exception as e:
        log_error(f"Failed to execute Snowflake SQL: {e}")
        raise

def fetch_sf_table(cursor, table_name):
    try:
        sql = f"SELECT * FROM {table_name}"
        safe_execute(cursor, sql)
        rows = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        df = pd.DataFrame(rows, columns=columns)
        log(f"Fetched {len(df)} rows from Snowflake table {table_name}.")
        return df
    except Exception as e:
        log_error(f"Failed to fetch table {table_name} from Snowflake: {e}")
        raise

# ------------------- COMPARISON FUNCTIONS -------------------
def compare_tables(df_td, df_sf, table_name):
    report = {}
    try:
        td_count = len(df_td)
        sf_count = len(df_sf)
        report['table'] = table_name
        report['td_row_count'] = td_count
        report['sf_row_count'] = sf_count
        report['row_count_match'] = td_count == sf_count

        # Compare columns
        td_cols = set(df_td.columns)
        sf_cols = set(df_sf.columns)
        report['column_match'] = td_cols == sf_cols
        report['td_columns'] = list(td_cols)
        report['sf_columns'] = list(sf_cols)
        col_discrepancies = list(td_cols.symmetric_difference(sf_cols))
        report['column_discrepancies'] = col_discrepancies

        # Data comparison
        match_rows = 0
        mismatches = []
        min_rows = min(td_count, sf_count)
        for i in range(min_rows):
            td_row = df_td.iloc[i].to_dict()
            sf_row = df_sf.iloc[i].to_dict()
            if td_row == sf_row:
                match_rows += 1
            else:
                mismatches.append({'td': td_row, 'sf': sf_row})

        match_pct = (match_rows / min_rows) * 100 if min_rows > 0 else 100
        report['match_percentage'] = match_pct
        if match_pct == 100 and report['row_count_match'] and report['column_match']:
            report['status'] = 'MATCH'
        elif match_pct > 0:
            report['status'] = 'PARTIAL MATCH'
        else:
            report['status'] = 'NO MATCH'
        report['sample_mismatches'] = mismatches[:5]
        return report
    except Exception as e:
        log_error(f"Error comparing tables: {e}")
        report['error'] = str(e)
        report['status'] = 'ERROR'
        return report

# ------------------- MAIN SCRIPT -------------------
def main():
    try:
        log("Starting Teradata to Snowflake migration validation process.")
        export_dir = os.path.abspath('td_exports')
        os.makedirs(export_dir, exist_ok=True)

        # 1. Connect to Teradata
        td_conn = teradata_connect()
        td_cursor = td_conn.cursor()

        # 2. Execute Teradata SQL logic (simulate BTEQ flow)
        # Drop and create employee_bkup, then insert join results
        safe_execute(td_cursor, "DROP TABLE employee_bkup")
        safe_execute(td_cursor, """
            CREATE TABLE employee_bkup (
                EmployeeNo INTEGER,
                FirstName CHAR(30),
                LastName CHAR(30),
                DepartmentNo SMALLINT,
                NetPay INTEGER
            )
            UNIQUE PRIMARY INDEX(EmployeeNo)
        """)
        safe_execute(td_cursor, """
            INSERT INTO employee_bkup
            SELECT a.EmployeeNo, a.FirstName, a.LastName, a.DepartmentNo, b.NetPay
            FROM Employee a INNER JOIN Salary b
            ON a.EmployeeNo = b.EmployeeNo
        """)

        # 3. Export employee_bkup to Parquet
        parquet_file = export_td_table_to_parquet(td_cursor, 'employee_bkup', export_dir)

        # 4. Connect to Snowflake
        sf_conn = snowflake_connect()
        sf_cursor = sf_conn.cursor()

        # 5. PUT Parquet file to Snowflake stage
        put_file_to_snowflake_stage(sf_cursor, parquet_file, SF_STAGE)

        # 6. Create external table in Snowflake
        # Schema: EmployeeNo INTEGER, FirstName VARCHAR(30), LastName VARCHAR(30), DepartmentNo SMALLINT, NetPay INTEGER
        ext_table_name = "employee_bkup_ext"
        schema = """
            EmployeeNo INTEGER,
            FirstName VARCHAR(30),
            LastName VARCHAR(30),
            DepartmentNo SMALLINT,
            NetPay INTEGER
        """
        create_external_table(sf_cursor, ext_table_name, parquet_file, schema)

        # 7. Execute Snowflake SQL logic (from provided code)
        snowflake_sql = """
        DROP TABLE IF EXISTS employee_bkup;
        CREATE OR REPLACE TABLE employee_bkup (
            EmployeeNo INTEGER,
            FirstName VARCHAR(30),
            LastName VARCHAR(30),
            DepartmentNo SMALLINT,
            NetPay INTEGER
        );
        INSERT INTO employee_bkup (
            EmployeeNo, FirstName, LastName, DepartmentNo, NetPay
        )
        SELECT a.EmployeeNo, a.FirstName, a.LastName, a.DepartmentNo, b.NetPay
        FROM Employee a
        INNER JOIN Salary b
        ON a.EmployeeNo = b.EmployeeNo;
        """
        for stmt in snowflake_sql.strip().split(';'):
            if stmt.strip():
                run_snowflake_sql(sf_cursor, stmt.strip())

        # 8. Fetch data from external and target tables
        df_td_ext = fetch_sf_table(sf_cursor, ext_table_name)
        df_sf = fetch_sf_table(sf_cursor, 'employee_bkup')

        # 9. Compare tables and generate report
        report = compare_tables(df_td_ext, df_sf, 'employee_bkup')
        summary = {
            'table': report['table'],
            'status': report['status'],
            'row_count_match': report['row_count_match'],
            'column_match': report['column_match'],
            'match_percentage': report['match_percentage'],
            'column_discrepancies': report['column_discrepancies'],
            'sample_mismatches': report['sample_mismatches']
        }
        log(f"Comparison Report: {summary}")

        # 10. Save report
        report_file = f"comparison_report_{get_timestamp()}.json"
        pd.DataFrame([summary]).to_json(report_file, orient='records', lines=True)
        log(f"Saved comparison report to {report_file}")

        # 11. Cleanup
        td_cursor.close()
        td_conn.close()
        sf_cursor.close()
        sf_conn.close()
        log("Migration validation process completed successfully.")

    except Exception as e:
        log_error(f"Fatal error: {e}\n{traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```