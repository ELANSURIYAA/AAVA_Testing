```
=============================================
Author:        Ascendion AVA+
Created on:    
Description:   Automates Teradata-to-Snowflake migration validation by executing Teradata BTEQ and Snowflake SQL, exporting data, transferring via Parquet, and performing comprehensive output comparison with robust error handling and reporting.
=============================================

import os
import sys
import re
import csv
import logging
import tempfile
import traceback
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

from datetime import datetime

# Optional: Install missing packages if running in a fresh environment
# import subprocess
# for pkg in ['teradatasql', 'snowflake-connector-python', 'pyarrow', 'pandas']:
#     subprocess.run([sys.executable, '-m', 'pip', 'install', pkg])

import teradatasql
import snowflake.connector

# ------------------ CONFIGURATION ------------------

# Environment variable keys
TD_HOST_ENV      = 'TD_HOST'
TD_USER_ENV      = 'TD_USER'
TD_PASS_ENV      = 'TD_PASS'
TD_DB_ENV        = 'TD_DB'

SF_ACCOUNT_ENV   = 'SF_ACCOUNT'
SF_USER_ENV      = 'SF_USER'
SF_PASS_ENV      = 'SF_PASS'
SF_WAREHOUSE_ENV = 'SF_WAREHOUSE'
SF_DB_ENV        = 'SF_DB'
SF_SCHEMA_ENV    = 'SF_SCHEMA'
SF_STAGE_ENV     = 'SF_STAGE'  # e.g., '@my_stage'

# Logging
LOG_FILE = f"migration_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    filename=LOG_FILE,
    filemode='w',
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s'
)
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
console.setFormatter(formatter)
logging.getLogger('').addHandler(console)

# ------------------ UTILITY FUNCTIONS ------------------

def log_and_raise(msg, exc=None):
    logging.error(msg)
    if exc:
        logging.error(traceback.format_exc())
    raise Exception(msg)

def get_env(var, required=True):
    val = os.getenv(var)
    if required and not val:
        log_and_raise(f"Environment variable {var} is required but not set.")
    return val

def parse_teradata_bteq(bteq_code):
    """
    Parse Teradata BTEQ code to extract table names and SQL statements.
    Returns: dict with 'tables', 'create_table', 'insert', etc.
    """
    # Remove comments and normalize whitespace
    code = re.sub(r'--.*', '', bteq_code)
    code = re.sub(r'\s+', ' ', code)
    # Find CREATE TABLE
    create_table = re.search(r'CREATE TABLE\s+(\w+)\s*\((.*?)\)\s*Unique Primary Index\((.*?)\);', code, re.IGNORECASE)
    # Find INSERT INTO
    insert_stmt = re.search(r'INSERT INTO\s+(\w+)\s+SELECT(.*?)FROM(.*?);', code, re.IGNORECASE)
    # Find all tables used
    tables = set(re.findall(r'FROM\s+(\w+)', code, re.IGNORECASE))
    tables.update(re.findall(r'JOIN\s+(\w+)', code, re.IGNORECASE))
    tables.update(re.findall(r'INTO\s+(\w+)', code, re.IGNORECASE))
    return {
        'tables': list(tables),
        'create_table': create_table.group(0) if create_table else None,
        'insert_stmt': insert_stmt.group(0) if insert_stmt else None,
        'target_table': create_table.group(1) if create_table else None
    }

def parse_snowflake_sql(sql_code):
    """
    Parse Snowflake SQL code to extract table names and SQL statements.
    Returns: dict with 'tables', 'create_table', 'insert', etc.
    """
    code = re.sub(r'--.*', '', sql_code)
    code = re.sub(r'\s+', ' ', code)
    create_table = re.search(r'CREATE TABLE\s+(\w+)\s*\((.*?)\);', code, re.IGNORECASE)
    insert_stmt = re.search(r'INSERT INTO\s+(\w+)\s+SELECT(.*?)FROM(.*?);', code, re.IGNORECASE)
    tables = set(re.findall(r'FROM\s+(\w+)', code, re.IGNORECASE))
    tables.update(re.findall(r'JOIN\s+(\w+)', code, re.IGNORECASE))
    tables.update(re.findall(r'INTO\s+(\w+)', code, re.IGNORECASE))
    return {
        'tables': list(tables),
        'create_table': create_table.group(0) if create_table else None,
        'insert_stmt': insert_stmt.group(0) if insert_stmt else None,
        'target_table': create_table.group(1) if create_table else None
    }

def teradata_connect():
    """Establish a secure Teradata connection using environment variables."""
    host = get_env(TD_HOST_ENV)
    user = get_env(TD_USER_ENV)
    pwd  = get_env(TD_PASS_ENV)
    db   = get_env(TD_DB_ENV)
    try:
        con = teradatasql.connect(host=host, user=user, password=pwd, database=db)
        logging.info("Connected to Teradata.")
        return con
    except Exception as e:
        log_and_raise("Teradata connection failed.", e)

def snowflake_connect():
    """Establish a secure Snowflake connection using environment variables."""
    try:
        con = snowflake.connector.connect(
            account=get_env(SF_ACCOUNT_ENV),
            user=get_env(SF_USER_ENV),
            password=get_env(SF_PASS_ENV),
            warehouse=get_env(SF_WAREHOUSE_ENV),
            database=get_env(SF_DB_ENV),
            schema=get_env(SF_SCHEMA_ENV)
        )
        logging.info("Connected to Snowflake.")
        return con
    except Exception as e:
        log_and_raise("Snowflake connection failed.", e)

def export_teradata_table_to_csv(con, table, csv_path):
    """Export a Teradata table to CSV using pandas for robust handling."""
    try:
        sql = f"SELECT * FROM {table}"
        df = pd.read_sql(sql, con)
        df.to_csv(csv_path, index=False, quoting=csv.QUOTE_NONNUMERIC)
        logging.info(f"Exported Teradata table {table} to CSV: {csv_path} (rows: {len(df)})")
        return df
    except Exception as e:
        log_and_raise(f"Failed to export Teradata table {table} to CSV.", e)

def csv_to_parquet(csv_path, parquet_path):
    """Convert CSV to Parquet for efficient transfer and Snowflake compatibility."""
    try:
        df = pd.read_csv(csv_path)
        table = pa.Table.from_pandas(df)
        pq.write_table(table, parquet_path)
        logging.info(f"Converted CSV {csv_path} to Parquet: {parquet_path}")
    except Exception as e:
        log_and_raise(f"Failed to convert CSV to Parquet: {csv_path}", e)

def snowflake_put_file(con, local_path, stage_path):
    """Upload Parquet file to Snowflake stage and verify integrity."""
    try:
        cs = con.cursor()
        put_sql = f"PUT file://{local_path} {stage_path} OVERWRITE = TRUE"
        cs.execute(put_sql)
        result = cs.fetchall()
        logging.info(f"PUT result: {result}")
        # Check for successful upload
        if not any('UPLOADED' in str(row) or 'SKIPPED' in str(row) for row in result):
            log_and_raise(f"Snowflake PUT failed for {local_path}")
    except Exception as e:
        log_and_raise(f"Snowflake PUT failed for {local_path}", e)
    finally:
        cs.close()

def create_external_table(con, ext_table_name, parquet_stage_path, schema):
    """Create a Snowflake external table pointing to the Parquet file."""
    try:
        cs = con.cursor()
        # Drop if exists
        cs.execute(f"DROP TABLE IF EXISTS {ext_table_name}")
        # Build column definitions
        col_defs = ', '.join([f"{col} {dtype}" for col, dtype in schema.items()])
        sql = (
            f"CREATE EXTERNAL TABLE {ext_table_name} ({col_defs}) "
            f"WITH LOCATION = '{parquet_stage_path}' "
            f"FILE_FORMAT = (TYPE = 'PARQUET')"
        )
        cs.execute(sql)
        logging.info(f"Created external table {ext_table_name} on {parquet_stage_path}")
    except Exception as e:
        log_and_raise(f"Failed to create external table {ext_table_name}", e)
    finally:
        cs.close()

def execute_snowflake_sql(con, sql_code):
    """Execute Snowflake SQL code, statement by statement."""
    cs = con.cursor()
    try:
        for stmt in [s.strip() for s in sql_code.split(';') if s.strip()]:
            cs.execute(stmt)
            logging.info(f"Executed Snowflake SQL: {stmt[:80]}")
    except Exception as e:
        log_and_raise("Failed to execute Snowflake SQL.", e)
    finally:
        cs.close()

def fetch_table_to_df(con, table):
    """Fetch a Snowflake table to pandas DataFrame."""
    try:
        sql = f"SELECT * FROM {table}"
        df = pd.read_sql(sql, con)
        logging.info(f"Fetched {len(df)} rows from {table}")
        return df
    except Exception as e:
        log_and_raise(f"Failed to fetch table {table} from Snowflake.", e)

def compare_dataframes(df1, df2, key_columns=None):
    """
    Compare two DataFrames: row count, column-by-column, match percentage.
    Handles nulls, data types, and large data.
    Returns: dict with match status, discrepancies, and samples.
    """
    report = {}
    report['row_count_1'] = len(df1)
    report['row_count_2'] = len(df2)
    report['row_count_match'] = len(df1) == len(df2)
    report['columns_1'] = list(df1.columns)
    report['columns_2'] = list(df2.columns)
    report['columns_match'] = set(df1.columns) == set(df2.columns)
    mismatches = []
    match_count = 0
    total = len(df1)
    if total == 0:
        report['match_percentage'] = 100.0 if len(df2) == 0 else 0.0
        report['mismatches'] = []
        return report
    # Sort both DataFrames by key columns if provided, else by all columns
    sort_cols = key_columns if key_columns else list(df1.columns)
    df1_sorted = df1.sort_values(by=sort_cols).reset_index(drop=True)
    df2_sorted = df2.sort_values(by=sort_cols).reset_index(drop=True)
    # Compare row by row
    for i in range(min(len(df1_sorted), len(df2_sorted))):
        row1 = df1_sorted.iloc[i]
        row2 = df2_sorted.iloc[i]
        row_match = True
        row_diff = {}
        for col in df1.columns:
            v1 = row1[col]
            v2 = row2[col]
            # Handle nulls and type differences
            if pd.isnull(v1) and pd.isnull(v2):
                continue
            if (isinstance(v1, float) or isinstance(v2, float)) and abs(float(v1) - float(v2)) < 1e-6:
                continue
            if str(v1) != str(v2):
                row_match = False
                row_diff[col] = (v1, v2)
        if row_match:
            match_count += 1
        else:
            mismatches.append({'row': i, 'diff': row_diff})
        if len(mismatches) > 10:
            break  # Limit sample mismatches
    report['match_percentage'] = round(100.0 * match_count / total, 2)
    report['mismatches'] = mismatches
    return report

def generate_report(comparison_result, output_path):
    """Generate a detailed comparison report as CSV and log summary."""
    try:
        with open(output_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Metric', 'Value'])
            for k, v in comparison_result.items():
                writer.writerow([k, str(v)])
        logging.info(f"Comparison report generated: {output_path}")
    except Exception as e:
        log_and_raise("Failed to generate comparison report.", e)

# ------------------ MAIN MIGRATION VALIDATION LOGIC ------------------

def main():
    # --- 1. Parse code and extract structure ---
    TERADATA_BTEQ_CODE = '''.LOGON 192.168.1.102/dbc,dbc; 
   DATABASE tduser;
   CREATE TABLE employee_bkup (EmployeeNo INTEGER,FirstName CHAR(30),LastName CHAR(30),DepartmentNo SMALLINT,NetPay INTEGER 
   )Unique Primary Index(EmployeeNo);.IF ERRORCODE <> 0 THEN .EXIT ERRORCODE;  
   SELECT * FROM  
   EmployeeSample 1;.IF ACTIVITYCOUNT <> 0 THEN .GOTO InsertEmployee;  

   DROP TABLE employee_bkup;.IF ERRORCODE <> 0 THEN .EXIT ERRORCODE;.LABEL InsertEmployee 
   INSERT INTO employee_bkup 
   SELECT a.EmployeeNo, 
      a.FirstName, 
      a.LastName, 
      a.DepartmentNo, 
      b.NetPay 
   FROM  
   Employee a INNER JOIN Salary b 
   ON (a.EmployeeNo = b.EmployeeNo);.IF ERRORCODE <> 0 THEN .EXIT ERRORCODE;.LOGOFF;'''
    SNOWFLAKE_SQL_CODE = '''-- Drop backup table if it exists
DROP TABLE IF EXISTS employee_bkup;
-- Create backup table
CREATE TABLE employee_bkup (
    EmployeeNo INTEGER,
    FirstName VARCHAR(30),
    LastName VARCHAR(30),
    DepartmentNo SMALLINT,
    NetPay INTEGER
);
-- Insert data into backup table from Employee and Salary tables
INSERT INTO employee_bkup
SELECT
    a.EmployeeNo,
    a.FirstName,
    a.LastName,
    a.DepartmentNo,
    b.NetPay
FROM
    Employee a
    INNER JOIN Salary b
        ON a.EmployeeNo = b.EmployeeNo;'''

    logging.info("Parsing Teradata BTEQ code...")
    td_info = parse_teradata_bteq(TERADATA_BTEQ_CODE)
    logging.info(f"Teradata tables: {td_info['tables']}, Target: {td_info['target_table']}")

    logging.info("Parsing Snowflake SQL code...")
    sf_info = parse_snowflake_sql(SNOWFLAKE_SQL_CODE)
    logging.info(f"Snowflake tables: {sf_info['tables']}, Target: {sf_info['target_table']}")

    target_table = td_info['target_table'] or sf_info['target_table'] or 'employee_bkup'
    # Define schema for external table (from CREATE TABLE)
    schema = {
        'EmployeeNo': 'INTEGER',
        'FirstName': 'VARCHAR(30)',
        'LastName': 'VARCHAR(30)',
        'DepartmentNo': 'SMALLINT',
        'NetPay': 'INTEGER'
    }

    # --- 2. Connect to Teradata and Snowflake ---
    td_con = teradata_connect()
    sf_con = snowflake_connect()

    # --- 3. Execute Teradata logic and export target table ---
    # For production, run BTEQ logic via orchestration or manual translation
    # Here, we assume the Teradata table exists and is populated
    with tempfile.TemporaryDirectory() as tmpdir:
        csv_path = os.path.join(tmpdir, f"{target_table}_teradata.csv")
        parquet_path = os.path.join(tmpdir, f"{target_table}_teradata.parquet")

        # Export Teradata table to CSV and convert to Parquet
        td_df = export_teradata_table_to_csv(td_con, target_table, csv_path)
        csv_to_parquet(csv_path, parquet_path)

        # --- 4. Transfer Parquet to Snowflake stage ---
        sf_stage = get_env(SF_STAGE_ENV)
        parquet_stage_path = f"{sf_stage}/{os.path.basename(parquet_path)}"
        snowflake_put_file(sf_con, parquet_path, sf_stage)

        # --- 5. Create external table in Snowflake ---
        ext_table_name = f"{target_table}_ext"
        create_external_table(sf_con, ext_table_name, parquet_stage_path, schema)

        # --- 6. Execute Snowflake SQL code ---
        execute_snowflake_sql(sf_con, SNOWFLAKE_SQL_CODE)

        # --- 7. Compare external table and Snowflake output ---
        ext_df = fetch_table_to_df(sf_con, ext_table_name)
        sf_df = fetch_table_to_df(sf_con, target_table)
        comparison = compare_dataframes(ext_df, sf_df, key_columns=['EmployeeNo'])

        # --- 8. Generate detailed comparison report ---
        report_path = os.path.join(tmpdir, f"comparison_report_{target_table}.csv")
        generate_report(comparison, report_path)

        # --- 9. Output structured results for downstream parsing ---
        print("==== MIGRATION VALIDATION RESULT ====")
        print(f"Target Table: {target_table}")
        print(f"External Table: {ext_table_name}")
        print(f"Row Count Teradata: {comparison['row_count_1']}")
        print(f"Row Count Snowflake: {comparison['row_count_2']}")
        print(f"Row Count Match: {comparison['row_count_match']}")
        print(f"Column Match: {comparison['columns_match']}")
        print(f"Match Percentage: {comparison['match_percentage']}%")
        print(f"Sample Mismatches: {comparison['mismatches'][:3]}")
        print(f"Report Path: {report_path}")
        print("==== END OF REPORT ====")

    # --- 10. Close connections ---
    td_con.close()
    sf_con.close()
    logging.info("All connections closed. Migration validation complete.")

# ------------------ ENTRY POINT ------------------

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.error(f"Migration validation failed: {e}")
        sys.exit(1)
```