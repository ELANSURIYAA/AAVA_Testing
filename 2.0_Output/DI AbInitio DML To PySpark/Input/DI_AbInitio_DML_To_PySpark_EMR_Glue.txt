# pyspark.sql.types schema definitions for Ab Initio DML files
# Cleaned and reusable StructType objects for PySpark pipelines

from pyspark.sql.types import StructType, StructField, StringType, DecimalType, DateType

# Cleaned and Enriched Transaction Data
enriched_schema = StructType([
    StructField("txn_id", DecimalType(10, 0), True),
    StructField("store_id", StringType(), True),
    StructField("txn_date", DateType(), True),
    StructField("product_sku", StringType(), True),
    StructField("category", StringType(), True),
    StructField("total_amount", DecimalType(10, 2), True),
    StructField("standard_cost", DecimalType(10, 2), True),
    StructField("tax_amount", DecimalType(10, 2), True),
    StructField("final_bill", DecimalType(10, 2), True),
    StructField("loyalty_points", DecimalType(5, 0), True)
])

# Product Dimension Table
product_dimension_schema = StructType([
    StructField("product_sku", StringType(), True),
    StructField("product_name", StringType(), True),
    StructField("category", StringType(), True),
    StructField("sub_category", StringType(), True),
    StructField("standard_cost", DecimalType(10, 2), True)
])

# Raw Transaction Data
raw_input_schema = StructType([
    StructField("txn_id", StringType(), True),
    StructField("store_id", StringType(), True),
    StructField("txn_date_str", StringType(), True),
    StructField("customer_id", StringType(), True),
    StructField("product_sku", StringType(), True),
    StructField("quantity_str", StringType(), True),
    StructField("unit_price_str", StringType(), True),
    StructField("payment_type", StringType(), True)
])

# Daily Store Aggregation Report
summary_schema = StructType([
    StructField("store_id", StringType(), True),
    StructField("report_date", DateType(), True),
    StructField("total_gross_sales", DecimalType(15, 2), True),
    StructField("total_tax_collected", DecimalType(15, 2), True),
    StructField("total_transaction_count", DecimalType(10, 0), True)
])
