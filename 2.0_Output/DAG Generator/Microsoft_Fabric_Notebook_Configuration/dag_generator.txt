"""
Airflow DAG for Orchestrating Microsoft Fabric Notebooks (Bronze/Silver/Gold Layers)
====================================================================================

This DAG orchestrates the ETL pipeline from source to bronze, bronze to silver, and silver to gold layers using Azure Databricks notebooks.
It is scheduled to run daily at 4 AM EST, with robust error handling, monitoring, and alerting.

Author: Data Engineering Team
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.utils.email import send_email
from airflow.utils.dates import days_ago
from airflow.models import Variable
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
import logging

# Constants
DAG_ID = "fabric_bronze_silver_gold_etl"
DAG_DESCRIPTION = """
ETL pipeline for Microsoft Fabric data lakehouse: orchestrates Databricks notebooks for source → bronze → silver → gold layers.
Handles extraction, transformation, and enrichment of customer, branch, and geocode data.
"""
SCHEDULE_INTERVAL = "0 4 * * *"  # 4 AM EST
TIMEZONE = "America/New_York"
MAX_CONCURRENT_RUNS = 1
TIMEOUT_SECONDS = 43200  # 12 hours
RETRY_ATTEMPTS = 3
RETRY_DELAY = timedelta(minutes=15)
NOTIFICATION_EMAIL = "data-pipeline-alerts@company.com"

# Databricks connection and environment variables (should be set in Airflow Variables or Connections)
DATABRICKS_CONN_ID = "databricks_default"  # Airflow connection with workspace host/token
DATABRICKS_CLUSTER_ID = Variable.get("DATABRICKS_CLUSTER_ID", default_var="0123-456789-abcd123")
DATABRICKS_WORKSPACE_URL = Variable.get("DATABRICKS_WORKSPACE_URL", default_var="https://adb-workspace.azuredatabricks.net")

# Notebook paths (relative to Databricks workspace root)
NOTEBOOKS = {
    "01_SRC_TO_BRONZE_CUSTOMER_MTH": "/notebooks/01_SRC_TO_BRONZE_CUSTOMER_MTH",
    "02_SRC_TO_BRONZE_BRANCH_HRCY": "/notebooks/02_SRC_TO_BRONZE_BRANCH_HRCY",
    "03_SRC_TO_BRONZE_GEO_CUSTOMER": "/notebooks/03_SRC_TO_BRONZE_GEO_CUSTOMER",
    "04_SRC_TO_BRONZE_GEOCODE": "/notebooks/04_SRC_TO_BRONZE_GEOCODE",
    "05_BRONZE_TO_SILVER_CUSTOMER": "/notebooks/05_BRONZE_TO_SILVER_CUSTOMER",
    "06_BRONZE_TO_SILVER_BRANCH": "/notebooks/06_BRONZE_TO_SILVER_BRANCH",
    "07_BRONZE_TO_SILVER_GEO": "/notebooks/07_BRONZE_TO_SILVER_GEO",
    "08_BRONZE_TO_SILVER_GEOCODE": "/notebooks/08_BRONZE_TO_SILVER_GEOCODE",
    "09_SILVER_TO_GOLD_CUSTOMER_GEO": "/notebooks/09_SILVER_TO_GOLD_CUSTOMER_GEO",
    "10_SILVER_TO_GOLD_BRANCH_MAPPING": "/notebooks/10_SILVER_TO_GOLD_BRANCH_MAPPING",
    "11_SILVER_TO_GOLD_GEO_ENRICHMENT": "/notebooks/11_SILVER_TO_GOLD_GEO_ENRICHMENT",
    "12_SILVER_TO_GOLD_TAMBR_RINGS": "/notebooks/12_SILVER_TO_GOLD_TAMBR_RINGS",
}

# Default arguments for the DAG
default_args = {
    "owner": "data-engineering",
    "depends_on_past": False,
    "email": [NOTIFICATION_EMAIL],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": RETRY_ATTEMPTS,
    "retry_delay": RETRY_DELAY,
    "start_date": days_ago(1),
    "execution_timeout": timedelta(seconds=TIMEOUT_SECONDS),
}

def failure_callback(context):
    """
    Send email notification on task failure.
    """
    subject = f"[Airflow] DAG {context['dag'].dag_id} - Task Failed"
    html_content = f"""
    DAG: {context['dag'].dag_id}<br>
    Task: {context['task_instance'].task_id}<br>
    Execution Time: {context['execution_date']}<br>
    Log: <a href="{context['task_instance'].log_url}">Log Link</a>
    """
    send_email(NOTIFICATION_EMAIL, subject, html_content)

with DAG(
    dag_id=DAG_ID,
    description=DAG_DESCRIPTION,
    schedule_interval=SCHEDULE_INTERVAL,
    default_args=default_args,
    max_active_runs=MAX_CONCURRENT_RUNS,
    catchup=False,
    tags=["fabric", "bronze", "silver", "gold", "databricks"],
    on_failure_callback=failure_callback,
    doc_md=DAG_DESCRIPTION,
) as dag:

    # 1. Source to Bronze Layer
    src_to_bronze_customer = DatabricksRunNowOperator(
        task_id="src_to_bronze_customer",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["01_SRC_TO_BRONZE_CUSTOMER_MTH"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=6),
    )

    src_to_bronze_branch = DatabricksRunNowOperator(
        task_id="src_to_bronze_branch",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["02_SRC_TO_BRONZE_BRANCH_HRCY"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=6),
    )

    src_to_bronze_geo_customer = DatabricksRunNowOperator(
        task_id="src_to_bronze_geo_customer",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["03_SRC_TO_BRONZE_GEO_CUSTOMER"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=6),
    )

    src_to_bronze_geocode = DatabricksRunNowOperator(
        task_id="src_to_bronze_geocode",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["04_SRC_TO_BRONZE_GEOCODE"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=6),
    )

    # 2. Bronze to Silver Layer
    bronze_to_silver_customer = DatabricksRunNowOperator(
        task_id="bronze_to_silver_customer",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["05_BRONZE_TO_SILVER_CUSTOMER"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=4),
    )

    bronze_to_silver_branch = DatabricksRunNowOperator(
        task_id="bronze_to_silver_branch",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["06_BRONZE_TO_SILVER_BRANCH"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=4),
    )

    bronze_to_silver_geo = DatabricksRunNowOperator(
        task_id="bronze_to_silver_geo",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["07_BRONZE_TO_SILVER_GEO"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=4),
    )

    bronze_to_silver_geocode = DatabricksRunNowOperator(
        task_id="bronze_to_silver_geocode",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["08_BRONZE_TO_SILVER_GEOCODE"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=4),
    )

    # 3. Silver to Gold Layer
    silver_to_gold_customer_geo = DatabricksRunNowOperator(
        task_id="silver_to_gold_customer_geo",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["09_SILVER_TO_GOLD_CUSTOMER_GEO"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=2),
    )

    silver_to_gold_branch_mapping = DatabricksRunNowOperator(
        task_id="silver_to_gold_branch_mapping",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["10_SILVER_TO_GOLD_BRANCH_MAPPING"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=2),
    )

    silver_to_gold_geo_enrichment = DatabricksRunNowOperator(
        task_id="silver_to_gold_geo_enrichment",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["11_SILVER_TO_GOLD_GEO_ENRICHMENT"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=2),
    )

    silver_to_gold_tambr_rings = DatabricksRunNowOperator(
        task_id="silver_to_gold_tambr_rings",
        databricks_conn_id=DATABRICKS_CONN_ID,
        job_id=None,
        notebook_params={},
        notebook_path=NOTEBOOKS["12_SILVER_TO_GOLD_TAMBR_RINGS"],
        existing_cluster_id=DATABRICKS_CLUSTER_ID,
        timeout_seconds=TIMEOUT_SECONDS,
        on_failure_callback=failure_callback,
        sla=timedelta(hours=2),
    )

    # Task Dependencies
    # Source to Bronze → Bronze to Silver
    src_to_bronze_customer >> bronze_to_silver_customer
    src_to_bronze_branch >> bronze_to_silver_branch
    src_to_bronze_geo_customer >> bronze_to_silver_geo
    src_to_bronze_geocode >> bronze_to_silver_geocode

    # Bronze to Silver → Silver to Gold
    bronze_to_silver_customer >> silver_to_gold_customer_geo
    bronze_to_silver_geo >> silver_to_gold_customer_geo
    bronze_to_silver_branch >> silver_to_gold_branch_mapping
    bronze_to_silver_geocode >> silver_to_gold_geo_enrichment

    # Silver to Gold → Final Gold Layer
    [silver_to_gold_customer_geo, silver_to_gold_branch_mapping, silver_to_gold_geo_enrichment] >> silver_to_gold_tambr_rings

    # Optionally, add data quality checks or monitoring tasks here if needed

    # Example logging
    logging.info("Fabric Bronze/Silver/Gold ETL DAG loaded successfully.")

# End of DAG