{
  "EDW_CC_Load_LossBodyParts.dtsx": {
    "1. Script Overview": {
      "Summary": "The SSIS package 'EDW_CC_Load_LossBodyParts.dtsx' orchestrates ETL for loss body part data from ClaimCenter to the EDW, integrating injury and claim details for regulatory reporting and analytics. It ensures data consistency, traceability, and supports compliance monitoring.",
      "Functional Modules": [
        "DATASET Declarations: OLE DB Source for extracting claim, incident, and body part data.",
        "TRANSFORMs: Lookup (LossBodyParts, ClaimId), Derived Column (BatchID, BeanVersion), Conditional Split (change detection), Row Count.",
        "PROJECTS: SQL SELECT statements for data extraction and enrichment.",
        "OUTPUTs: OLE DB Destination for LossBodyParts, Update and Insert operations."
      ],
      "Data Pipelines Overview": "Data is extracted from ClaimCenter, joined with reference tables for enrichment, transformed for batch and versioning, routed by change detection, and loaded into the EDW. Metrics are tracked for inserts, updates, and unchanged records."
    },
    "2. Complexity Metrics": {
      "Total Lines of Code": 480,
      "Dataset Count": 11,
      "Transform Count and Types": [
        {
          "Lookup": 2
        },
        {
          "Derived Column": 1
        },
        {
          "Conditional Split": 1
        },
        {
          "Row Count": 4
        }
      ],
      "Join Analysis": {
        "Join Count": 7,
        "Join Types": [
          "LEFT",
          "INNER",
          "Lookup"
        ]
      },
      "Project, Sort, Dedup, Rollup Counts": {
        "Project Count": 1,
        "Sort Count": 0,
        "Dedup Count": 0,
        "Rollup Count": 0
      },
      "Child Workflows or Module Calls": 0,
      "Output or Store Operations": 2,
      "Conditional Logic Count": 3,
      "Macro or Function Module Reuse": 0,
      "Conversion Complexity Score": {
        "Score (0\u2013100)": 78,
        "Reasoning": [
          "Multiple incompatible features (e.g., Conditional Split, Derived Column).",
          "Manual refactoring required for lookups and change detection.",
          "Complex workflow orchestration with precedence constraints.",
          "High volume of datasets and compound transforms."
        ]
      }
    },
    "3. Feature Compatibility Check": {
      "Incompatible Features": [
        "Conditional Split (requires manual translation to Spark filter logic).",
        "Derived Column transformations (must be mapped to UDFs or withColumn in Spark).",
        "Row Count metrics (manual aggregation in Spark).",
        "Precedence constraints and event handlers (require orchestration outside Spark, e.g., Airflow)."
      ],
      "Examples of Challenging Constructs": [
        "Implicit schema typing in OLE DB Source.",
        "Complex join chains with both LEFT and INNER joins.",
        "Change detection logic using BeanVersion concatenation.",
        "SQL-based enrichment embedded in pipeline."
      ]
    },
    "4. Manual Adjustments for PySpark Migration": {
      "Transform Refactoring": "Rewrite Lookup and Derived Column as DataFrame joins and withColumn/UDFs. Conditional Split logic should be implemented with DataFrame filters.",
      "Schema Redefinition": "Explicitly define DataFrame schemas to match SSIS source and target structures. Map RECORD-like logic to case classes or StructType definitions.",
      "Join Handling Strategy": "Translate SQL joins to DataFrame joins, ensuring join keys and null-handling match SSIS logic. Use left_outer and inner joins as appropriate.",
      "Complex Ops Handling": "Implement change detection (BeanVersion logic) as a computed column. Use groupBy and agg for any deduplication or rollup if needed.",
      "Output Refactoring": "Replace OLE DB Destination with DataFrame.write to Parquet, Delta, or JDBC sinks. Ensure batch and transactional semantics are preserved."
    },
    "5. Optimization Techniques in Spark": {
      "Join Optimization": "Use broadcast joins for small reference tables (e.g., cctl_*), shuffle joins for large tables. Cache lookup tables if reused.",
      "Partitioning Strategy": "Partition main DataFrame by ClaimID or PublicId to parallelize processing and optimize shuffles.",
      "Caching Strategy": "Cache intermediate DataFrames after expensive joins or lookups to avoid recomputation.",
      "Code Optimization Techniques": "Leverage Catalyst optimizer, avoid UDFs where possible, use DataFrame API for projections and filters.",
      "Refactor vs Rebuild Recommendation": {
        "Approach": "Refactor with minimal changes",
        "Justification": "The pipeline is modular and maps closely to Spark DataFrame operations. Minimal refactoring is needed for lookups, splits, and derived columns. Full rebuild is not necessary unless business logic changes."
      }
    },
    "6. Cost Estimation": {
      "PySpark Runtime Cost": {
        "Cluster Configuration": {
          "Number of Executors": 8,
          "Executor Memory": "16 GB",
          "Driver Memory": "8 GB"
        },
        "Approximate Data Volume Processed": {
          "Input Data": "50\u2013100 GB",
          "Output Data": "10\u201315 GB"
        },
        "Time Taken for Each Phase": {
          "Shuffle-heavy Joins": "2 hours",
          "Wide Transforms": "1 hour",
          "Output Writes": "0.5 hour"
        },
        "Cost Model": {
          "Pricing Basis": "AWS EMR m5.xlarge (4 vCPU, 16GB) at $0.192/hr per node",
          "Total Estimated Cost": "8 nodes * 3.5 hours * $0.192 = $5.38"
        },
        "Justification": [
          "8 executors chosen for parallelism on 100GB input.",
          "16GB memory per executor supports large joins and caching.",
          "Estimated durations based on join complexity and output size.",
          "Pricing based on standard EMR on-demand rates."
        ]
      }
    },
    "7. Code Fixing and Data Recon Testing Effort Estimation": {
      "Estimated Effort in Hours": {
        "Manual Refactoring": 12,
        "Data Reconciliation Testing": 8,
        "Syntax Translation Adjustments": 6,
        "Optimization and Performance Tuning": 6
      },
      "Major Contributors to Effort": {
        "Nested TRANSFORM Refactoring": 4,
        "Output Refactoring for Spark Writes": 3,
        "Schema Management Effort": 2
      }
    },
    "8. API Cost": {
      "apiCost": "0.013452 USD"
    }
  }
}