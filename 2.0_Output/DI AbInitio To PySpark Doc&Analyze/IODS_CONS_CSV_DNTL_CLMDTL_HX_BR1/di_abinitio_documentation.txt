====================================================
Author:        Ascendion AVA+
Date:          
Description:   Documentation for Ab Initio Graph: IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1
====================================================

# Ab Initio Graph Documentation

## 1. Overview of Graph/Component

This Ab Initio graph, `IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1`, orchestrates the extraction, transformation, and loading (ETL) of dental claim detail history data from BigQuery sources into downstream datasets and tables. It handles data ingestion from multiple joined sources, applies business logic transformations, deduplication, and data quality checks, and loads the results into both file-based and BigQuery table targets. The graph supports regulatory and analytical reporting by ensuring high-quality, consolidated dental claim data.

---

## 2. Component Structure and Design

- **Input Table**: Reads from BigQuery using a complex SQL join across multiple tables.
- **Reformat Components**: Used for adapting and mapping input data to internal layouts and for applying business logic via `.xfr` transforms.
- **Sort & Dedup**: Ensures uniqueness of records based on key columns.
- **Partition by Key**: Distributes data for parallel processing.
- **Output File**: Writes processed data to a delimited file (MFS).
- **Output Table**: Loads data into a BigQuery table for downstream consumption.
- **Error, Reject, and Log Ports**: Present on most components for robust error handling and auditing.

**Key Components:**
- Input Table (BigQuery)
- Reformat (multiple, with .xfr transforms)
- Sort
- Dedup Sorted
- Partition by Key
- Output File
- Output Table

**Connection Flow:**
1. Input Table → Reformat (Adaptor) → Sort → Dedup → Partition by Key → Reformat (Join/Business Logic) → Output File & Output Table.
2. Each transformation step uses parameters and DMLs for metadata-driven processing.

---

## 3. Data Flow and Processing Logic

### Processed Datasets
- Input: BigQuery tables (`CSV_5010_DENTAL_SERVICE_LINE_HX`, `CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX`, `CONS_CSV_DENTAL_CLM_HX`)
- Intermediate: MFS temp files, deduped/sorted datasets
- Output: 
  - File: `$IODS_GCS_TEMP/DNTLCLM_DTL_inserts_2019082807120189.tmp`
  - Table: `${IODS_PUB_BQ_DATASET_STG}.STG_CONS_CSV_DENTAL_CLM_DTL_HX`

### Data Flow Description

1. **Extraction**: Data is extracted from BigQuery using a SQL join, filtered by load date.
2. **Initial Reformat**: Adapts the raw input to the internal Ab Initio layout.
3. **Sort & Deduplication**: Data is sorted and deduplicated on key columns (`AK_UCK_ID`, `AK_UCK_ID_PREFIX_CD`, `AK_UCK_ID_SEGMENT_NO`, `AK_SUBMT_SVC_LN_NO`).
4. **Partitioning**: Data is partitioned for parallel downstream processing.
5. **Business Logic Reformat**: Applies business rules and field mappings via `.xfr` transforms.
6. **Output**: Writes to both a delimited file and a BigQuery staging table.
7. **Error/Reject Handling**: All major steps have reject/error/log ports for capturing issues.

---

## 4. Data Mapping (Lineage)

```
Target Table Name : ${IODS_PUB_BQ_DATASET_STG}.STG_CONS_CSV_DENTAL_CLM_DTL_HX
Target Column Name : AK_UCK_ID
Source Table Name : ${IODS_PUB_BQ_DATASET_ENR}.CSV_5010_DENTAL_SERVICE_LINE_HX
Source Column Name : UCK_ID
Remarks : 1:1 Mapping

Target Table Name : ${IODS_PUB_BQ_DATASET_STG}.STG_CONS_CSV_DENTAL_CLM_DTL_HX
Target Column Name : AK_UCK_ID_PREFIX_CD
Source Table Name : ${IODS_PUB_BQ_DATASET_ENR}.CSV_5010_DENTAL_SERVICE_LINE_HX
Source Column Name : UCK_ID_PREFIX_CD
Remarks : 1:1 Mapping

Target Table Name : ${IODS_PUB_BQ_DATASET_STG}.STG_CONS_CSV_DENTAL_CLM_DTL_HX
Target Column Name : AK_UCK_ID_SEGMENT_NO
Source Table Name : ${IODS_PUB_BQ_DATASET_ENR}.CSV_5010_DENTAL_SERVICE_LINE_HX
Source Column Name : UCK_ID_SEGMENT_NO
Remarks : 1:1 Mapping

Target Table Name : ${IODS_PUB_BQ_DATASET_STG}.STG_CONS_CSV_DENTAL_CLM_DTL_HX
Target Column Name : AK_SUBMT_SVC_LN_NO
Source Table Name : ${IODS_PUB_BQ_DATASET_ENR}.CSV_5010_DENTAL_SERVICE_LINE_HX
Source Column Name : SUBMT_SVC_LN_NO
Remarks : 1:1 Mapping

Target Table Name : ${IODS_GCS_TEMP}/DNTLCLM_DTL_inserts_2019082807120189.tmp
Target Column Name : [All columns as per DML]
Source Table Name : [Joined BigQuery tables]
Source Column Name : [Mapped via .xfr logic]
Remarks : Transformation/Enrichment as per business rules in .xfr
```

---

## 5. Transformation Logic

### .xfr Functions Used

- `$AI_XFR/table_adaptor.xfr`: Adapts raw input to internal layout.
- `$AI_XFR/GEN_CSV_FIRST_DEFINED.xfr`: Applies "first defined" logic for certain fields.
- `$AI_XFR/IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2.xfr` and `$AI_XFR/IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P3.xfr`: Main business logic for field mapping, enrichment, and validation.
- Inline: `out :: reformat(in) = begin out.* :: in.*; end` (simple passthrough).

**Fields Involved:** All columns from the input SQL, with business rules applied for null handling, coalescing, and data type conversions.

**Reusable Components:** Standard Ab Initio Reformat, Dedup, Sort, Partition by Key.

---

## 6. Complexity Analysis

- **Number of Graph Components:** ~20 (major vertices: Input Table, multiple Reformats, Sort, Dedup, Partition, Output File, Output Table, etc.)
- **Number of Lines of Code (.xfr/.plan):** >500 (including SQL, DML, and .xfr logic)
- **Transform Functions Used:** 4+ (including custom and standard .xfr)
- **Joins Used:** 1 Inner Join, 1 Left Outer Join (in SQL extraction)
- **Lookup Files or Datasets:** None (all joins in SQL)
- **Parameter Sets (.pset) or Plan Files Used:** 1+ (parameters for environment, DML, dates, etc.)
- **Number of Output Datasets:** 2 (file + table)
- **Conditional Logic or if-else Flows:** Multiple (in .xfr and SQL, e.g., COALESCE, CASE)
- **External Dependencies:** BigQuery JDBC/DBC, shell environment variables, DML/XFR files
- **Overall Complexity Score:** 85/100 (due to multi-source join, complex mapping, error handling, and dual output)

---

## 7. Key Outputs

- **File Output:** `${IODS_GCS_TEMP}/DNTLCLM_DTL_inserts_2019082807120189.tmp` (Delimited, for downstream batch or archival)
- **Table Output:** `${IODS_PUB_BQ_DATASET_STG}.STG_CONS_CSV_DENTAL_CLM_DTL_HX` (BigQuery, for analytics/reporting)
- **Format:** Delimited (file), BigQuery Table (variable/fixed-width as per DML)
- **Intended Use:** Regulatory reporting, analytics, and downstream data lake ingestion.

---

## 8. Error Handling and Logging

- **Reject Ports:** All major components (Reformat, Output Table, etc.) have reject ports for invalid records.
- **Error Ports:** Capture transformation or load errors.
- **Log Ports:** Audit and operational logs for monitoring.
- **.xfr-based Error Tagging:** Used in transformation steps for tagging and routing bad records.
- **Reject Thresholds:** Configurable (Abort on first reject, or as per parameter).
- **Control Files:** Not explicitly used, but DML and parameterization ensure metadata-driven error handling.
- **Error Handling Actions:** Errors routed to reject/error files, logs written for operational review, and auto-abort on critical failures.

---

## 9. API Cost (Cloud Ab Initio Deployments)

**Estimated Compute/I/O Cost:**
- **BigQuery Extraction:** Cost = (Input Rows) x (Bytes Processed) x (BigQuery $/TB)
- **Ab Initio Compute:** Cost = (CPU Hours) x (Instance Type Rate)
- **File Write:** Cost = (Output File Size) x (Cloud Storage $/GB)
- **BigQuery Load:** Cost = (Rows Inserted) x (Bytes Loaded) x (BigQuery $/TB)

**Formula Example:**
```
Total Cost = 
  (BigQuery Read Bytes / 1TB) * $5 +
  (Ab Initio Compute Hours) * (VM $/hr) +
  (Output File Size in GB) * (Storage $/GB) +
  (BigQuery Load Bytes / 1TB) * $5
```
*Actual cost depends on data volume and cloud pricing.*

---

**End of Documentation**