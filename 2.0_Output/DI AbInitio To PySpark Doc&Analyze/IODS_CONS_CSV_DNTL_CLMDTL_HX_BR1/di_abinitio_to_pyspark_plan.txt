========================================================
Author:        Ascendion AVA+
Created on:    
Description:   Pre-conversion analysis and cost estimation for Ab Initio to PySpark migration of dental claim detail history ETL on GCP Dataproc.
========================================================

# 1. GCP Runtime Cost Estimation

## 1.1 Dataproc/Spark Job Cost Breakdown

- **Cluster Configuration**:  
  - Master Node: n1-standard-4 (4 vCPUs, 15 GB RAM), 1 node  
  - Worker Nodes: n1-standard-4 (4 vCPUs, 15 GB RAM), 4 nodes  
  - **Total vCPUs**: 1 × 4 (master) + 4 × 4 (workers) = **20 vCPUs**  
  - **Total Memory**: 1 × 15 GB + 4 × 15 GB = **75 GB**  
- **Job Duration Estimate**: **60 minutes** (based on graph complexity, multi-table join, deduplication, enrichment, and BigQuery IO)
- **GCP Pricing**:  
  - Compute: **$0.156 per vCPU/hour**  
  - Storage: **$0.01 per GB/hour** (for attached persistent disk; assume 100 GB for shuffle/temp)  
- **Cost Formula Used**:  
  ```
  Total Cost = (Total vCPUs × Duration in hours × Compute (per vCPU/hr)) 
             + (Storage GB × Duration in hours × Storage (per GB/hr))
  ```
  Where:
  - Total vCPUs = 20
  - Duration = 1 hour
  - Compute = $0.156/vCPU/hr
  - Storage = 100 GB × $0.01/GB/hr = $1.00/hr

- **Estimated Runtime Cost (USD)**:  
  - Compute: 20 × 1 × $0.156 = **$3.12**
  - Storage: 100 × 1 × $0.01 = **$1.00**
  - **Total Estimated Runtime Cost:** **$4.12**

---

# 2. Manual Code Fixing and Data Reconciliation Effort

## 2.1 Estimated Effort (Hours)

| Task                                             | Estimated Hours |
|--------------------------------------------------|----------------|
| Logic Corrections (custom .xfr transformations)  | 12             |
| Metadata Alignment (DML type/schema fixes)       | 6              |
| Rejected Row Handling / Edge Case Logic          | 4              |
| Data Reconciliation & Output Validation          | 8              |
| **Total Effort**                                | **30**         |

### **Effort Rationale:**
- **Logic Corrections:** Multiple custom `.xfr` files (e.g., business enrichment, COALESCE/CASE logic, multi-output ports) require translation to PySpark UDFs or DataFrame logic.
- **Metadata Alignment:** Mapping Ab Initio DML types (e.g., utf8 string, decimal(40.9), date) to PySpark schema, ensuring BigQuery compatibility.
- **Reject/Edge Case Handling:** Ab Initio’s reject/error ports must be explicitly implemented in PySpark (filters, logging, error DataFrames).
- **Data Reconciliation:** Validation of output row counts, field-level mapping, deduplication correctness, and BigQuery load verification.

## 2.2 Developer Cost

- **Developer Rate:** `$50/hr`
- **Total Developer Cost:** `30 × $50 = $1,500 USD`

---

# 3. API Cost

- **apiCost:** `0.0125 USD`

---

# 4. Summary Table

| Cost Component                | Value (USD)   |
|-------------------------------|--------------|
| GCP Dataproc Runtime Cost     | $4.12        |
| Developer Effort Cost         | $1,500.00    |
| API Processing Cost           | $0.0125      |
| **Total Estimated Cost**      | **$1,504.13**|

---

# 5. Migration Analysis & Recommendations

- **Manual Interventions Required:**
  - Translate all custom `.xfr` logic to PySpark functions/UDFs.
  - Map DML-defined field types to PySpark schema and BigQuery types.
  - Implement explicit error/reject handling for each transformation.
  - Validate business rules, deduplication, and enrichment logic.
  - Test partitioning and BigQuery/GCS integration.

- **Complexity Drivers:**
  - Multi-table BigQuery joins with COALESCE/CASE logic.
  - Deduplication on composite keys.
  - Multiple chained Reformat/Enrichment steps.
  - Robust error/reject/log handling at each stage.

- **GCP Runtime Cost is minimal** compared to developer effort, but should be monitored for large data volumes or frequent runs.

---

# 6. References

- **Ab Initio Source File(s):**  
  - Main graph, parameter set, DMLs, and custom XFRs as detailed above.
- **GCP Cluster/Dataproc Configuration:**  
  - n1-standard-4, 1 master, 4 workers, 100 GB storage, 1 hour runtime.

---

# 7. Appendix

- **Assumptions:**  
  - 1 hour Dataproc runtime for a single ETL cycle.
  - 100 GB attached storage for shuffle/temp.
  - Developer rate and GCP pricing as per current public rates.
  - Actual runtime and effort may vary based on data volume and unforeseen logic gaps.

---

**End of Report**