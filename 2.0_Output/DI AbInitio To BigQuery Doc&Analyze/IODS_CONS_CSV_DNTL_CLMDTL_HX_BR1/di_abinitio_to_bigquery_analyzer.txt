```
=======================================================================================
Author:        Ascendion AVA+
Created on:    
Description:   Pre-conversion analysis of Ab Initio ETL flow for BigQuery SQL migration
=======================================================================================

## Syntax Differences

### Major Ab Initio Components and BigQuery Mapping

| Ab Initio Component           | Purpose / Logic                                      | BigQuery SQL Mapping / Notes                          |
|------------------------------|------------------------------------------------------|-------------------------------------------------------|
| **Input Table**              | Reads/join/filter from BigQuery tables               | `SELECT ... FROM ... JOIN ... WHERE ...`              |
| **Reformat**                 | Field mapping, enrichment, DML adaptation            | `SELECT` with expressions, CTEs, or UDFs              |
| **Sort**                     | Orders records by business key                       | `ORDER BY` in SQL; may require window functions       |
| **Dedup Sorted**             | Removes duplicates by composite key                  | `ROW_NUMBER() OVER (PARTITION BY ...)` + filter       |
| **Partition by Key**         | Parallelizes by key                                  | Not directly needed; BigQuery parallelizes internally  |
| **Output File/Table**        | Writes to GCS multifile, loads to BigQuery           | `INSERT INTO` or `CREATE TABLE AS SELECT`             |
| **Reject/Error/Log Ports**   | Error handling, logging                              | Use `EXCEPTIONS`, logging tables, or scripting        |
| **Parameterization**         | Env, dataset, date, DML paths                        | Use scripting, parameterized queries, or variables    |

#### Incompatible/Non-native SQL Behaviors

- **Reject Ports:** Ab Initio's explicit reject/error ports must be replaced by error handling logic, e.g., using `SAFE_CAST`, `TRY_CAST`, or scripting to capture load errors.
- **Procedural/Branching:** Multi-output reformats and procedural flows (e.g., conditional branching) must be flattened into CTE chains or nested queries.
- **Record-level Transformations:** `.xfr` logic (custom transforms) must be rewritten as SQL expressions or UDFs.
- **Variable-width Output:** BigQuery requires explicit schema; variable-width files must be mapped to fixed schemas.

---

## Anticipated Manual Interventions

- **.xfr Logic Translation:**  
  - Custom logic in `GEN_CSV_FIRST_DEFINED.xfr`, `table_adaptor.xfr`, and custom business logic `.xfr` files must be manually rewritten as SQL expressions or, if complex, as BigQuery UDFs.
  - Example: `GEN_CSV_FIRST_DEFINED.xfr` (first non-null value) → `COALESCE()` in SQL.
- **.dml Schema Restructuring:**  
  - Ab Initio DMLs define record-level schemas; BigQuery requires explicit column-level schemas. Manual mapping and validation are needed, especially for complex or nested records.
- **Parameterization:**  
  - Ab Initio parameters (e.g., `${IODS_PUB_BQ_DATASET_ENR}`) must be handled via scripting or templating in SQL or orchestration tools (e.g., Airflow, dbt).
- **Error/Reject Handling:**  
  - All reject/error ports must be replaced by SQL error handling or logging tables. BigQuery does not natively support row-level reject ports.
- **Procedural Constructs:**  
  - Conditional logic in `.xfr` or multi-branch reformats must be expressed as `CASE WHEN` or split into separate CTEs/subqueries.
- **Performance Tuning:**  
  - Sort/dedup operations on large datasets (~2TB) may require query optimization, partitioning, and clustering in BigQuery.
- **Load/Unload to GCS:**  
  - GCS multifile logic must be replaced with BigQuery's native staging or external table features.

---

## Complexity Evaluation

**Score: 85/100**

### Justification

- **Component Variety:**  
  - 24+ components: Input Table, multiple Reformats, Sort, Dedup, Partition, Output, etc.
- **Custom `.xfr` Logic:**  
  - Multiple custom and generic transforms, including business rules and field derivations.
- **Joins and Data Dependencies:**  
  - Multi-table joins (1 LEFT OUTER, 1 INNER), complex field mapping, and enrichment.
- **Schema Complexity:**  
  - Large, wide schemas with hundreds of fields, requiring careful mapping.
- **Error Handling:**  
  - Robust reject/error/log ports on all major transforms.
- **Parameterization:**  
  - Extensive use of parameters for environment, dataset, and DML paths.
- **Manual UDF/Scripting Need:**  
  - Several `.xfr` functions require manual translation to SQL or UDFs.
- **Data Volume:**  
  - Large data volumes (200–400GB per run) increase operational complexity.
- **No Feedback Loops/Iterative Components:**  
  - No explicit feedback loops, but procedural dependencies exist.

---

## Optimization Recommendation

### **Rebuild**

- **Why:**  
  - The graph contains complex, multi-step ETL logic with custom transformations, robust error handling, and parameterization that do not map 1:1 to SQL. The procedural nature, error/reject ports, and `.xfr` logic require a full redesign for BigQuery.
- **How:**  
  - **CTE Chains:** Use `WITH` clauses to modularize transformation steps.
  - **Subqueries:** For intermediate mapping and enrichment.
  - **UDFs:** For complex field derivations and business rules.
  - **CASE/COALESCE:** For conditional logic and first-defined value selection.
  - **Window Functions:** For deduplication (`ROW_NUMBER() OVER (PARTITION BY ...)`).
  - **Scripting/Orchestration:** For parameterization, error handling, and multi-step orchestration (e.g., dbt, Airflow).
  - **Logging Tables:** For capturing rejects/errors, as BigQuery lacks reject ports.

---

## SQL Design Suggestions

- **Express each ETL step as a CTE:**  
  - E.g., `WITH raw AS (...), mapped AS (...), deduped AS (...), final AS (...) SELECT * FROM final`
- **Use BigQuery UDFs for complex logic:**  
  - Translate `.xfr` functions to JavaScript or SQL UDFs.
- **Deduplication:**  
  - `ROW_NUMBER() OVER (PARTITION BY key_cols ORDER BY ...) = 1`
- **Error Handling:**  
  - Use `SAFE_CAST`, `TRY_CAST`, or scripting to log errors.
- **Parameterization:**  
  - Use scripting or dbt variables for dynamic dataset/table names.
- **Performance:**  
  - Use partitioned and clustered tables for large datasets.
- **Validation Checkpoints:**  
  - After each major transformation, validate row counts and key integrity.

---

## Validation and Data Assurance Checkpoints

- **Row Count Validation:**  
  - Compare input and output row counts at each stage.
- **Key Uniqueness:**  
  - Validate deduplication by checking for duplicate keys post-process.
- **Null/Type Checks:**  
  - Ensure all required fields are non-null and types match target schema.
- **Error/Reject Logging:**  
  - Capture and review all transformation and load errors.
- **Data Lineage:**  
  - Document field-level lineage from source to target.

---

## API Cost

`apiCost: 0.048 USD`

---

**End of Pre-conversion Analysis**
```