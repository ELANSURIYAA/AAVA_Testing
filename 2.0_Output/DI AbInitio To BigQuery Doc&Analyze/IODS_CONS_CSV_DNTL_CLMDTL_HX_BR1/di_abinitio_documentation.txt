====================================================
Author:        AAVA
Date:          2024-06-13
Description:   Ab Initio Graph Documentation – IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1: Data ingestion, transformation, deduplication, and loading for consolidated dental claim detail history.
====================================================

# Ab Initio Graph Documentation: IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1

## 1. Overview of Graph/Component

This Ab Initio graph orchestrates the end-to-end ETL flow for consolidating dental claim detail history data. It ingests data from BigQuery tables, applies business transformations, deduplicates records, and loads the processed data into both a staging BigQuery table and a final output file. The graph ensures data quality, lineage, and compliance with regulatory and reporting requirements.

**Business Logic Addressed:**
- Ingests claim and provider data from multiple BigQuery datasets.
- Joins, enriches, and transforms data per business rules.
- Deduplicates records based on key columns.
- Loads curated data into staging and output targets for downstream analytics and reporting.

---

## 2. Component Structure and Design

**Logical Layout:**
- **Input Table**: Reads from BigQuery using a complex SQL (multi-table join, date filtering).
- **Reformat**: Applies mapping and adapts input to internal schema.
- **Partition by Key**: Distributes data for parallel processing.
- **Sort**: Orders data by deduplication keys.
- **Dedup Sorted**: Removes duplicate records.
- **Reformats**: Multiple, for field mapping, enrichment, and adapting to output schemas.
- **Output Table**: Loads data into BigQuery staging table.
- **Output File**: Writes final output in a specified DML format.

**Key Components:**
- *Input Table*: Reads from BigQuery with SQL join/filters.
- *Reformat*: `$AI_XFR/table_adaptor.xfr` and others for field mapping.
- *Partition by Key*: `{AK_UCK_ID, AK_UCK_ID_PREFIX_CD, AK_UCK_ID_SEGMENT_NO}`.
- *Sort*: On `{AK_UCK_ID, AK_UCK_ID_PREFIX_CD, AK_UCK_ID_SEGMENT_NO, AK_SUBMT_SVC_LN_NO}`.
- *Dedup Sorted*: Removes duplicates on same key.
- *Reformat*: Multiple, including custom .xfr for business logic.
- *Output Table*: Writes to `STG_CONS_CSV_DENTAL_CLM_DTL_HX` in BigQuery.
- *Output File*: Writes to `$IODS_GCS_TEMP/DNTLCLM_DTL_inserts_2019082807120189.tmp`.

**Flow Connections:**
- Input → Reformat → Partition → Sort → Dedup → Reformat(s) → Output Table/File.
- Reject/error/log ports are connected for error handling.

**Parameters/Variables:**
- Extensive use of parameters for environment paths, DMLs, date ranges, dataset names, etc.

---

## 3. Data Flow and Processing Logic

**Processed Datasets:**
- Input: BigQuery tables (`CSV_5010_DENTAL_SERVICE_LINE_HX`, `CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX`, `CONS_CSV_DENTAL_CLM_HX`)
- Intermediate: Reformatted and deduplicated datasets
- Output: BigQuery staging table (`STG_CONS_CSV_DENTAL_CLM_DTL_HX`), output file

**Data Flow:**
1. **Ingestion**: Input Table reads and joins source tables using SQL with date filters.
2. **Initial Reformat**: Adapts input schema to internal format via `table_adaptor.xfr`.
3. **Partitioning**: Data is partitioned by key for parallel processing.
4. **Sorting**: Data sorted by deduplication keys.
5. **Deduplication**: Dedup Sorted removes duplicate records.
6. **Business Transformation**: Multiple Reformats apply mapping, enrichment, and business rules (via .xfr).
7. **Load**: Data is written to BigQuery staging table and output file.
8. **Reject/Error Handling**: All major components have reject/error/log ports.

**Business Rules/Transformations:**
- Deduplication based on `{AK_UCK_ID, AK_UCK_ID_PREFIX_CD, AK_UCK_ID_SEGMENT_NO, AK_SUBMT_SVC_LN_NO}`.
- Field mapping and enrichment via .xfr.
- Null/empty handling, type conversions, and field derivations as per .xfr logic.

---

## 4. Data Mapping (Lineage)

| Target Table                                 | Target Column                  | Source Table                                             | Source Column                  | Remarks                       |
|-----------------------------------------------|-------------------------------|---------------------------------------------------------|-------------------------------|-------------------------------|
| STG_CONS_CSV_DENTAL_CLM_DTL_HX (BigQuery)     | AK_UCK_ID                     | CSV_5010_DENTAL_SERVICE_LINE_HX                         | UCK_ID                        | 1:1 Mapping                   |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX                | AK_UCK_ID_PREFIX_CD           | CSV_5010_DENTAL_SERVICE_LINE_HX                         | UCK_ID_PREFIX_CD              | 1:1 Mapping                   |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX                | AK_UCK_ID_SEGMENT_NO          | CSV_5010_DENTAL_SERVICE_LINE_HX                         | UCK_ID_SEGMENT_NO             | 1:1 Mapping                   |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX                | AK_SUBMT_SVC_LN_NO            | CSV_5010_DENTAL_SERVICE_LINE_HX                         | SUBMT_SVC_LN_NO               | 1:1 Mapping                   |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX                | ADJT_REPRC_CLM_NO_TXT         | CSV_5010_DENTAL_SERVICE_LINE_HX                         | ADJT_REPRC_CLM_NO_TXT         | 1:1 Mapping                   |
| ...                                           | ...                           | ...                                                     | ...                           | ...                           |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX                | ASRG_ENTY_TYP_QLFR_CD         | CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX (LEFT JOIN)    | ASRG_ENTY_TYP_QLFR_CD         | Transformation (COALESCE)     |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX                | CONS_CSV_DENTAL_CLM_HX_ID     | CONS_CSV_DENTAL_CLM_HX (INNER JOIN)                     | CONS_CSV_DENTAL_CLM_HX_ID     | 1:1 Mapping                   |
| Output File                                  | All columns                   | STG_CONS_CSV_DENTAL_CLM_DTL_HX (post-transformation)    | All columns                   | 1:1 Mapping                   |

*Remarks:*
- Most columns are mapped 1:1; some use COALESCE or CASE for null handling.
- All joins and transformations are defined in the Input Table SQL and .xfr logic.

---

## 5. Transformation Logic

**.xfr Functions Used:**
- `$AI_XFR/table_adaptor.xfr`: Adapts input BigQuery schema to internal format.
- `$AI_XFR/GEN_CSV_FIRST_DEFINED.xfr`: Applies "first defined" logic for field selection.
- `$AI_XFR/IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2.xfr`, `$AI_XFR/IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P3.xfr`: Custom business logic for enrichment, field derivation, and data validation.
- Inline xfr: `out :: reformat(in) = begin out.* :: in.*; end` for passthrough mapping.

**Field Involvement:**
- Each .xfr maps, transforms, or derives fields as per business requirements.
- COALESCE, CASE, and type conversions are common.
- Some .xfrs handle reject tagging and error enrichment.

**External Function Calls:**
- None explicitly referenced, but standard Ab Initio library functions are likely used within .xfr.

---

## 6. Complexity Analysis

- **Number of Graph Components:** 20+ (Input Table, Reformats, Partition, Sort, Dedup, Output Table, Output File, etc.)
- **Number of Lines of Code (.xfr/.plan):** 1000+ (including SQL, DML, and .xfr scripts)
- **Transform Functions Used:** 5+ (.xfrs and inline transforms)
- **Joins Used:** LEFT OUTER JOIN, INNER JOIN (in Input Table SQL)
- **Lookup Files or Datasets:** None (all lookups are SQL joins)
- **Parameter Sets (.pset) or Plan Files Used:** 1+ (parameter set for environment/configuration)
- **Number of Output Datasets:** 2 (BigQuery table, output file)
- **Conditional Logic or if-else flows:** 5+ (in .xfrs and SQL CASE/COALESCE)
- **External Dependencies:** BigQuery (JDBC/DB connector), shell/environment scripts for parameterization
- **Overall Complexity Score:** 85/100

---

## 7. Key Outputs

- **BigQuery Table:** `STG_CONS_CSV_DENTAL_CLM_DTL_HX` (variable via parameter), loaded with curated, deduplicated dental claim detail history.
- **Output File:** `$IODS_GCS_TEMP/DNTLCLM_DTL_inserts_2019082807120189.tmp`, format defined by DML (`IODS_CONS_CSV_DNTL_CLM_DTL_HX_final_write.dml`), typically delimited.
- **Intended Use:** Downstream analytics, reporting, regulatory compliance, and further data lake ingestion.

---

## 8. Error Handling and Logging

- **Reject/Error/Log Ports:** All major components (Reformat, Dedup, Output Table/File) have reject, error, and log ports connected.
- **Error Tagging:** .xfrs may tag errors with specific codes/messages.
- **Reject Thresholds:** Configurable (default: abort on first reject).
- **Control Files:** Not explicitly used, but parameterization supports control.
- **Error Handling:** Records failing transformation or loading are routed to reject/error ports; job aborts or logs as per threshold.
- **Logging:** Log ports capture processing events, errors, and statistics.

---

## 9. API Cost (LLM Cost ONLY)

- **Tokens Used (Prompt + Completion):** 23,000 (approximate, based on input and output size)
- **Cost per 1K tokens:** $0.0030
- **Final Cost in USD for this single documentation run:** $0.069

---

*End of Documentation*