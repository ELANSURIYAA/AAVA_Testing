====================================================
Author:        AAVA
Date:          2024-06-11
Description:   Ab Initio graph documentation for IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1 – data ingestion, transformation, deduplication, and load to BigQuery staging.
====================================================

# Ab Initio Graph Documentation

## 1. Overview of Graph/Component

The Ab Initio graph `IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1` orchestrates the extraction, transformation, deduplication, and loading of consolidated dental claim detail data from multiple BigQuery sources into a BigQuery staging table. The graph integrates service line, provider, and claim history data, applies business rules, deduplicates records, and prepares outputs for downstream analytics and reporting.

**Business Purpose:**  
- Consolidate and enrich dental claim detail data from multiple sources.
- Apply business logic for field mapping, enrichment, and deduplication.
- Prepare and load high-quality, deduplicated data into a BigQuery staging table for further processing.

## 2. Component Structure and Design

**Key Components:**
- **Input Table**: Reads data from BigQuery using a complex SQL join across `CSV_5010_DENTAL_SERVICE_LINE_HX`, `CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX`, and `CONS_CSV_DENTAL_CLM_HX`.
- **Reformat (Multiple)**: Used for field mapping, enrichment, and adapting data layouts using `.xfr` transforms.
- **Sort**: Orders data by deduplication keys before deduplication.
- **Dedup Sorted**: Removes duplicate records based on business keys.
- **Partition by Key**: Distributes data for parallel processing.
- **Output File**: Writes intermediate results in Ab Initio multifile format.
- **Output Table**: Loads the final dataset into the BigQuery staging table.
- **Error/Reject/Log Ports**: Present on all major components for error handling and auditing.

**Logical Grouping & Flow:**
- Extraction → Transformation (Reformat) → Sort → Deduplication → Partition → Further Transformation → Output (File/Table)
- Parameters and environment variables are used for dynamic dataset names, date ranges, and DML file references.

## 3. Data Flow and Processing Logic

**Processed Datasets:**
- Input: 
  - `CSV_5010_DENTAL_SERVICE_LINE_HX`
  - `CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX`
  - `CONS_CSV_DENTAL_CLM_HX`
- Intermediate:
  - Multifile datasets (e.g., `DNTLCLM_DTL_inserts_*.tmp`)
- Output:
  - BigQuery table: `STG_CONS_CSV_DENTAL_CLM_DTL_HX`

**Data Flow:**
1. **Input Table**: Executes a SQL join to fetch and combine service line, provider, and claim history data, with COALESCE and CASE logic for enrichment.
2. **Reformat (Adaptor)**: Applies initial field mapping and layout adaptation.
3. **Sort**: Orders records by deduplication keys (`AK_UCK_ID`, `AK_UCK_ID_PREFIX_CD`, `AK_UCK_ID_SEGMENT_NO`, `AK_SUBMT_SVC_LN_NO`).
4. **Dedup Sorted**: Removes duplicate records, keeping the first by key.
5. **Reformat (Join/Business Logic)**: Applies further business rules and field mapping.
6. **Partition by Key**: Distributes data for parallel downstream processing.
7. **Reformat (Final Layout)**: Prepares data for output, mapping to target DML.
8. **Output File**: Writes to intermediate multifile system.
9. **Output Table**: Loads the processed data into the BigQuery staging table.

**Business Rules/Transformations:**
- Use of COALESCE for provider fields.
- CASE logic for numeric conversions and defaulting.
- Deduplication by business keys.
- Field mapping and enrichment via `.xfr` transforms.

## 4. Data Mapping (Lineage)

| Target Table                         | Target Column                | Source Table                                      | Source Column                       | Remarks                    |
|-------------------------------------- |-----------------------------|---------------------------------------------------|-------------------------------------|----------------------------|
| STG_CONS_CSV_DENTAL_CLM_DTL_HX       | AK_UCK_ID                   | CSV_5010_DENTAL_SERVICE_LINE_HX                   | UCK_ID                              | 1:1 Mapping                |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX       | AK_UCK_ID_PREFIX_CD         | CSV_5010_DENTAL_SERVICE_LINE_HX                   | UCK_ID_PREFIX_CD                    | 1:1 Mapping                |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX       | AK_UCK_ID_SEGMENT_NO        | CSV_5010_DENTAL_SERVICE_LINE_HX                   | UCK_ID_SEGMENT_NO                   | 1:1 Mapping                |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX       | AK_SUBMT_SVC_LN_NO          | CSV_5010_DENTAL_SERVICE_LINE_HX                   | SUBMT_SVC_LN_NO                     | 1:1 Mapping                |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX       | ADJT_REPRC_CLM_NO_TXT       | CSV_5010_DENTAL_SERVICE_LINE_HX                   | ADJT_REPRC_CLM_NO_TXT               | 1:1 Mapping                |
| ...                                  | ...                         | ...                                               | ...                                 | ...                        |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX       | ASRG_ENTY_TYP_QLFR_CD       | CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX (joined) | ASRG_ENTY_TYP_QLFR_CD               | COALESCE(B.field, '')      |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX       | CONS_CSV_DENTAL_CLM_HX_ID   | CONS_CSV_DENTAL_CLM_HX (joined)                   | CONS_CSV_DENTAL_CLM_HX_ID           | 1:1 Mapping                |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX       | STK_UCK_ID                  | CSV_5010_DENTAL_SERVICE_LINE_HX                   | STK_UCK_ID                          | CASE WHEN ... ELSE 0 END   |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX       | STK_REND_PROV_NO            | CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX (joined) | STK_REND_PROV_NO                    | CASE WHEN ... ELSE 0 END   |
| ...                                  | ...                         | ...                                               | ...                                 | ...                        |

*Remarks*:  
- Most fields are 1:1 mapped from the input SQL SELECT.
- Provider and facility fields use COALESCE for null handling.
- Numeric fields use CASE for type safety.
- Deduplication ensures only unique records per business key are loaded.

## 5. Transformation Logic

**.xfr Functions Used:**
- `$AI_XFR/table_adaptor.xfr`: Adapts input table layout to internal format.
- `$AI_XFR/GEN_CSV_FIRST_DEFINED.xfr`: Ensures first non-null value selection for certain fields.
- `$AI_XFR/IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2.xfr` and `$AI_XFR/IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P3.xfr`: Custom business logic for field mapping and enrichment.
- Inline `.xfr` in Reformat: `out.* :: in.*;` (direct field copy).

**Field Involvement:**
- All fields from the SELECT are mapped and/or transformed as per DML and business rules.
- COALESCE and CASE logic are implemented in SQL and/or transformation functions.

**External Function Calls/Reusables:**
- Standard Ab Initio components for deduplication, sorting, and partitioning.
- Custom and generic `.xfr` for field mapping and enrichment.

## 6. Complexity Analysis

- **Number of Graph Components:** ~20 (Input Table, multiple Reformat, Sort, Dedup, Partition, Output File, Output Table, plus error/reject/log ports)
- **Number of Lines of Code (in .xfr or .plan):** ~2000+ (including SQL, DML, and transformation logic)
- **Transform Functions Used:** 5+ (including custom and generic)
- **Joins Used:** 2 (LEFT OUTER JOIN, INNER JOIN in SQL)
- **Lookup Files or Datasets:** None (joins are in SQL)
- **Parameter Sets (.pset) or Plan Files Used:** Multiple parameter sets for environment, DML, and output paths
- **Number of Output Datasets:** 1 main (BigQuery staging table), 1 intermediate multifile
- **Conditional Logic or if-else flows:** 3+ (CASE, COALESCE, deduplication logic)
- **External Dependencies:** BigQuery (JDBC/ODBC), environment variables, shell for file paths
- **Overall Complexity Score:** 85/100

## 7. Key Outputs

- **Final Output:**  
  - BigQuery table: `STG_CONS_CSV_DENTAL_CLM_DTL_HX`
  - Format: Variable-width (BigQuery table), intermediate multifile (Ab Initio MFS)
  - Intended Use: Downstream analytics, reporting, regulatory processing

## 8. Error Handling and Logging

- **Reject Ports:** All major components have reject ports for bad records.
- **Error Ports:** Capture and log errors using Ab Initio error DML.
- **Log Ports:** Audit and operational logs written for monitoring.
- **Thresholds:** Dedup and Reformat components use `Abort on first reject` or configurable thresholds.
- **Control Files:** Not explicitly used, but environment and parameter sets control file paths and DML references.
- **Error Handling:**  
  - Records failing transformation or deduplication are routed to reject/error ports.
  - Logging is enabled for traceability.
  - Auto-abort on critical errors, with logs for investigation.

## 9. API Cost (LLM Cost ONLY)

- **Tokens Used (Prompt + Completion):** [Estimated: 16,000]
- **Cost per 1K tokens:** $0.003
- **Final Cost in USD for this single documentation run:** $0.048

---

**Note:**  
- Field-level mapping and transformation logic are based on the provided Ab Initio graph and SQL.
- For full lineage, refer to the SQL SELECT and DML definitions.
- For detailed `.xfr` logic, see referenced `.xfr` files in the Ab Initio repository.

---