====================================================
Author:        AAVA
Date:          2024-06-11
Description:   Ab Initio Graph Documentation for IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1
====================================================

# IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1 â€“ Ab Initio Graph Documentation

## 1. Overview of Graph/Component

This Ab Initio graph (`IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1`) orchestrates the extraction, transformation, deduplication, and loading of consolidated dental claim detail data from multiple BigQuery datasets into a target BigQuery staging table. The business requirement is to consolidate and enrich dental claim service line data, joining provider and claim header information, applying business rules, and preparing the data for downstream analytics and reporting.

**Component Types:**
- `.mp` (graph): Main orchestration logic.
- `.xfr` (transform): Custom and generic transformation logic.
- `.dml` (data definition): Metadata for input/output datasets.

---

## 2. Component Structure and Design

### Logical Layout

- **Input Table (BigQuery):** Reads from `CSV_5010_DENTAL_SERVICE_LINE_HX` joined with `CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX` and `CONS_CSV_DENTAL_CLM_HX`.
- **Reformat Components:** Used for field mapping, enrichment, and adaptation to target DML.
- **Sort & Dedup:** Sorts by business keys and removes duplicates.
- **Partition by Key:** Ensures parallel processing by partitioning on unique claim identifiers.
- **Output File/Table:** Writes to a GCS temp multifile and loads to BigQuery staging table.
- **Error/Reject/Log Ports:** Present on all major transforms for robust error handling.

### Key Components

- **Input Table:** Extracts and joins raw data from BigQuery.
- **Reformat (Adaptor, Join, Final):** Applies field mapping, business rules, and prepares output.
- **Sort/Dedup:** Removes duplicates based on composite business keys.
- **Output Table:** Loads processed data into `STG_CONS_CSV_DENTAL_CLM_DTL_HX` in BigQuery.
- **Parameterization:** Uses parameters for environment, dataset names, date ranges, and DML paths.

---

## 3. Data Flow and Processing Logic

### Processed Datasets

- **Source:** 
  - `CSV_5010_DENTAL_SERVICE_LINE_HX`
  - `CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX`
  - `CONS_CSV_DENTAL_CLM_HX`
- **Intermediate:** 
  - Reformat outputs, deduped/sorted multifiles, temp GCS files
- **Target:** 
  - `STG_CONS_CSV_DENTAL_CLM_DTL_HX` (BigQuery)

### Data Flow

1. **Extract:** Input Table component runs a SQL join across three BigQuery tables, filtering by load date.
2. **Initial Reformat:** Adapts raw input to internal DML, applies initial mapping.
3. **Sort:** Orders records by business key (`AK_UCK_ID`, `AK_UCK_ID_PREFIX_CD`, `AK_UCK_ID_SEGMENT_NO`, `AK_SUBMT_SVC_LN_NO`).
4. **Deduplication:** Removes duplicate records, keeping the first occurrence per key.
5. **Partition:** Distributes data for parallel processing.
6. **Business Reformat:** Applies business rules, field derivations, and prepares for output.
7. **Output File:** Writes to a temp multifile in GCS.
8. **Output Table:** Loads the processed data into the BigQuery staging table.

#### Logical Steps

- **Filtering:** Only records within the specified date range are processed.
- **Joining:** Left join with provider, inner join with claim header.
- **Field Mapping:** Direct and derived mappings using `.xfr` logic.
- **Deduplication:** Composite key-based deduplication.
- **Transformation:** Custom logic via `.xfr` (e.g., `GEN_CSV_FIRST_DEFINED.xfr`).
- **Output:** Data written in variable-width format to BigQuery.

---

## 4. Data Mapping (Lineage)

| Target Table                               | Target Column                | Source Table                                 | Source Column                | Remarks                     |
|--------------------------------------------|------------------------------|----------------------------------------------|------------------------------|-----------------------------|
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | AK_UCK_ID                    | CSV_5010_DENTAL_SERVICE_LINE_HX              | UCK_ID                       | 1:1 Mapping                 |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | AK_UCK_ID_PREFIX_CD          | CSV_5010_DENTAL_SERVICE_LINE_HX              | UCK_ID_PREFIX_CD             | 1:1 Mapping                 |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | AK_UCK_ID_SEGMENT_NO         | CSV_5010_DENTAL_SERVICE_LINE_HX              | UCK_ID_SEGMENT_NO            | 1:1 Mapping                 |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | AK_SUBMT_SVC_LN_NO           | CSV_5010_DENTAL_SERVICE_LINE_HX              | SUBMT_SVC_LN_NO              | 1:1 Mapping                 |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | ASRG_ENTY_TYP_QLFR_CD        | CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX     | ASRG_ENTY_TYP_QLFR_CD        | Transformation (COALESCE)   |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | CONS_CSV_DENTAL_CLM_HX_ID    | CONS_CSV_DENTAL_CLM_HX                       | CONS_CSV_DENTAL_CLM_HX_ID    | 1:1 Mapping                 |
| ...                                        | ...                          | ...                                          | ...                          | ...                         |

**Remarks:**  
- Most fields are 1:1 mapped; some use transformation logic (e.g., `COALESCE` for provider fields).
- All fields are validated for nulls and data types as per `.dml` definitions.
- See the SQL in Input Table for full mapping.

---

## 5. Transformation Logic

### .xfr Functions Used

- **GEN_CSV_FIRST_DEFINED.xfr:** Used to select the first non-null value among candidate fields for output.
- **table_adaptor.xfr:** Adapts input table fields to internal DML structure.
- **IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2.xfr / V353S6P3.xfr:** Custom business logic for field derivation and validation.
- **Generic Reformat:** Used for direct field mapping (`out.* :: in.*`).

**Fields Involved:**  
- All claim, provider, and header fields as per the SQL and DML.

**External Functions:**  
- Standard Ab Initio built-ins for error/log handling.
- No external shell scripts or non-Ab Initio code.

---

## 6. Complexity Analysis

- **Number of Graph Components:** 24 (major vertices: Input Table, multiple Reformats, Sort, Dedup, Partition, Output File, Output Table, etc.)
- **Number of Lines of Code (.xfr/.plan):** ~200+ (across all .xfr and SQL)
- **Transform Functions Used:** 5+ (including custom and generic)
- **Joins Used:** 2 (1 LEFT OUTER, 1 INNER)
- **Lookup Files or Datasets:** 0 (all joins are SQL-based)
- **Parameter Sets (.pset) or Plan Files Used:** 1 (parameterization for environment, DML, and dataset names)
- **Number of Output Datasets:** 2 (GCS multifile, BigQuery staging table)
- **Conditional Logic or if-else flows:** 3+ (in .xfr and SQL)
- **External Dependencies:** JDBC (BigQuery), GCS, Ab Initio standard components
- **Overall Complexity Score:** 85/100 (complex multi-join, multi-step ETL with robust error handling and parameterization)

---

## 7. Key Outputs

- **Final Output:**  
  - `STG_CONS_CSV_DENTAL_CLM_DTL_HX` (BigQuery table)
  - Format: Variable-width, as per `.dml` definition
  - Intended Use: Downstream analytics, reporting, and regulatory submissions

- **Intermediate Output:**  
  - GCS multifile for staging and backup

---

## 8. Error Handling and Logging

- **Reject Ports:** All major transforms (Reformat, Dedup, Output) have reject ports.
- **Error Ports:** Capture transformation and load errors.
- **Log Ports:** Capture operational and data flow logs.
- **Error Tagging:** `.xfr` logic tags records with error codes/messages.
- **Reject Thresholds:** Configurable; typically set to abort on first reject.
- **Control Files:** Not explicitly used; error handling is via reject/error ports.
- **Alerting:** Not shown in code, but logs can be integrated with enterprise alerting.

---

## 9. API Cost (LLM Cost ONLY)

- **Tokens Used (Prompt + Completion):** _Estimate: 13,000 (prompt) + 3,000 (completion) = 16,000_
- **Cost per 1K tokens:** $0.003 (gpt-3.5-turbo, example; adjust as per your provider)
- **Final Cost in USD for this single documentation run:**  
  - 16,000 tokens / 1,000 = 16  
  - 16 * $0.003 = **$0.048**

---

**End of Documentation**