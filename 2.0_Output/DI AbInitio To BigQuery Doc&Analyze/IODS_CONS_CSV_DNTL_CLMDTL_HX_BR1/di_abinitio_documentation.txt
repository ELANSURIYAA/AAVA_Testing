====================================================
Author:        AAVA
Date:          2024-06-10
Description:   Ab Initio Graph Documentation for IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1: Data Integration and Transformation from BigQuery to Staging and Output
====================================================

# IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1 Ab Initio Graph Documentation

## 1. Overview of Graph/Component

This Ab Initio graph orchestrates the extraction, transformation, deduplication, and loading of dental claim detail data from Google BigQuery sources into staging and output datasets. It consolidates multiple BigQuery tables, applies business rules, deduplicates on key columns, and prepares data for downstream reporting and analytics. The graph is designed for high-volume, parallel processing, ensuring data quality and lineage traceability.

**Key Business Requirement:**  
Integrate and enrich dental service line and provider history data with claim header details, deduplicate records, and load the result into both staging and output datasets for regulatory and analytical consumption.

---

## 2. Component Structure and Design

**Major Components:**
- **Input Table:** Reads from BigQuery using a complex SQL join (CSV_5010_DENTAL_SERVICE_LINE_HX, CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX, CONS_CSV_DENTAL_CLM_HX).
- **Reformat (RFMT_V351S3P1_Adaptor_CSV_5010_DNTL_CLMDTL):** Adapts input data to internal DML structure.
- **Sort (SORT_V353S0P3_S_Rmv_Dup_keycols):** Orders data by deduplication keys.
- **Dedup Sorted (DEDU_V353S0_Rmv_Dup_keycols):** Removes duplicates based on composite key.
- **Multiple Reformat Steps:** For field mapping, enrichment, and output dataset adaptation.
- **Partition by Key:** Ensures parallel processing by distributing records.
- **Output Table:** Loads data into BigQuery staging table.
- **Output File:** Writes to a multifile system for downstream use.

**Logical Groupings:**
- **Ingestion:** Input Table → Reformat
- **Transformation:** Reformat → Sort → Dedup → Reformat(s)
- **Output:** Output Table (to BigQuery), Output File (to MFS)
- **Error/Reject/Log:** Each major component has reject, error, and log ports for robust error handling.

**Parameters/Variables:**  
Parameters are used for dynamic dataset names, date ranges, DML paths, and environment-specific variables (e.g., IODS_GCS_TEMP, CSVDNTL_START_DATE/END_DATE).

---

## 3. Data Flow and Processing Logic

### Processed Datasets
- **Input:**  
  - `CSV_5010_DENTAL_SERVICE_LINE_HX` (BigQuery)
  - `CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX` (BigQuery)
  - `CONS_CSV_DENTAL_CLM_HX` (BigQuery)
- **Intermediate:**  
  - Internal multifile datasets (e.g., DNTLCLM_DTL_inserts_2019082807120189.tmp)
- **Output:**  
  - `STG_CONS_CSV_DENTAL_CLM_DTL_HX` (BigQuery staging table)
  - Output multifile (MFS) for downstream processing

### Data Flow Description

1. **Ingestion:**  
   - Input Table component executes a SQL query joining three BigQuery tables, filtering by load date range.
2. **Initial Reformat:**  
   - Adapts input records to internal DML using `$AI_XFR/table_adaptor.xfr`.
3. **Deduplication Preparation:**  
   - Data is sorted on composite key (`AK_UCK_ID`, `AK_UCK_ID_PREFIX_CD`, `AK_UCK_ID_SEGMENT_NO`, `AK_SUBMT_SVC_LN_NO`).
4. **Deduplication:**  
   - Dedup Sorted component removes duplicate records, keeping the first occurrence.
5. **Field Mapping and Enrichment:**  
   - Multiple Reformat components apply business rules, transformations, and field mappings.
6. **Partitioning:**  
   - Partition by Key component distributes data for parallel output.
7. **Output:**  
   - Data is written to both a BigQuery staging table and a multifile output.
8. **Error Handling:**  
   - Rejects, errors, and logs are captured at each transformation step.

---

## 4. Data Mapping (Lineage)

| Target Table                               | Target Column                  | Source Table                                 | Source Column                  | Remarks                                |
|--------------------------------------------|-------------------------------|----------------------------------------------|-------------------------------|----------------------------------------|
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | AK_UCK_ID                     | CSV_5010_DENTAL_SERVICE_LINE_HX              | UCK_ID                        | 1:1 Mapping                            |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | AK_UCK_ID_PREFIX_CD           | CSV_5010_DENTAL_SERVICE_LINE_HX              | UCK_ID_PREFIX_CD              | 1:1 Mapping                            |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | AK_UCK_ID_SEGMENT_NO          | CSV_5010_DENTAL_SERVICE_LINE_HX              | UCK_ID_SEGMENT_NO             | 1:1 Mapping                            |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | AK_SUBMT_SVC_LN_NO            | CSV_5010_DENTAL_SERVICE_LINE_HX              | SUBMT_SVC_LN_NO               | 1:1 Mapping                            |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | ADJT_REPRC_CLM_NO_TXT         | CSV_5010_DENTAL_SERVICE_LINE_HX              | ADJT_REPRC_CLM_NO_TXT         | 1:1 Mapping                            |
| ...                                        | ...                           | ...                                          | ...                           | ...                                    |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | ASRG_ENTY_TYP_QLFR_CD         | CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX      | ASRG_ENTY_TYP_QLFR_CD         | Transformation (COALESCE, default '')  |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | CONS_CSV_DENTAL_CLM_HX_ID     | CONS_CSV_DENTAL_CLM_HX                        | CONS_CSV_DENTAL_CLM_HX_ID     | 1:1 Mapping (via JOIN)                 |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | STK_UCK_ID                    | CSV_5010_DENTAL_SERVICE_LINE_HX              | STK_UCK_ID                    | Transformation (CASE, COALESCE)        |
| STG_CONS_CSV_DENTAL_CLM_DTL_HX             | STK_REND_PROV_NO              | CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX      | STK_REND_PROV_NO              | Transformation (CASE, COALESCE)        |
| ...                                        | ...                           | ...                                          | ...                           | ...                                    |

*Remarks:*  
- 1:1 Mapping: Direct field copy  
- Transformation: COALESCE, CASE, or other logic as per SQL  
- Validation: Data type conversions, null handling, and deduplication

---

## 5. Transformation Logic

**.xfr Functions Used:**
- `$AI_XFR/table_adaptor.xfr`: Adapts BigQuery input to internal DML.
- `$AI_XFR/GEN_CSV_FIRST_DEFINED.xfr`: Applies "first defined" logic for field population.
- `$AI_XFR/IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2.xfr` and `$AI_XFR/IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P3.xfr`: Custom business logic for output mapping.
- Inline Reformat: `out.* :: in.*;` (direct field pass-through).

**Key Transformations:**
- Null handling via COALESCE
- Conditional logic for numeric fields (e.g., STK_UCK_ID: `CASE WHEN ... THEN COALESCE(...) ELSE 0 END`)
- Field enrichment and data type conversions
- Deduplication on composite key

---

## 6. Complexity Analysis

- **Number of Graph Components:** 20+ (Input Table, Reformat, Sort, Dedup, Partition, Output Table, Output File, multiple Reformat steps, error/reject/log ports)
- **Number of Lines of Code (in .xfr or .plan):** ~2000+ (including SQL, DML, and XFR logic)
- **Transform Functions Used:** 5+ (.xfr and inline)
- **Joins Used:** 2 (LEFT OUTER JOIN, INNER JOIN in SQL)
- **Lookup Files or Datasets:** None (joins handled in SQL)
- **Parameter Sets (.pset) or Plan Files Used:** 1+ (parameterized DMLs, environment variables)
- **Number of Output Datasets:** 2 (BigQuery staging table, MFS output file)
- **Conditional Logic or if-else flows:** 5+ (CASE, COALESCE, deduplication, null handling)
- **External Dependencies:** JDBC (BigQuery), shell/environment variables, DML/XFR includes
- **Overall Complexity Score:** 85/100 (high due to multi-source joins, deduplication, parallelization, and robust error handling)

---

## 7. Key Outputs

- **BigQuery Table:**  
  - `STG_CONS_CSV_DENTAL_CLM_DTL_HX` (variable, parameterized name)
  - Format: Variable-width, delimited (BigQuery table)
  - Use: Downstream analytics, regulatory reporting

- **Multifile Output:**  
  - Path: `$IODS_GCS_TEMP/DNTLCLM_DTL_inserts_*.tmp`
  - Format: Ab Initio multifile system (MFS), delimited
  - Use: Downstream ETL, archiving, or further processing

---

## 8. Error Handling and Logging

- **Reject Ports:**  
  - Each major component (Reformat, Dedup, Output Table/File) has reject ports capturing records failing business rules or DML constraints.
- **Error Ports:**  
  - Errors are routed to error files with detailed error_info DML.
- **Log Ports:**  
  - Logging is enabled for operational events, with log_info DML.
- **Thresholds:**  
  - `Abort on first reject` is set for critical components.
- **Control Files:**  
  - Not explicitly used, but environment variables and parameters control output paths and DMLs.
- **Alerting:**  
  - Not specified, but logs/errors can be integrated with enterprise alerting.

---

## 9. API Cost (LLM Cost ONLY)

- **Tokens Used (Prompt + Completion):** *Estimate: 18,000 tokens*  
- **Cost per 1K tokens:** $0.003  
- **Final Cost in USD for this single documentation run:** $0.054

---

*End of Documentation*