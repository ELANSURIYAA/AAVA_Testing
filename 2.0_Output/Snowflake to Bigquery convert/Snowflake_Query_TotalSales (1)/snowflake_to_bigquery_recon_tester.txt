```python
"""
Automated Snowflake to BigQuery Migration Validation Script

This script executes the following end-to-end workflow:
1. Executes provided Snowflake SQL code and extracts result data.
2. Exports target tables from Snowflake to CSV, then converts to Parquet.
3. Uploads Parquet files to Google Cloud Storage.
4. Creates BigQuery external tables referencing the Parquet files.
5. Executes the provided BigQuery SQL code.
6. Compares the results (row counts, column-by-column, sample mismatches).
7. Generates a detailed comparison report and summary.
8. Implements robust error handling, logging, and security best practices.

Requirements:
- snowflake-connector-python
- pandas
- pyarrow
- google-cloud-storage
- google-cloud-bigquery
- python-dotenv (for environment variable management)
"""

import os
import sys
import time
import logging
import tempfile
import traceback
import uuid
import json
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from datetime import datetime

# Snowflake
import snowflake.connector

# GCP
from google.cloud import storage
from google.cloud import bigquery

# Load environment variables securely
from dotenv import load_dotenv
load_dotenv()

# ------------------- CONFIGURATION -------------------
# Environment variables (do NOT hardcode secrets)
SNOWFLAKE_USER = os.getenv('SNOWFLAKE_USER')
SNOWFLAKE_PASSWORD = os.getenv('SNOWFLAKE_PASSWORD')
SNOWFLAKE_ACCOUNT = os.getenv('SNOWFLAKE_ACCOUNT')
SNOWFLAKE_WAREHOUSE = os.getenv('SNOWFLAKE_WAREHOUSE')
SNOWFLAKE_DATABASE = os.getenv('SNOWFLAKE_DATABASE')
SNOWFLAKE_SCHEMA = os.getenv('SNOWFLAKE_SCHEMA')

GCP_PROJECT = os.getenv('GCP_PROJECT')
GCS_BUCKET = os.getenv('GCS_BUCKET')
BQ_DATASET = os.getenv('BQ_DATASET')
GOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')

# Input SQL files (or pass as arguments)
SNOWFLAKE_SQL_FILE = "Snowflake_Query_TotalSales (1).sql"
BIGQUERY_SQL_FILE = None  # If None, use string below

# BigQuery SQL (from migration)
BIGQUERY_SQL = """
WITH customer_sales AS (
    SELECT 
        c.customer_id,
        c.customer_name,
        c.region,
        CAST(c.metadata['loyalty_level'] AS STRING) AS loyalty_level,
        SUM(s.sale_amount) AS total_sales,
        COUNT(s.sale_id) AS total_orders,
        ARRAY_AGG(DISTINCT s.product_id) AS product_list
    FROM 
        `{dataset}.Customers` c
    LEFT JOIN 
        `{dataset}.Sales` s ON c.customer_id = s.customer_id
    WHERE 
        s.sale_date >= '2023-01-01' 
        AND s.sale_date < '2024-01-01'
        AND CAST(s.sale_metadata['discount_applied'] AS BOOL) = TRUE
    GROUP BY 
        c.customer_id, c.customer_name, c.region, c.metadata['loyalty_level']
),
top_customers AS (
    SELECT 
        customer_id,
        customer_name,
        region,
        loyalty_level,
        total_sales,
        total_orders,
        product_list,
        RANK() OVER (PARTITION BY region ORDER BY total_sales DESC) AS region_rank
    FROM 
        customer_sales
),
sales_performance AS (
    SELECT
        s.sale_id,
        s.sale_date,
        s.sale_amount,
        s.discount_percentage,
        CAST(s.sale_metadata['source'] AS STRING) AS source,
        p.product_name,
        CASE 
            WHEN s.sale_amount > 1000 THEN 'High Value'
            WHEN s.sale_amount > 500 THEN 'Medium Value'
            ELSE 'Low Value'
        END AS sale_category
    FROM 
        `{dataset}.Sales` s
    INNER JOIN 
        `{dataset}.Products` p ON s.product_id = p.product_id
    WHERE 
        s.sale_date >= '2023-01-01'
        AND CAST(s.sale_metadata['country'] AS STRING) = 'USA'
)
SELECT 
    tc.customer_name,
    tc.region,
    tc.loyalty_level,
    tc.total_sales,
    tc.total_orders,
    sp.sale_id,
    sp.sale_date,
    sp.product_name,
    sp.sale_category,
    sp.source
FROM 
    top_customers tc
LEFT JOIN 
    sales_performance sp ON tc.customer_id = sp.customer_id
WHERE 
    tc.region_rank <= 5
ORDER BY 
    tc.region, tc.region_rank, sp.sale_date
"""

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('migration_validation.log', mode='w')
    ]
)

def log_and_print(msg):
    logging.info(msg)
    print(msg)

# ------------------- UTILITY FUNCTIONS -------------------

def get_snowflake_connection():
    try:
        ctx = snowflake.connector.connect(
            user=SNOWFLAKE_USER,
            password=SNOWFLAKE_PASSWORD,
            account=SNOWFLAKE_ACCOUNT,
            warehouse=SNOWFLAKE_WAREHOUSE,
            database=SNOWFLAKE_DATABASE,
            schema=SNOWFLAKE_SCHEMA,
            autocommit=True
        )
        log_and_print("Connected to Snowflake.")
        return ctx
    except Exception as e:
        logging.error(f"Snowflake connection error: {e}")
        raise

def get_bq_client():
    try:
        client = bigquery.Client(project=GCP_PROJECT)
        log_and_print("Connected to BigQuery.")
        return client
    except Exception as e:
        logging.error(f"BigQuery connection error: {e}")
        raise

def get_gcs_client():
    try:
        client = storage.Client(project=GCP_PROJECT)
        log_and_print("Connected to Google Cloud Storage.")
        return client
    except Exception as e:
        logging.error(f"GCS connection error: {e}")
        raise

def safe_execute(cursor, sql):
    try:
        cursor.execute(sql)
        log_and_print("Executed SQL successfully.")
        return cursor
    except Exception as e:
        logging.error(f"SQL execution error: {e}\nSQL: {sql}")
        raise

def get_table_schema_from_snowflake(cursor, table_name):
    cursor.execute(f"DESC TABLE {table_name}")
    schema = cursor.fetchall()
    # schema: list of tuples (name, type, ...)
    return [(row[0], row[1]) for row in schema]

def fetch_table_to_df(cursor, table_name):
    cursor.execute(f"SELECT * FROM {table_name}")
    columns = [desc[0] for desc in cursor.description]
    rows = cursor.fetchall()
    df = pd.DataFrame(rows, columns=columns)
    return df

def export_df_to_parquet(df, table_name, export_dir):
    ts = datetime.utcnow().strftime('%Y%m%d%H%M%S')
    file_name = f"{table_name}_{ts}.parquet"
    file_path = os.path.join(export_dir, file_name)
    table = pa.Table.from_pandas(df)
    pq.write_table(table, file_path)
    return file_path

def upload_to_gcs(local_path, bucket_name, dest_blob_name):
    client = get_gcs_client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(dest_blob_name)
    blob.upload_from_filename(local_path)
    log_and_print(f"Uploaded {local_path} to gs://{bucket_name}/{dest_blob_name}")
    # Integrity check
    remote_md5 = blob.md5_hash
    local_md5 = None
    try:
        import hashlib, base64
        with open(local_path, 'rb') as f:
            local_md5 = base64.b64encode(hashlib.md5(f.read()).digest()).decode('utf-8')
        assert local_md5 == remote_md5, f"MD5 mismatch: {local_md5} != {remote_md5}"
        log_and_print(f"Integrity check passed for {dest_blob_name}")
    except Exception as e:
        logging.error(f"Integrity check failed: {e}")
        raise
    return f"gs://{bucket_name}/{dest_blob_name}"

def create_bq_external_table(bq_client, table_name, gcs_uri, schema):
    dataset_ref = bq_client.dataset(BQ_DATASET)
    table_ref = dataset_ref.table(table_name)
    external_config = bigquery.ExternalConfig('PARQUET')
    external_config.source_uris = [gcs_uri]
    # Map schema to BigQuery format
    bq_schema = []
    for col_name, col_type in schema:
        # Simple mapping, can be extended for more types
        if col_type.upper().startswith('VARCHAR') or col_type.upper().startswith('STRING'):
            bq_type = 'STRING'
        elif col_type.upper().startswith('NUMBER') or col_type.upper().startswith('INT'):
            bq_type = 'INTEGER'
        elif col_type.upper().startswith('FLOAT') or col_type.upper().startswith('DOUBLE'):
            bq_type = 'FLOAT'
        elif col_type.upper().startswith('BOOLEAN'):
            bq_type = 'BOOLEAN'
        elif col_type.upper().startswith('DATE'):
            bq_type = 'DATE'
        elif col_type.upper().startswith('TIMESTAMP'):
            bq_type = 'TIMESTAMP'
        elif col_type.upper().startswith('ARRAY'):
            bq_type = 'STRING'  # Store as JSON string
        else:
            bq_type = 'STRING'
        bq_schema.append(bigquery.SchemaField(col_name, bq_type))
    external_config.schema = bq_schema
    table = bigquery.Table(table_ref)
    table.external_data_configuration = external_config
    table = bq_client.create_table(table, exists_ok=True)
    log_and_print(f"Created BigQuery external table {table_name} for {gcs_uri}")
    return table

def compare_dataframes(df1, df2, key_columns=None, float_tol=1e-6):
    """
    Compare two DataFrames, return match status, row count diff, column diffs, and sample mismatches.
    """
    report = {}
    report['row_count_1'] = len(df1)
    report['row_count_2'] = len(df2)
    report['row_count_match'] = (len(df1) == len(df2))
    report['columns_1'] = list(df1.columns)
    report['columns_2'] = list(df2.columns)
    report['column_match'] = (set(df1.columns) == set(df2.columns))
    mismatches = []
    if not report['column_match']:
        report['column_discrepancies'] = {
            'only_in_1': list(set(df1.columns) - set(df2.columns)),
            'only_in_2': list(set(df2.columns) - set(df1.columns))
        }
    # Row-by-row comparison
    if report['row_count_match'] and report['column_match']:
        # Sort if key_columns provided
        if key_columns:
            df1 = df1.sort_values(key_columns).reset_index(drop=True)
            df2 = df2.sort_values(key_columns).reset_index(drop=True)
        else:
            df1 = df1.reset_index(drop=True)
            df2 = df2.reset_index(drop=True)
        for idx in range(len(df1)):
            row1 = df1.iloc[idx]
            row2 = df2.iloc[idx]
            row_diff = {}
            for col in df1.columns:
                v1 = row1[col]
                v2 = row2[col]
                if pd.isnull(v1) and pd.isnull(v2):
                    continue
                elif isinstance(v1, float) and isinstance(v2, float):
                    if abs(v1 - v2) > float_tol:
                        row_diff[col] = (v1, v2)
                elif v1 != v2:
                    row_diff[col] = (v1, v2)
            if row_diff:
                mismatches.append({'row': idx, 'diff': row_diff})
            if len(mismatches) >= 10:
                break
    report['num_mismatches'] = len(mismatches)
    report['sample_mismatches'] = mismatches
    if report['row_count_match'] and report['column_match'] and len(mismatches) == 0:
        report['match_status'] = 'MATCH'
    elif report['row_count_match'] and report['column_match'] and len(mismatches) > 0:
        report['match_status'] = 'PARTIAL MATCH'
    else:
        report['match_status'] = 'NO MATCH'
    return report

def save_report(report, file_path):
    with open(file_path, 'w') as f:
        json.dump(report, f, indent=2)
    log_and_print(f"Saved comparison report to {file_path}")

# ------------------- MAIN WORKFLOW -------------------

def main():
    try:
        log_and_print("=== Snowflake to BigQuery Migration Validation Script ===")
        # Step 1: Read Snowflake SQL
        with open(SNOWFLAKE_SQL_FILE, 'r') as f:
            snowflake_sql = f.read()
        log_and_print("Loaded Snowflake SQL.")
        # Step 2: Read BigQuery SQL
        if BIGQUERY_SQL_FILE:
            with open(BIGQUERY_SQL_FILE, 'r') as f:
                bigquery_sql = f.read()
        else:
            bigquery_sql = BIGQUERY_SQL
        # Step 3: Connect to Snowflake
        sf_conn = get_snowflake_connection()
        sf_cursor = sf_conn.cursor()
        # Step 4: Execute Snowflake SQL (materialize as temp table)
        temp_table = f"TEMP_TOTALSALES_{uuid.uuid4().hex[:8].upper()}"
        create_temp_sql = f"CREATE OR REPLACE TEMPORARY TABLE {temp_table} AS {snowflake_sql}"
        log_and_print(f"Creating temp table in Snowflake: {temp_table}")
        safe_execute(sf_cursor, create_temp_sql)
        # Step 5: Export temp table to CSV, then Parquet
        export_dir = tempfile.mkdtemp()
        log_and_print(f"Export directory: {export_dir}")
        # Fetch data to DataFrame
        df = fetch_table_to_df(sf_cursor, temp_table)
        log_and_print(f"Fetched {len(df)} rows from Snowflake temp table.")
        # Get schema for BQ mapping
        schema = get_table_schema_from_snowflake(sf_cursor, temp_table)
        # Export to Parquet
        parquet_path = export_df_to_parquet(df, temp_table, export_dir)
        log_and_print(f"Exported Parquet file: {parquet_path}")
        # Step 6: Upload Parquet to GCS
        gcs_blob_name = f"snowflake_exports/{os.path.basename(parquet_path)}"
        gcs_uri = upload_to_gcs(parquet_path, GCS_BUCKET, gcs_blob_name)
        # Step 7: Create BigQuery external table
        bq_client = get_bq_client()
        ext_table_name = f"{temp_table}_EXT"
        create_bq_external_table(bq_client, ext_table_name, gcs_uri, schema)
        # Step 8: Execute BigQuery SQL
        # Patch dataset name in SQL
        bigquery_sql_fmt = bigquery_sql.replace("{dataset}", BQ_DATASET)
        log_and_print("Executing BigQuery SQL...")
        job = bq_client.query(bigquery_sql_fmt)
        bq_result = job.result()
        bq_rows = [dict(row) for row in bq_result]
        df_bq = pd.DataFrame(bq_rows)
        log_and_print(f"BigQuery query returned {len(df_bq)} rows.")
        # Step 9: Compare DataFrames
        log_and_print("Comparing Snowflake and BigQuery results...")
        # Key columns for sorting (if present)
        key_columns = ['customer_name', 'region', 'sale_id', 'sale_date']
        for col in key_columns:
            if col not in df.columns or col not in df_bq.columns:
                key_columns = None
                break
        report = compare_dataframes(df, df_bq, key_columns=key_columns)
        # Step 10: Save report
        ts = datetime.utcnow().strftime('%Y%m%d%H%M%S')
        report_file = f"comparison_report_{ts}.json"
        save_report(report, report_file)
        # Step 11: Print summary
        log_and_print("=== SUMMARY REPORT ===")
        log_and_print(json.dumps(report, indent=2))
        log_and_print("=== END OF VALIDATION ===")
    except Exception as e:
        logging.error(f"Fatal error: {e}")
        logging.error(traceback.format_exc())
        print(f"ERROR: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```