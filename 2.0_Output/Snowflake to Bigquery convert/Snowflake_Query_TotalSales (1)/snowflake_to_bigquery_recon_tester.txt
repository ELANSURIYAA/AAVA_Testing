import os
import sys
import logging
import time
import traceback
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import snowflake.connector
from google.cloud import storage
from google.cloud import bigquery

# ==============================
# CONFIGURATION & ENV VARIABLES
# ==============================
# Set these environment variables securely before running:
# SNOWFLAKE_USER, SNOWFLAKE_PASSWORD, SNOWFLAKE_ACCOUNT, SNOWFLAKE_DATABASE, SNOWFLAKE_SCHEMA, SNOWFLAKE_WAREHOUSE
# GCP_PROJECT, GOOGLE_APPLICATION_CREDENTIALS (path to service account JSON)
# GCS_BUCKET (target bucket for Parquet files)
# BQ_DATASET (target BigQuery dataset)

# ==============================
# LOGGING SETUP
# ==============================
LOG_FILE = "migration_validation.log"
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s"
)
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
console.setFormatter(formatter)
logging.getLogger('').addHandler(console)

def log_status(msg):
    logging.info(msg)
    print(msg)

# ==============================
# ERROR HANDLING DECORATOR
# ==============================
def robust(func):
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logging.error(f"Error in {func.__name__}: {str(e)}\n{traceback.format_exc()}")
            print(f"Error in {func.__name__}: {str(e)}")
            return None
    return wrapper

# ==============================
# STEP 1: ANALYZE INPUTS
# ==============================
# For this script, we expect the Snowflake SQL and BigQuery SQL as files.
SNOWFLAKE_SQL_FILE = "Snowflake_Query_TotalSales.sql"
BIGQUERY_SQL_FILE = "BigQuery_Query_TotalSales.sql"

@robust
def read_sql_file(path):
    with open(path, "r") as f:
        return f.read()

# ==============================
# STEP 2: CREATE CONNECTION COMPONENTS
# ==============================
@robust
def get_snowflake_connection():
    return snowflake.connector.connect(
        user=os.environ["SNOWFLAKE_USER"],
        password=os.environ["SNOWFLAKE_PASSWORD"],
        account=os.environ["SNOWFLAKE_ACCOUNT"],
        warehouse=os.environ["SNOWFLAKE_WAREHOUSE"],
        database=os.environ["SNOWFLAKE_DATABASE"],
        schema=os.environ["SNOWFLAKE_SCHEMA"]
    )

@robust
def get_gcs_client():
    return storage.Client(project=os.environ["GCP_PROJECT"])

@robust
def get_bq_client():
    return bigquery.Client(project=os.environ["GCP_PROJECT"])

# ==============================
# STEP 3: EXECUTE SNOWFLAKE CODE
# ==============================
@robust
def execute_snowflake_query(conn, query):
    cur = conn.cursor()
    cur.execute(query)
    columns = [desc[0] for desc in cur.description]
    rows = cur.fetchall()
    df = pd.DataFrame(rows, columns=columns)
    cur.close()
    return df

# ==============================
# STEP 4: EXPORT & TRANSFORM DATA
# ==============================
@robust
def export_table_to_parquet(conn, table_name, output_dir):
    query = f"SELECT * FROM {table_name}"
    cur = conn.cursor()
    cur.execute(query)
    columns = [desc[0] for desc in cur.description]
    rows = cur.fetchall()
    df = pd.DataFrame(rows, columns=columns)
    cur.close()
    parquet_path = os.path.join(
        output_dir,
        f"{table_name}_{int(time.time())}.parquet"
    )
    table = pa.Table.from_pandas(df)
    pq.write_table(table, parquet_path)
    log_status(f"Exported {table_name} to {parquet_path}")
    return parquet_path

# ==============================
# STEP 5: TRANSFER TO GCS
# ==============================
@robust
def upload_to_gcs(gcs_client, bucket_name, local_path, remote_path):
    bucket = gcs_client.bucket(bucket_name)
    blob = bucket.blob(remote_path)
    blob.upload_from_filename(local_path)
    # Integrity check
    local_size = os.path.getsize(local_path)
    remote_size = blob.size
    if local_size != remote_size:
        raise Exception(f"File size mismatch for {remote_path}: local={local_size}, remote={remote_size}")
    log_status(f"Uploaded {local_path} to gs://{bucket_name}/{remote_path}")
    return f"gs://{bucket_name}/{remote_path}"

# ==============================
# STEP 6: CREATE BIGQUERY EXTERNAL TABLES
# ==============================
@robust
def create_external_table(bq_client, dataset, table_name, gcs_uri, schema):
    table_id = f"{bq_client.project}.{dataset}.{table_name}_external"
    external_config = bigquery.ExternalConfig("PARQUET")
    external_config.source_uris = [gcs_uri]
    external_config.autodetect = True
    table = bigquery.Table(table_id, schema=schema)
    table.external_data_configuration = external_config
    bq_client.delete_table(table_id, not_found_ok=True)
    table = bq_client.create_table(table)
    log_status(f"Created external table {table_id} pointing to {gcs_uri}")
    return table_id

# ==============================
# STEP 7: EXECUTE BIGQUERY CODE
# ==============================
@robust
def execute_bigquery_query(bq_client, query):
    job = bq_client.query(query)
    df = job.result().to_dataframe()
    return df

# ==============================
# STEP 8: COMPARISON LOGIC
# ==============================
@robust
def compare_tables(df1, df2, key_columns=None):
    report = {}
    # Row count comparison
    report['row_count_snowflake'] = len(df1)
    report['row_count_bigquery'] = len(df2)
    report['row_count_match'] = len(df1) == len(df2)
    # Column comparison
    report['columns_snowflake'] = list(df1.columns)
    report['columns_bigquery'] = list(df2.columns)
    report['columns_match'] = set(df1.columns) == set(df2.columns)
    # Data comparison
    mismatches = []
    match_count = 0
    total = min(len(df1), len(df2))
    if total == 0:
        report['match_percentage'] = 1.0 if len(df1) == len(df2) else 0.0
        report['mismatches'] = []
        return report
    # Use key columns if provided, else index
    if key_columns and all(col in df1.columns and col in df2.columns for col in key_columns):
        df1_sorted = df1.sort_values(key_columns).reset_index(drop=True)
        df2_sorted = df2.sort_values(key_columns).reset_index(drop=True)
    else:
        df1_sorted = df1.reset_index(drop=True)
        df2_sorted = df2.reset_index(drop=True)
    for i in range(total):
        row1 = df1_sorted.iloc[i].to_dict()
        row2 = df2_sorted.iloc[i].to_dict()
        if row1 == row2:
            match_count += 1
        else:
            mismatches.append({'row_snowflake': row1, 'row_bigquery': row2})
            if len(mismatches) >= 10:
                break
    report['match_percentage'] = match_count / total if total else 1.0
    if report['match_percentage'] == 1.0 and report['row_count_match'] and report['columns_match']:
        report['status'] = "MATCH"
    elif report['match_percentage'] > 0.8:
        report['status'] = "PARTIAL MATCH"
    else:
        report['status'] = "NO MATCH"
    report['mismatches'] = mismatches
    return report

# ==============================
# STEP 9: REPORTING
# ==============================
@robust
def generate_report(table_reports):
    summary = []
    for table, report in table_reports.items():
        summary.append({
            "table": table,
            "status": report.get("status"),
            "row_count_snowflake": report.get("row_count_snowflake"),
            "row_count_bigquery": report.get("row_count_bigquery"),
            "match_percentage": report.get("match_percentage"),
            "column_match": report.get("columns_match"),
            "sample_mismatches": report.get("mismatches")[:3] if report.get("mismatches") else []
        })
    report_df = pd.DataFrame(summary)
    report_df.to_csv("comparison_report.csv", index=False)
    log_status("Comparison report generated: comparison_report.csv")
    return summary

# ==============================
# STEP 10: MAIN MIGRATION & VALIDATION LOGIC
# ==============================
@robust
def main():
    log_status("Starting Snowflake to BigQuery migration validation process.")
    # Read SQL files
    snowflake_sql = read_sql_file(SNOWFLAKE_SQL_FILE)
    bigquery_sql = read_sql_file(BIGQUERY_SQL_FILE)
    # Connect to Snowflake
    conn_sf = get_snowflake_connection()
    # Execute Snowflake SQL
    log_status("Executing Snowflake SQL code...")
    df_sf = execute_snowflake_query(conn_sf, snowflake_sql)
    log_status(f"Snowflake query returned {len(df_sf)} rows.")
    # Export Snowflake output to Parquet
    output_dir = "exports"
    os.makedirs(output_dir, exist_ok=True)
    parquet_path = os.path.join(output_dir, f"final_output_{int(time.time())}.parquet")
    table = pa.Table.from_pandas(df_sf)
    pq.write_table(table, parquet_path)
    log_status(f"Snowflake output exported to Parquet: {parquet_path}")
    # Upload Parquet to GCS
    gcs_client = get_gcs_client()
    bucket_name = os.environ["GCS_BUCKET"]
    remote_path = os.path.basename(parquet_path)
    gcs_uri = upload_to_gcs(gcs_client, bucket_name, parquet_path, remote_path)
    # Create BigQuery external table
    bq_client = get_bq_client()
    dataset = os.environ["BQ_DATASET"]
    # Autodetect schema from Parquet
    schema = []
    for col, dtype in zip(df_sf.columns, df_sf.dtypes):
        if pd.api.types.is_integer_dtype(dtype):
            field_type = "INTEGER"
        elif pd.api.types.is_float_dtype(dtype):
            field_type = "FLOAT"
        elif pd.api.types.is_bool_dtype(dtype):
            field_type = "BOOLEAN"
        elif pd.api.types.is_datetime64_any_dtype(dtype):
            field_type = "TIMESTAMP"
        else:
            field_type = "STRING"
        schema.append(bigquery.SchemaField(col, field_type))
    ext_table_id = create_external_table(bq_client, dataset, "final_output", gcs_uri, schema)
    # Execute BigQuery SQL
    log_status("Executing BigQuery SQL code...")
    df_bq = execute_bigquery_query(bq_client, bigquery_sql)
    log_status(f"BigQuery query returned {len(df_bq)} rows.")
    # Compare outputs
    log_status("Comparing Snowflake and BigQuery outputs...")
    table_reports = {}
    table_reports["final_output"] = compare_tables(df_sf, df_bq)
    # Generate report
    report_summary = generate_report(table_reports)
    log_status("Migration validation complete.")
    return report_summary

if __name__ == "__main__":
    result = main()
    print("Final Comparison Summary:")
    print(result)

# ==============================
# SECURITY NOTES
# ==============================
# - All credentials must be set via environment variables.
# - No credentials are hardcoded.
# - Uses secure connections for Snowflake, GCS, and BigQuery.
# - Logging is enabled for audit and troubleshooting.

# ==============================
# PERFORMANCE NOTES
# ==============================
# - Efficient Parquet export and GCS upload.
# - Batching can be added for large tables.
# - Progress reporting via logs.
# - Handles large datasets with pandas and pyarrow.

# ==============================
# ERROR HANDLING
# ==============================
# - All major steps wrapped in robust error handling.
# - Clear error messages and logs for troubleshooting.
# - Recovers from failures where possible.

# ==============================
# USAGE
# ==============================
# 1. Set all required environment variables.
# 2. Place the Snowflake SQL and BigQuery SQL files in the working directory.
# 3. Run the script in an automated environment.
# 4. Review logs and the generated comparison_report.csv for results.

# ==============================
# END OF SCRIPT
# ==============================

# API Cost for this call: 0.0083 USD