Test Case List:
-------------------------------------------------
Test ID: TC01
Test Description: Happy path - Multiple users with completed orders, each with multiple products/categories.
Expected Output: Correct aggregation per user: total_orders, total_spent, avg_order_value, distinct categories_bought, sorted by total_spent descending.

Test ID: TC02
Test Description: Edge case - User with no completed orders.
Expected Output: User is not present in the output.

Test ID: TC03
Test Description: Edge case - Order with zero total_amount.
Expected Output: Order is counted, total_spent and avg_order_value reflect zero values appropriately.

Test ID: TC04
Test Description: Boundary - Single user with a single completed order and single product.
Expected Output: Aggregation returns correct values for one order, one category.

Test ID: TC05
Test Description: Null handling - Orders with null product_category.
Expected Output: Null product_category is either excluded from categories_bought or handled as per warehouse semantics.

Test ID: TC06
Test Description: Data integrity - Ensure join logic excludes orders with missing user/product/order_item references.
Expected Output: Only fully joined records are included.

Test ID: TC07
Test Description: Aggregation - Multiple orders for a user with overlapping product categories.
Expected Output: categories_bought contains distinct categories only.

Test ID: TC08
Test Description: Performance - Large input dataset.
Expected Output: Query completes successfully and returns correct aggregations.

Test ID: TC09
Test Description: Error handling - Invalid data types in total_amount.
Expected Output: Query fails gracefully or ignores invalid records.

Test ID: TC10
Test Description: Compliance - DBT ref syntax is used for all table references.
Expected Output: All table references use {{ ref('table_name') }} syntax.

-------------------------------------------------
Pytest Script (for PySpark, covering above test cases):

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.functions import col, count, sum as _sum, avg, collect_set

@pytest.fixture(scope="function")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("DBTUnitTest").getOrCreate()
    yield spark
    spark.stop()

def create_test_data(spark, orders, users, order_items, products):
    df_orders = spark.createDataFrame([Row(**x) for x in orders])
    df_users = spark.createDataFrame([Row(**x) for x in users])
    df_order_items = spark.createDataFrame([Row(**x) for x in order_items])
    df_products = spark.createDataFrame([Row(**x) for x in products])
    return df_orders, df_users, df_order_items, df_products

def run_customer_orders(spark, df_orders, df_users, df_order_items, df_products):
    # Simulate the SQL logic in PySpark
    customer_orders = (
        df_orders.alias("o")
        .join(df_users.alias("u"), col("o.user_id") == col("u.id"))
        .join(df_order_items.alias("oi"), col("o.id") == col("oi.order_id"))
        .join(df_products.alias("p"), col("oi.product_id") == col("p.id"))
        .where(col("o.status") == "completed")
        .select(
            col("o.id").alias("order_id"),
            col("o.user_id"),
            col("u.name").alias("customer_name"),
            col("u.email").alias("customer_email"),
            col("o.total_amount"),
            col("o.created_at").alias("order_date"),
            col("p.category").alias("product_category"),
            col("p.price").alias("product_price"),
        )
    )
    order_summary = (
        customer_orders.groupBy("user_id", "customer_name", "customer_email")
        .agg(
            count("order_id").alias("total_orders"),
            _sum("total_amount").alias("total_spent"),
            avg("total_amount").alias("avg_order_value"),
            collect_set("product_category").alias("categories_bought"),
        )
        .orderBy(col("total_spent").desc())
    )
    return order_summary

# TC01: Happy path
def test_happy_path(spark):
    """
    Test multiple users with completed orders, each with multiple products/categories.
    """
    orders = [
        {"id": 1, "user_id": 10, "total_amount": 100.0, "created_at": "2024-06-01", "status": "completed"},
        {"id": 2, "user_id": 10, "total_amount": 200.0, "created_at": "2024-06-02", "status": "completed"},
        {"id": 3, "user_id": 20, "total_amount": 300.0, "created_at": "2024-06-03", "status": "completed"},
    ]
    users = [
        {"id": 10, "name": "Alice", "email": "alice@example.com"},
        {"id": 20, "name": "Bob", "email": "bob@example.com"},
    ]
    order_items = [
        {"order_id": 1, "product_id": 100},
        {"order_id": 1, "product_id": 101},
        {"order_id": 2, "product_id": 102},
        {"order_id": 3, "product_id": 103},
    ]
    products = [
        {"id": 100, "category": "Books", "price": 50.0},
        {"id": 101, "category": "Electronics", "price": 50.0},
        {"id": 102, "category": "Books", "price": 200.0},
        {"id": 103, "category": "Clothing", "price": 300.0},
    ]
    df_orders, df_users, df_order_items, df_products = create_test_data(spark, orders, users, order_items, products)
    result = run_customer_orders(spark, df_orders, df_users, df_order_items, df_products).collect()
    assert len(result) == 2
    assert result[0]["user_id"] == 20
    assert result[0]["total_spent"] == 300.0
    assert set(result[0]["categories_bought"]) == {"Clothing"}
    assert result[1]["user_id"] == 10
    assert result[1]["total_spent"] == 300.0
    assert set(result[1]["categories_bought"]) == {"Books", "Electronics"}

# TC02: User with no completed orders
def test_user_no_completed_orders(spark):
    """
    Test user with no completed orders is not present in output.
    """
    orders = [
        {"id": 1, "user_id": 10, "total_amount": 100.0, "created_at": "2024-06-01", "status": "pending"},
    ]
    users = [
        {"id": 10, "name": "Alice", "email": "alice@example.com"},
    ]
    order_items = [
        {"order_id": 1, "product_id": 100},
    ]
    products = [
        {"id": 100, "category": "Books", "price": 50.0},
    ]
    df_orders, df_users, df_order_items, df_products = create_test_data(spark, orders, users, order_items, products)
    result = run_customer_orders(spark, df_orders, df_users, df_order_items, df_products).collect()
    assert len(result) == 0

# TC03: Order with zero total_amount
def test_order_zero_total_amount(spark):
    """
    Test order with zero total_amount is counted and reflected in aggregations.
    """
    orders = [
        {"id": 1, "user_id": 10, "total_amount": 0.0, "created_at": "2024-06-01", "status": "completed"},
    ]
    users = [
        {"id": 10, "name": "Alice", "email": "alice@example.com"},
    ]
    order_items = [
        {"order_id": 1, "product_id": 100},
    ]
    products = [
        {"id": 100, "category": "Books", "price": 50.0},
    ]
    df_orders, df_users, df_order_items, df_products = create_test_data(spark, orders, users, order_items, products)
    result = run_customer_orders(spark, df_orders, df_users, df_order_items, df_products).collect()
    assert len(result) == 1
    assert result[0]["total_orders"] == 1
    assert result[0]["total_spent"] == 0.0
    assert result[0]["avg_order_value"] == 0.0

# TC04: Single user, single order, single product
def test_single_user_single_order_single_product(spark):
    """
    Test single user with single completed order and single product.
    """
    orders = [
        {"id": 1, "user_id": 10, "total_amount": 50.0, "created_at": "2024-06-01", "status": "completed"},
    ]
    users = [
        {"id": 10, "name": "Alice", "email": "alice@example.com"},
    ]
    order_items = [
        {"order_id": 1, "product_id": 100},
    ]
    products = [
        {"id": 100, "category": "Books", "price": 50.0},
    ]
    df_orders, df_users, df_order_items, df_products = create_test_data(spark, orders, users, order_items, products)
    result = run_customer_orders(spark, df_orders, df_users, df_order_items, df_products).collect()
    assert len(result) == 1
    assert result[0]["total_orders"] == 1
    assert result[0]["total_spent"] == 50.0
    assert result[0]["avg_order_value"] == 50.0
    assert set(result[0]["categories_bought"]) == {"Books"}

# TC05: Null product_category
def test_null_product_category(spark):
    """
    Test orders with null product_category.
    """
    orders = [
        {"id": 1, "user_id": 10, "total_amount": 50.0, "created_at": "2024-06-01", "status": "completed"},
    ]
    users = [
        {"id": 10, "name": "Alice", "email": "alice@example.com"},
    ]
    order_items = [
        {"order_id": 1, "product_id": 100},
    ]
    products = [
        {"id": 100, "category": None, "price": 50.0},
    ]
    df_orders, df_users, df_order_items, df_products = create_test_data(spark, orders, users, order_items, products)
    result = run_customer_orders(spark, df_orders, df_users, df_order_items, df_products).collect()
    assert len(result) == 1
    # categories_bought should contain None or be empty depending on collect_set behavior
    assert result[0]["categories_bought"] == [None] or result[0]["categories_bought"] == []

# TC06: Data integrity - missing references
def test_missing_references(spark):
    """
    Test join logic excludes orders with missing user/product/order_item references.
    """
    orders = [
        {"id": 1, "user_id": 10, "total_amount": 50.0, "created_at": "2024-06-01", "status": "completed"},
        {"id": 2, "user_id": 99, "total_amount": 100.0, "created_at": "2024-06-02", "status": "completed"},
    ]
    users = [
        {"id": 10, "name": "Alice", "email": "alice@example.com"},
    ]
    order_items = [
        {"order_id": 1, "product_id": 100},
        # order_id 2 missing
    ]
    products = [
        {"id": 100, "category": "Books", "price": 50.0},
    ]
    df_orders, df_users, df_order_items, df_products = create_test_data(spark, orders, users, order_items, products)
    result = run_customer_orders(spark, df_orders, df_users, df_order_items, df_products).collect()
    assert len(result) == 1
    assert result[0]["user_id"] == 10

# TC07: Aggregation - overlapping product categories
def test_overlapping_categories(spark):
    """
    Test multiple orders for a user with overlapping product categories.
    """
    orders = [
        {"id": 1, "user_id": 10, "total_amount": 50.0, "created_at": "2024-06-01", "status": "completed"},
        {"id": 2, "user_id": 10, "total_amount": 100.0, "created_at": "2024-06-02", "status": "completed"},
    ]
    users = [
        {"id": 10, "name": "Alice", "email": "alice@example.com"},
    ]
    order_items = [
        {"order_id": 1, "product_id": 100},
        {"order_id": 2, "product_id": 100},
        {"order_id": 2, "product_id": 101},
    ]
    products = [
        {"id": 100, "category": "Books", "price": 50.0},
        {"id": 101, "category": "Electronics", "price": 100.0},
    ]
    df_orders, df_users, df_order_items, df_products = create_test_data(spark, orders, users, order_items, products)
    result = run_customer_orders(spark, df_orders, df_users, df_order_items, df_products).collect()
    assert len(result) == 1
    assert set(result[0]["categories_bought"]) == {"Books", "Electronics"}

# TC08: Performance - large input dataset
def test_large_dataset(spark):
    """
    Test performance and correctness with large input dataset.
    """
    orders = [{"id": i, "user_id": i % 10, "total_amount": float(i), "created_at": "2024-06-01", "status": "completed"} for i in range(1, 1001)]
    users = [{"id": i, "name": f"User{i}", "email": f"user{i}@example.com"} for i in range(10)]
    order_items = [{"order_id": i, "product_id": i % 20} for i in range(1, 1001)]
    products = [{"id": i, "category": f"Cat{i%5}", "price": float(i)} for i in range(20)]
    df_orders, df_users, df_order_items, df_products = create_test_data(spark, orders, users, order_items, products)
    result = run_customer_orders(spark, df_orders, df_users, df_order_items, df_products).collect()
    assert len(result) == 10
    # Check that total_orders for each user is 100
    for row in result:
        assert row["total_orders"] == 100

# TC09: Error handling - invalid data types in total_amount
def test_invalid_total_amount(spark):
    """
    Test invalid data types in total_amount are handled gracefully.
    """
    orders = [
        {"id": 1, "user_id": 10, "total_amount": "invalid", "created_at": "2024-06-01", "status": "completed"},
    ]
    users = [
        {"id": 10, "name": "Alice", "email": "alice@example.com"},
    ]
    order_items = [
        {"order_id": 1, "product_id": 100},
    ]
    products = [
        {"id": 100, "category": "Books", "price": 50.0},
    ]
    df_orders, df_users, df_order_items, df_products = create_test_data(spark, orders, users, order_items, products)
    try:
        result = run_customer_orders(spark, df_orders, df_users, df_order_items, df_products).collect()
        # Should raise an exception or skip invalid record
        assert False, "Expected exception for invalid total_amount"
    except Exception:
        assert True

# TC10: Compliance - DBT ref syntax
def test_dbt_ref_syntax():
    """
    Test that DBT ref syntax is used for all table references.
    """
    with open("/src/878eecfb-4160-4c61-a754-2e5d070878c8/tmpkjc95km6/SQL_TO_DBT.txt__wnhu5tv2", "r") as f:
        sql_code = f.read()
    assert "{{ ref('orders') }}" in sql_code
    assert "{{ ref('users') }}" in sql_code
    assert "{{ ref('order_items') }}" in sql_code
    assert "{{ ref('products') }}" in sql_code

# Note: All tests use setup/teardown via Spark fixture.
# Helper functions are used for test data creation.
# Each test is logically grouped and commented.
```

*Cost consumed by the API for this call: 1 file list + 1 file read = 2 API calls*