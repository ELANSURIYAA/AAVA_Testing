Test Case List:

| Test ID | Test Description | Expected Output |
|--------|------------------|----------------|
| TC01   | Happy path: Multiple completed orders for a user, multiple product categories | Correct aggregation of total_orders, total_spent, avg_order_value, and distinct categories_bought for each user |
| TC02   | Edge case: User with no completed orders | User should not appear in the output |
| TC03   | Edge case: Order with null product category | Null category should be handled in categories_bought aggregation |
| TC04   | Boundary: User with exactly one completed order | Aggregates should reflect single order values |
| TC05   | Boundary: Order with zero total_amount | total_spent and avg_order_value should reflect zero values |
| TC06   | Error handling: Orders with missing user_id (foreign key violation) | Such orders should be excluded due to join failure |
| TC07   | Data integrity: Duplicate orders for a user | total_orders should count all completed orders, categories_bought should be distinct |
| TC08   | Aggregation: Multiple users, overlapping categories | Each user's categories_bought should only include their own categories |
| TC09   | Null handling: Orders with null total_amount | Nulls should be excluded from SUM and AVG calculations |
| TC10   | Performance: Large dataset (simulate with many rows) | Query should return correct aggregates and not fail/performance degrade |
| TC11   | DBT compliance: Use of {{ ref('table_name') }} and Jinja conditional for array aggregation | Syntax should be correct for DBT rendering |

Pytest Script (for PySpark):

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.functions import col, count, sum as _sum, avg, collect_set

@pytest.fixture(scope="function")
def spark():
    """Fixture to initialize and tear down SparkSession for each test."""
    spark = SparkSession.builder.master("local[1]").appName("dbt_test").getOrCreate()
    yield spark
    spark.stop()

def create_test_data(spark):
    """Helper to create test DataFrames for orders, users, order_items, products."""
    orders = spark.createDataFrame([
        Row(id=1, user_id=101, total_amount=100.0, created_at="2024-06-01", status="completed"),
        Row(id=2, user_id=101, total_amount=200.0, created_at="2024-06-02", status="completed"),
        Row(id=3, user_id=102, total_amount=150.0, created_at="2024-06-03", status="pending"),
        Row(id=4, user_id=103, total_amount=0.0, created_at="2024-06-04", status="completed"),
        Row(id=5, user_id=104, total_amount=None, created_at="2024-06-05", status="completed"),
        Row(id=6, user_id=None, total_amount=120.0, created_at="2024-06-06", status="completed"),
    ])
    users = spark.createDataFrame([
        Row(id=101, name="Alice", email="alice@example.com"),
        Row(id=102, name="Bob", email="bob@example.com"),
        Row(id=103, name="Charlie", email="charlie@example.com"),
        Row(id=104, name="Dana", email="dana@example.com"),
    ])
    order_items = spark.createDataFrame([
        Row(order_id=1, product_id=1001),
        Row(order_id=1, product_id=1002),
        Row(order_id=2, product_id=1003),
        Row(order_id=4, product_id=1004),
        Row(order_id=5, product_id=1005),
    ])
    products = spark.createDataFrame([
        Row(id=1001, category="Books", price=30.0),
        Row(id=1002, category="Electronics", price=70.0),
        Row(id=1003, category="Books", price=200.0),
        Row(id=1004, category=None, price=0.0),
        Row(id=1005, category="Toys", price=50.0),
    ])
    return orders, users, order_items, products

def customer_orders_df(spark, orders, users, order_items, products):
    """Implements the customer_orders CTE logic."""
    o = orders.alias("o")
    u = users.alias("u")
    oi = order_items.alias("oi")
    p = products.alias("p")
    joined = o.join(u, o.user_id == u.id, "inner") \
              .join(oi, o.id == oi.order_id, "inner") \
              .join(p, oi.product_id == p.id, "inner") \
              .where(col("o.status") == "completed") \
              .select(
                  col("o.id").alias("order_id"),
                  col("o.user_id"),
                  col("u.name").alias("customer_name"),
                  col("u.email").alias("customer_email"),
                  col("o.total_amount"),
                  col("o.created_at").alias("order_date"),
                  col("p.category").alias("product_category"),
                  col("p.price").alias("product_price")
              )
    return joined

def order_summary_df(customer_orders):
    """Implements the order_summary CTE logic."""
    return customer_orders.groupBy(
        "user_id", "customer_name", "customer_email"
    ).agg(
        count("order_id").alias("total_orders"),
        _sum("total_amount").alias("total_spent"),
        avg("total_amount").alias("avg_order_value"),
        collect_set("product_category").alias("categories_bought")  # distinct categories
    )

# TC01: Happy path
def test_happy_path(spark):
    """Test aggregation for user with multiple completed orders and categories."""
    orders, users, order_items, products = create_test_data(spark)
    customer_orders = customer_orders_df(spark, orders, users, order_items, products)
    summary = order_summary_df(customer_orders)
    alice = summary.filter(col("user_id") == 101).collect()[0]
    assert alice.total_orders == 3  # 2 orders, 3 items
    assert alice.total_spent == 300.0
    assert round(alice.avg_order_value, 2) == 150.0
    assert set(alice.categories_bought) == {"Books", "Electronics"}

# TC02: User with no completed orders
def test_user_with_no_completed_orders(spark):
    """Test that user with no completed orders is excluded."""
    orders, users, order_items, products = create_test_data(spark)
    customer_orders = customer_orders_df(spark, orders, users, order_items, products)
    summary = order_summary_df(customer_orders)
    assert summary.filter(col("user_id") == 102).count() == 0

# TC03: Order with null product category
def test_null_product_category(spark):
    """Test handling of null product category in aggregation."""
    orders, users, order_items, products = create_test_data(spark)
    customer_orders = customer_orders_df(spark, orders, users, order_items, products)
    summary = order_summary_df(customer_orders)
    charlie = summary.filter(col("user_id") == 103).collect()[0]
    assert charlie.categories_bought == [None]

# TC04: User with one completed order
def test_single_order_user(spark):
    """Test aggregates for user with one completed order."""
    orders, users, order_items, products = create_test_data(spark)
    customer_orders = customer_orders_df(spark, orders, users, order_items, products)
    summary = order_summary_df(customer_orders)
    charlie = summary.filter(col("user_id") == 103).collect()[0]
    assert charlie.total_orders == 1
    assert charlie.total_spent == 0.0
    assert charlie.avg_order_value == 0.0

# TC05: Order with zero total_amount
def test_zero_total_amount(spark):
    """Test handling of zero total_amount in aggregates."""
    orders, users, order_items, products = create_test_data(spark)
    customer_orders = customer_orders_df(spark, orders, users, order_items, products)
    summary = order_summary_df(customer_orders)
    charlie = summary.filter(col("user_id") == 103).collect()[0]
    assert charlie.total_spent == 0.0
    assert charlie.avg_order_value == 0.0

# TC06: Orders with missing user_id
def test_missing_user_id(spark):
    """Test that orders with missing user_id are excluded."""
    orders, users, order_items, products = create_test_data(spark)
    customer_orders = customer_orders_df(spark, orders, users, order_items, products)
    summary = order_summary_df(customer_orders)
    # user_id None should not appear
    assert summary.filter(col("user_id").isNull()).count() == 0

# TC07: Duplicate orders for a user
def test_duplicate_orders(spark):
    """Test that total_orders counts all completed orders, categories_bought is distinct."""
    orders, users, order_items, products = create_test_data(spark)
    # Add duplicate order for Alice
    orders_dup = orders.union(spark.createDataFrame([
        Row(id=7, user_id=101, total_amount=100.0, created_at="2024-06-07", status="completed")
    ]))
    customer_orders = customer_orders_df(spark, orders_dup, users, order_items, products)
    summary = order_summary_df(customer_orders)
    alice = summary.filter(col("user_id") == 101).collect()[0]
    assert alice.total_orders > 3
    assert "Books" in alice.categories_bought

# TC08: Multiple users, overlapping categories
def test_overlapping_categories(spark):
    """Test that each user's categories_bought only includes their own categories."""
    orders, users, order_items, products = create_test_data(spark)
    # Add an order for Dana with category "Books"
    orders_ext = orders.union(spark.createDataFrame([
        Row(id=8, user_id=104, total_amount=50.0, created_at="2024-06-08", status="completed")
    ]))
    order_items_ext = order_items.union(spark.createDataFrame([
        Row(order_id=8, product_id=1001)
    ]))
    customer_orders = customer_orders_df(spark, orders_ext, users, order_items_ext, products)
    summary = order_summary_df(customer_orders)
    dana = summary.filter(col("user_id") == 104).collect()[0]
    assert "Books" in dana.categories_bought
    assert "Toys" in dana.categories_bought

# TC09: Orders with null total_amount
def test_null_total_amount(spark):
    """Test that null total_amount is excluded from SUM and AVG."""
    orders, users, order_items, products = create_test_data(spark)
    customer_orders = customer_orders_df(spark, orders, users, order_items, products)
    summary = order_summary_df(customer_orders)
    dana = summary.filter(col("user_id") == 104).collect()[0]
    assert dana.total_spent is None or dana.total_spent == 0.0
    assert dana.avg_order_value is None or dana.avg_order_value == 0.0

# TC10: Performance with large dataset
def test_large_dataset_performance(spark):
    """Test that aggregation works with large number of rows."""
    orders, users, order_items, products = create_test_data(spark)
    # Simulate large dataset by duplicating rows
    for _ in range(100):
        orders = orders.union(create_test_data(spark)[0])
        order_items = order_items.union(create_test_data(spark)[2])
    customer_orders = customer_orders_df(spark, orders, users, order_items, products)
    summary = order_summary_df(customer_orders)
    # Should not raise and should return correct schema
    assert "total_orders" in summary.columns

# TC11: DBT compliance (syntax check)
def test_dbt_syntax_compliance():
    """Test that DBT Jinja and ref macros are present in the SQL."""
    sql = """
    WITH customer_orders AS (
        SELECT
            o.id AS order_id,
            o.user_id,
            u.name AS customer_name,
            u.email AS customer_email,
            o.total_amount,
            o.created_at AS order_date,
            p.category AS product_category,
            p.price AS product_price
        FROM {{ ref('orders') }} o
        JOIN {{ ref('users') }} u ON o.user_id = u.id
        JOIN {{ ref('order_items') }} oi ON o.id = oi.order_id
        JOIN {{ ref('products') }} p ON oi.product_id = p.id
        WHERE o.status = 'completed'
    ),
    order_summary AS (
        SELECT 
            user_id,
            customer_name,
            customer_email,
            COUNT(order_id) AS total_orders,
            SUM(total_amount) AS total_spent,
            AVG(total_amount) AS avg_order_value,
            {% if target.type == 'bigquery' %}
                ARRAY_AGG(DISTINCT product_category) AS categories_bought
            {% elif target.type == 'snowflake' %}
                ARRAY_AGG(DISTINCT product_category) AS categories_bought
            {% elif target.type == 'redshift' %}
                LISTAGG(DISTINCT product_category, ',') AS categories_bought
            {% else %}
                ARRAY_AGG(product_category) AS categories_bought
            {% endif %}
        FROM customer_orders
        GROUP BY user_id, customer_name, customer_email
    )
    SELECT * FROM order_summary
    ORDER BY total_spent DESC;
    """
    assert "{{ ref('orders') }}" in sql
    assert "{% if target.type" in sql

# API cost consumed for this call: $0.002 USD
```