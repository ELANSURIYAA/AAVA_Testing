1. Test Case List:

| Test ID | Test Description                                                                 | Expected Output                                                                                                 |
|-------- |----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| TC01    | Happy path: Multiple completed orders for multiple users                          | Correct aggregation per user: total_orders, total_spent, avg_order_value, categories_bought, sorted by spent     |
| TC02    | Edge case: User with a single completed order                                    | Aggregation returns correct counts and arrays for single record                                                  |
| TC03    | Edge case: User with multiple orders for same product category                   | categories_bought contains unique categories only                                                               |
| TC04    | Edge case: User with no completed orders                                         | User not present in output                                                                                      |
| TC05    | Boundary: Order with zero total_amount                                           | Zero values correctly aggregated and reflected in total_spent and avg_order_value                               |
| TC06    | Null handling: Orders with null product_category                                 | categories_bought does not include null, or includes if logic allows                                            |
| TC07    | Null handling: Orders with null total_amount                                     | Nulls ignored in sum/avg, or handled per SQL logic                                                              |
| TC08    | Error: Orders with missing user_id (should not join)                             | No output for orders with missing user_id                                                                       |
| TC09    | Performance: Large dataset                                                       | Query completes, output correct, no performance degradation                                                     |
| TC10    | Aggregation: Multiple users with overlapping categories                          | Each userâ€™s categories_bought is correct and independent                                                        |
| TC11    | DBT compliance: Table references use {{ ref('table_name') }}                     | Model compiles and runs in DBT                                                                                  |
| TC12    | Sorting: Output sorted by total_spent desc                                       | Output order matches expected                                                                                   |
| TC13    | Data integrity: No duplicate users in output                                     | Each user appears only once                                                                                     |
| TC14    | Error: Orders with status other than 'completed'                                 | These orders are excluded from output                                                                           |
| TC15    | Empty input tables                                                               | Output is empty                                                                                                 |

2. Pytest Script for each test case

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.functions import col, array_distinct, collect_set, count, sum as _sum, avg

# Fixtures for SparkSession
@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("dbt_test").getOrCreate()
    yield spark
    spark.stop()

# Helper to create DataFrames for input tables
def create_orders_df(spark, data):
    return spark.createDataFrame(data)

def create_users_df(spark, data):
    return spark.createDataFrame(data)

def create_order_items_df(spark, data):
    return spark.createDataFrame(data)

def create_products_df(spark, data):
    return spark.createDataFrame(data)

# Core logic simulating the DBT SQL in PySpark
def run_order_summary(orders_df, users_df, order_items_df, products_df):
    # Only completed orders
    completed_orders = orders_df.filter(col("status") == "completed")
    # Joins
    joined = completed_orders.join(users_df, completed_orders.user_id == users_df.id, "inner") \
        .join(order_items_df, completed_orders.id == order_items_df.order_id, "inner") \
        .join(products_df, order_items_df.product_id == products_df.id, "inner")
    # Select fields
    customer_orders = joined.select(
        completed_orders.id.alias("order_id"),
        completed_orders.user_id,
        users_df.name.alias("customer_name"),
        users_df.email.alias("customer_email"),
        completed_orders.total_amount,
        completed_orders.created_at.alias("order_date"),
        products_df.category.alias("product_category"),
        products_df.price.alias("product_price")
    )
    # Group and aggregate
    order_summary = customer_orders.groupBy(
        "user_id", "customer_name", "customer_email"
    ).agg(
        count("order_id").alias("total_orders"),
        _sum("total_amount").alias("total_spent"),
        avg("total_amount").alias("avg_order_value"),
        collect_set("product_category").alias("categories_bought")
    )
    # Sort by total_spent desc
    order_summary = order_summary.orderBy(col("total_spent").desc())
    return order_summary

# TC01: Happy path: Multiple completed orders for multiple users
def test_tc01_happy_path(spark):
    orders = [
        Row(id=1, user_id=10, total_amount=100, created_at="2024-06-01", status="completed"),
        Row(id=2, user_id=10, total_amount=150, created_at="2024-06-02", status="completed"),
        Row(id=3, user_id=20, total_amount=200, created_at="2024-06-03", status="completed"),
    ]
    users = [
        Row(id=10, name="Alice", email="alice@example.com"),
        Row(id=20, name="Bob", email="bob@example.com"),
    ]
    order_items = [
        Row(order_id=1, product_id=100),
        Row(order_id=2, product_id=101),
        Row(order_id=3, product_id=102),
    ]
    products = [
        Row(id=100, category="Books", price=50),
        Row(id=101, category="Electronics", price=150),
        Row(id=102, category="Books", price=200),
    ]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = {row['user_id']: row for row in df.collect()}
    # Alice
    assert result[10]['total_orders'] == 2
    assert result[10]['total_spent'] == 250
    assert round(result[10]['avg_order_value'], 2) == 125.0
    assert set(result[10]['categories_bought']) == {"Books", "Electronics"}
    # Bob
    assert result[20]['total_orders'] == 1
    assert result[20]['total_spent'] == 200
    assert round(result[20]['avg_order_value'], 2) == 200.0
    assert set(result[20]['categories_bought']) == {"Books"}

# TC02: Edge case: User with a single completed order
def test_tc02_single_order(spark):
    orders = [Row(id=1, user_id=10, total_amount=100, created_at="2024-06-01", status="completed")]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [Row(order_id=1, product_id=100)]
    products = [Row(id=100, category="Books", price=50)]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 1
    row = result[0]
    assert row['total_orders'] == 1
    assert row['total_spent'] == 100
    assert round(row['avg_order_value'], 2) == 100.0
    assert set(row['categories_bought']) == {"Books"}

# TC03: Edge case: User with multiple orders for same product category
def test_tc03_duplicate_category(spark):
    orders = [
        Row(id=1, user_id=10, total_amount=100, created_at="2024-06-01", status="completed"),
        Row(id=2, user_id=10, total_amount=150, created_at="2024-06-02", status="completed"),
    ]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [
        Row(order_id=1, product_id=100),
        Row(order_id=2, product_id=100),
    ]
    products = [Row(id=100, category="Books", price=50)]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 1
    row = result[0]
    assert set(row['categories_bought']) == {"Books"}

# TC04: Edge case: User with no completed orders
def test_tc04_no_completed_orders(spark):
    orders = [Row(id=1, user_id=10, total_amount=100, created_at="2024-06-01", status="pending")]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [Row(order_id=1, product_id=100)]
    products = [Row(id=100, category="Books", price=50)]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 0

# TC05: Boundary: Order with zero total_amount
def test_tc05_zero_total_amount(spark):
    orders = [Row(id=1, user_id=10, total_amount=0, created_at="2024-06-01", status="completed")]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [Row(order_id=1, product_id=100)]
    products = [Row(id=100, category="Books", price=50)]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 1
    row = result[0]
    assert row['total_spent'] == 0
    assert round(row['avg_order_value'], 2) == 0.0

# TC06: Null handling: Orders with null product_category
def test_tc06_null_product_category(spark):
    orders = [Row(id=1, user_id=10, total_amount=100, created_at="2024-06-01", status="completed")]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [Row(order_id=1, product_id=100)]
    products = [Row(id=100, category=None, price=50)]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 1
    row = result[0]
    # Should contain None if logic allows, or empty set
    assert row['categories_bought'] == [None] or row['categories_bought'] == []

# TC07: Null handling: Orders with null total_amount
def test_tc07_null_total_amount(spark):
    orders = [Row(id=1, user_id=10, total_amount=None, created_at="2024-06-01", status="completed")]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [Row(order_id=1, product_id=100)]
    products = [Row(id=100, category="Books", price=50)]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 1
    row = result[0]
    # sum/avg of None should be None
    assert row['total_spent'] is None
    assert row['avg_order_value'] is None

# TC08: Error: Orders with missing user_id (should not join)
def test_tc08_missing_user_id(spark):
    orders = [Row(id=1, user_id=None, total_amount=100, created_at="2024-06-01", status="completed")]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [Row(order_id=1, product_id=100)]
    products = [Row(id=100, category="Books", price=50)]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 0

# TC09: Performance: Large dataset
def test_tc09_performance_large_dataset(spark):
    orders = [Row(id=i, user_id=10, total_amount=1, created_at="2024-06-01", status="completed") for i in range(1000)]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [Row(order_id=i, product_id=100) for i in range(1000)]
    products = [Row(id=100, category="Books", price=50)]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 1
    row = result[0]
    assert row['total_orders'] == 1000
    assert row['total_spent'] == 1000
    assert round(row['avg_order_value'], 2) == 1.0

# TC10: Aggregation: Multiple users with overlapping categories
def test_tc10_overlapping_categories(spark):
    orders = [
        Row(id=1, user_id=10, total_amount=100, created_at="2024-06-01", status="completed"),
        Row(id=2, user_id=20, total_amount=150, created_at="2024-06-02", status="completed"),
    ]
    users = [
        Row(id=10, name="Alice", email="alice@example.com"),
        Row(id=20, name="Bob", email="bob@example.com"),
    ]
    order_items = [
        Row(order_id=1, product_id=100),
        Row(order_id=2, product_id=100),
    ]
    products = [Row(id=100, category="Books", price=50)]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = {row['user_id']: row for row in df.collect()}
    assert set(result[10]['categories_bought']) == {"Books"}
    assert set(result[20]['categories_bought']) == {"Books"}

# TC11: DBT compliance: Table references use {{ ref('table_name') }}
def test_tc11_dbt_compliance():
    # This test is a placeholder to check that the SQL uses DBT ref syntax.
    # In actual DBT, this is checked by compilation.
    sql = """
    select * from {{ ref('orders') }} o
    join {{ ref('users') }} u on o.user_id = u.id
    join {{ ref('order_items') }} oi on o.id = oi.order_id
    join {{ ref('products') }} p on oi.product_id = p.id
    """
    assert "{{ ref('orders') }}" in sql
    assert "{{ ref('users') }}" in sql
    assert "{{ ref('order_items') }}" in sql
    assert "{{ ref('products') }}" in sql

# TC12: Sorting: Output sorted by total_spent desc
def test_tc12_sorting(spark):
    orders = [
        Row(id=1, user_id=10, total_amount=100, created_at="2024-06-01", status="completed"),
        Row(id=2, user_id=20, total_amount=200, created_at="2024-06-02", status="completed"),
    ]
    users = [
        Row(id=10, name="Alice", email="alice@example.com"),
        Row(id=20, name="Bob", email="bob@example.com"),
    ]
    order_items = [
        Row(order_id=1, product_id=100),
        Row(order_id=2, product_id=101),
    ]
    products = [
        Row(id=100, category="Books", price=50),
        Row(id=101, category="Electronics", price=150),
    ]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    # Should be sorted by total_spent desc
    assert result[0]['user_id'] == 20
    assert result[1]['user_id'] == 10

# TC13: Data integrity: No duplicate users in output
def test_tc13_no_duplicate_users(spark):
    orders = [
        Row(id=1, user_id=10, total_amount=100, created_at="2024-06-01", status="completed"),
        Row(id=2, user_id=10, total_amount=150, created_at="2024-06-02", status="completed"),
    ]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [
        Row(order_id=1, product_id=100),
        Row(order_id=2, product_id=101),
    ]
    products = [
        Row(id=100, category="Books", price=50),
        Row(id=101, category="Electronics", price=150),
    ]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = [row['user_id'] for row in df.collect()]
    assert result.count(10) == 1

# TC14: Error: Orders with status other than 'completed'
def test_tc14_non_completed_orders_excluded(spark):
    orders = [
        Row(id=1, user_id=10, total_amount=100, created_at="2024-06-01", status="pending"),
        Row(id=2, user_id=10, total_amount=150, created_at="2024-06-02", status="cancelled"),
    ]
    users = [Row(id=10, name="Alice", email="alice@example.com")]
    order_items = [
        Row(order_id=1, product_id=100),
        Row(order_id=2, product_id=101),
    ]
    products = [
        Row(id=100, category="Books", price=50),
        Row(id=101, category="Electronics", price=150),
    ]
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 0

# TC15: Empty input tables
def test_tc15_empty_input(spark):
    orders = []
    users = []
    order_items = []
    products = []
    df = run_order_summary(
        create_orders_df(spark, orders),
        create_users_df(spark, users),
        create_order_items_df(spark, order_items),
        create_products_df(spark, products)
    )
    result = df.collect()
    assert len(result) == 0

# End of tests

# Cost consumed by the API for this call: 1 file list + 2 file reads = 3 operations
```
**Estimated API Cost for this call: 3 operations**