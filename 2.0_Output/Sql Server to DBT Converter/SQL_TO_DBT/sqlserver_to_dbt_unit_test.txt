1. Test Case List:

| Test ID | Test Description | Expected Output |
|---------|------------------|----------------|
| TC01    | Happy path: Multiple completed orders for multiple users, multiple categories | Correct aggregation per user: total_orders, total_spent, avg_order_value, categories_bought as array of distinct categories, sorted by total_spent DESC |
| TC02    | Edge case: User with no completed orders | User does not appear in output |
| TC03    | Edge case: Orders with null product_category | categories_bought does not include null; aggregation is correct |
| TC04    | Edge case: Orders with null total_amount | total_spent and avg_order_value skip nulls (per Spark SQL default) |
| TC05    | Boundary: User with only one completed order | total_orders = 1, total_spent = order amount, avg_order_value = order amount, categories_bought = [category] |
| TC06    | Boundary: Order with multiple items, same category | categories_bought contains category once (DISTINCT) |
| TC07    | Boundary: Order with multiple items, multiple categories | categories_bought contains all distinct categories |
| TC08    | Error handling: Empty input tables | Output is empty DataFrame |
| TC09    | Performance: Large input datasets | Query completes and aggregations are correct (no assertion, just that it runs) |
| TC10    | Data integrity: Orders with status other than 'completed' | Only 'completed' orders are included in output |

2. Pytest Script (for PySpark, using pytest and SparkSession fixture):

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType, FloatType, ArrayType

@pytest.fixture(scope="function")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("unit-test").getOrCreate()
    yield spark
    spark.stop()

def create_test_tables(spark, orders_data, users_data, order_items_data, products_data):
    orders_schema = StructType([
        StructField("id", IntegerType(), False),
        StructField("user_id", IntegerType(), False),
        StructField("total_amount", FloatType(), True),
        StructField("created_at", StringType(), True),
        StructField("status", StringType(), True),
    ])
    users_schema = StructType([
        StructField("id", IntegerType(), False),
        StructField("name", StringType(), True),
        StructField("email", StringType(), True),
    ])
    order_items_schema = StructType([
        StructField("id", IntegerType(), False),
        StructField("order_id", IntegerType(), False),
        StructField("product_id", IntegerType(), False),
    ])
    products_schema = StructType([
        StructField("id", IntegerType(), False),
        StructField("category", StringType(), True),
        StructField("price", FloatType(), True),
    ])
    orders = spark.createDataFrame(orders_data, schema=orders_schema)
    users = spark.createDataFrame(users_data, schema=users_schema)
    order_items = spark.createDataFrame(order_items_data, schema=order_items_schema)
    products = spark.createDataFrame(products_data, schema=products_schema)
    return orders, users, order_items, products

def run_customer_order_summary(spark, orders, users, order_items, products):
    # customer_orders CTE
    customer_orders = (
        orders.alias("o")
        .join(users.alias("u"), orders.user_id == users.id)
        .join(order_items.alias("oi"), orders.id == order_items.order_id)
        .join(products.alias("p"), order_items.product_id == products.id)
        .where(orders.status == "completed")
        .select(
            orders.id.alias("order_id"),
            orders.user_id,
            users.name.alias("customer_name"),
            users.email.alias("customer_email"),
            orders.total_amount,
            orders.created_at.alias("order_date"),
            products.category.alias("product_category"),
            products.price.alias("product_price"),
        )
    )
    from pyspark.sql import functions as F
    order_summary = (
        customer_orders
        .groupBy("user_id", "customer_name", "customer_email")
        .agg(
            F.count("order_id").alias("total_orders"),
            F.sum("total_amount").alias("total_spent"),
            F.avg("total_amount").alias("avg_order_value"),
            F.array_distinct(F.collect_list("product_category")).alias("categories_bought"),
        )
        .orderBy(F.desc("total_spent"))
    )
    return order_summary

# TC01: Happy path
def test_happy_path_multiple_users(spark):
    """
    Test multiple completed orders for multiple users, multiple categories.
    """
    orders_data = [
        (1, 10, 100.0, "2024-01-01", "completed"),
        (2, 10, 50.0, "2024-01-02", "completed"),
        (3, 20, 200.0, "2024-01-03", "completed"),
        (4, 20, 20.0, "2024-01-04", "pending"),
    ]
    users_data = [
        (10, "Alice", "alice@example.com"),
        (20, "Bob", "bob@example.com"),
    ]
    order_items_data = [
        (1, 1, 100),
        (2, 1, 101),
        (3, 2, 102),
        (4, 3, 100),
        (5, 3, 103),
    ]
    products_data = [
        (100, "Books", 10.0),
        (101, "Toys", 20.0),
        (102, "Books", 10.0),
        (103, "Electronics", 100.0),
    ]
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products).collect()
    # Bob: 1 order (id 3), total_spent 200.0, avg 200.0, categories ["Books", "Electronics"]
    # Alice: 2 orders (id 1,2), total_spent 150.0, avg 75.0, categories ["Books", "Toys"]
    assert len(result) == 2
    assert result[0]['customer_name'] == "Bob"
    assert result[0]['total_orders'] == 1
    assert result[0]['total_spent'] == 200.0
    assert result[0]['avg_order_value'] == 200.0
    assert set(result[0]['categories_bought']) == {"Books", "Electronics"}
    assert result[1]['customer_name'] == "Alice"
    assert result[1]['total_orders'] == 2
    assert result[1]['total_spent'] == 150.0
    assert result[1]['avg_order_value'] == 75.0
    assert set(result[1]['categories_bought']) == {"Books", "Toys"}

# TC02: User with no completed orders
def test_user_with_no_completed_orders(spark):
    """
    Test that users with no completed orders do not appear in output.
    """
    orders_data = [
        (1, 10, 100.0, "2024-01-01", "pending"),
    ]
    users_data = [
        (10, "Alice", "alice@example.com"),
    ]
    order_items_data = [
        (1, 1, 100),
    ]
    products_data = [
        (100, "Books", 10.0),
    ]
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products).collect()
    assert len(result) == 0

# TC03: Orders with null product_category
def test_null_product_category(spark):
    """
    Test that null product_category is not included in categories_bought.
    """
    orders_data = [
        (1, 10, 100.0, "2024-01-01", "completed"),
    ]
    users_data = [
        (10, "Alice", "alice@example.com"),
    ]
    order_items_data = [
        (1, 1, 100),
        (2, 1, 101),
    ]
    products_data = [
        (100, None, 10.0),
        (101, "Toys", 20.0),
    ]
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products).collect()
    assert len(result) == 1
    assert set(result[0]['categories_bought']) == {"Toys", None} or set(filter(lambda x: x is not None, result[0]['categories_bought'])) == {"Toys"}

# TC04: Orders with null total_amount
def test_null_total_amount(spark):
    """
    Test that null total_amount is skipped in sum/avg.
    """
    orders_data = [
        (1, 10, None, "2024-01-01", "completed"),
        (2, 10, 50.0, "2024-01-02", "completed"),
    ]
    users_data = [
        (10, "Alice", "alice@example.com"),
    ]
    order_items_data = [
        (1, 1, 100),
        (2, 2, 101),
    ]
    products_data = [
        (100, "Books", 10.0),
        (101, "Toys", 20.0),
    ]
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products).collect()
    assert len(result) == 1
    assert result[0]['total_orders'] == 2
    assert result[0]['total_spent'] == 50.0  # None is skipped
    assert result[0]['avg_order_value'] == 50.0

# TC05: User with only one completed order
def test_single_order_user(spark):
    """
    Test user with only one completed order.
    """
    orders_data = [
        (1, 10, 99.0, "2024-01-01", "completed"),
    ]
    users_data = [
        (10, "Alice", "alice@example.com"),
    ]
    order_items_data = [
        (1, 1, 100),
    ]
    products_data = [
        (100, "Books", 10.0),
    ]
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products).collect()
    assert len(result) == 1
    assert result[0]['total_orders'] == 1
    assert result[0]['total_spent'] == 99.0
    assert result[0]['avg_order_value'] == 99.0
    assert set(result[0]['categories_bought']) == {"Books"}

# TC06: Order with multiple items, same category
def test_multiple_items_same_category(spark):
    """
    Test that categories_bought contains category only once if multiple items in same category.
    """
    orders_data = [
        (1, 10, 100.0, "2024-01-01", "completed"),
    ]
    users_data = [
        (10, "Alice", "alice@example.com"),
    ]
    order_items_data = [
        (1, 1, 100),
        (2, 1, 100),
    ]
    products_data = [
        (100, "Books", 10.0),
    ]
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products).collect()
    assert len(result) == 1
    assert set(result[0]['categories_bought']) == {"Books"}

# TC07: Order with multiple items, multiple categories
def test_multiple_items_multiple_categories(spark):
    """
    Test that categories_bought contains all distinct categories.
    """
    orders_data = [
        (1, 10, 100.0, "2024-01-01", "completed"),
    ]
    users_data = [
        (10, "Alice", "alice@example.com"),
    ]
    order_items_data = [
        (1, 1, 100),
        (2, 1, 101),
    ]
    products_data = [
        (100, "Books", 10.0),
        (101, "Toys", 20.0),
    ]
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products).collect()
    assert len(result) == 1
    assert set(result[0]['categories_bought']) == {"Books", "Toys"}

# TC08: Empty input tables
def test_empty_input_tables(spark):
    """
    Test that empty input tables produce empty output.
    """
    orders_data = []
    users_data = []
    order_items_data = []
    products_data = []
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products).collect()
    assert len(result) == 0

# TC09: Performance on large datasets (not asserting, just runs)
def test_performance_large_dataset(spark):
    """
    Test performance on large input datasets.
    """
    orders_data = [(i, i % 10, float(i), "2024-01-01", "completed") for i in range(1000)]
    users_data = [(i, f"User{i}", f"user{i}@example.com") for i in range(10)]
    order_items_data = [(i, i % 1000, i % 50) for i in range(5000)]
    products_data = [(i, f"Category{i%5}", float(i)) for i in range(50)]
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products)
    # Just ensure it runs and returns a DataFrame
    assert result.count() > 0

# TC10: Orders with status other than 'completed'
def test_orders_with_non_completed_status(spark):
    """
    Test that only orders with status 'completed' are included.
    """
    orders_data = [
        (1, 10, 100.0, "2024-01-01", "completed"),
        (2, 10, 50.0, "2024-01-02", "pending"),
    ]
    users_data = [
        (10, "Alice", "alice@example.com"),
    ]
    order_items_data = [
        (1, 1, 100),
        (2, 2, 101),
    ]
    products_data = [
        (100, "Books", 10.0),
        (101, "Toys", 20.0),
    ]
    orders, users, order_items, products = create_test_tables(spark, orders_data, users_data, order_items_data, products_data)
    result = run_customer_order_summary(spark, orders, users, order_items, products).collect()
    assert len(result) == 1
    assert result[0]['total_orders'] == 1
    assert result[0]['total_spent'] == 100.0
    assert result[0]['avg_order_value'] == 100.0
    assert set(result[0]['categories_bought']) == {"Books"}

# Helper: You may add more helper functions as needed.

# End of tests

# API cost for this call: $0.002 USD
```