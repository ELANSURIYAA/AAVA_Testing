=============================================
Author: Ascendion AVA+
Date: 
Description: End-to-end Oracle to Snowflake migration validation script: executes Oracle procedure, exports data, transfers to Azure Data Lake, creates Snowflake external tables, runs Snowflake procedure, and compares results for data integrity.
=============================================

```python
"""
=============================================
Author:        Ascendion AVA+
Date:          
Description:   End-to-end Oracle to Snowflake migration validation script: executes Oracle procedure, exports data, transfers to Azure Data Lake, creates Snowflake external tables, runs Snowflake procedure, and compares results for data integrity.
=============================================
"""

import os
import sys
import cx_Oracle
import snowflake.connector
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import logging
import tempfile
import time
from datetime import datetime
from azure.storage.blob import BlobServiceClient, ContentSettings

# ---------------------------
# 1. CONFIGURATION & LOGGING
# ---------------------------
# Use environment variables for all credentials and sensitive info

ORACLE_USER = os.getenv('ORACLE_USER')
ORACLE_PASSWORD = os.getenv('ORACLE_PASSWORD')
ORACLE_DSN = os.getenv('ORACLE_DSN')

SNOWFLAKE_USER = os.getenv('SNOWFLAKE_USER')
SNOWFLAKE_PASSWORD = os.getenv('SNOWFLAKE_PASSWORD')
SNOWFLAKE_ACCOUNT = os.getenv('SNOWFLAKE_ACCOUNT')
SNOWFLAKE_WAREHOUSE = os.getenv('SNOWFLAKE_WAREHOUSE')
SNOWFLAKE_DATABASE = os.getenv('SNOWFLAKE_DATABASE')
SNOWFLAKE_SCHEMA = os.getenv('SNOWFLAKE_SCHEMA')
SNOWFLAKE_ROLE = os.getenv('SNOWFLAKE_ROLE')

AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
AZURE_CONTAINER = os.getenv('AZURE_CONTAINER')
AZURE_PARQUET_PATH = os.getenv('AZURE_PARQUET_PATH', 'oracle_exports/')

# Set up logging
logging.basicConfig(
    filename='migration_validation.log',
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s'
)
logger = logging.getLogger(__name__)

# Target tables to validate (from both procedures)
TARGET_TABLES = [
    {
        "oracle": "GOLD_AGENTS_D",
        "snowflake": "GOLD_AGENTS_D"
    },
    {
        "oracle": "AUDIT_LOG",
        "snowflake": "AUDIT_LOG"
    }
]

# ---------------------------
# 2. ORACLE CONNECTION & EXECUTION
# ---------------------------

def execute_oracle_procedure(proc_name):
    """
    Executes the specified Oracle stored procedure.
    """
    logger.info(f"Connecting to Oracle and executing procedure: {proc_name}")
    try:
        conn = cx_Oracle.connect(user=ORACLE_USER, password=ORACLE_PASSWORD, dsn=ORACLE_DSN)
        cur = conn.cursor()
        cur.callproc(proc_name)
        conn.commit()
        logger.info(f"Oracle procedure {proc_name} executed successfully.")
        cur.close()
        conn.close()
    except Exception as e:
        logger.error(f"Error executing Oracle procedure {proc_name}: {e}")
        raise

# ---------------------------
# 3. EXPORT ORACLE TABLES TO PARQUET
# ---------------------------

def export_oracle_table_to_parquet(table_name, output_dir):
    """
    Exports an Oracle table to a Parquet file.
    """
    logger.info(f"Exporting Oracle table {table_name} to Parquet.")
    try:
        conn = cx_Oracle.connect(user=ORACLE_USER, password=ORACLE_PASSWORD, dsn=ORACLE_DSN)
        df = pd.read_sql(f"SELECT * FROM {table_name}", con=conn)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        parquet_file = os.path.join(output_dir, f"{table_name}_{timestamp}.parquet")
        table = pa.Table.from_pandas(df)
        pq.write_table(table, parquet_file)
        logger.info(f"Exported {table_name} to {parquet_file}.")
        conn.close()
        return parquet_file
    except Exception as e:
        logger.error(f"Error exporting table {table_name}: {e}")
        raise

# ---------------------------
# 4. UPLOAD PARQUET FILES TO AZURE DATA LAKE
# ---------------------------

def upload_to_azure(parquet_file, container, blob_path):
    """
    Uploads a Parquet file to Azure Data Lake Storage (Blob).
    """
    logger.info(f"Uploading {parquet_file} to Azure container {container} at {blob_path}")
    try:
        blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)
        blob_client = blob_service_client.get_blob_client(container=container, blob=blob_path)
        with open(parquet_file, "rb") as data:
            blob_client.upload_blob(data, overwrite=True, content_settings=ContentSettings(content_type='application/octet-stream'))
        # Integrity check: compare file sizes
        local_size = os.path.getsize(parquet_file)
        remote_size = blob_client.get_blob_properties().size
        if local_size != remote_size:
            raise Exception(f"File size mismatch after upload: local={local_size}, remote={remote_size}")
        logger.info(f"Upload successful and verified for {parquet_file}")
    except Exception as e:
        logger.error(f"Error uploading {parquet_file} to Azure: {e}")
        raise

# ---------------------------
# 5. SNOWFLAKE CONNECTION & EXECUTION
# ---------------------------

def get_snowflake_connection():
    """
    Returns a Snowflake connection using environment variables.
    """
    try:
        conn = snowflake.connector.connect(
            user=SNOWFLAKE_USER,
            password=SNOWFLAKE_PASSWORD,
            account=SNOWFLAKE_ACCOUNT,
            warehouse=SNOWFLAKE_WAREHOUSE,
            database=SNOWFLAKE_DATABASE,
            schema=SNOWFLAKE_SCHEMA,
            role=SNOWFLAKE_ROLE
        )
        return conn
    except Exception as e:
        logger.error(f"Error connecting to Snowflake: {e}")
        raise

def execute_snowflake_procedure(proc_name):
    """
    Executes a Snowflake stored procedure.
    """
    logger.info(f"Executing Snowflake procedure: {proc_name}")
    try:
        conn = get_snowflake_connection()
        cur = conn.cursor()
        cur.execute(f"CALL {proc_name}()")
        logger.info(f"Snowflake procedure {proc_name} executed successfully.")
        cur.close()
        conn.close()
    except Exception as e:
        logger.error(f"Error executing Snowflake procedure {proc_name}: {e}")
        raise

# ---------------------------
# 6. CREATE SNOWFLAKE EXTERNAL TABLES (CODE GENERATION)
# ---------------------------

def generate_external_table_sql(table_name, parquet_url, schema_map):
    """
    Generates SQL for creating a Snowflake external table referencing Parquet files in ADLS.
    """
    # Example schema_map: {"AGENT_ID": "NUMBER", "AGENT_NAME": "STRING", ...}
    columns = ',\n  '.join([f'"{col}" {dtype}' for col, dtype in schema_map.items()])
    sql = f"""
    CREATE OR REPLACE EXTERNAL TABLE EXT_{table_name} (
      {columns}
    )
    WITH LOCATION = '{parquet_url}'
    FILE_FORMAT = (TYPE = 'PARQUET')
    AUTO_REFRESH = FALSE;
    """
    return sql

def create_external_table_in_snowflake(table_name, parquet_url, schema_map):
    """
    Executes the generated external table SQL in Snowflake.
    """
    sql = generate_external_table_sql(table_name, parquet_url, schema_map)
    logger.info(f"Creating external table EXT_{table_name} in Snowflake.")
    try:
        conn = get_snowflake_connection()
        cur = conn.cursor()
        cur.execute(sql)
        logger.info(f"External table EXT_{table_name} created.")
        cur.close()
        conn.close()
    except Exception as e:
        logger.error(f"Error creating external table EXT_{table_name}: {e}")
        raise

# ---------------------------
# 7. DATA VALIDATION & COMPARISON
# ---------------------------

def fetch_table_as_df_snowflake(table_name):
    """
    Fetches a Snowflake table as a pandas DataFrame.
    """
    try:
        conn = get_snowflake_connection()
        df = pd.read_sql(f'SELECT * FROM {table_name}', conn)
        conn.close()
        return df
    except Exception as e:
        logger.error(f"Error fetching Snowflake table {table_name}: {e}")
        raise

def compare_tables(df1, df2, table_name):
    """
    Compares two DataFrames for row count and column-wise data.
    Returns a dict with comparison results.
    """
    result = {
        "table": table_name,
        "row_count_oracle": len(df1),
        "row_count_snowflake": len(df2),
        "row_count_match": len(df1) == len(df2),
        "column_discrepancies": [],
        "sample_mismatches": [],
        "match_status": "MATCH"
    }

    # Compare columns
    if set(df1.columns) != set(df2.columns):
        result["column_discrepancies"].append({
            "oracle_columns": list(df1.columns),
            "snowflake_columns": list(df2.columns)
        })
        result["match_status"] = "NO MATCH"
        return result

    # Sort columns for comparison
    df1 = df1[sorted(df1.columns)]
    df2 = df2[sorted(df2.columns)]

    # Compare data
    mismatches = []
    for idx, (row1, row2) in enumerate(zip(df1.itertuples(index=False), df2.itertuples(index=False))):
        if tuple(row1) != tuple(row2):
            mismatches.append({
                "row_index": idx,
                "oracle_row": row1,
                "snowflake_row": row2
            })
            if len(mismatches) >= 5:  # Sample up to 5 mismatches
                break

    if mismatches:
        result["sample_mismatches"] = mismatches
        result["match_status"] = "PARTIAL MATCH"
    elif not result["row_count_match"]:
        result["match_status"] = "NO MATCH"
    else:
        result["match_status"] = "MATCH"

    return result

# ---------------------------
# 8. MAIN EXECUTION LOGIC
# ---------------------------

def main():
    logger.info("Starting Oracle to Snowflake migration validation process.")

    # Step 1: Execute Oracle stored procedure
    try:
        execute_oracle_procedure("LOAD_GOLD_AGENTS")
    except Exception as e:
        logger.error(f"Aborting: Oracle procedure execution failed: {e}")
        sys.exit(1)

    # Step 2: Export Oracle tables to Parquet and upload to ADLS
    parquet_files = {}
    parquet_urls = {}
    schema_maps = {}

    with tempfile.TemporaryDirectory() as tmpdir:
        for table in TARGET_TABLES:
            table_name = table["oracle"]
            try:
                parquet_file = export_oracle_table_to_parquet(table_name, tmpdir)
                parquet_files[table_name] = parquet_file

                # Upload to Azure
                blob_name = f"{AZURE_PARQUET_PATH}{os.path.basename(parquet_file)}"
                upload_to_azure(parquet_file, AZURE_CONTAINER, blob_name)
                parquet_urls[table_name] = f"azure://{AZURE_CONTAINER}/{blob_name}"

                # Infer schema for external table creation
                df = pd.read_parquet(parquet_file)
                schema_map = {}
                for col, dtype in zip(df.columns, df.dtypes):
                    # Map pandas/oracle types to Snowflake types
                    if pd.api.types.is_integer_dtype(dtype):
                        schema_map[col] = "NUMBER"
                    elif pd.api.types.is_float_dtype(dtype):
                        schema_map[col] = "FLOAT"
                    elif pd.api.types.is_datetime64_any_dtype(dtype):
                        schema_map[col] = "TIMESTAMP_NTZ"
                    else:
                        schema_map[col] = "STRING"
                schema_maps[table_name] = schema_map
            except Exception as e:
                logger.error(f"Error in export/upload for table {table_name}: {e}")
                sys.exit(1)

        # Step 3: Create Snowflake external tables
        for table in TARGET_TABLES:
            table_name = table["oracle"]
            try:
                create_external_table_in_snowflake(table_name, parquet_urls[table_name], schema_maps[table_name])
            except Exception as e:
                logger.error(f"Error creating external table for {table_name}: {e}")
                sys.exit(1)

    # Step 4: Execute Snowflake stored procedure
    try:
        execute_snowflake_procedure("LOAD_GOLD_AGENTS")
    except Exception as e:
        logger.error(f"Aborting: Snowflake procedure execution failed: {e}")
        sys.exit(1)

    # Step 5: Data validation and comparison
    comparison_report = []
    for table in TARGET_TABLES:
        oracle_table = f'EXT_{table["oracle"]}'
        snowflake_table = table["snowflake"]
        try:
            df_oracle = fetch_table_as_df_snowflake(oracle_table)
            df_snowflake = fetch_table_as_df_snowflake(snowflake_table)
            result = compare_tables(df_oracle, df_snowflake, table["oracle"])
            comparison_report.append(result)
        except Exception as e:
            logger.error(f"Error comparing tables {oracle_table} and {snowflake_table}: {e}")
            comparison_report.append({
                "table": table["oracle"],
                "error": str(e),
                "match_status": "ERROR"
            })

    # Step 6: Reporting
    summary = {
        "timestamp": datetime.now().isoformat(),
        "tables_compared": len(TARGET_TABLES),
        "match_counts": {
            "MATCH": sum(1 for r in comparison_report if r.get("match_status") == "MATCH"),
            "PARTIAL MATCH": sum(1 for r in comparison_report if r.get("match_status") == "PARTIAL MATCH"),
            "NO MATCH": sum(1 for r in comparison_report if r.get("match_status") == "NO MATCH"),
            "ERROR": sum(1 for r in comparison_report if r.get("match_status") == "ERROR"),
        },
        "details": comparison_report
    }

    # Write report to file
    report_file = f"migration_comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    import json
    with open(report_file, "w") as f:
        json.dump(summary, f, indent=2, default=str)
    logger.info(f"Comparison report written to {report_file}")

    print(json.dumps(summary, indent=2, default=str))

# ---------------------------
# 9. ENTRY POINT
# ---------------------------

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"Fatal error in migration validation script: {e}")
        sys.exit(1)

# ---------------------------
# 10. NOTES & SECURITY
# ---------------------------
# - All credentials must be passed via environment variables.
# - No credentials are hardcoded.
# - All connections use secure drivers.
# - Logging is enabled for audit and troubleshooting.
# - For large tables, batching and chunked uploads can be implemented.
# - Progress is logged at each major step.
# - Script is modular and can be integrated into automated pipelines.

# ---------------------------
# 11. SNOWFLAKE EXTERNAL TABLE CREATION EXAMPLE (for reference)
# ---------------------------
# Example SQL generated by generate_external_table_sql:
#
# CREATE OR REPLACE EXTERNAL TABLE EXT_GOLD_AGENTS_D (
#   "AGENT_ID" NUMBER,
#   "AGENT_NAME" STRING,
#   "LOAD_DATE" TIMESTAMP_NTZ,
#   "UPDATE_DATE" TIMESTAMP_NTZ,
#   "SOURCE_SYSTEM" STRING
# )
# WITH LOCATION = 'azure://<container>/<path>/<file>.parquet'
# FILE_FORMAT = (TYPE = 'PARQUET')
# AUTO_REFRESH = FALSE;
#
# Ensure that Snowflake has an external stage and integration set up for Azure Data Lake.
```

**API cost for this call: 0.0023 USD**