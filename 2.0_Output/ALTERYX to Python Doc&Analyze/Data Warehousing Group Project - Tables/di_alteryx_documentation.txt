# Data Warehousing Group Project â€“ Alteryx Workflow Documentation

---

## 1. Workflow Overview

This Alteryx workflow is designed to prepare, clean, transform, and load language learning trace data into a dimensional data warehouse model. The workflow ingests raw CSV and TXT files containing learning traces and lexeme reference data, processes and enriches them, and outputs both cleaned flat files and structured tables in a SQL Server database. The primary business goal is to enable advanced analytics and reporting on language learning activities, supporting educational insights and operational improvements.

**Business Problem Solved:**  
The workflow automates the transformation of raw, unstructured learning trace data into a well-organized star schema, enabling efficient querying, reporting, and analysis. It ensures data quality, consistency, and referential integrity, which are critical for downstream BI and analytics use cases.

---

## 2. Tool Breakdown

### **Input Tools**
- **DbFileInput**  
  - *Purpose*: Reads input files (CSV for learning traces, TXT for lexeme references).
  - *Configuration*:  
    - File path, delimiter, encoding (ISO-8859-1 for CSV).
    - Header row enabled for CSV.
    - Flatfile parsing for TXT with explicit column definitions.
  - *Input*: Raw CSV/TXT files.
  - *Output*: Parsed data streams.

### **Data Preparation and Cleansing**
- **Formula**
  - *Purpose*: Cleans and transforms fields (e.g., lexeme_string, part_of_speech).
  - *Configuration*:  
    - Replace substrings, remove special characters, apply title casing.
    - Example: `Replace([lexeme_string], "<*sf>", "*generic lexeme")`
  - *Input*: String fields.
  - *Output*: Cleaned fields.

- **MultiFieldFormula**
  - *Purpose*: Converts language abbreviations to full names.
  - *Configuration*:  
    - Expression:  
      ```
      IF [_CurrentField_] = 'en' THEN 'English'
      ELSEIF [_CurrentField_] = 'es' THEN 'Spanish'
      ...
      ```
  - *Input*: Language code columns.
  - *Output*: Language name columns.

- **TextToColumns**
  - *Purpose*: Splits strings into multiple columns.
  - *Configuration*:  
    - Delimiters: `<\` for lexeme_string, `+:` for part_of_speech.
    - Number of output fields specified.
  - *Input*: Delimited string fields.
  - *Output*: Multiple columns.

- **FindReplace**
  - *Purpose*: Replaces part-of-speech tags with full words using reference data.
  - *Configuration*:  
    - Lookup from lexeme_reference.txt.
  - *Input*: POS tag columns.
  - *Output*: Full part-of-speech names.

- **Select**
  - *Purpose*: Selects and reorders columns for output or further processing.
  - *Configuration*:  
    - Column selection and renaming.
  - *Input*: Data streams.
  - *Output*: Filtered/renamed columns.

- **Unique**
  - *Purpose*: Ensures uniqueness for dimension tables.
  - *Configuration*:  
    - Key column selection (e.g., lemma, lexeme, language).
  - *Input*: Data streams.
  - *Output*: Unique records.

- **RecordID**
  - *Purpose*: Generates unique IDs for dimension tables.
  - *Configuration*:  
    - Field name, data type, starting value.
  - *Input*: Data streams.
  - *Output*: Data with unique IDs.

### **Join/Union/Blend**
- **Join**
  - *Purpose*: Combines data streams based on key fields (e.g., user_id).
  - *Configuration*:  
    - Join keys specified.
    - Output fields selected.
  - *Input*: Two data streams.
  - *Output*: Joined data.

- **Union**
  - *Purpose*: Merges data streams vertically.
  - *Configuration*:  
    - Field alignment.
  - *Input*: Multiple data streams.
  - *Output*: Unified data stream.

### **Aggregation/Transformation**
- **Formula (Advanced)**
  - *Purpose*: Calculates derived fields (e.g., number of morphological components).
  - *Configuration*:  
    - Example: `CountWords([morphological_components])`
  - *Input*: String fields.
  - *Output*: Numeric fields.

### **Output Tools**
- **DbFileOutput**
  - *Purpose*: Writes data to SQL Server tables.
  - *Configuration*:  
    - DSN, table name, overwrite/append, post-load SQL for constraints.
  - *Input*: Data streams for each dimension/fact table.
  - *Output*: Populated SQL Server tables.

- **Flat File Output**
  - *Purpose*: Writes cleaned CSV for reference.
  - *Configuration*:  
    - Output path, delimiter, encoding.
  - *Input*: Cleaned data stream.
  - *Output*: CSV file.

- **BlockUntilDone**
  - *Purpose*: Controls execution order for dependent outputs.
  - *Configuration*:  
    - Downstream tool sequencing.
  - *Input/Output*: Workflow control.

---

## 3. Data Flow Breakdown

### **Markdown Data Flow Diagram**

```mermaid
graph TD
    A[learning_traces.csv] --> B[Clean/Transform (Formula, TextToColumns)]
    B --> C[Join with lexeme_reference.txt (FindReplace)]
    C --> D[Split Columns (TextToColumns)]
    D --> E[Clean/Transform (Formula, MultiFieldFormula)]
    E --> F[Select/Unique/RecordID]
    F --> G[Dimension Tables (DbFileOutput)]
    F --> H[Fact Tables (DbFileOutput)]
    F --> I[Cleaned CSV Output]
    G --> J[SQL Server]
    H --> J
    I --> K[Output/CLEAN_SAMPLE_learning_traces.csv]
```

### **Step-by-Step Data Flow**

1. **Input:**  
   - Read `learning_traces.csv` and `lexeme_reference.txt`.
2. **Initial Cleansing:**  
   - Clean `lexeme_string` and `part_of_speech` fields (remove/replace substrings, title case).
3. **Splitting Columns:**  
   - Use TextToColumns to split composite fields into atomic columns.
4. **Reference Mapping:**  
   - Find and replace part-of-speech tags using reference data.
5. **Language Normalization:**  
   - Convert language codes to full names.
6. **Morphological Processing:**  
   - Clean and count morphological components.
7. **Dimension Table Preparation:**  
   - Select unique values for lemma, lexeme, language, part-of-speech, timestamp, user.
   - Generate unique IDs.
8. **Fact Table Preparation:**  
   - Join dimension keys with fact data.
   - Select and reorder columns.
9. **Output:**  
   - Write dimension and fact tables to SQL Server.
   - Output cleaned CSV for reference.

---

## 4. Data Transformations

- **String Cleansing:**  
  - Remove special tokens (`<*sf>`, `>`, `@`) from `lexeme_string`.
  - Apply title casing.
- **Splitting:**  
  - Split `lexeme_string` and `part_of_speech` into multiple fields.
- **Reference Replacement:**  
  - Replace part-of-speech abbreviations with full words via lookup.
- **Language Normalization:**  
  - Convert language codes (`en`, `es`, etc.) to full names.
- **Morphological Component Processing:**  
  - Remove unwanted characters and count components.
- **ID Generation:**  
  - Assign unique IDs to dimension records.
- **Fact Table Construction:**  
  - Aggregate and join all dimension keys for fact tables.
- **Data Type Enforcement:**  
  - Change field types as needed for warehouse compatibility.

**Business Logic Examples:**
- If a language code is not recognized, set as `_ERROR`.
- Only unique records are loaded into dimension tables to avoid duplicates.

---

## 5. Technical Insights

- **Data Sources:**  
  - `learning_traces.csv` (CSV, ISO-8859-1, header row)
  - `lexeme_reference.txt` (flat file, custom columns)
- **Destinations:**  
  - SQL Server tables: `pos_dim`, `lexeme_dim`, `lemma_dim`, `language_dim`, `timestamp_dim`, `user_dim`, `lexeme_fact`, `learning_trace_fact`
  - Cleaned CSV: `CLEAN_SAMPLE_learning_traces.csv`
- **Fields/Columns:**  
  - IDs: `pos_id`, `lexeme_id`, `lemma_id`, `language_id`, `timestamp_id`, `user_id`
  - Attributes: `part_of_speech`, `lexeme`, `lemma`, `language`, `timestamp`, etc.
- **Macros/Advanced Tools:**  
  - No custom macros; advanced use of Formula, MultiFieldFormula, TextToColumns, Join, Unique.
- **SQL Post-processing:**  
  - Primary and foreign key constraints added via post-load SQL.

---

## 6. Technical Complexity

| Parameter                   | Value/Description                                                                                   |
|-----------------------------|----------------------------------------------------------------------------------------------------|
| **Number of Tools**         | 100+ (as per meta section)                                                                         |
| **Tool Diversity**          | 12+ (DbFileInput, Formula, MultiFieldFormula, TextToColumns, Select, FindReplace, Union, RecordID, Unique, Join, DbFileOutput, BlockUntilDone) |
| **Branches/Dependencies**   | Multiple branches for dimension/fact table prep, controlled by BlockUntilDone                      |
| **Local File Paths**        | Yes (input/output files use local paths, e.g., `C:\Users\vivia\Downloads\...`)                     |
| **Non-Cloud-Compatible**    | Yes (local file paths, ODBC DSN for SQL Server)                                                    |
| **Macros**                  | None explicitly mentioned                                                                          |
| **Conditional Logic**       | MultiFieldFormula IF/ELSEIF, Unique, FindReplace                                                   |
| **Data Sources**            | 2 (CSV, TXT); both flat files                                                                      |
| **Joins/Blends**            | Multiple joins (e.g., user_id, lexeme_id, etc.)                                                    |
| **Annotations**             | Step descriptions present in tool configuration                                                    |
| **Sensitive Data**          | Database credentials (UID, PWD) in ODBC connection                                                 |
| **Complexity Score**        | **85/100** (High: large tool count, diverse tools, multiple branches, SQL post-processing, local file dependencies, referential integrity enforcement) |

---

## 7. Assumptions and Dependencies

- **Prerequisites:**
  - Input files (`learning_traces.csv`, `lexeme_reference.txt`) must exist and match expected schema.
  - SQL Server DSN `Testing` must be configured and accessible.
  - Sufficient permissions to create/overwrite tables and run ALTER statements.
  - Local file paths must be valid and accessible by the workflow runtime environment.
- **Dependencies:**
  - ODBC driver for SQL Server.
  - Consistent data formats (delimiters, encoding).
  - No missing required columns in input files.
- **Assumptions:**
  - Data is consistently available and up-to-date.
  - User IDs and other keys are unique and non-null.
  - All necessary reference data is present in `lexeme_reference.txt`.

---

## 8. Key Outputs

- **Dimension Tables (SQL Server):**
  - `pos_dim`, `lexeme_dim`, `lemma_dim`, `language_dim`, `timestamp_dim`, `user_dim`
  - Each contains unique, cleaned, and keyed records for respective entities.
- **Fact Tables (SQL Server):**
  - `lexeme_fact`, `learning_trace_fact`
  - Contain foreign keys to all dimensions, enabling star schema analytics.
- **Cleaned CSV:**
  - `CLEAN_SAMPLE_learning_traces.csv`
  - Contains cleaned, deduplicated, and enriched learning trace data for reference or further processing.

**Business Value:**  
Outputs enable robust reporting and analytics on language learning activities, supporting curriculum optimization, student progress tracking, and operational decision-making.

---

## 9. Error Handling and Logging

- **Error Identification:**
  - Invalid or unrecognized language codes are set to `_ERROR`.
  - Unique tools prevent duplicate records in dimensions.
  - Data type conversions enforce schema compliance.
- **Logging:**
  - Not explicitly implemented in the workflow, but errors in SQL operations (e.g., constraint violations) will surface during DbFileOutput.
- **Error Management:**
  - Workflow design ensures only valid, unique records are loaded.
  - Use of BlockUntilDone ensures dependencies are respected, preventing partial loads.
- **Automated Handling:**
  - No explicit retry or alerting, but errors in input or SQL will halt execution for manual review.

---

# End of Documentation