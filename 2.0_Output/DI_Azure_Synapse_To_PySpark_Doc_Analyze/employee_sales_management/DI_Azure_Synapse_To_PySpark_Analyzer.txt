=============================================
Author:        Ascendion AVA
Created on:    
Description:   Analysis and migration guidance for the Azure Synapse stored procedure `dw.sp_load_sales_fact`, which loads, validates, and transforms sales transaction data into the enterprise data warehouse.
=============================================

## 1. Procedure Overview

The `dw.sp_load_sales_fact` stored procedure orchestrates the end-to-end ETL process for sales transaction data within Azure Synapse Analytics. It automates audit logging, data validation, cleansing, transformation, enrichment, and loading into the enterprise data warehouse fact table (`dw.Fact_Sales`). The procedure ensures only high-quality, validated data is loaded, logs all data quality failures, and maintains a robust audit trail for operational and regulatory requirements. This workflow supports business objectives such as accurate analytics, compliance, and data lineage.

- **Number of mappings per workflow/session:** 1 (single stored procedure encapsulating the entire ETL logic)

---

## 2. Complexity Metrics

| Metric                        | Value / Type                                                                                       |
|-------------------------------|----------------------------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (stg.Sales_Transactions; SQL Server table)                                                       |
| Number of Transformations     | 3 (Validation [Filter], Transformation/Enrichment [Expression/Join], Aggregation [none explicit])  |
| Lookup Usage                  | 2 (Connected: dw.Dim_Customer [SQL Server], dw.Dim_Date [SQL Server])                              |
| Expression Logic              | 2 (Total_Sales_Amount calculation, SYSDATETIME usage; simple arithmetic and timestamp)             |
| Join Conditions               | 2 (INNER JOIN: Customer_ID, INNER JOIN: Sales_Date to Date_Value)                                  |
| Conditional Logic             | 2 (Validation filters: Customer_ID IS NULL, Quantity <= 0)                                         |
| Reusable Components           | 0 (No reusable transformations or mapplets; all logic is inline)                                   |
| Data Sources                  | 3 (SQL Server: stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date)                              |
| Data Targets                  | 3 (SQL Server: dw.Fact_Sales, dw.DQ_Failures, dw.Audit_Log)                                       |
| Pre/Post SQL Logic            | 2 (Audit log insert/update, DQ failure logging)                                                    |
| Session/Workflow Controls     | 1 (TRY...CATCH for error handling; procedural control)                                             |
| DML Logic                     | 4 (INSERT, DELETE, TRUNCATE, UPDATE)                                                              |
| Complexity Score (0–100)      | 35 (Matches DI_Azure_Synapse_Documentation; moderate complexity, mostly standard ETL patterns)     |

**High-complexity areas:**
- Multiple joins (dimension enrichment)
- Branching logic via TRY...CATCH and validation filter conditions
- Audit and DQ logging for traceability

---

## 3. Syntax Differences

- **Functions without direct PySpark equivalents:**
    - `SYSDATETIME()` → Use `current_timestamp()` in PySpark.
    - `NEWID()` → Use `uuid.uuid4()` or `F.monotonically_increasing_id()` for batch IDs.
    - `OBJECT_NAME(@@PROCID)` → No direct equivalent; procedure name must be passed as a parameter or hardcoded.
    - `@@ROWCOUNT` → Use DataFrame `.count()` after actions in PySpark.
    - `TRY...CATCH` → Use Python `try...except` blocks.
    - Temporary tables (`#InvalidRows`) → Use temporary DataFrames or persisted tables in Spark.
    - `TRUNCATE TABLE` → Use `overwrite` mode when writing to tables in Spark, or explicit `spark.sql("TRUNCATE TABLE ...")`.

- **Data type conversions:**
    - `DATETIME` → PySpark `TimestampType`
    - `UNIQUEIDENTIFIER` → PySpark `StringType` (UUID as string)
    - `NVARCHAR` → PySpark `StringType`
    - `BIGINT`, `INT` → PySpark `LongType`, `IntegerType`

- **Workflow/control logic:**
    - Audit logging and DQ failure logging must be implemented as DataFrame writes/appends.
    - Batch variables and counters must be managed in Python scope, not as SQL variables.
    - No direct equivalent for session variables; use Python variables.

---

## 4. Manual Adjustments

- **Components requiring manual implementation:**
    - Audit log insert/update logic must be re-implemented using DataFrame operations and explicit writes.
    - Data quality validation (invalid row capture) must use DataFrame filters and union operations.
    - Error handling (`TRY...CATCH`) must be mapped to Python `try...except` blocks, with explicit logging.
    - Row count tracking (`@@ROWCOUNT`) must be replaced with DataFrame `.count()` after each relevant operation.
    - Temporary tables (`#InvalidRows`) must be replaced with temporary DataFrames or persisted tables as needed.
    - Procedure name tracking (`OBJECT_NAME(@@PROCID)`) must be hardcoded or passed as a parameter.
    - Batch ID generation must use Python's `uuid` or Spark's unique ID functions.
    - Any orchestration or scheduling logic must be migrated to Databricks Jobs or external orchestrators (e.g., Azure Data Factory).

- **External dependencies:**
    - If pre/post SQLs, external stored procedures, or shell scripts are referenced outside this procedure, they must be identified and re-implemented in PySpark or Databricks workflows.
    - Business logic for validation and enrichment should be reviewed with SMEs to ensure semantic equivalence post-migration.

---

## 5. Optimization Techniques

- **Spark best practices:**
    - Use DataFrame partitioning on large tables (e.g., by `Sales_Date` or `Region_ID`) to optimize parallelism.
    - Cache intermediate DataFrames if reused (e.g., dimension lookups).
    - Use broadcast joins for small dimension tables (`dw.Dim_Customer`, `dw.Dim_Date`) to avoid shuffles.
    - Replace chain filters and joins with a single, pipelined DataFrame transformation for efficiency.
    - Use Spark window functions for any aggregations or row-wise calculations if logic expands.
    - Prefer `overwrite` mode for truncating tables, and `append` for audit/DQ logs.
    - Leverage Spark's built-in error handling and logging for robust monitoring.

- **Refactor vs. Rebuild:**
    - **Refactor**: The logic can be largely retained and mapped to PySpark DataFrame operations, as the ETL flow is standard and modular.
    - **Rebuild**: Consider a rebuild only if significant performance improvements or architectural changes are required (e.g., for very large datasets, or if integrating with new data sources).

---

**API Cost:** apiCost: 0.0025 USD