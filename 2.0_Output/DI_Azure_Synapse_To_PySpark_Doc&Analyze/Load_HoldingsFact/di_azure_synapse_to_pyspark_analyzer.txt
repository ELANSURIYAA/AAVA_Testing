=============================================
Author:        Ascendion AVA
Created on:   
Description:   Analysis and migration mapping of Synapse SQL stored procedure 'LOAD_FACT_EXECUTIVE_SUMMARY' for Databricks PySpark conversion.
=============================================

# 1. Procedure Overview

The provided Azure Synapse stored procedure (`dbo.LOAD_FACT_EXECUTIVE_SUMMARY`) is designed to load the `FACT_EXECUTIVE_SUMMARY` fact table with summarized holding metrics. It ingests data from the staging table `STG_HOLDING_METRICS`, applies business rules (such as income amount validation), and ensures referential integrity by joining with four dimension tables (`DIM_DATE`, `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`). The process supports executive dashboards, regulatory reporting, and downstream analytics by providing a cleansed, integrated fact table.

- **Number of mappings per workflow/session:** 1 stored procedure mapping (single workflow)

# 2. Complexity Metrics

| Metric                               | Value/Details                                                                                  |
|---------------------------------------|-----------------------------------------------------------------------------------------------|
| Number of Source Qualifiers           | 1 (STG_HOLDING_METRICS staging table)                                                         |
| Number of Transformations             | 1 (income_amount transformation via CASE statement)                                            |
| Lookup Usage                          | 4 (Joins to DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT)                          |
| Expression Logic                      | 1 (CASE WHEN for income_amount validation)                                                     |
| Join Conditions                       | 4 INNER JOINs (all normal joins for referential integrity)                                     |
| Conditional Logic                     | 1 (income_amount CASE statement)                                                               |
| Reusable Components                   | 0 (No reusable transformations or mapplets)                                                    |
| Data Sources                          | 1 (SQL Server/Synapse table: STG_HOLDING_METRICS)                                             |
| Data Targets                          | 1 (SQL Server/Synapse table: FACT_EXECUTIVE_SUMMARY)                                          |
| Pre/Post SQL Logic                    | 0 (No explicit pre/post SQLs, but includes audit logging and cleanup within the procedure)     |
| Session/Workflow Controls             | 0 (No session/workflow controls; all logic is in the stored procedure)                         |
| DML Logic                             | 1 INSERT (into FACT_EXECUTIVE_SUMMARY), 1 SELECT INTO (for temp table), 1 DROP TABLE          |
| Complexity Score (0â€“100)              | 25 (matches DI_Azure_Synapse_Documentation Complexity Score)                                   |

**High-complexity areas:**  
- Multiple dimension lookups (4 INNER JOINs)
- Data quality logic (CASE WHEN for income_amount)
- Audit logging and temp table management

**Data source/target types:**  
- Source: SQL Server/Synapse table (`STG_HOLDING_METRICS`)
- Targets: SQL Server/Synapse table (`FACT_EXECUTIVE_SUMMARY`)
- Lookup/reference: SQL Server/Synapse tables (`DIM_DATE`, `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`)

# 3. Syntax Differences

- **Temporary Tables:**  
  - Synapse uses `SELECT ... INTO #temp` and `DROP TABLE #temp`; in PySpark, use DataFrame caching or temporary views.
- **CASE WHEN Logic:**  
  - `CASE WHEN stg.income_amount IS NULL OR stg.income_amount < 0 THEN 0 ELSE stg.income_amount END`  
    - In PySpark: Use `when`/`otherwise` from `pyspark.sql.functions`.
- **Joins:**  
  - SQL INNER JOINs map to DataFrame `.join()` with appropriate join conditions.
- **Audit Logging:**  
  - `PRINT` and variable assignment (`@v_row_count = @@ROWCOUNT`) need to be replaced with DataFrame `count()` and logging via notebook/cluster logs.
- **Variables:**  
  - SQL variables (`DECLARE @v_row_count INT`) are replaced by Python variables.
- **No direct equivalents:**  
  - `@@ROWCOUNT` (use DataFrame count)
  - `PRINT` (use Python `print()` or logging)
- **Data Types:**  
  - Ensure numeric/decimal types are mapped correctly between SQL and Spark DataFrame schemas.

# 4. Manual Adjustments

- **Temp Table Handling:**  
  - Replace `#staging_metrics` with a cached DataFrame or a Spark temporary view.
- **Audit Logging:**  
  - Manual implementation needed for row count logging.
- **Error Handling:**  
  - SQL error handling (`@error_message`) is not present in the code, but if needed, must be implemented using Python `try/except`.
- **Cleanup:**  
  - Explicit temp table drops are unnecessary; manage DataFrame persistence and unpersist as needed.
- **External Dependencies:**  
  - All dimension tables must be accessible as Spark tables or DataFrames.
- **Business Logic Validation:**  
  - Post-migration, validate that the CASE logic for `income_amount` and join integrity is preserved.

# 5. Optimization Techniques

- **Partitioning:**  
  - Partition DataFrames on join keys (e.g., `date_key`, `institution_id`) to optimize join performance.
- **Broadcast Joins:**  
  - Use broadcast joins for small dimension tables to speed up lookups.
- **Caching:**  
  - Cache staging DataFrame if reused.
- **Pipeline Transformation:**  
  - Chain transformations using DataFrame API for efficient execution.
- **Window Functions:**  
  - Not required for this logic, but consider for future nested aggregations.
- **Refactor vs. Rebuild:**  
  - **Refactor** is recommended: The logic is straightforward and can be mapped directly to PySpark DataFrame operations with minor restructuring.

---

**API Cost Calculation:**  
apiCost: 0.007 USD

---

**Note:**  
- This analysis is based on the provided Synapse SQL stored procedure code and associated documentation.
- No additional pipeline/dataflow artifacts were available for review.
- All recommendations assume equivalent data availability in Databricks and that business logic remains unchanged.