Test Case List:

Test Case ID: TC01
Test Case Description: Happy path - All rows are valid, all columns present, correct types, data is loaded successfully.
Expected Outcome: Data is written to the target table, no errors are printed, and all rows are present in the output.

Test Case ID: TC02
Test Case Description: Edge case - Input DataFrame is empty (no rows).
Expected Outcome: No data is written to the target table, no errors are printed.

Test Case ID: TC03
Test Case Description: Edge case - Some rows have missing columns (null values).
Expected Outcome: Only valid rows are written to the target table; invalid rows are logged as errors.

Test Case ID: TC04
Test Case Description: Edge case - All rows are invalid (all have missing columns).
Expected Outcome: No data is written to the target table; all rows are logged as errors.

Test Case ID: TC05
Test Case Description: Edge case - Amount column contains non-numeric values.
Expected Outcome: Rows with non-numeric values in amount are written with amount as null (PySpark cast behavior); these can be validated.

Test Case ID: TC06
Test Case Description: Error handling - File does not exist.
Expected Outcome: File open error is caught and printed; no data is loaded.

Test Case ID: TC07
Test Case Description: Error handling - Exception during table write (simulate write failure).
Expected Outcome: Rollback is called, error is printed, and no data is committed.

Test Case ID: TC08
Test Case Description: Boundary condition - Maximum allowed field lengths for each column.
Expected Outcome: Data is written successfully if all columns are present.

Test Case ID: TC09
Test Case Description: Edge case - Nulls in non-amount columns.
Expected Outcome: Rows with nulls in any column are considered invalid and logged as errors.

Test Case ID: TC10
Test Case Description: Happy path - Multiple valid rows, mixture of currencies and posting dates.
Expected Outcome: All valid rows are written to the target table with correct values.

Pytest Script:

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from pyspark.sql.functions import col
from unittest import mock

# Fixture for SparkSession
@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("TestFinanceLoad").getOrCreate()
    yield spark
    spark.stop()

# Helper function to create DataFrame with the schema
def make_df(spark, data):
    schema = StructType([
        StructField("bukrs", StringType(), True),
        StructField("fiscyear", StringType(), True),
        StructField("costcenter", StringType(), True),
        StructField("gl_account", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("currency", StringType(), True),
        StructField("posting_date", StringType(), True)
    ])
    return spark.createDataFrame(data, schema=schema)

# Mock for DataFrame.write.saveAsTable
class DummyWriter:
    def __init__(self):
        self.mode_called = None
        self.table_written = None
    def mode(self, mode):
        self.mode_called = mode
        return self
    def saveAsTable(self, table):
        self.table_written = table

# Test TC01: Happy path
def test_happy_path(spark, capsys):
    data = [
        ("1000", "2023", "CC01", "GL1000", "123.45", "USD", "2023-01-01"),
        ("2000", "2023", "CC02", "GL2000", "678.90", "EUR", "2023-01-02"),
    ]
    df = make_df(spark, data)
    with mock.patch("pyspark.sql.DataFrame.write", new_callable=mock.PropertyMock) as mock_write:
        dummy_writer = DummyWriter()
        mock_write.return_value = dummy_writer
        # Filtering logic as in main script
        valid_rows = df.filter(
            col("bukrs").isNotNull() &
            col("fiscyear").isNotNull() &
            col("costcenter").isNotNull() &
            col("gl_account").isNotNull() &
            col("amount").isNotNull() &
            col("currency").isNotNull() &
            col("posting_date").isNotNull()
        )
        invalid_rows = df.filter(
            col("bukrs").isNull() |
            col("fiscyear").isNull() |
            col("costcenter").isNull() |
            col("gl_account").isNull() |
            col("amount").isNull() |
            col("currency").isNull() |
            col("posting_date").isNull()
        )
        assert valid_rows.count() == 2
        assert invalid_rows.count() == 0
        bw_data = valid_rows.withColumn("amount", col("amount").cast(DoubleType()))
        # Simulate writing
        bw_data.write.mode("append").saveAsTable("zbw_finance_data")
        assert dummy_writer.mode_called == "append"
        assert dummy_writer.table_written == "zbw_finance_data"

# Test TC02: Empty DataFrame
def test_empty_dataframe(spark, capsys):
    df = make_df(spark, [])
    valid_rows = df.filter(
        col("bukrs").isNotNull() &
        col("fiscyear").isNotNull() &
        col("costcenter").isNotNull() &
        col("gl_account").isNotNull() &
        col("amount").isNotNull() &
        col("currency").isNotNull() &
        col("posting_date").isNotNull()
    )
    assert valid_rows.count() == 0

# Test TC03: Some rows invalid
def test_some_rows_invalid(spark, capsys):
    data = [
        ("1000", "2023", "CC01", "GL1000", "123.45", "USD", "2023-01-01"),
        (None, "2023", "CC02", "GL2000", "678.90", "EUR", "2023-01-02"),
    ]
    df = make_df(spark, data)
    valid_rows = df.filter(
        col("bukrs").isNotNull() &
        col("fiscyear").isNotNull() &
        col("costcenter").isNotNull() &
        col("gl_account").isNotNull() &
        col("amount").isNotNull() &
        col("currency").isNotNull() &
        col("posting_date").isNotNull()
    )
    invalid_rows = df.filter(
        col("bukrs").isNull() |
        col("fiscyear").isNull() |
        col("costcenter").isNull() |
        col("gl_account").isNull() |
        col("amount").isNull() |
        col("currency").isNull() |
        col("posting_date").isNull()
    )
    assert valid_rows.count() == 1
    assert invalid_rows.count() == 1

# Test TC04: All rows invalid
def test_all_rows_invalid(spark, capsys):
    data = [
        (None, None, None, None, None, None, None),
        (None, "2023", None, "GL2000", None, "EUR", None),
    ]
    df = make_df(spark, data)
    valid_rows = df.filter(
        col("bukrs").isNotNull() &
        col("fiscyear").isNotNull() &
        col("costcenter").isNotNull() &
        col("gl_account").isNotNull() &
        col("amount").isNotNull() &
        col("currency").isNotNull() &
        col("posting_date").isNotNull()
    )
    invalid_rows = df.filter(
        col("bukrs").isNull() |
        col("fiscyear").isNull() |
        col("costcenter").isNull() |
        col("gl_account").isNull() |
        col("amount").isNull() |
        col("currency").isNull() |
        col("posting_date").isNull()
    )
    assert valid_rows.count() == 0
    assert invalid_rows.count() == 2

# Test TC05: Non-numeric amount
def test_non_numeric_amount(spark, capsys):
    data = [
        ("1000", "2023", "CC01", "GL1000", "abc", "USD", "2023-01-01"),
    ]
    df = make_df(spark, data)
    bw_data = df.withColumn("amount", col("amount").cast(DoubleType()))
    result = bw_data.collect()
    # amount should be None (null) after cast
    assert result[0]["amount"] is None

# Test TC06: File does not exist
def test_file_not_exist(monkeypatch, capsys):
    # Simulate file open error by raising an exception
    with mock.patch("pyspark.sql.SparkSession.read", side_effect=Exception("File not found")):
        try:
            spark = SparkSession.builder.master("local[1]").appName("Test").getOrCreate()
            spark.read.csv("nonexistent.csv")
        except Exception as e:
            assert "File not found" in str(e)
        finally:
            spark.stop()

# Test TC07: Exception during table write
def test_write_exception(spark, capsys):
    data = [
        ("1000", "2023", "CC01", "GL1000", "123.45", "USD", "2023-01-01"),
    ]
    df = make_df(spark, data)
    bw_data = df.withColumn("amount", col("amount").cast(DoubleType()))
    with mock.patch("pyspark.sql.DataFrame.write", new_callable=mock.PropertyMock) as mock_write:
        class FailingWriter:
            def mode(self, mode):
                return self
            def saveAsTable(self, table):
                raise Exception("Simulated write failure")
        mock_write.return_value = FailingWriter()
        with pytest.raises(Exception) as excinfo:
            bw_data.write.mode("append").saveAsTable("zbw_finance_data")
        assert "Simulated write failure" in str(excinfo.value)

# Test TC08: Maximum field lengths
def test_max_field_lengths(spark, capsys):
    max_str = "X" * 255
    data = [
        (max_str, max_str, max_str, max_str, "123.45", max_str, max_str),
    ]
    df = make_df(spark, data)
    valid_rows = df.filter(
        col("bukrs").isNotNull() &
        col("fiscyear").isNotNull() &
        col("costcenter").isNotNull() &
        col("gl_account").isNotNull() &
        col("amount").isNotNull() &
        col("currency").isNotNull() &
        col("posting_date").isNotNull()
    )
    assert valid_rows.count() == 1

# Test TC09: Nulls in non-amount columns
def test_nulls_in_non_amount_columns(spark, capsys):
    data = [
        ("1000", None, "CC01", "GL1000", "123.45", "USD", "2023-01-01"),
    ]
    df = make_df(spark, data)
    valid_rows = df.filter(
        col("bukrs").isNotNull() &
        col("fiscyear").isNotNull() &
        col("costcenter").isNotNull() &
        col("gl_account").isNotNull() &
        col("amount").isNotNull() &
        col("currency").isNotNull() &
        col("posting_date").isNotNull()
    )
    invalid_rows = df.filter(
        col("bukrs").isNull() |
        col("fiscyear").isNull() |
        col("costcenter").isNull() |
        col("gl_account").isNull() |
        col("amount").isNull() |
        col("currency").isNull() |
        col("posting_date").isNull()
    )
    assert valid_rows.count() == 0
    assert invalid_rows.count() == 1

# Test TC10: Multiple valid rows, mixture
def test_multiple_valid_rows_mixture(spark, capsys):
    data = [
        ("1000", "2023", "CC01", "GL1000", "123.45", "USD", "2023-01-01"),
        ("2000", "2022", "CC02", "GL2000", "678.90", "EUR", "2022-12-31"),
    ]
    df = make_df(spark, data)
    valid_rows = df.filter(
        col("bukrs").isNotNull() &
        col("fiscyear").isNotNull() &
        col("costcenter").isNotNull() &
        col("gl_account").isNotNull() &
        col("amount").isNotNull() &
        col("currency").isNotNull() &
        col("posting_date").isNotNull()
    )
    assert valid_rows.count() == 2
    rows = valid_rows.collect()
    assert rows[0]["currency"] == "USD"
    assert rows[1]["currency"] == "EUR"

# End of test script
```

# apiCost: 0.004 USD