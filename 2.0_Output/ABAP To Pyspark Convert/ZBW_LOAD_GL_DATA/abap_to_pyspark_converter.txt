# PySpark conversion of ABAP program "ZBW_LOAD_GL_DATA.txt"
# This script reads a CSV file, processes its contents, and writes the data to a target table.
# The logic and structure closely follow the original ABAP code, with optimizations for Spark.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType

# Initialize Spark session
spark = SparkSession.builder \
    .appName("LoadFinanceDataToBW") \
    .getOrCreate()

# Define the schema matching the SAP BW table structure
schema = StructType([
    StructField("bukrs", StringType(), True),         # Company Code
    StructField("fiscyear", StringType(), True),      # Fiscal Year
    StructField("costcenter", StringType(), True),    # Cost Center
    StructField("gl_account", StringType(), True),    # GL Account
    StructField("amount", DoubleType(), True),        # Transaction Amount
    StructField("currency", StringType(), True),      # Currency
    StructField("posting_date", StringType(), True),  # Posting Date (could be DateType if format is known)
])

# File path (update as needed)
input_file = "/usr/sap/interfaces/finance_data.csv"

try:
    # Read CSV file into DataFrame
    # Assuming the CSV does not have a header row
    df = spark.read.csv(input_file, schema=schema, header=False)

    # Validate row length (should be 7 fields per row)
    # If the CSV is well-formed, this is not required; otherwise, filter out malformed rows
    df_valid = df.dropna(subset=["bukrs", "fiscyear", "costcenter", "gl_account", "amount", "currency", "posting_date"])

    # Optionally, log or collect malformed rows for review
    df_invalid = df.filter(
        col("bukrs").isNull() |
        col("fiscyear").isNull() |
        col("costcenter").isNull() |
        col("gl_account").isNull() |
        col("amount").isNull() |
        col("currency").isNull() |
        col("posting_date").isNull()
    )
    invalid_count = df_invalid.count()
    if invalid_count > 0:
        print(f"Error: Incorrect file format in {invalid_count} line(s).")

    # Insert valid data into the target table (replace with actual target, e.g., JDBC, Hive, etc.)
    # Example: Write to a Hive table named 'zbw_finance_data'
    target_table = "zbw_finance_data"
    df_valid.write.mode("append").saveAsTable(target_table)

    print("Data successfully loaded into SAP BW table.")

except Exception as e:
    print(f"Error while inserting data into SAP BW: {e}")

finally:
    spark.stop()

# Notes:
# - The schema should match the target table's structure and data types.
# - If the posting_date format is known, use DateType and proper parsing.
# - For production, add logging and error handling as needed.
# - Update 'input_file' and 'target_table' as per your environment.

# apiCost: 0.002 USD