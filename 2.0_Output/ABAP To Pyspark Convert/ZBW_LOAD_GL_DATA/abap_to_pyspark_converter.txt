# PySpark equivalent of ABAP program "zload_finance_to_bw"
# This script reads a CSV file, validates and transforms its data, and writes it to a target table.
# The logic is mapped from ABAP constructs (internal tables, loops, conditions, error handling, and DB operations) to PySpark DataFrame operations.
# Follows PEP 8 style guidelines.

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType
from pyspark.sql.functions import col, lit, when, count, to_date

# Initialize Spark session
spark = SparkSession.builder.appName("LoadFinanceToBW").getOrCreate()

# Define the schema equivalent to ABAP structure zbw_finance_data
finance_schema = StructType([
    StructField("bukrs", StringType(), True),         # Company Code
    StructField("fiscyear", StringType(), True),      # Fiscal Year
    StructField("costcenter", StringType(), True),    # Cost Center
    StructField("gl_account", StringType(), True),    # GL Account
    StructField("amount", StringType(), True),        # Transaction Amount (can be cast to DoubleType if needed)
    StructField("currency", StringType(), True),      # Currency
    StructField("posting_date", StringType(), True)   # Posting Date (can be cast to DateType if needed)
])

# File path (replace with actual path as needed)
filename = "/usr/sap/interfaces/finance_data.csv"

# Read CSV file into DataFrame
df_raw = spark.read.csv(filename, schema=None, sep=",", header=False)

# Validate: Ensure each row has exactly 7 fields
df_valid = df_raw.filter(df_raw._c0.isNotNull() & df_raw._c6.isNotNull())
df_valid = df_valid.filter(
    (count("*").over(Window.partitionBy()) == 7)
)

# Alternatively, check for correct number of columns
if len(df_raw.columns) != 7:
    print("Error: Incorrect file format. Expected 7 columns, got {}".format(len(df_raw.columns)))
    spark.stop()
    exit(1)

# Rename columns to match target schema
df_finance = df_raw.toDF(
    "bukrs", "fiscyear", "costcenter", "gl_account", "amount", "currency", "posting_date"
)

# Optional: Data type conversions
# df_finance = df_finance.withColumn("amount", col("amount").cast(DoubleType()))
# df_finance = df_finance.withColumn("posting_date", to_date(col("posting_date"), "yyyy-MM-dd"))

# Error handling for file access is implicit in Spark read; will throw exception if file not found

# Insert data into target table (replace with actual table name and write method as per environment)
try:
    # For demonstration, write to a managed table or external location
    df_finance.write.mode("append").saveAsTable("zbw_finance_data")
    print("Data successfully loaded into SAP BW table (zbw_finance_data).")
except Exception as e:
    print("Error while inserting data into SAP BW: {}".format(str(e)))
    # Optionally, implement rollback logic if using transactional tables

# Stop Spark session
spark.stop()

# ----------------------
# API Cost Consumed: 1 file listing + 1 file read = 2 operations
# ----------------------