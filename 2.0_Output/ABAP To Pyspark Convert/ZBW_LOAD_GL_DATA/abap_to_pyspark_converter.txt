# PySpark equivalent of ABAP program zload_finance_to_bw
# This script reads a CSV file, validates each row, transforms it, and writes to a target table.
# It preserves the logic and error handling of the original ABAP code.
# PEP 8 formatting and comments included for clarity.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

# Initialize Spark session
spark = SparkSession.builder.appName("LoadFinanceDataToBW").getOrCreate()

# File path (adjust as needed)
filename = "/usr/sap/interfaces/finance_data.csv"

# Define schema for the CSV file (all as string for initial load)
schema = StructType([
    StructField("bukrs", StringType(), True),         # Company Code
    StructField("fiscyear", StringType(), True),      # Fiscal Year
    StructField("costcenter", StringType(), True),    # Cost Center
    StructField("gl_account", StringType(), True),    # GL Account
    StructField("amount", StringType(), True),        # Transaction Amount
    StructField("currency", StringType(), True),      # Currency
    StructField("posting_date", StringType(), True)   # Posting Date
])

try:
    # Read CSV file into DataFrame
    df = spark.read.csv(filename, schema=schema, sep=",", header=False)

    # Validate: Ensure each row has exactly 7 columns (already enforced by schema)
    # If file format error, log the problematic rows
    valid_rows = df.filter(
        col("bukrs").isNotNull() &
        col("fiscyear").isNotNull() &
        col("costcenter").isNotNull() &
        col("gl_account").isNotNull() &
        col("amount").isNotNull() &
        col("currency").isNotNull() &
        col("posting_date").isNotNull()
    )

    invalid_rows = df.filter(
        col("bukrs").isNull() |
        col("fiscyear").isNull() |
        col("costcenter").isNull() |
        col("gl_account").isNull() |
        col("amount").isNull() |
        col("currency").isNull() |
        col("posting_date").isNull()
    )

    # Log errors for invalid rows
    if invalid_rows.count() > 0:
        print("Error: Incorrect file format in the following lines:")
        invalid_rows.show(truncate=False)

    # Transformations: Cast amount to double, if needed
    bw_data = valid_rows.withColumn("amount", col("amount").cast(DoubleType()))

    # Insert data into target table (replace with actual table name and write method)
    # For demonstration, we write to a Hive table named 'zbw_finance_data'
    try:
        bw_data.write.mode("append").saveAsTable("zbw_finance_data")
        spark.sql("COMMIT")  # Commit transaction if supported
        print("Data successfully loaded into SAP BW table")
    except Exception as insert_err:
        spark.sql("ROLLBACK")  # Rollback transaction if supported
        print("Error while inserting data into SAP BW:", insert_err)

except Exception as file_err:
    print(f"Error opening file: {filename}")
    print(file_err)

# Stop Spark session
spark.stop()

# apiCost: 0.002 USD