# PySpark Conversion of ABAP Program: zload_finance_to_bw
# This script reads a CSV file, validates and transforms the data, and writes it to a target table.
# The logic is mapped from the provided ABAP code, with error handling and logging.
# PEP 8 formatting and comments are included for clarity.

from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, DateType
)
from pyspark.sql.functions import col, lit, to_date

# Initialize Spark session
spark = SparkSession.builder \
    .appName("LoadFinanceToBW") \
    .getOrCreate()

# File path (replace with actual path if needed)
input_file = "/usr/sap/interfaces/finance_data.csv"

# Define schema for the CSV file
schema = StructType([
    StructField("bukrs",        StringType(), True),  # Company Code
    StructField("fiscyear",     StringType(), True),  # Fiscal Year
    StructField("costcenter",   StringType(), True),  # Cost Center
    StructField("gl_account",   StringType(), True),  # GL Account
    StructField("amount",       DoubleType(), True),  # Transaction Amount
    StructField("currency",     StringType(), True),  # Currency
    StructField("posting_date", StringType(), True),  # Posting Date (will be converted)
])

try:
    # Read the CSV file into a DataFrame
    df = spark.read.csv(
        input_file,
        schema=schema,
        header=False,
        mode="DROPMALFORMED"
    )

    # Validate number of columns (should be 7)
    # Rows with missing columns are dropped by schema enforcement and mode="DROPMALFORMED"
    # Additional validation can be added if needed

    # Convert posting_date to DateType (assuming format 'yyyy-MM-dd', adjust if needed)
    df = df.withColumn(
        "posting_date",
        to_date(col("posting_date"), "yyyy-MM-dd")
    )

    # Log number of records read
    print(f"Number of records read: {df.count()}")

    # Write DataFrame to target table (replace with actual table name and connection if needed)
    # Example: writing to a Hive table named 'zbw_finance_data'
    df.write.mode("append").saveAsTable("zbw_finance_data")

    print("Data successfully loaded into SAP BW table (zbw_finance_data).")

except Exception as e:
    # Error handling similar to ABAP's sy-subrc checks
    print(f"Error while processing file or inserting data: {str(e)}")

# Stop the Spark session
spark.stop()

# API cost for this call: 0.002 USD