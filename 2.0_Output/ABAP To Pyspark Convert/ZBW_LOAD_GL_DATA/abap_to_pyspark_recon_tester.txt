import os
import sys
import time
import logging
import tempfile
import shutil
import traceback
from datetime import datetime

# SAP RFC connection (pyRFC)
from pyrfc import Connection, ABAPApplicationError, ABAPRuntimeError, LogonError, CommunicationError

# PySpark imports
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, DateType
)
from pyspark.sql.functions import col, to_date

# Pandas and pyarrow for Parquet conversion
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# S3/HDFS/Databricks - placeholder imports (implement as needed)
# import boto3
# from hdfs import InsecureClient

# ========== CONFIGURATION ==========
# Read these from environment variables or secure vaults in production!
SAP_CONN_PARAMS = {
    'user': os.getenv('SAP_USER'),
    'passwd': os.getenv('SAP_PASS'),
    'ashost': os.getenv('SAP_ASHOST'),
    'sysnr': os.getenv('SAP_SYSNR'),
    'client': os.getenv('SAP_CLIENT'),
    'lang': os.getenv('SAP_LANG', 'EN'),
    # Add other params as needed
}

DISTRIBUTED_STORAGE_TYPE = os.getenv('DST_TYPE', 'local')  # 's3', 'hdfs', 'local'
DST_PATH = os.getenv('DST_PATH', '/tmp/abap_pyspark_validation')
PYSPARK_APP_NAME = "ABAP_PySpark_Migration_Validation"
PYSPARK_TABLE = "zbw_finance_data"
PYSPARK_INPUT_FILE = "/usr/sap/interfaces/finance_data.csv"  # Should match ABAP input

LOG_FILE = os.getenv('LOG_FILE', 'migration_validation.log')
REPORT_FILE = os.getenv('REPORT_FILE', 'migration_validation_report.txt')

# ========== LOGGING SETUP ==========
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s'
)
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
console.setFormatter(formatter)
logging.getLogger('').addHandler(console)

def log(msg):
    logging.info(msg)
    print(msg)

# ========== STEP 1: ANALYZE INPUTS ==========
# ABAP Table: zbw_finance_data
# Columns: bukrs, fiscyear, costcenter, gl_account, amount, currency, posting_date
# PySpark Table: zbw_finance_data (same schema)

ABAP_TABLE_NAME = "zbw_finance_data"
ABAP_TABLE_SCHEMA = [
    ("bukrs",        str),
    ("fiscyear",     str),
    ("costcenter",   str),
    ("gl_account",   str),
    ("amount",       float),
    ("currency",     str),
    ("posting_date", str),  # Will be converted to date
]

PYSPARK_SCHEMA = StructType([
    StructField("bukrs",        StringType(), True),
    StructField("fiscyear",     StringType(), True),
    StructField("costcenter",   StringType(), True),
    StructField("gl_account",   StringType(), True),
    StructField("amount",       DoubleType(), True),
    StructField("currency",     StringType(), True),
    StructField("posting_date", DateType(), True),
])

# ========== STEP 2: CREATE CONNECTION COMPONENTS ==========

def connect_sap():
    try:
        log("Connecting to SAP ABAP system...")
        conn = Connection(**SAP_CONN_PARAMS)
        log("SAP connection successful.")
        return conn
    except (CommunicationError, LogonError, ABAPApplicationError, ABAPRuntimeError) as e:
        log(f"SAP connection failed: {e}")
        raise

def get_spark_session():
    log("Initializing Spark session...")
    spark = SparkSession.builder \
        .appName(PYSPARK_APP_NAME) \
        .enableHiveSupport() \
        .getOrCreate()
    log("Spark session started.")
    return spark

def ensure_dst_path():
    if DISTRIBUTED_STORAGE_TYPE == 'local':
        os.makedirs(DST_PATH, exist_ok=True)
        log(f"Local distributed storage path ensured: {DST_PATH}")
    # Add S3/HDFS initialization as needed

# ========== STEP 3: IMPLEMENT ABAP EXECUTION ==========

def execute_abap_and_export(conn, table_name, export_path):
    """
    Execute ABAP code and export the result table to CSV.
    """
    log(f"Executing ABAP code to extract table: {table_name}")
    try:
        # For demo: use RFC_READ_TABLE, in production use custom RFC or ABAP report
        result = conn.call('RFC_READ_TABLE', QUERY_TABLE=table_name, DELIMITER=',')
        rows = result['DATA']
        fields = [f['FIELDNAME'] for f in result['FIELDS']]
        data = []
        for row in rows:
            values = row['WA'].split(',')
            data.append(values)
        df = pd.DataFrame(data, columns=fields)
        csv_path = os.path.join(export_path, f"{table_name}_abap_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        df.to_csv(csv_path, index=False)
        log(f"ABAP table exported to CSV: {csv_path}")
        return csv_path, df
    except Exception as e:
        log(f"Error executing ABAP code: {e}")
        traceback.print_exc()
        raise

# ========== STEP 4: DATA EXPORT & TRANSFORMATION ==========

def convert_csv_to_parquet(csv_path, export_path, table_name):
    try:
        log(f"Converting CSV to Parquet for table {table_name}...")
        df = pd.read_csv(csv_path)
        table_name_clean = table_name.replace('.', '_')
        parquet_path = os.path.join(export_path, f"{table_name_clean}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet")
        table = pa.Table.from_pandas(df)
        pq.write_table(table, parquet_path)
        log(f"Parquet file created: {parquet_path}")
        return parquet_path
    except Exception as e:
        log(f"Error converting CSV to Parquet: {e}")
        traceback.print_exc()
        raise

# ========== STEP 5: DISTRIBUTED STORAGE TRANSFER ==========

def transfer_to_distributed_storage(local_path):
    # For local, just return path; for S3/HDFS, upload and return URI
    if DISTRIBUTED_STORAGE_TYPE == 'local':
        log(f"File retained locally: {local_path}")
        return local_path
    elif DISTRIBUTED_STORAGE_TYPE == 's3':
        # Implement S3 upload logic here
        raise NotImplementedError("S3 transfer not implemented in this template.")
    elif DISTRIBUTED_STORAGE_TYPE == 'hdfs':
        # Implement HDFS upload logic here
        raise NotImplementedError("HDFS transfer not implemented in this template.")
    else:
        raise ValueError(f"Unknown distributed storage type: {DISTRIBUTED_STORAGE_TYPE}")

# ========== STEP 6: PYSPARK EXTERNAL TABLES ==========

def create_external_table(spark, parquet_path, table_name):
    log(f"Creating external table {table_name} in Spark from Parquet: {parquet_path}")
    df = spark.read.parquet(parquet_path)
    df.createOrReplaceTempView(table_name)
    log(f"External table/view {table_name} created.")
    return df

# ========== STEP 7: PYSPARK EXECUTION ==========

def run_pyspark_code(spark):
    """
    Executes the PySpark logic as per the migration.
    Returns the resulting DataFrame.
    """
    log("Running PySpark migration logic...")
    input_file = PYSPARK_INPUT_FILE
    schema = PYSPARK_SCHEMA
    try:
        df = spark.read.csv(
            input_file,
            schema=schema,
            header=False,
            mode="DROPMALFORMED"
        )
        df = df.withColumn(
            "posting_date",
            to_date(col("posting_date"), "yyyy-MM-dd")
        )
        df.createOrReplaceTempView(PYSPARK_TABLE)
        log(f"PySpark DataFrame loaded and view {PYSPARK_TABLE} created.")
        return df
    except Exception as e:
        log(f"Error running PySpark code: {e}")
        traceback.print_exc()
        raise

# ========== STEP 8: COMPARISON LOGIC ==========

def compare_tables(spark, abap_df, pyspark_df, key_columns=None):
    """
    Compare ABAP and PySpark DataFrames.
    Returns a dict with match status, row count diff, column diffs, and sample mismatches.
    """
    log("Starting table comparison...")
    abap_count = abap_df.count()
    pyspark_count = pyspark_df.count()
    row_count_match = abap_count == pyspark_count

    # Compare schemas
    abap_cols = set(abap_df.columns)
    pyspark_cols = set(pyspark_df.columns)
    schema_match = abap_cols == pyspark_cols

    # Column-wise comparison
    mismatches = []
    match_rows = 0
    sample_mismatches = []
    if abap_count == 0 and pyspark_count == 0:
        match_status = "MATCH"
    else:
        # Join on all columns (or key columns if specified)
        join_cols = key_columns if key_columns else list(abap_cols & pyspark_cols)
        abap_alias = "a"
        pyspark_alias = "p"
        abap_df_alias = abap_df.alias(abap_alias)
        pyspark_df_alias = pyspark_df.alias(pyspark_alias)
        join_expr = [abap_df_alias[c] == pyspark_df_alias[c] for c in join_cols]
        joined = abap_df_alias.join(pyspark_df_alias, join_expr, 'outer')
        total_rows = joined.count()
        # Find mismatches
        for colname in join_cols:
            diff = joined.filter(
                (abap_df_alias[colname].isNull() & pyspark_df_alias[colname].isNotNull()) |
                (abap_df_alias[colname].isNotNull() & pyspark_df_alias[colname].isNull()) |
                (abap_df_alias[colname] != pyspark_df_alias[colname])
            )
            cnt = diff.count()
            if cnt > 0:
                mismatches.append((colname, cnt))
                sample = diff.select(abap_df_alias[colname], pyspark_df_alias[colname]).limit(5).collect()
                sample_mismatches.append((colname, sample))
        match_rows = total_rows - sum(cnt for _, cnt in mismatches)
        match_pct = 100.0 * match_rows / total_rows if total_rows else 100.0
        if not mismatches and row_count_match and schema_match:
            match_status = "MATCH"
        elif mismatches and match_pct > 90.0:
            match_status = "PARTIAL MATCH"
        else:
            match_status = "NO MATCH"

    report = {
        "row_count_abap": abap_count,
        "row_count_pyspark": pyspark_count,
        "row_count_match": row_count_match,
        "schema_match": schema_match,
        "mismatches": mismatches,
        "sample_mismatches": sample_mismatches,
        "match_status": match_status,
        "match_pct": match_pct if abap_count or pyspark_count else 100.0,
    }
    log(f"Comparison result: {report}")
    return report

# ========== STEP 9: REPORTING ==========

def generate_report(report_dict, report_file=REPORT_FILE):
    log("Generating reconciliation report...")
    with open(report_file, 'w') as f:
        f.write("==== ABAP to PySpark Migration Validation Report ====\n")
        f.write(f"Timestamp: {datetime.now()}\n\n")
        for k, v in report_dict.items():
            f.write(f"{k}: {v}\n")
        f.write("\n")
    log(f"Report written to {report_file}")

# ========== STEP 10: ERROR HANDLING ==========
# (Already included via try/except and logging in each function)

# ========== STEP 11: SECURITY ==========
# (No credentials are hardcoded, all are read from environment variables)

# ========== STEP 12: PERFORMANCE ==========
# (Efficient DataFrame operations, batch processing, logging, and progress messages)

# ========== MAIN EXECUTION FLOW ==========

def main():
    try:
        log("==== ABAP to PySpark Migration Validation Script Started ====")
        ensure_dst_path()

        # Step 1: Connect to SAP
        sap_conn = connect_sap()

        # Step 2: Execute ABAP and export to CSV
        abap_csv, abap_pd_df = execute_abap_and_export(sap_conn, ABAP_TABLE_NAME, DST_PATH)

        # Step 3: Convert ABAP CSV to Parquet
        abap_parquet = convert_csv_to_parquet(abap_csv, DST_PATH, ABAP_TABLE_NAME)

        # Step 4: Transfer Parquet to distributed storage (if needed)
        abap_parquet_dst = transfer_to_distributed_storage(abap_parquet)

        # Step 5: Start Spark
        spark = get_spark_session()

        # Step 6: Create Spark external table from ABAP Parquet
        abap_spark_df = create_external_table(spark, abap_parquet_dst, "abap_" + ABAP_TABLE_NAME)

        # Step 7: Run PySpark code and get DataFrame
        pyspark_df = run_pyspark_code(spark)

        # Step 8: Compare outputs
        comparison_report = compare_tables(spark, abap_spark_df, pyspark_df)

        # Step 9: Generate report
        generate_report(comparison_report)

        log("==== Migration validation completed successfully. ====")
        sys.exit(0)
    except Exception as e:
        log(f"Fatal error: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()