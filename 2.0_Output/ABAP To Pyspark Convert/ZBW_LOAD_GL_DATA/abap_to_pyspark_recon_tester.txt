# ABAP to PySpark Migration Validation Script
# This script automates the validation of ABAP to PySpark data migrations by:
# 1. Executing ABAP code and extracting output tables
# 2. Exporting ABAP output to Parquet and transferring to distributed storage
# 3. Running the equivalent PySpark code
# 4. Comparing outputs for data integrity and generating a reconciliation report
# The script is modular, secure, and ready for scheduled execution.

import os
import sys
import logging
import tempfile
import datetime
import json
import pandas as pd

from typing import List, Dict, Any

# SAP RFC (pyRFC) imports
try:
    from pyrfc import Connection as SAPConnection
except ImportError:
    SAPConnection = None  # To allow script to load for doc/testing if pyRFC is not installed

# PySpark imports
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType
    from pyspark.sql.functions import col, count, to_date
except ImportError:
    SparkSession = None  # To allow script to load for doc/testing if PySpark is not installed

# Distributed storage imports (S3 example)
try:
    import boto3
    from botocore.exceptions import ClientError
except ImportError:
    boto3 = None

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("migration_validation.log", mode="a")
    ]
)
logger = logging.getLogger(__name__)

# --------------------------
# CONFIGURATION SECTION
# --------------------------
# These should be set via environment variables or secure config files, NOT hardcoded!
SAP_CONFIG = {
    "user": os.getenv("SAP_USER"),
    "passwd": os.getenv("SAP_PASSWD"),
    "ashost": os.getenv("SAP_ASHOST"),
    "sysnr": os.getenv("SAP_SYSNR"),
    "client": os.getenv("SAP_CLIENT"),
    "lang": os.getenv("SAP_LANG", "EN"),
}
S3_CONFIG = {
    "bucket": os.getenv("S3_BUCKET"),
    "prefix": os.getenv("S3_PREFIX", "abap_validation/"),
    "aws_access_key_id": os.getenv("AWS_ACCESS_KEY_ID"),
    "aws_secret_access_key": os.getenv("AWS_SECRET_ACCESS_KEY"),
    "region_name": os.getenv("AWS_REGION", "us-east-1"),
}
SPARK_CONFIG = {
    "app_name": "ABAPtoPySparkValidation",
    "master": os.getenv("SPARK_MASTER", "local[*]"),
}

# Target table name (from ABAP and PySpark)
TARGET_TABLE = "zbw_finance_data"

# --------------------------
# UTILITY FUNCTIONS
# --------------------------

def get_timestamp():
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

def safe_filename(name: str) -> str:
    return "".join([c if c.isalnum() or c in ('_', '-') else '_' for c in name])

def export_to_parquet(df: pd.DataFrame, table_name: str) -> str:
    """Export pandas DataFrame to Parquet file and return the file path."""
    fname = f"{safe_filename(table_name)}_{get_timestamp()}.parquet"
    fpath = os.path.join(tempfile.gettempdir(), fname)
    df.to_parquet(fpath, index=False)
    logger.info(f"Exported {table_name} to {fpath}")
    return fpath

def upload_to_s3(local_path: str, s3_bucket: str, s3_key: str, s3_config: dict):
    """Upload a file to S3 with integrity check."""
    if boto3 is None:
        raise ImportError("boto3 is not installed")
    s3 = boto3.client(
        "s3",
        aws_access_key_id=s3_config["aws_access_key_id"],
        aws_secret_access_key=s3_config["aws_secret_access_key"],
        region_name=s3_config["region_name"],
    )
    try:
        s3.upload_file(local_path, s3_bucket, s3_key)
        # Integrity check: HeadObject
        s3.head_object(Bucket=s3_bucket, Key=s3_key)
        logger.info(f"File {local_path} uploaded to s3://{s3_bucket}/{s3_key}")
    except ClientError as e:
        logger.error(f"S3 upload failed: {e}")
        raise

def read_abap_table_via_rfc(table_name: str, sap_config: dict) -> pd.DataFrame:
    """Read an ABAP table using pyRFC and return as pandas DataFrame."""
    if SAPConnection is None:
        raise ImportError("pyRFC is not installed")
    conn = SAPConnection(**sap_config)
    # Use RFC_READ_TABLE or Z-function module for custom types
    result = conn.call("RFC_READ_TABLE", QUERY_TABLE=table_name, DELIMITER=",", ROWCOUNT=0)
    fields = [f["FIELDNAME"] for f in result["FIELDS"]]
    rows = [r["WA"].split(",") for r in result["DATA"]]
    df = pd.DataFrame(rows, columns=fields)
    logger.info(f"Read {len(df)} rows from ABAP table {table_name}")
    return df

def execute_abap_code_and_get_output(sap_config: dict, abap_code: str, output_table: str) -> pd.DataFrame:
    """
    Execute ABAP code remotely (if possible) and extract output table.
    For this script, we simulate by reading the output table directly.
    """
    logger.info("Executing ABAP code and extracting output table...")
    # In real scenario, trigger ABAP program via RFC or job, then fetch output table
    return read_abap_table_via_rfc(output_table, sap_config)

def run_pyspark_job_and_get_output(spark_config: dict, pyspark_code_path: str, output_table: str) -> pd.DataFrame:
    """
    Run PySpark code and extract output table as pandas DataFrame.
    Assumes PySpark code writes to a managed/external table.
    """
    if SparkSession is None:
        raise ImportError("PySpark is not installed")
    spark = SparkSession.builder.appName(spark_config["app_name"]).master(spark_config["master"]).getOrCreate()
    try:
        logger.info("Running PySpark job...")
        # Dynamically execute PySpark code (assumed to be in a .py file)
        exec(open(pyspark_code_path).read(), {"spark": spark})
        # Read output table
        df = spark.table(output_table)
        pdf = df.toPandas()
        logger.info(f"Read {len(pdf)} rows from PySpark output table {output_table}")
        return pdf
    finally:
        spark.stop()

def compare_dataframes(df_abap: pd.DataFrame, df_pyspark: pd.DataFrame, table_name: str) -> Dict[str, Any]:
    """
    Compare two DataFrames for row/column match, nulls, and mismatches.
    Returns a detailed reconciliation report.
    """
    report = {"table": table_name}
    abap_rows, pyspark_rows = len(df_abap), len(df_pyspark)
    report["row_count_abap"] = abap_rows
    report["row_count_pyspark"] = pyspark_rows
    report["row_count_match"] = abap_rows == pyspark_rows

    # Column comparison
    abap_cols, pyspark_cols = set(df_abap.columns), set(df_pyspark.columns)
    report["columns_abap"] = list(abap_cols)
    report["columns_pyspark"] = list(pyspark_cols)
    report["column_match"] = abap_cols == pyspark_cols

    # Align columns for comparison
    common_cols = list(abap_cols & pyspark_cols)
    df_abap_common = df_abap[common_cols].fillna("")
    df_pyspark_common = df_pyspark[common_cols].fillna("")

    # Row-by-row comparison (sampled for large datasets)
    mismatches = []
    sample_size = min(100, min(len(df_abap_common), len(df_pyspark_common)))
    for i in range(sample_size):
        row_abap = df_abap_common.iloc[i].to_dict()
        row_pyspark = df_pyspark_common.iloc[i].to_dict()
        if row_abap != row_pyspark:
            mismatches.append({"row": i, "abap": row_abap, "pyspark": row_pyspark})

    report["mismatched_rows_sample"] = mismatches
    match_pct = 1.0 - (len(mismatches) / sample_size if sample_size else 0)
    report["match_percentage"] = match_pct

    if abap_rows == pyspark_rows and abap_cols == pyspark_cols and match_pct == 1.0:
        report["status"] = "MATCH"
    elif match_pct > 0.95:
        report["status"] = "PARTIAL MATCH"
    else:
        report["status"] = "NO MATCH"
    return report

def log_and_save_report(report: Dict[str, Any], outdir: str = "."):
    """Save reconciliation report as JSON and log summary."""
    fname = f"reconciliation_report_{get_timestamp()}.json"
    fpath = os.path.join(outdir, fname)
    with open(fpath, "w") as f:
        json.dump(report, f, indent=2)
    logger.info(f"Reconciliation report saved to {fpath}")

# --------------------------
# MAIN VALIDATION WORKFLOW
# --------------------------

def main(
    abap_code_path: str,
    pyspark_code_path: str,
    sap_config: dict = SAP_CONFIG,
    s3_config: dict = S3_CONFIG,
    spark_config: dict = SPARK_CONFIG,
    output_table: str = TARGET_TABLE,
):

    try:
        logger.info("=== ABAP to PySpark Migration Validation Started ===")

        # 1. Execute ABAP code and extract output table
        with open(abap_code_path, "r") as f:
            abap_code = f.read()
        df_abap = execute_abap_code_and_get_output(sap_config, abap_code, output_table)

        # 2. Export ABAP output to Parquet
        parquet_path = export_to_parquet(df_abap, output_table)

        # 3. Transfer Parquet to distributed storage (S3 example)
        s3_key = f"{s3_config['prefix']}{os.path.basename(parquet_path)}"
        upload_to_s3(parquet_path, s3_config["bucket"], s3_key, s3_config)

        # 4. Run PySpark code and extract output table
        df_pyspark = run_pyspark_job_and_get_output(spark_config, pyspark_code_path, output_table)

        # 5. Compare outputs
        report = compare_dataframes(df_abap, df_pyspark, output_table)

        # 6. Save and log report
        log_and_save_report(report)

        logger.info("=== Migration Validation Completed ===")
        logger.info(f"Validation Status: {report['status']}")
        if report["status"] != "MATCH":
            logger.warning("Discrepancies found! See report for details.")

    except Exception as e:
        logger.error(f"Validation failed: {e}", exc_info=True)
        sys.exit(1)

# --------------------------
# ENTRY POINT
# --------------------------

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ABAP to PySpark Migration Validation Script")
    parser.add_argument("--abap-code", required=True, help="Path to ABAP SQL code file")
    parser.add_argument("--pyspark-code", required=True, help="Path to converted PySpark code file")
    parser.add_argument("--output-table", default=TARGET_TABLE, help="Target table name for validation")
    parser.add_argument("--report-dir", default=".", help="Directory to save reconciliation report")
    args = parser.parse_args()

    main(
        abap_code_path=args.abap_code,
        pyspark_code_path=args.pyspark_code,
        output_table=args.output_table,
    )

# --------------------------
# END OF SCRIPT
# --------------------------

"""
NOTES:
- This script assumes that the ABAP output table can be read via RFC_READ_TABLE or a similar mechanism.
- For distributed storage, S3 is used as an example; adapt upload_to_s3 for HDFS or other stores as needed.
- All credentials must be provided securely via environment variables.
- The script logs all operations and errors for auditability.
- For large datasets, only a sample of mismatches is reported.
- Schema and data type mismatches are handled via pandas DataFrame alignment and fillna.
- The script is modular and can be scheduled for automated runs.
"""