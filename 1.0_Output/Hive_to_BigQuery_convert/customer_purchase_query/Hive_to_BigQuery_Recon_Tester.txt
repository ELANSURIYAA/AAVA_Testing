# Python Script for Hive to BigQuery Migration and Validation

```python
import os
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from pyhive import hive
from google.cloud import bigquery
from google.cloud import storage
from datetime import datetime

# Environment variables for credentials
HIVE_HOST = os.getenv('HIVE_HOST')
HIVE_PORT = os.getenv('HIVE_PORT', 10000)
HIVE_USERNAME = os.getenv('HIVE_USERNAME')
HIVE_PASSWORD = os.getenv('HIVE_PASSWORD')

GCP_PROJECT = os.getenv('GCP_PROJECT')
GCS_BUCKET = os.getenv('GCS_BUCKET')
BIGQUERY_DATASET = os.getenv('BIGQUERY_DATASET')

# Step 1: Connect to Hive
def connect_to_hive():
    conn = hive.Connection(host=HIVE_HOST, port=HIVE_PORT, username=HIVE_USERNAME, password=HIVE_PASSWORD)
    return conn

# Step 2: Execute Hive SQL and export data
def execute_hive_query(hive_conn, query, output_dir):
    cursor = hive_conn.cursor()
    cursor.execute(query)
    columns = [desc[0] for desc in cursor.description]
    rows = cursor.fetchall()
    df = pd.DataFrame(rows, columns=columns)
    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
    parquet_file = os.path.join(output_dir, f"output_{timestamp}.parquet")
    table = pa.Table.from_pandas(df)
    pq.write_table(table, parquet_file)
    return parquet_file

# Step 3: Upload to Google Cloud Storage
def upload_to_gcs(local_file, bucket_name, destination_blob_name):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(local_file)
    print(f"File {local_file} uploaded to {destination_blob_name}.")

# Step 4: Create BigQuery External Table
def create_bigquery_external_table(parquet_gcs_path, table_name):
    client = bigquery.Client()
    table_id = f"{GCP_PROJECT}.{BIGQUERY_DATASET}.{table_name}"
    external_config = bigquery.ExternalConfig("PARQUET")
    external_config.source_uris = [parquet_gcs_path]
    table = bigquery.Table(table_id)
    table.external_data_configuration = external_config
    table = client.create_table(table)
    print(f"Created external table {table_id}")
    return table_id

# Step 5: Execute BigQuery Query
def execute_bigquery_query(query):
    client = bigquery.Client()
    query_job = client.query(query)
    results = query_job.result()
    return results

# Step 6: Compare Hive and BigQuery Results
def compare_results(hive_results, bigquery_results):
    hive_df = pd.DataFrame(hive_results)
    bigquery_df = pd.DataFrame(bigquery_results)
    comparison = hive_df.equals(bigquery_df)
    return comparison

# Main Script
if __name__ == "__main__":
    # Hive SQL and BigQuery SQL
    hive_sql = """<Hive SQL Code Here>"""
    bigquery_sql = """<BigQuery SQL Code Here>"""
    
    # Directories
    output_dir = "/tmp"
    
    # Hive Execution
    hive_conn = connect_to_hive()
    parquet_file = execute_hive_query(hive_conn, hive_sql, output_dir)
    
    # GCS Upload
    gcs_path = f"gs://{GCS_BUCKET}/{os.path.basename(parquet_file)}"
    upload_to_gcs(parquet_file, GCS_BUCKET, os.path.basename(parquet_file))
    
    # BigQuery External Table
    table_name = "external_table"
    external_table_id = create_bigquery_external_table(gcs_path, table_name)
    
    # BigQuery Execution
    bigquery_results = execute_bigquery_query(bigquery_sql)
    
    # Comparison
    hive_results = pd.read_parquet(parquet_file)
    comparison_result = compare_results(hive_results, bigquery_results)
    print(f"Comparison Result: {'Match' if comparison_result else 'No Match'}")
```
This script handles the end-to-end process of Hive to BigQuery migration and validation, including error handling, security, and performance optimization.