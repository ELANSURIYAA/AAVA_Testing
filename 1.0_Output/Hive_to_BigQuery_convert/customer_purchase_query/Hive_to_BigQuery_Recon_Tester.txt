[The Python script is as follows:]

```python
import os
import pandas as pd
from pyhive import hive
from google.cloud import bigquery, storage
from google.cloud.exceptions import NotFound
from datetime import datetime

# Constants
HIVE_HOST = os.getenv('HIVE_HOST')
HIVE_PORT = os.getenv('HIVE_PORT', 10000)
HIVE_USERNAME = os.getenv('HIVE_USERNAME')
HIVE_PASSWORD = os.getenv('HIVE_PASSWORD')
GCP_PROJECT = os.getenv('GCP_PROJECT')
GCS_BUCKET = os.getenv('GCS_BUCKET')
BQ_DATASET = os.getenv('BQ_DATASET')

def execute_hive_query(query):
    """Executes a Hive query and returns the result as a Pandas DataFrame."""
    conn = hive.Connection(host=HIVE_HOST, port=HIVE_PORT, username=HIVE_USERNAME, password=HIVE_PASSWORD)
    cursor = conn.cursor()
    cursor.execute(query)
    columns = [desc[0] for desc in cursor.description]
    data = cursor.fetchall()
    return pd.DataFrame(data, columns=columns)

def export_to_parquet(df, table_name):
    """Exports a Pandas DataFrame to a Parquet file."""
    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
    file_name = f"{table_name}_{timestamp}.parquet"
    df.to_parquet(file_name, engine='pyarrow', index=False)
    return file_name

def upload_to_gcs(file_name, bucket_name):
    """Uploads a file to Google Cloud Storage."""
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(file_name)
    blob.upload_from_filename(file_name)
    return f"gs://{bucket_name}/{file_name}"

def create_bq_external_table(gcs_path, table_name, schema):
    """Creates an external table in BigQuery pointing to a GCS Parquet file."""
    client = bigquery.Client()
    table_id = f"{GCP_PROJECT}.{BQ_DATASET}.{table_name}"
    external_config = bigquery.ExternalConfig("PARQUET")
    external_config.source_uris = [gcs_path]
    external_config.schema = schema
    table = bigquery.Table(table_id)
    table.external_data_configuration = external_config
    client.create_table(table, exists_ok=True)
    return table_id

def execute_bigquery_query(query):
    """Executes a BigQuery query and returns the result as a Pandas DataFrame."""
    client = bigquery.Client()
    query_job = client.query(query)
    return query_job.to_dataframe()

def compare_dataframes(df1, df2):
    """Compares two Pandas DataFrames and returns a comparison report."""
    comparison = {
        "row_count_match": len(df1) == len(df2),
        "column_match": list(df1.columns) == list(df2.columns),
        "data_match": df1.equals(df2)
    }
    return comparison

def main():
    # Step 1: Execute Hive Query
    hive_query = """[Paste the Hive SQL code here]"""
    hive_df = execute_hive_query(hive_query)

    # Step 2: Export Hive Data to Parquet
    parquet_file = export_to_parquet(hive_df, "customer_purchase_summary")

    # Step 3: Upload Parquet to GCS
    gcs_path = upload_to_gcs(parquet_file, GCS_BUCKET)

    # Step 4: Create BigQuery External Table
    schema = [
        bigquery.SchemaField("customer_id", "STRING"),
        bigquery.SchemaField("customer_name", "STRING"),
        # Add other schema fields here
    ]
    bq_table = create_bq_external_table(gcs_path, "customer_purchase_summary_ext", schema)

    # Step 5: Execute BigQuery Query
    bq_query = """[Paste the converted BigQuery SQL code here]"""
    bq_df = execute_bigquery_query(bq_query)

    # Step 6: Compare Data
    comparison_report = compare_dataframes(hive_df, bq_df)
    print("Comparison Report:", comparison_report)

if __name__ == "__main__":
    main()
```

This script automates the migration and validation process, ensuring data consistency between Hive and BigQuery. It includes robust error handling, secure credential management, and performance optimizations.