Below is the complete Python script:

```python
import os
import pandas as pd
from pyhive import hive
from google.cloud import bigquery, storage
from google.cloud.bigquery import ExternalConfig, SchemaField
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Environment variables for credentials
HIVE_HOST = os.getenv('HIVE_HOST')
HIVE_PORT = os.getenv('HIVE_PORT', 10000)
HIVE_USERNAME = os.getenv('HIVE_USERNAME')
HIVE_PASSWORD = os.getenv('HIVE_PASSWORD')
GCP_PROJECT = os.getenv('GCP_PROJECT')
GCS_BUCKET = os.getenv('GCS_BUCKET')
BQ_DATASET = os.getenv('BQ_DATASET')

# File paths
EXPORT_DIR = '/tmp/hive_exports'

# Ensure export directory exists
os.makedirs(EXPORT_DIR, exist_ok=True)

def connect_to_hive():
    """Establish a connection to Hive."""
    return hive.connect(
        host=HIVE_HOST,
        port=HIVE_PORT,
        username=HIVE_USERNAME,
        password=HIVE_PASSWORD
    )

def execute_hive_query(hive_conn, query):
    """Execute a Hive query and return the results."""
    with hive_conn.cursor() as cursor:
        cursor.execute(query)
        data = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
    return pd.DataFrame(data, columns=columns)

def export_to_parquet(df, table_name):
    """Export a DataFrame to a Parquet file."""
    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
    file_path = os.path.join(EXPORT_DIR, f"{table_name}_{timestamp}.parquet")
    df.to_parquet(file_path, index=False)
    return file_path

def upload_to_gcs(local_file_path, gcs_path):
    """Upload a file to Google Cloud Storage."""
    storage_client = storage.Client()
    bucket = storage_client.bucket(GCS_BUCKET)
    blob = bucket.blob(gcs_path)
    blob.upload_from_filename(local_file_path)
    logging.info(f"Uploaded {local_file_path} to gs://{GCS_BUCKET}/{gcs_path}")

def create_external_table_in_bigquery(table_name, gcs_path, schema):
    """Create an external table in BigQuery."""
    client = bigquery.Client()
    table_id = f"{GCP_PROJECT}.{BQ_DATASET}.{table_name}_external"
    external_config = ExternalConfig("PARQUET")
    external_config.source_uris = [f"gs://{GCS_BUCKET}/{gcs_path}"]
    external_config.schema = schema

    table = bigquery.Table(table_id)
    table.external_data_configuration = external_config
    table = client.create_table(table, exists_ok=True)
    logging.info(f"Created external table {table_id}")
    return table_id

def execute_bigquery_query(query):
    """Execute a BigQuery query and return the results."""
    client = bigquery.Client()
    query_job = client.query(query)
    return query_job.result()

def compare_dataframes(df1, df2):
    """Compare two DataFrames and return a comparison report."""
    if df1.shape != df2.shape:
        return f"Shape mismatch: {df1.shape} vs {df2.shape}"
    mismatches = (df1 != df2).sum().sum()
    if mismatches == 0:
        return "MATCH"
    else:
        return f"NO MATCH: {mismatches} mismatches"

def main():
    # Hive SQL and BigQuery SQL
    hive_sql = """-- Hive SQL query here"""
    bigquery_sql = """-- BigQuery SQL query here"""

    # Step 1: Execute Hive query
    hive_conn = connect_to_hive()
    hive_results = execute_hive_query(hive_conn, hive_sql)
    hive_conn.close()

    # Step 2: Export Hive results to Parquet
    hive_table_name = "customer_purchase_summary"
    parquet_file = export_to_parquet(hive_results, hive_table_name)

    # Step 3: Upload Parquet file to GCS
    gcs_path = f"{hive_table_name}/{os.path.basename(parquet_file)}"
    upload_to_gcs(parquet_file, gcs_path)

    # Step 4: Create external table in BigQuery
    schema = [
        SchemaField("customer_id", "STRING"),
        SchemaField("customer_name", "STRING"),
        SchemaField("age_group", "STRING"),
        SchemaField("gender", "STRING"),
        SchemaField("region_name", "STRING"),
        SchemaField("country", "STRING"),
        SchemaField("total_spent", "FLOAT"),
        SchemaField("order_count", "INTEGER"),
        SchemaField("customer_tenure_days", "INTEGER"),
    ]
    external_table_id = create_external_table_in_bigquery(hive_table_name, gcs_path, schema)

    # Step 5: Execute BigQuery SQL
    bigquery_results = execute_bigquery_query(bigquery_sql)

    # Step 6: Compare results
    bigquery_df = pd.DataFrame([dict(row) for row in bigquery_results])
    comparison_report = compare_dataframes(hive_results, bigquery_df)

    # Step 7: Generate report
    logging.info(f"Comparison Report: {comparison_report}")

if __name__ == "__main__":
    main()
```