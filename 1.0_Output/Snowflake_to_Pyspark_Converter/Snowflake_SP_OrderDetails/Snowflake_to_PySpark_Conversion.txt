from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, count, lit
from datetime import datetime, timedelta

# Initialize Spark session
spark = SparkSession.builder.appName("OrderDetailsProcessing").getOrCreate()

# Create TempOrders DataFrame
orders_df = spark.read.format("csv").option("header", "true").load("path_to_orders_csv")  # Replace with actual path
temp_orders_df = orders_df.filter(col("OrderDate") >= datetime.now() - timedelta(days=30))

# Get distinct CustomerIDs
distinct_customers_df = temp_orders_df.select("CustomerID").distinct()

# Process each customer
for row in distinct_customers_df.collect():
    customer_id = row["CustomerID"]
    
    # Calculate total amount and order count for the current customer
    customer_summary_df = temp_orders_df.filter(col("CustomerID") == customer_id).agg(
        sum(col("Amount")).alias("TotalAmount"),
        count(lit(1)).alias("OrderCount")
    )
    
    # Print customer summary
    customer_summary = customer_summary_df.collect()[0]
    print(f"CustomerID: {customer_id} | TotalAmount: {customer_summary['TotalAmount']} | OrderCount: {customer_summary['OrderCount']}")

# Clean up
spark.stop()

# API cost consumed for this call: 0.0012 USD