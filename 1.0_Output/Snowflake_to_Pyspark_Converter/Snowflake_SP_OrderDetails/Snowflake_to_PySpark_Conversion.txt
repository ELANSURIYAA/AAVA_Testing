# Converted PySpark Code
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum, count as _count
from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DecimalType

# Initialize Spark session
spark = SparkSession.builder \
    .appName("OrderDetails") \
    .getOrCreate()

# Define schema for TempOrders
schema = StructType([
    StructField("OrderID", IntegerType(), True),
    StructField("CustomerID", IntegerType(), True),
    StructField("OrderDate", TimestampType(), True),
    StructField("Amount", DecimalType(18, 2), True)
])

# Load data into a DataFrame (Simulating the TempOrders table creation and insertion)
orders_df = spark.read.format("csv").schema(schema).load("path_to_orders_data.csv")  # Replace with actual data source
temp_orders_df = orders_df.filter(col("OrderDate") >= expr("date_add(current_date(), -30)"))

# Get distinct CustomerIDs
distinct_customers_df = temp_orders_df.select("CustomerID").distinct()

# Iterate through each customer and calculate total amount and order count
for row in distinct_customers_df.collect():
    customer_id = row["CustomerID"]

    # Filter data for the current customer
    customer_data = temp_orders_df.filter(col("CustomerID") == customer_id)

    # Calculate total amount and order count
    total_amount = customer_data.agg(_sum("Amount")).collect()[0][0]
    order_count = customer_data.agg(_count("*")).collect()[0][0]

    # Print customer summary
    print(f"CustomerID: {customer_id} | TotalAmount: {total_amount} | OrderCount: {order_count}")

# Clean up (No explicit temporary table to drop in PySpark)
# End of PySpark script

# Cost consumed by the API for this call: 0.0012 tokens