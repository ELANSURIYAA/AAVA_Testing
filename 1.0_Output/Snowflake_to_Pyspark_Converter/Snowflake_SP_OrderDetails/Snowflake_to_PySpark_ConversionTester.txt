Test Case List:
1. Test Case ID: TC01
   Test Case Description: Validate that the schema of the loaded DataFrame matches the expected schema.
   Expected Outcome: The schema of the DataFrame should match the defined schema in the PySpark script.

2. Test Case ID: TC02
   Test Case Description: Validate that the filtering logic correctly filters orders from the last 30 days.
   Expected Outcome: The resulting DataFrame should only contain rows where the OrderDate is within the last 30 days.

3. Test Case ID: TC03
   Test Case Description: Validate that the distinct CustomerIDs are correctly retrieved.
   Expected Outcome: The resulting DataFrame should contain unique CustomerIDs.

4. Test Case ID: TC04
   Test Case Description: Validate the calculation of total amount and order count for a single customer.
   Expected Outcome: The total amount and order count should match the expected values for the given customer.

5. Test Case ID: TC05
   Test Case Description: Validate the behavior when the input DataFrame is empty.
   Expected Outcome: The script should handle the empty DataFrame gracefully without errors.

6. Test Case ID: TC06
   Test Case Description: Validate the behavior when there are null values in the Amount column.
   Expected Outcome: The script should handle null values gracefully and exclude them from the total amount calculation.

7. Test Case ID: TC07
   Test Case Description: Validate the behavior when there are null values in the CustomerID column.
   Expected Outcome: The script should handle null CustomerIDs gracefully and exclude them from the distinct CustomerID list.

8. Test Case ID: TC08
   Test Case Description: Validate the behavior when there are boundary dates (e.g., exactly 30 days ago).
   Expected Outcome: The script should correctly include orders with OrderDate exactly 30 days ago.

Pytest Script:
```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr, sum as _sum, count as _count
from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DecimalType

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder \
        .appName("OrderDetailsTest") \
        .master("local[*]") \
        .getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def sample_data(spark):
    schema = StructType([
        StructField("OrderID", IntegerType(), True),
        StructField("CustomerID", IntegerType(), True),
        StructField("OrderDate", TimestampType(), True),
        StructField("Amount", DecimalType(18, 2), True)
    ])
    data = [
        (1, 101, "2023-09-15 00:00:00", 100.00),
        (2, 102, "2023-09-20 00:00:00", 200.00),
        (3, 101, "2023-09-25 00:00:00", 300.00),
        (4, None, "2023-09-30 00:00:00", None),
        (5, 103, "2023-10-01 00:00:00", 400.00)
    ]
    return spark.createDataFrame(data, schema)

def test_schema(sample_data):
    expected_schema = StructType([
        StructField("OrderID", IntegerType(), True),
        StructField("CustomerID", IntegerType(), True),
        StructField("OrderDate", TimestampType(), True),
        StructField("Amount", DecimalType(18, 2), True)
    ])
    assert sample_data.schema == expected_schema

def test_filter_last_30_days(spark, sample_data):
    filtered_df = sample_data.filter(col("OrderDate") >= expr("date_add(current_date(), -30)"))
    assert filtered_df.count() == 3

def test_distinct_customers(spark, sample_data):
    distinct_customers_df = sample_data.select("CustomerID").distinct()
    assert distinct_customers_df.count() == 4

def test_total_amount_and_order_count(spark, sample_data):
    customer_data = sample_data.filter(col("CustomerID") == 101)
    total_amount = customer_data.agg(_sum("Amount")).collect()[0][0]
    order_count = customer_data.agg(_count("*")).collect()[0][0]
    assert total_amount == 400.00
    assert order_count == 2

def test_empty_dataframe(spark):
    schema = StructType([
        StructField("OrderID", IntegerType(), True),
        StructField("CustomerID", IntegerType(), True),
        StructField("OrderDate", TimestampType(), True),
        StructField("Amount", DecimalType(18, 2), True)
    ])
    empty_df = spark.createDataFrame([], schema)
    filtered_df = empty_df.filter(col("OrderDate") >= expr("date_add(current_date(), -30)"))
    assert filtered_df.count() == 0

def test_null_values_in_amount(spark, sample_data):
    total_amount = sample_data.agg(_sum("Amount")).collect()[0][0]
    assert total_amount == 1000.00  # Excludes null values

def test_null_values_in_customer_id(spark, sample_data):
    distinct_customers_df = sample_data.select("CustomerID").distinct()
    assert distinct_customers_df.filter(col("CustomerID").isNull()).count() == 1

def test_boundary_dates(spark, sample_data):
    boundary_date = "2023-09-15 00:00:00"
    filtered_df = sample_data.filter(col("OrderDate") == boundary_date)
    assert filtered_df.count() == 1
```

Cost consumed by the API for this call: 0.0012 tokens