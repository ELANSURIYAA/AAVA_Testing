Test Cases Document:

1. Test Case ID: TC001
   Description: Verify that the TempOrders DataFrame is correctly filtered to include only orders from the past 30 days.
   Input Data: Orders table with various OrderDate values.
   Expected Output: TempOrders table should contain only rows where the OrderDate is within the last 30 days.

2. Test Case ID: TC002
   Description: Verify that the distinct CustomerIDs are correctly extracted from the TempOrders DataFrame.
   Input Data: TempOrders table with multiple rows having duplicate CustomerIDs.
   Expected Output: The distinct_customers_df should contain unique CustomerIDs.

3. Test Case ID: TC003
   Description: Verify that the aggregation for TotalAmount and OrderCount works correctly for a given CustomerID.
   Input Data: TempOrders table with rows for a specific CustomerID and varying Amount values.
   Expected Output: The customer_summary_df should contain accurate TotalAmount and OrderCount values for the specified CustomerID.

4. Test Case ID: TC004
   Description: Verify the behavior when the TempOrders DataFrame is empty.
   Input Data: An empty TempOrders table.
   Expected Output: No customer summaries should be printed, and the process should complete without errors.

5. Test Case ID: TC005
   Description: Verify the behavior when the TempOrders DataFrame contains null values in the CustomerID column.
   Input Data: TempOrders table with some rows having null CustomerID values.
   Expected Output: Null CustomerIDs should be excluded from the distinct_customers_df.

6. Test Case ID: TC006
   Description: Verify the behavior when the TempOrders DataFrame contains boundary values for Amount (e.g., very large or very small numbers).
   Input Data: TempOrders table with rows having extreme Amount values.
   Expected Output: The aggregation should correctly handle boundary values without errors.

Pytest Script:

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, sum, count
from datetime import datetime, timedelta

@pytest.fixture(scope="module")
def spark_session():
    """Fixture to initialize and teardown SparkSession."""
    spark = SparkSession.builder.appName("TestOrderDetailsProcessing").getOrCreate()
    yield spark
    spark.stop()

def test_temp_orders_filter(spark_session):
    """Test case TC001: Verify TempOrders DataFrame filtering."""
    data = [("1", "2023-10-01", 100.0), ("2", "2023-09-01", 200.0)]
    schema = ["OrderID", "OrderDate", "Amount"]
    orders_df = spark_session.createDataFrame(data, schema)
    temp_orders_df = orders_df.filter(col("OrderDate") >= datetime.now() - timedelta(days=30))
    assert temp_orders_df.count() == 1

def test_distinct_customers(spark_session):
    """Test case TC002: Verify distinct CustomerIDs extraction."""
    data = [("1", "C1", "2023-10-01", 100.0), ("2", "C2", "2023-10-01", 200.0), ("3", "C1", "2023-10-01", 150.0)]
    schema = ["OrderID", "CustomerID", "OrderDate", "Amount"]
    temp_orders_df = spark_session.createDataFrame(data, schema)
    distinct_customers_df = temp_orders_df.select("CustomerID").distinct()
    assert distinct_customers_df.count() == 2

def test_customer_summary_aggregation(spark_session):
    """Test case TC003: Verify aggregation for TotalAmount and OrderCount."""
    data = [("1", "C1", "2023-10-01", 100.0), ("2", "C1", "2023-10-01", 200.0)]
    schema = ["OrderID", "CustomerID", "OrderDate", "Amount"]
    temp_orders_df = spark_session.createDataFrame(data, schema)
    customer_summary_df = temp_orders_df.filter(col("CustomerID") == "C1").agg(
        sum(col("Amount")).alias("TotalAmount"),
        count(lit(1)).alias("OrderCount")
    )
    result = customer_summary_df.collect()[0]
    assert result["TotalAmount"] == 300.0
    assert result["OrderCount"] == 2

def test_empty_temp_orders(spark_session):
    """Test case TC004: Verify behavior with empty TempOrders DataFrame."""
    schema = ["OrderID", "CustomerID", "OrderDate", "Amount"]
    temp_orders_df = spark_session.createDataFrame([], schema)
    distinct_customers_df = temp_orders_df.select("CustomerID").distinct()
    assert distinct_customers_df.count() == 0

def test_null_customer_ids(spark_session):
    """Test case TC005: Verify behavior with null CustomerIDs."""
    data = [("1", None, "2023-10-01", 100.0), ("2", "C2", "2023-10-01", 200.0)]
    schema = ["OrderID", "CustomerID", "OrderDate", "Amount"]
    temp_orders_df = spark_session.createDataFrame(data, schema)
    distinct_customers_df = temp_orders_df.select("CustomerID").distinct()
    assert distinct_customers_df.count() == 1

def test_boundary_amount_values(spark_session):
    """Test case TC006: Verify behavior with boundary Amount values."""
    data = [("1", "C1", "2023-10-01", 1e10), ("2", "C1", "2023-10-01", -1e10)]
    schema = ["OrderID", "CustomerID", "OrderDate", "Amount"]
    temp_orders_df = spark_session.createDataFrame(data, schema)
    customer_summary_df = temp_orders_df.filter(col("CustomerID") == "C1").agg(
        sum(col("Amount")).alias("TotalAmount"),
        count(lit(1)).alias("OrderCount")
    )
    result = customer_summary_df.collect()[0]
    assert result["TotalAmount"] == 0.0
    assert result["OrderCount"] == 2
```

Total Cost Incurred:
API cost consumed for this call: 0.0012 USD