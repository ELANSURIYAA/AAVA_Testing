=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Informatica workflow and mapping analysis for loading vendor data from flat file to Oracle, supporting PySpark migration.
=============================================

# 1. Workflow Overview

This Informatica workflow (`wkf_aufi016d_PS_Vendor`) and mapping (`m_aufi016d_PS_Vendor`) automate the ingestion and transformation of vendor data from a flat file source (`PS_VENDOR_S4`) into an Oracle target table (`PS_VENDOR`). The primary business objective is to synchronize vendor master data from PeopleSoft extracts into the enterprise data warehouse for downstream analytics and operational reporting.

- **Number of Mappings per Workflow/Session:** 1 mapping (`m_aufi016d_PS_Vendor`) in 1 session (`s_m_aufi016d_PS_Vendor`).

---

# 2. Complexity Metrics

| Metric                       | Value / Type                                                                 |
|------------------------------|------------------------------------------------------------------------------|
| Number of Source Qualifiers  | 1 (`SQ_PS_VENDOR_S4`)                                                        |
| Number of Transformations    | 2 (Source Qualifier, Expression)                                             |
| Lookup Usage                 | 0                                                                            |
| Expression Logic             | 1 (Simple assignment, no nested logic)                                       |
| Join Conditions              | 0                                                                            |
| Conditional Logic            | 0                                                                            |
| Reusable Components          | 0                                                                            |
| Data Sources                 | 1 (Flat File: `PS_VENDOR_S4`, delimited by `||`)                             |
| Data Targets                 | 1 (Oracle Table: `PS_VENDOR`)                                                |
| Pre/Post SQL Logic           | 0                                                                            |
| Session/Workflow Controls    | 1 Start task, 1 session, basic workflow variables                            |
| DML Logic                    | Insert only (target load type: Normal, Truncate target table: YES)           |
| Complexity Score (0â€“100)     | 10                                                                           |

**Data Source/Target Types:**  
- Source: Flat File (Delimited, US-ASCII)  
- Target: Oracle (Relational Table)

**High-Complexity Areas:**  
- None. No lookups, joins, or branching logic. All mappings are 1:1 or simple assignments.

---

# 3. Syntax Differences

- **Informatica Functions Used:**  
  - Mapping variable: `$$Audit_Year` (assigned to `AUD_YR_NBR`)
  - Session variable: `SESSSTARTTIME` (assigned to `LOAD_DT`)
- **PySpark Equivalents:**  
  - Mapping/session variables must be handled as parameters or environment variables in PySpark.
  - Date/time assignment (`SESSSTARTTIME`) should be replaced with Spark's `current_timestamp()` or equivalent.
- **Data Type Conversions:**  
  - Flat file string/numeric types must be explicitly cast to match Oracle target schema.
  - Informatica's `number`/`decimal` maps to PySpark `IntegerType` or `DecimalType`.
  - Oracle `date` maps to PySpark `TimestampType`.
- **Workflow/Control Logic:**  
  - No Router, Filter, or Transaction Control transformations to restructure.

---

# 4. Manual Adjustments

- **Components Requiring Manual Implementation:**  
  - Mapping/session variables (`$$Audit_Year`, `SESSSTARTTIME`) must be passed as parameters or set in PySpark.
  - Flat file parsing: Delimiter `||` and null character `*` handling must be explicitly coded in PySpark.
  - Error/reject handling: Informatica writes rejects to `.bad` files; in PySpark, explicit error handling and logging must be implemented.
- **External Dependencies:**  
  - Oracle connection (`BAT_AUFI_RWH`) must be replaced with a Spark JDBC connection.
  - File paths (e.g., `$PMSourceFileDir/corp/SrcFiles/profrec_vendor.txt`) must be parameterized or configured in PySpark.
- **Business Logic Review:**  
  - Ensure audit year and load date logic are correctly implemented.
  - Validate that all source-to-target mappings are 1:1 and data types are compatible.

---

# 5. Optimization Techniques

- **Spark Best Practices:**  
  - Use schema inference or explicit schema definition when reading the flat file.
  - Partition data if processing large files for parallelism.
  - Use caching only if the data is reused in multiple operations (not needed here).
- **Pipeline Optimization:**  
  - Chain file read, transformation (audit year, load date), and write to Oracle in a single pipeline.
- **Window Functions:**  
  - Not applicable (no aggregations or window logic present).
- **Refactor vs. Rebuild:**  
  - **Refactor** is recommended: The logic is simple and can be directly mapped to PySpark with minimal changes.

---

**Summary:**  
This Informatica workflow is a straightforward ETL process with no complex transformations, joins, or lookups. The migration to PySpark is direct, with the main considerations being parameter handling, flat file parsing, and error management. No major manual intervention is required beyond adapting Informatica-specific variables and file handling to PySpark paradigms.