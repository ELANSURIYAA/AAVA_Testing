=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Informatica workflow and mapping analysis for PS_VENDOR ingestion to support PySpark migration.
=============================================

---

# 1. Workflow Overview

This Informatica workflow (`wkf_aufi016d_PS_Vendor`) orchestrates the ingestion of vendor master data from a flat file (`PS_VENDOR_S4`) and loads it into an Oracle target table (`PS_VENDOR`). The mapping (`m_aufi016d_PS_Vendor`) applies audit year and load timestamp enrichment, supporting regulatory and operational reporting. The workflow consists of a single mapping and session, with error handling and logging configured for traceability.

- **Number of Mappings per Workflow/Session:** 1 mapping (`m_aufi016d_PS_Vendor`) in 1 session (`s_m_aufi016d_PS_Vendor`).

**Business Objective:**  
Data integration and enrichment for vendor master data, ensuring auditability and compliance for downstream analytics and reporting.

---

# 2. Complexity Metrics

| Metric                       | Value/Type                                                                 |
|------------------------------|---------------------------------------------------------------------------|
| Number of Source Qualifiers  | 1 (`SQ_PS_VENDOR_S4`)                                                     |
| Number of Transformations    | 2 (Source Qualifier, Expression)                                          |
| Lookup Usage                 | 0                                                                         |
| Expression Logic             | 2 (Simple: parameter assignment, session variable usage)                  |
| Join Conditions              | 0                                                                         |
| Conditional Logic            | 0                                                                         |
| Reusable Components          | 0                                                                         |
| Data Sources                 | 1 (Flat File: US-ASCII, delimited by `||`)                               |
| Data Targets                 | 1 (Oracle table: `PS_VENDOR`)                                             |
| Pre/Post SQL Logic           | 0 (No pre/post SQL in mapping/session)                                    |
| Session/Workflow Controls    | 1 Start task, session variables, workflow variables                       |
| DML Logic                    | Insert (YES), Delete (YES - truncate), Update (NO), Merge (NO)            |
| Complexity Score (0â€“100)     | 15                                                                        |

**Data Source/Target Types:**  
- **Source:** Flat File (US-ASCII, `||` delimited)  
- **Target:** Oracle (Relational Table)

**High-Complexity Areas:**  
- None. No nested expressions, no lookups, no branching logic, no unstructured sources, no external scripts.

---

# 3. Syntax Differences

- **Parameter Usage:** Informatica mapping variables (e.g., `$$Audit_Year`) must be replaced with PySpark/Pandas parameter passing (e.g., config files, environment variables).
- **Session Variables:** Informatica's `SESSSTARTTIME` (session start time) must be replaced with a PySpark equivalent (e.g., `current_timestamp()` at job start).
- **Data Types:**  
  - Informatica `number`, `string`, `date/time` types must be mapped to Spark SQL types (`IntegerType`, `StringType`, `TimestampType`).
  - Flat file reading must handle delimiters (`||`) and US-ASCII encoding.
- **No Informatica-specific functions** (e.g., `IIF`, `DECODE`, `TO_DATE`) are used in this mapping.
- **Control Logic:** No Routers, Filters, or Transaction Control transformations to restructure.

---

# 4. Manual Adjustments

- **Parameterization:**  
  - Replace `$$Audit_Year` with a PySpark parameter (e.g., passed via config or notebook parameter).
  - Assign session start time (`SESSSTARTTIME`) using PySpark's `current_timestamp()` or job start variable.
- **Error Handling:**  
  - Implement reject/bad file logic in PySpark (capture and log failed records).
  - Replicate session/workflow logging (write logs to designated directories).
- **External Dependencies:**  
  - Flat file location (`$PMSourceFileDir/corp/SrcFiles/profrec_vendor.txt`) and bad file output (`$PMBadFileDir/corp/BadFiles/ps_vendor1.bad`) must be parameterized in PySpark.
  - Oracle connection (`BAT_AUFI_RWH`) must be set up using Spark JDBC.
- **DML Logic:**  
  - Truncate target table before load (if required) using a pre-load SQL in PySpark.
- **Business Logic Validation:**  
  - Validate that audit year and load timestamp are correctly assigned post-migration.

---

# 5. Optimization Techniques

- **Partitioning:**  
  - Partition input data based on a logical key (e.g., `SET_ID` or `VENDOR_NBR_ID`) for parallelism.
- **Caching:**  
  - Cache intermediate DataFrames if reused in multiple steps (not needed here due to linear flow).
- **Pipeline Processing:**  
  - Chain flat file read, transformation, and write operations in a single pipeline for efficiency.
- **Bulk Load:**  
  - Use Spark's bulk JDBC write for efficient Oracle loading.
- **Error Handling:**  
  - Use DataFrame filters and exception handling to capture and log rejects.
- **Refactor vs. Rebuild:**  
  - **Refactor**: Retain original logic, as the mapping is simple and direct. No need for a full rebuild.

---

**Summary:**  
This Informatica workflow is a straightforward ETL pipeline with a single mapping, no complex transformations, and no branching logic. Migration to PySpark is direct, requiring only parameterization and session variable handling adjustments, along with standard Spark best practices for data ingestion and loading.