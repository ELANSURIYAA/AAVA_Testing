=============================================
Author:        Ascendion AVA+
Created on:   
Description:   Informatica workflow and mapping analysis for PS_VENDOR data ingestion and transformation, supporting PySpark migration.
=============================================

---

# 1. Workflow Overview

**Workflow Name:** `wkf_aufi016d_PS_Vendor`  
**Number of Mappings per Workflow/Session:** 1 (Mapping: `m_aufi016d_PS_Vendor`)

This Informatica workflow ingests vendor data from a flat file (`PS_VENDOR_S4`), applies audit and transformation logic, and loads the processed data into the Oracle target table `PS_VENDOR`. The workflow supports audit year parameterization and session-based load timestamping, ensuring traceability and data quality for downstream financial and reporting applications.

**Business Objective:**  
Data integration and audit enrichment for vendor master data, supporting financial reporting and reconciliation.

---

# 2. Complexity Metrics

| Metric                       | Value / Type                          |
|------------------------------|---------------------------------------|
| Number of Source Qualifiers  | 1 (`SQ_PS_VENDOR_S4`)                 |
| Number of Transformations    | 2 (Source Qualifier, Expression)      |
| Lookup Usage                 | 0                                     |
| Expression Logic             | 2 (Simple assignment, no nesting)     |
| Join Conditions              | 0                                     |
| Conditional Logic            | 0                                     |
| Reusable Components          | 0                                     |
| Data Sources                 | 1 (Flat File)                         |
| Data Targets                 | 1 (Oracle Table)                      |
| Pre/Post SQL Logic           | 0                                     |
| Session/Workflow Controls    | Basic Start/Session only              |
| DML Logic                    | Insert only (with truncate option)    |
| Complexity Score (0–100)     | 15                                    |

**Source Type:** Flat File (Delimited: `||`, US-ASCII)  
**Target Type:** Oracle Table (`PS_VENDOR`)

**High-Complexity Areas:**  
- None. The mapping is straightforward with no nested logic, lookups, or branching.

---

# 3. Syntax Differences

- **Informatica Functions Used:**  
  - Direct field mapping in Expression transformation.
  - Assignment of parameter (`$$Audit_Year`) and session variable (`SESSSTARTTIME`).

- **PySpark Equivalents:**  
  - Parameter assignment: Use a variable or config parameter in PySpark.
  - Session timestamp: Use `current_timestamp()` or equivalent.
  - Field mapping: Use DataFrame column assignments.

- **Data Type Conversions:**  
  - `number` (Informatica) → `IntegerType` or `LongType` (PySpark)
  - `varchar2`/`string` → `StringType`
  - `date` → `TimestampType` or `DateType`
  - No Informatica-specific functions (e.g., `DECODE`, `IIF`, `TO_DATE`) are used.

- **Workflow/Control Logic:**  
  - No Routers, Filters, or Transaction Control present.

---

# 4. Manual Adjustments

- **Components Requiring Manual Implementation:**  
  - Parameter substitution for `$$Audit_Year` (can be handled via config or argument).
  - Session timestamp (`SESSSTARTTIME`) needs to be set using PySpark's `current_timestamp()`.

- **External Dependencies:**  
  - Source file path: `$PMSourceFileDir/corp/SrcFiles/profrec_vendor.txt` (must be referenced or parameterized in PySpark).
  - Target Oracle connection: Must be configured in Spark with JDBC URL, user, and password.
  - Reject/bad file handling: Informatica writes rejects to `$PMBadFileDir/corp/BadFiles/ps_vendor1.bad`; in PySpark, explicit error handling and logging must be implemented if needed.

- **Business Logic Review:**  
  - Ensure the audit year and session timestamp logic is preserved.
  - Validate field data types and nullability constraints for Oracle target compatibility.

---

# 5. Optimization Techniques

- **Spark Best Practices:**
  - Use schema inference or explicit schema definition when reading the flat file for performance and data quality.
  - Partition data if the input file is large for parallel processing.
  - Use `cache()` or `persist()` only if the DataFrame is reused.
  - Use JDBC batch writes for efficient loading into Oracle.

- **Pipeline Simplification:**
  - Chain the read, transformation, and write operations in a single pipeline.
  - No need for intermediate DataFrames unless required for debugging.

- **Window Functions:**
  - Not required as there are no aggregations or row-level calculations.

- **Refactor or Rebuild Recommendation:**  
  - **Refactor.** The original logic is simple and can be retained as-is in PySpark, with minor adjustments for parameter and timestamp handling.

---

**Summary:**  
This Informatica workflow is low in complexity, with a direct mapping from a flat file to an Oracle table and minimal transformation logic. The migration to PySpark is straightforward, requiring only basic parameterization and timestamp handling, with attention to data types and error logging.

---