=============================================
Author:        Ascendion AVA+
Date:          
Description:   Cost and effort estimation for PySpark conversion
=============================================

# 1. Cost Estimation

## 1.1 PySpark Runtime Cost

**Estimated Data Volume:**  
- Source: PS_VENDOR_S4 ~ 500 MB  
- Target: PS_VENDOR ~ 500 MB

**Azure Databricks Environment Details:**  
- Standard job cluster: ~$0.22 per DBU/hour  
- Estimated compute cost: ~$0.60 per hour per node  
- Storage: $18.40 per TB/month  
- Optimizations (e.g., Delta Lake, caching) can further reduce costs.

**Job Complexity:**  
- No joins, lookups, or aggregations; only direct mapping and simple field assignments.
- Expected job runtime: ~10-15 minutes for 500 MB with a small cluster (2 nodes).

**Cost Breakdown:**  
- Compute: 2 nodes × $0.60/hr × 0.25 hr = $0.30
- Storage (transient, negligible for 500 MB): < $0.01
- Total estimated PySpark job cost: **$0.30 USD** (rounded for a single run)

**Justification:**  
- The workflow is low complexity (score: 15), with no heavy transformations.
- Input size is moderate (500 MB), easily handled by a small Databricks cluster.
- No advanced Spark features (joins, windowing, aggregations) required.
- Cost can be further optimized with job scheduling and cluster auto-termination.

---

# 2. Code Fixing and Testing Effort Estimation

## 2.1 PySpark Code Manual Fixes and Unit Testing Effort

**Manual Code Fixes:**  
- Parameter substitution for `$$Audit_Year` (PySpark config/argument): 0.5 hr
- Session timestamp assignment (`SESSSTARTTIME` → `current_timestamp()`): 0.5 hr
- Source/target path and JDBC connection setup: 0.5 hr
- Error/reject file handling (optional): 0.5 hr
- Data type mapping validation: 0.5 hr

**Total for code fixes:** **2.5 hours**

**Unit Testing Effort:**  
- Basic validation of field mapping and transformation: 0.5 hr
- Test parameter and timestamp logic: 0.5 hr

**Total for unit testing:** **1.0 hour**

**Combined code fixes + unit testing:** **3.5 hours**

---

## 2.2 Output Validation Effort

- Compare Informatica output (Oracle table, reject file) with PySpark output: 1.0 hr
- Data reconciliation and row count validation: 0.5 hr

**Total output validation effort:** **1.5 hours**

---

## 2.3 Total Estimated Effort in Hours

| Activity                           | Effort (hrs) |
|-------------------------------------|--------------|
| Manual code fixes                   | 2.5          |
| Unit testing                        | 1.0          |
| Output validation                   | 1.5          |
| **Total Estimated Effort**          | **5.0**      |

**Justification:**  
- The mapping is simple, with only parameter and timestamp logic requiring manual intervention.
- No joins, lookups, or aggregations, so minimal transformation complexity.
- Output validation is straightforward due to 1:1 mapping and small data volume.

---

# 3. API Cost Consumption

```
apiCost: 0.0520 USD
```

---

**Summary:**  
- PySpark runtime cost (Azure Databricks): ~$0.30 per run for 500 MB data, 2-node cluster.
- Manual code fix and testing effort: ~5.0 hours total.
- API cost for this analysis: $0.0520 USD.

This estimate provides a clear financial and resource plan for Informatica to PySpark migration for the PS_VENDOR workflow.