=============================================
Author:        Ascendion AVA+
Date:          
Description:   Cost and effort estimation for PySpark conversion
=============================================

# 1. Cost Estimation

## 1.1 PySpark Runtime Cost

**Environment:** Azure Databricks (Serverless)

- **Data Volume:**  
  - Source: PS_VENDOR_S4 ~ 500 MB  
  - Target: PS_VENDOR ~ 500 MB

- **Transformation Complexity:**  
  - Only 1:1 mappings and simple assignments (no lookups, joins, or aggregations).
  - No conditional logic or complex expressions.

- **Expected Job Execution Time:**  
  - For 500 MB of data, a single standard Databricks job cluster (1-2 nodes) should complete the job in under 10 minutes, assuming efficient file I/O and JDBC write.

- **Performance Optimizations:**  
  - Use of schema inference or explicit schema.
  - No caching or partitioning required given the data size.
  - Delta Lake and optimized JDBC writes recommended for reliability.

- **Cost Calculation:**  
  - Databricks Unit (DBU) cost: ~$0.22 per DBU/hour.
  - Estimated compute cost: $0.60 per hour per node.
  - For a 10-minute job on 1 node:  
    - Compute: ($0.60/hour) * (1/6 hour) ≈ **$0.10**  
    - DBU: ($0.22/hour) * (1/6 hour) ≈ **$0.04**
    - Storage: Negligible for transient data (~$0.01)

**Total Estimated Runtime Cost per Run:**  
**~$0.15 USD** (rounded up for buffer and minor overheads)

---

# 2. Code Fixing and Testing Effort Estimation

## 2.1 PySpark Code Manual Fixes and Unit Testing Effort

| Area                        | Manual Effort (hrs) | Notes                                                      |
|-----------------------------|---------------------|------------------------------------------------------------|
| Parameter Handling          | 0.5                 | Mapping/session variables to PySpark params                |
| Flat File Parsing           | 0.5                 | Delimiter `||`, null char `*`, schema definition           |
| Data Type Conversion        | 0.5                 | Ensure Oracle/PySpark types match                          |
| Error/Reject Handling       | 0.5                 | Implement error logging, reject file logic                 |
| Oracle JDBC Integration     | 0.5                 | Setup and test JDBC write                                  |
| Audit/Load Date Logic       | 0.25                | Use current_timestamp() for LOAD_DT                        |
| Unit Testing (PySpark)      | 0.5                 | Test data flow, error paths                                |
| **Total**                   | **3.25**            |                                                            |

*No lookups, joins, or aggregations are present, so no additional effort for those.*

## 2.2 Output Validation Effort

| Task                                   | Manual Effort (hrs) | Notes                                      |
|-----------------------------------------|---------------------|--------------------------------------------|
| Data Reconciliation (Source vs Target)  | 0.5                 | Compare Informatica and PySpark outputs    |
| Validation Testing                      | 0.5                 | Row counts, field-by-field checks         |
| **Total**                               | **1.0**             |                                            |

## 2.3 Total Estimated Effort in Hours

- **Manual Code Fixes & Unit Testing:** 3.25 hours
- **Output Validation Effort:** 1.0 hour

**Total Estimated Effort:**  
**4.25 hours**

**Justification:**  
- The workflow is simple, with only basic parameter, file, and JDBC handling required.
- No complex business logic, joins, or aggregations.
- Testing and validation are straightforward due to 1:1 mappings and small data volume.
- Minor overlap in setup and validation tasks is already considered in the estimates.

---

# 3. API Cost Consumption

```
apiCost: 0.0523 USD
```