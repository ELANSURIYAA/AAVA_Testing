=============================================
Author:        AAVA
Created on:    
Description:   Cost estimation and testing effort analysis for Synapse to Databricks conversion of LOAD_FACT_EXECUTIVE_SUMMARY procedure
=============================================

---

### 1. Cost Estimation

#### 1.1 Databricks Runtime Cost

**Estimated Monthly Runtime Cost: $285 - $420 USD**

**Cost Breakdown:**
- **Cluster Configuration:** Standard_DS3_v2 (4 cores, 14GB RAM) - $0.15/hour
- **DBU Cost:** 1 DBU/hour for Standard cluster - $0.20/hour  
- **Total Hourly Cost:** $0.35/hour
- **Estimated Daily Runtime:** 2-3 hours (assuming daily batch processing)
- **Monthly Cost:** $0.35 × 2.5 hours × 30 days = $262.50 base cost
- **With overhead and scaling:** $285 - $420 USD/month

**Cost Justification:**
- The procedure involves moderate complexity with 4 dimension table joins
- Processing volume estimated at 100K-1M records daily based on fact table structure
- Single INSERT operation with minimal transformations keeps compute requirements moderate
- No complex aggregations or window functions reduce processing time
- Temporary table logic converted to CTEs will improve performance and reduce storage costs

---

### 2. Code Fixing and Testing Effort Estimation

#### 2.1 Databricks Code Manual Fixes

**Estimated Manual Fix Effort: 16 hours**

**Breakdown by Component:**
- **Variable Declaration Removal:** 2 hours
  - Remove `@v_row_count` and `@error_message` variables
  - Implement alternative row counting mechanism
  
- **Temporary Table Conversion:** 4 hours
  - Convert `#staging_metrics` temp table to CTE
  - Restructure query logic for single-statement execution
  
- **Procedural Logic Removal:** 3 hours
  - Remove `PRINT` statements and procedural blocks
  - Convert to declarative SQL structure
  
- **Join Optimization:** 3 hours
  - Validate dimension table joins work correctly in Databricks
  - Optimize join order and predicates
  
- **Business Rule Validation:** 2 hours
  - Ensure `CASE WHEN` logic for `income_amount` works identically
  - Validate data type conversions
  
- **Error Handling Implementation:** 2 hours
  - Implement external error handling via notebook or workflow
  - Add data quality checks

#### 2.2 Output Validation Effort

**Estimated Validation Effort: 24 hours**

**Breakdown by Validation Type:**
- **Schema Validation:** 4 hours
  - Compare output table schemas between Synapse and Databricks
  - Validate data types and column mappings
  
- **Data Reconciliation:** 12 hours
  - Row count validation between source and target
  - Sample data comparison (10% of records)
  - Validate all 20 metric columns for accuracy
  
- **Business Rule Testing:** 4 hours
  - Test `income_amount` transformation logic
  - Validate dimension key lookups
  
- **Performance Testing:** 2 hours
  - Compare execution times
  - Validate resource utilization
  
- **End-to-End Testing:** 2 hours
  - Full pipeline testing with production-like data volumes
  - Integration testing with downstream processes

#### 2.3 Total Estimated Effort in Hours

**Total Effort: 40 hours**

**Calculation:**
- Manual Code Fixes: 16 hours
- Output Validation: 24 hours
- **Total: 40 hours**

**Effort Justification:**
The total effort accounts for:
- Moderate complexity (25/100 complexity score)
- 4 dimension table joins requiring careful validation
- Single business transformation rule requiring thorough testing
- 20 metric columns requiring individual validation
- No overlap between fixing and testing phases as they are sequential
- Buffer time included for unexpected issues during conversion

---

### 3. API Cost Consumption

```
apiCost: 0.0523 USD
```