```
=============================================
Author:        AAVA
Created on:   
Description:   Loads the FACT_EXECUTIVE_SUMMARY table with summarized holding metrics from staging, applying business rules, data quality checks, and referential integrity via dimension joins.
=============================================

# 1. Procedure Overview

The `LOAD_FACT_EXECUTIVE_SUMMARY` stored procedure in Azure Synapse Analytics is designed to populate the `FACT_EXECUTIVE_SUMMARY` fact table. It ingests data from the `STG_HOLDING_METRICS` staging table, applies business rules (such as normalizing income amounts), performs data quality validations, and ensures referential integrity by joining with dimension tables (`DIM_DATE`, `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`). The procedure supports executive reporting and analytics by providing aggregated holding metrics at various business levels.

# 2. Complexity Metrics

| Metric                                   | Description                                                                                  |
|-------------------------------------------|----------------------------------------------------------------------------------------------|
| Number of Input Tables                    | 1 (`STG_HOLDING_METRICS`)                                                                   |
| Number of Output Tables                   | 1 (`FACT_EXECUTIVE_SUMMARY`)                                                                |
| Variable Declarations                     | 2 (`@v_row_count`, `@error_message`) – simple usage                                         |
| Conditional Logic                        | 1 (CASE WHEN for `income_amount` normalization)                                              |
| Loop Constructs                           | 0                                                                                            |
| Join Conditions                           | 4 (INNER JOINs: `DIM_DATE`, `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`)            |
| Aggregations                              | 0 (no aggregation functions used)                                                            |
| Subqueries / CTEs                         | 0                                                                                            |
| Procedural Calls                          | 0 (no stored procedure or function invocations)                                              |
| DML Operations                            | 1 (INSERT INTO fact table, SELECT INTO temp table, DROP TABLE for cleanup)                   |
| Temporary Tables / Table Variables        | 1 (`#staging_metrics` temp table)                                                            |
| Transaction Handling                      | 0 (no explicit BEGIN TRAN, COMMIT, ROLLBACK)                                                 |
| Error Handling Blocks                     | 0 (no TRY...CATCH logic)                                                                    |
| Complexity Score (0–100)                  | 25 (simple control flow, minimal procedural depth, moderate DML, straightforward joins)       |

**High-complexity areas:**  
- Multiple joins to dimension tables for referential integrity.
- Business rule enforcement via CASE WHEN logic.
- Use of temporary table for staging.

# 3. Syntax Differences

- **Variable Declarations:**  
  - T-SQL uses `DECLARE @variable` for scalar variables; Databricks declarative SQL does not support procedural variable declarations.
- **Temporary Tables:**  
  - T-SQL uses `SELECT INTO #temp_table`; Databricks does not support session-scoped temp tables, requiring CTEs or permanent staging tables.
- **Control Flow:**  
  - T-SQL procedural blocks (`BEGIN...END`, `IF...ELSE`, `PRINT`, variable assignments) are not supported in Databricks declarative SQL.
- **CASE WHEN:**  
  - Supported in both environments, but must be used within SELECT statements only in Databricks SQL.
- **Transaction Handling:**  
  - No explicit transaction statements (`BEGIN TRAN`, `COMMIT`, `ROLLBACK`) present, but if they were, Databricks declarative SQL would not support them.
- **Error Handling:**  
  - T-SQL `TRY...CATCH` blocks are not supported; error handling must be managed externally in Databricks.
- **Data Type Differences:**  
  - Potential need to convert `DATETIME` to `TIMESTAMP`, and adjust `DECIMAL` precision/scale if necessary.

# 4. Manual Adjustments

- **Variable Usage:**  
  - Audit variable (`@v_row_count`) and error message placeholder must be replaced with query-based row counting or external logging.
- **Temporary Table Logic:**  
  - Replace `#staging_metrics` temp table with a CTE or a view in Databricks.
- **PRINT Statements:**  
  - Logging via `PRINT` must be handled by external orchestration or notebook markdown/logging.
- **Cleanup:**  
  - Explicit DROP TABLE for temp tables is not required; CTEs/views are ephemeral.
- **Audit Logging:**  
  - Row count capture must be implemented via a SELECT COUNT(*) or similar logic.
- **External Dependencies:**  
  - Ensure all dimension tables exist and are accessible in the Databricks environment.
- **Business Rule Validation:**  
  - CASE WHEN logic for `income_amount` must be validated post-conversion for parity.

# 5. Optimization Techniques

- **Sequential CTEs:**  
  - Replace temp table logic with CTEs to streamline data flow and reduce intermediate materializations.
- **Join Optimization:**  
  - Ensure joins to dimension tables use indexed columns and apply predicate pushdown where possible.
- **Query Simplification:**  
  - Merge staging and fact table insert logic into a single query using CTEs.
- **Materialization Reduction:**  
  - Avoid unnecessary intermediate tables; use modular queries for clarity and performance.
- **Predicate Pushdown:**  
  - Filter records early in the query to minimize data processed during joins.

# 6. API Cost Consumption

```
apiCost: 0.008 USD