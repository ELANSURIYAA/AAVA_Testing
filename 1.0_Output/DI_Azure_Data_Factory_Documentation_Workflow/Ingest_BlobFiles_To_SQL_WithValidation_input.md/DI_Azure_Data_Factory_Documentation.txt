=============================================
Author:        Ascendion AVA+
Created on:    
Description:   Pipeline to ingest multiple blob files into SQL staging with validation and cleanup
=============================================

# Azure Data Factory Pipeline Documentation

## Overview of Pipeline

The **Ingest_BlobFiles_To_SQL_WithValidation** pipeline automates the ingestion of multiple files from Azure Blob Storage into a SQL staging table, with built-in validation and cleanup steps. This pipeline supports enterprise data integration and analytics by enabling scalable, auditable, and repeatable ingestion of raw data files into a curated SQL environment, ensuring data quality and compliance.

### Business Purpose

- Automates ingestion of raw files from Blob Storage into SQL staging.
- Validates existing data and ensures staging table is clean before loading.
- Supports downstream analytics, reporting, and business intelligence.
- Enables auditability and operational excellence for enterprise data integration.

### High-Level Components

- **Pipelines:** Ingest_BlobFiles_To_SQL_WithValidation
- **Activities:** Lookup, IfCondition, Stored Procedure, ForEach, Copy
- **Datasets:** InputBlobDataset, OutputSqlDataset
- **Linked Services:** AzureBlobStorageLS, AzureSqlDatabaseLS
- **Triggers:** (Not explicitly defined in JSON, but pipeline can be scheduled)
- **Integration Runtimes:** (Default Azure IR unless specified)

---

## Pipeline Structure and Design

### Structure

- **Linked Services**
  - `AzureBlobStorageLS`: Connects to Blob Storage (input files).
  - `AzureSqlDatabaseLS`: Connects to Azure SQL Database (staging table).

- **Datasets**
  - `InputBlobDataset`: Represents binary files in Blob Storage (container: input, folderPath: incoming).
  - `OutputSqlDataset`: Represents SQL table `Sales_Staging`.

- **Pipeline Parameters**
  - `SourceFileList`: Array of files to ingest (e.g., ["file1.csv", "file2.csv"]).
  - `RunDate`: Execution timestamp.

- **Variables**
  - `rowCount`: Holds count of rows in staging table.

- **Activities**
  - `LookupRowCount`: Checks row count in staging table.
  - `IfHasData`: If staging table has data, truncates it.
  - `ForEachFiles`: Iterates over file list and copies each file to SQL.
    - `CopyBlobToSQL`: Executes file copy from Blob to SQL.

### Dependencies

- `IfHasData` depends on completion of `LookupRowCount`.
- `ForEachFiles` depends on completion of `IfHasData`.
- `CopyBlobToSQL` executes for each item in `SourceFileList`.

### Performance/Optimization Notes

- Bulk copy operations optimize ingestion.
- Table truncation ensures clean staging for each run.
- Parallel execution possible in `ForEachFiles` (if enabled).

---

## Data Flow and Processing Logic

### Step-by-Step Flow

```markdown
[Start]
   |
   v
[LookupRowCount]
   |
   v
[IfCondition: IfHasData]
   |-------------------|
   |                   |
   v                   v
[TruncateTable]     [No Action]
   |
   v
[ForEachFiles]
   |
   v
[CopyBlobToSQL] (for each file)
   |
   v
[End]
```

#### Conditional Paths

- If `Sales_Staging` has rows, execute `TruncateTable`.
- If not, skip truncation.
- For each file in `SourceFileList`, execute `CopyBlobToSQL`.

---

## Transformation Breakdown (Mandatory)

### 1. LookupRowCount

- **Input Source:** `Sales_Staging` table (Azure SQL)
- **Transformation Logic:** SQL query: `SELECT COUNT(*) as cnt FROM Sales_Staging`
- **Output Destination:** Pipeline variable `rowCount`
- **Mapping to KB:** Staging table referenced in knowledge base as `staging_employee` and `Sales_Staging`.

### 2. IfHasData (IfCondition)

- **Input Source:** Output from `LookupRowCount`
- **Transformation Logic:** Expression: `@greater(activity('LookupRowCount').output.firstRow.cnt,0)`
- **Output Destination:** Conditional path
- **Mapping to KB:** Ensures staging table is clean for new data, as per best practices in KB.

### 3. TruncateTable (Stored Procedure)

- **Input Source:** Azure SQL Database
- **Transformation Logic:** Executes `usp_Truncate_Staging`
- **Output Destination:** `Sales_Staging` table
- **Mapping to KB:** Table truncation aligns with KB scripts for staging cleanup.

### 4. ForEachFiles

- **Input Source:** Pipeline parameter `SourceFileList`
- **Transformation Logic:** Iterates over each file name.
- **Output Destination:** Executes `CopyBlobToSQL` for each file.

### 5. CopyBlobToSQL

- **Input Source:** Blob file (`InputBlobDataset`)
- **Transformation Logic:** Binary copy to SQL sink (`OutputSqlDataset`)
- **Output Destination:** `Sales_Staging` table
- **Mapping to KB:** Data mapping and transformation rules for staging ingestion referenced from KB.

---

## Data Mapping

| Target Dataset   | Target Field | Source Dataset   | Source Field | Transformation / Rule                |
|------------------|--------------|------------------|-------------|--------------------------------------|
| Sales_Staging    | All columns  | InputBlobDataset | Binary file  | Direct copy, staging cleanup applied |
| Sales_Staging    | rowCount     | OutputSqlDataset | COUNT(*)     | Validation before ingestion          |

*Transformation rules are referenced from KB: staging_employee, Sales_Staging, and related scripts.*

---

## Complexity Analysis

| Metric                  | Count / Description                  |
|-------------------------|--------------------------------------|
| Pipelines               | 1                                    |
| Activities              | 4 (Lookup, IfCondition, ForEach, Copy)|
| Datasets                | 2 (InputBlobDataset, OutputSqlDataset)|
| Triggers                | 0 (manual/scheduled)                 |
| Parameters              | 2 (SourceFileList, RunDate)          |
| Conditional Logic       | 1 (IfCondition)                      |
| Transformations         | 2 (Validation, Cleanup)              |
| Dependencies            | 3 (Lookup -> IfCondition -> ForEach) |
| Overall Complexity Score| 35/100                               |

---

## Key Outputs

- **Final Output Table:** `Sales_Staging`
- **Schema/Table Names:** Referenced from KB: `staging_employee`, `Sales_Staging`
- **Fields:** All columns from ingested files
- **Storage Format:** Azure SQL Database (relational table)
- **Alignment to Business Requirements:** Provides curated, validated, and auditable data for downstream analytics and reporting.

---

## API Cost Calculations

- **API Calls:**
  - LookupRowCount: 1
  - TruncateTable: 1 (conditional)
  - CopyBlobToSQL: N (number of files in `SourceFileList`)
- **Estimated Cost Calculation:**
  - Azure Data Factory pipeline activity cost (as per Microsoft pricing): $0.25 per 1,000 activity runs
  - Example: For 2 files, total activities = 1 (Lookup) + 1 (IfCondition) + 1 (TruncateTable) + 2 (Copy) = 5
  - Total cost: (5/1000) * $0.25 = **$0.00125 USD**

---

**References:**  
- Knowledge base Excel: KB - DI_Azure_Data_Factory_Documentation_kb  
- SQL scripts and data mapping: staging_employee, Sales_Staging, and related transformation scripts

---

*This document is a single source of truth for the pipeline logic, dependencies, and operational details, bridging technical and business stakeholders and promoting operational excellence.*