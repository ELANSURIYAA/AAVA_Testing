========================================================
Author:        AAVA
Created on:    
Description:   Pre-conversion analysis of Ab Initio ETL flow for PySpark EMR/Glue migration
========================================================

#### 1. AWS EMR Glue Runtime Cost Estimation

##### 1.1 EMR/Spark Job Cost Breakdown

- **Cluster Configuration**:  
  - AWS Glue (equivalent to EMR: m5.xlarge, 4 workers, 1 master)
  - **Master Node**: 1 DPU (4 vCPUs, 16GB RAM)
  - **Worker Nodes**: 9 DPUs (total 10 DPUs typical for this workload)
  - **Total vCPUs & Memory**: 40 vCPUs, 160GB RAM

- **Job Duration Estimate**: 90 minutes (1.5 hours)  
  - (Based on 200GB input, heavy shuffle, and aggregation steps)

- **AWS Pricing**:  
  - Compute: $0.44 per DPU-hour (AWS Glue standard rate)
  - Storage: $0.023 per GB-month (S3 temp shuffle, not direct job cost, but ~100GB used for 1.5 hours)

- **Cost Formula Used**:

  ```
  Total Compute Cost = (DPUs × Duration in hours × DPU-hour rate)
  Storage Cost = (Temp Storage GB × Duration in hours × (Storage $/GB-month) / 720)
  Total Cost = Compute Cost + Storage Cost
  ```

  - Compute: 10 DPUs × 1.5 hours × $0.44 = $6.60
  - Storage: 100 GB × 1.5 hr × ($0.023/720 hr) ≈ $0.0048

- **Estimated Runtime Cost (USD)**: **$6.60** (compute) + **$0.005** (storage) = **$6.61**

---

#### 2. Manual Code Fixing and Data Reconciliation Effort

##### 2.1 Estimated Effort (Hours)

| Task                                      | Effort (hrs) |
|-------------------------------------------|--------------|
| Logic Corrections (.xfr transformations)  | 12           |
| Metadata Alignment (.dml type fixes)      | 4            |
| Rejected Row Handling / Edge Case Logic   | 3            |
| Data Reconciliation & Output Validation   | 5            |
| **Total Effort**                         | **24**       |

- **Logic Corrections**: Manual porting of `cleanse_validate.xfr`, `pricing_rules.xfr`, and `store_rollup.xfr` to PySpark UDFs and DataFrame logic (validation, pricing, aggregation).
- **Metadata Alignment**: Mapping Ab Initio DMLs to PySpark/Glue schemas, handling type mismatches and nullability.
- **Rejected Row Handling**: Implementing reject flows for cleansing and product misses, including error tagging.
- **Data Reconciliation & Output Validation**: Ensuring output matches business rules, validating summary and reject outputs.

##### 2.2 Developer Cost

- Developer Rate: `$50/hr`  
- **Total Developer Cost**: `24 × $50 = $1,200 USD`

---

#### 3. API Cost

- apiCost: **$0.0159** (in USD)

---

### **Summary Table**

| Cost Component                    | Value (USD)  |
|-----------------------------------|--------------|
| AWS EMR/Glue Runtime Cost         | $6.61        |
| Developer/Test Effort (24 hrs)    | $1,200.00    |
| API Processing Cost               | $0.0159      |
| **Total Projected Cost**          | **$1,206.63**|

---

### **Notes & Assumptions**

- Input data: 200GB (transactions), 2GB (product dim)
- Output data: ~5GB (summary), ~1GB (rejects), ~0.5GB (misses)
- Job duration: 1.5 hours (conservative estimate for 200GB batch with heavy shuffle)
- DPU count: 10 (typical for this batch size and transformation complexity)
- Storage cost for temp shuffle is negligible compared to compute for this run
- Developer effort assumes experienced PySpark/ETL engineer and includes both code and test/validation

---

### **Input:**

* AbInitio Source File(s): Retail_Data_Mart_Ingest.mp
* Environmental variable file : Retail_AWS_Env_Variable.txt

---