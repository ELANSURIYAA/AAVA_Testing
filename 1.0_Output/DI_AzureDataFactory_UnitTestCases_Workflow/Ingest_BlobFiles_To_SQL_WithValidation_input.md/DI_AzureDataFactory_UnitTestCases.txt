# Azure Data Factory (ADF) Pipeline Unit Test Framework

---

## 1. Unit Test Case Document (Markdown)

### Pipeline: `Ingest_BlobFiles_To_SQL_WithValidation`

#### **Test Case 1: LookupRowCount Activity**

- **Test Case ID:** TC-ADF-001
- **Description:** Validate that the Lookup activity retrieves the correct row count from `Sales_Staging` table.
- **Preconditions:**
  - Azure SQL Database linked service configured.
  - `Sales_Staging` table exists and contains test data.
  - Pipeline parameter `SourceFileList` set to sample files.
- **Test Steps:**
  1. Load sample data into `Sales_Staging` (see Test Data section).
  2. Execute `LookupRowCount` activity.
  3. Capture output: `output.firstRow.cnt`.
- **Validation SQL Queries:**  
  ```sql
  SELECT COUNT(*) as cnt FROM Sales_Staging;
  ```
  *(Reference: Knowledge Base Excel - SelectQuery for staging tables)*
- **Expected Results:**  
  - Output row count matches SQL query result.
- **Edge Cases:**  
  - Table is empty (expect 0).
  - Table contains duplicate rows.
  - Table contains null values.
- **Pass/Fail Criteria:**  
  - Pass if output matches SQL query result for all edge cases.

---

#### **Test Case 2: IfHasData Activity**

- **Test Case ID:** TC-ADF-002
- **Description:** Validate conditional execution based on Lookup output.
- **Preconditions:**
  - `LookupRowCount` activity executed.
  - Variable `rowCount` set.
- **Test Steps:**
  1. If `rowCount > 0`, execute `TruncateTable` stored procedure.
  2. If `rowCount == 0`, skip truncation.
- **Validation SQL Queries:**  
  ```sql
  EXEC usp_Truncate_Staging;
  SELECT COUNT(*) FROM Sales_Staging;
  ```
  *(Reference: Knowledge Base Excel - RawInsertScript/TransformScript for truncation and validation)*
- **Expected Results:**  
  - Table is empty after truncation if executed.
- **Edge Cases:**  
  - Table already empty.
  - Table contains non-removable rows (constraint violation).
- **Pass/Fail Criteria:**  
  - Pass if table is empty after truncation when `rowCount > 0`.

---

#### **Test Case 3: ForEachFiles Activity**

- **Test Case ID:** TC-ADF-003
- **Description:** Validate that each file in `SourceFileList` is processed and ingested.
- **Preconditions:**
  - Sample files exist in Blob Storage.
  - `Sales_Staging` table is empty.
- **Test Steps:**
  1. For each file in `SourceFileList`, execute `CopyBlobToSQL`.
  2. Validate data ingestion for each file.
- **Validation SQL Queries:**  
  ```sql
  SELECT * FROM Sales_Staging WHERE file_name IN ('file1.csv', 'file2.csv');
  ```
  *(Reference: Knowledge Base Excel - SelectQuery for file ingestion)*
- **Expected Results:**  
  - Data from each file is present in `Sales_Staging`.
  - Field mappings and transformations applied as per Excel.
- **Edge Cases:**  
  - File missing in Blob Storage.
  - File contains incorrect data types.
  - File contains nulls, duplicates, empty rows.
- **Pass/Fail Criteria:**  
  - Pass if all files are ingested and validated as per mappings.

---

#### **Test Case 4: CopyBlobToSQL Activity**

- **Test Case ID:** TC-ADF-004
- **Description:** Validate copy operation from Blob to SQL with transformation.
- **Preconditions:**
  - Blob files contain sample data.
  - SQL table schema matches Excel mappings.
- **Test Steps:**
  1. Execute `CopyBlobToSQL` for each file.
  2. Validate schema, transformations, and row counts.
- **Validation SQL Queries:**  
  ```sql
  -- Example transformation validation
  SELECT id, name, v2organization_id FROM Sales_Staging;
  ```
  *(Reference: Knowledge Base Excel - field-level TransformScript and mappings)*
- **Expected Results:**  
  - Data matches schema and transformation rules from Excel.
- **Edge Cases:**  
  - Data type mismatch.
  - Missing columns.
  - Extra columns.
- **Pass/Fail Criteria:**  
  - Pass if all rows conform to schema and transformation rules.

---

## 2. Sample Test Data Files

### **CSV Example (file1.csv):**
```csv
id,name,v2organization_id,is_default_dept,dept_category_id
1,DeptA,245,0,2
2,DeptB,245,1,2
3,DeptC,245,0,2
```
*(Schema and values derived from Excel: department table mappings)*

### **CSV Example (file2.csv):**
```csv
id,name,v2organization_id,is_default_dept,dept_category_id
4,DeptD,245,1,2
5,DeptE,245,0,2
```

### **Edge Case CSV Example (file_edge.csv):**
```csv
id,name,v2organization_id,is_default_dept,dept_category_id
6,,245,0,2
7,DeptG,notanumber,1,2
8,DeptH,245,,2
```

---

## 3. Execution Scripts

### **SQL Scripts for Loading Test Data:**
```sql
-- Load sample data into Sales_Staging
BULK INSERT Sales_Staging
FROM 'file1.csv'
WITH (
    FIELDTERMINATOR = ',',
    ROWTERMINATOR = '\n',
    FIRSTROW = 2
);

BULK INSERT Sales_Staging
FROM 'file2.csv'
WITH (
    FIELDTERMINATOR = ',',
    ROWTERMINATOR = '\n',
    FIRSTROW = 2
);
```

### **PowerShell/Azure CLI to Trigger Pipeline:**
```powershell
# PowerShell
$resourceGroup = "yourResourceGroup"
$dataFactoryName = "yourADF"
$pipelineName = "Ingest_BlobFiles_To_SQL_WithValidation"
$parameters = @{
    "SourceFileList" = @("file1.csv", "file2.csv")
    "RunDate" = (Get-Date).ToString("yyyy-MM-dd")
}
Invoke-AzDataFactoryV2Pipeline -ResourceGroupName $resourceGroup -DataFactoryName $dataFactoryName -PipelineName $pipelineName -Parameter $parameters
```

### **SQL Validation Script:**
```sql
-- Validate row count and transformations
SELECT COUNT(*) FROM Sales_Staging;
SELECT id, name, v2organization_id, is_default_dept, dept_category_id FROM Sales_Staging;
```
*(Reference: Excel mappings for department and staging tables)*

---

## 4. Test Coverage Summary

| Activity             | Test Cases | Coverage (%) | Complexity (SQL/Transform) |
|----------------------|------------|--------------|----------------------------|
| LookupRowCount       | 1          | 100%         | Low                        |
| IfHasData            | 1          | 100%         | Medium (Stored Proc)       |
| ForEachFiles         | 1          | 100%         | Medium                     |
| CopyBlobToSQL        | 1          | 100%         | High (Transform, Mapping)  |

- **Total Activities:** 4
- **Total Test Cases:** 4
- **Coverage:** 100%
- **Complex SQL-driven Transformations:** CopyBlobToSQL (field-level mapping, schema validation, transformation rules from Excel)

---

## 5. Output Formatting

- **Unit Test Case Document:** Markdown (above)
- **Sample Test Data Files:** CSV (examples above)
- **Execution Scripts:** SQL, PowerShell (examples above)
- **References:** All validation queries and expected results reference Knowledge Base Excel mappings, SQL scripts, and transformation rules.

---

## 6. Quality Notes

- All test cases reference Excel-derived SQL scripts and mappings.
- Edge cases include nulls, duplicates, incorrect data types, and missing files.
- Test data is anonymized, realistic, and lightweight.
- Execution scripts provided for loading, triggering, and validating.
- Test coverage summary highlights all pipeline components and complex transformations.

---

**This framework ensures robust, automated unit testing for Azure Data Factory pipelines, leveraging both pipeline logic and client-specific transformation rules from the Knowledge Base Excel.**