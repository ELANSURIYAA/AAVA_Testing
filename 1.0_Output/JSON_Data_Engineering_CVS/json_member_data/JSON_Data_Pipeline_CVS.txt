```python
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when
from pyspark.sql.types import StringType, IntegerType, DateType, StructType, StructField

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def create_spark_session(app_name="JsonToBigQueryPipeline"):
    """Creates and returns a Spark session."""
    try:
        spark = SparkSession.builder \
            .appName(app_name) \
            .config("spark.jars.packages", "com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2") \
            .getOrCreate()
        logging.info("Spark session created successfully.")
        return spark
    except Exception as e:
        logging.error(f"Error creating Spark session: {e}")
        raise

def read_json_data(spark, source_path):
    """Reads data from a JSON file."""
    try:
        # Define a schema to handle nested structures and ensure data types
        source_schema = StructType([
            StructField("member_id", StringType(), True),
            StructField("personal_info", StructType([
                StructField("first_name", StringType(), True),
                StructField("last_name", StringType(), True),
                StructField("date_of_birth", StringType(), True)
            ]), True),
            StructField("contact", StructType([
                StructField("email", StringType(), True),
                StructField("phone", StringType(), True)
            ]), True),
            StructField("subscription", StructType([
                StructField("plan", StringType(), True),
                StructField("start_date", StringType(), True),
                StructField("status", StringType(), True)
            ]), True)
        ])
        df = spark.read.format("json").schema(source_schema).load(source_path)
        logging.info(f"Successfully read data from {source_path}")
        return df
    except Exception as e:
        logging.error(f"Error reading JSON data from {source_path}: {e}")
        raise

def transform_data(df):
    """Applies transformations based on data mapping rules."""
    try:
        # Flatten the nested JSON structure
        transformed_df = df.select(
            col("member_id").alias("user_id"),
            col("personal_info.first_name").alias("first_name"),
            col("personal_info.last_name").alias("last_name"),
            col("personal_info.date_of_birth").cast(DateType()).alias("dob"),
            col("contact.email").alias("email_address"),
            col("contact.phone").alias("phone_number"),
            col("subscription.plan").alias("subscription_plan"),
            col("subscription.start_date").cast(DateType()).alias("subscription_start"),
            col("subscription.status").alias("subscription_status")
        )

        # Handle null values and apply default values
        transformed_df = transformed_df.withColumn(
            "subscription_status",
            when(col("subscription_status").isNull(), "inactive").otherwise(col("subscription_status"))
        )

        logging.info("Data transformation completed successfully.")
        return transformed_df
    except Exception as e:
        logging.error(f"Error during data transformation: {e}")
        raise

def write_to_bigquery(df, project, dataset, table, temporary_gcs_bucket):
    """Writes a DataFrame to a BigQuery table."""
    try:
        # Check if the target table exists
        # This is a conceptual check. In a real-world scenario, you'd use the BigQuery client library.
        # For this example, we'll assume the table exists and proceed.
        
        # Configure BigQuery connector options
        df.write.format("bigquery") \
            .option("project", project) \
            .option("temporaryGcsBucket", temporary_gcs_bucket) \
            .mode("append") \
            .save(f"{dataset}.{table}")
        
        logging.info(f"Successfully wrote data to BigQuery table {dataset}.{table}")
    except Exception as e:
        logging.error(f"Error writing data to BigQuery: {e}")
        raise

def main():
    """Main function to run the data pipeline."""
    # Configuration
    source_json_path = "json_member_data.json"
    bq_project = "your-gcp-project-id"  # Replace with your GCP project ID
    bq_dataset = "your_bigquery_dataset" # Replace with your BigQuery dataset
    bq_table = "your_bigquery_table"     # Replace with your BigQuery table
    temporary_gcs_bucket = "your-gcs-bucket" # Replace with your GCS bucket for temporary storage

    spark = None
    try:
        # Create dummy JSON file for demonstration
        with open(source_json_path, "w") as f:
            f.write("""
            {"member_id": "101", "personal_info": {"first_name": "John", "last_name": "Doe", "date_of_birth": "1990-05-15"}, "contact": {"email": "john.doe@example.com", "phone": "123-456-7890"}, "subscription": {"plan": "premium", "start_date": "2023-01-10", "status": "active"}}
            {"member_id": "102", "personal_info": {"first_name": "Jane", "last_name": "Smith", "date_of_birth": "1988-09-20"}, "contact": {"email": "jane.smith@example.com", "phone": "987-654-3210"}, "subscription": {"plan": "basic", "start_date": "2022-11-01", "status": null}}
            """)

        # Initialize Spark session
        spark = create_spark_session()

        # Read data from JSON source
        source_df = read_json_data(spark, source_json_path)

        # Apply transformations
        transformed_df = transform_data(source_df)

        # Write data to BigQuery
        write_to_bigquery(transformed_df, bq_project, bq_dataset, bq_table, temporary_gcs_bucket)

    except Exception as e:
        logging.error(f"Pipeline failed: {e}")
    finally:
        if spark:
            spark.stop()
            logging.info("Spark session stopped.")

if __name__ == "__main__":
    main()

# API Cost Calculation
# The cost is an estimate and depends on various factors like the amount of data processed,
# the complexity of transformations, and the specific pricing of the cloud provider.
# For this example, we'll assume a hypothetical cost.
api_cost = {
    "apiCost": 0.05  # Example cost in USD
}

print(api_cost)
```