### **1. Test Case List**

| Test Case ID | Test Case Description                                                              | Expected Outcome                                                                                                                              |
| :----------- | :--------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------- |
| **`create_spark_session`** |
| TC-001       | `test_create_spark_session`                                                        | A valid `SparkSession` object is created and returned without errors.                                                                         |
| **`read_json_data`** |
| TC-002       | `test_read_json_data_happy_path` - Read a well-formed JSON file.                   | A DataFrame is returned with the correct schema and data, matching the content of the JSON file.                                            |
| TC-003       | `test_read_json_data_empty_file` - Read an empty JSON file.                        | An empty DataFrame with the predefined schema is returned.                                                                                    |
| TC-004       | `test_read_json_data_missing_fields` - Read a JSON where records have missing keys. | A DataFrame is returned where missing fields are populated with `null` values, according to the schema.                                     |
| TC-005       | `test_read_json_data_invalid_path` - Attempt to read from a non-existent path.     | The function raises an exception (e.g., `AnalysisException` in Spark) indicating the path does not exist.                                   |
| **`transform_data`** |
| TC-006       | `test_transform_data_happy_path` - Transform a standard DataFrame.                 | The DataFrame is flattened, columns are renamed, and data types are cast as specified. The output DataFrame matches the expected structure and values. |
| TC-007       | `test_transform_data_empty_df` - Transform an empty DataFrame.                     | An empty DataFrame with the correct transformed schema is returned.                                                                           |
| TC-008       | `test_transform_data_null_status` - Transform data with `null` subscription status. | The `subscription_status` column is correctly populated with the default value "inactive" where it was originally `null`.                     |
| TC-009       | `test_transform_data_invalid_date` - Transform data with invalid date formats.     | The `dob` and `subscription_start` columns contain `null` for records with malformed date strings.                                            |
| TC-010       | `test_transform_data_null_nested_struct` - Transform data with null nested objects. | All columns derived from the null nested object (e.g., `personal_info`) are `null` in the resulting DataFrame.                                |
| **`write_to_bigquery`** |
| TC-011       | `test_write_to_bigquery_mocked` - Verify the BigQuery write call.                  | The DataFrame's `write` method is called with the correct format (`bigquery`), mode (`append`), and options (`project`, `temporaryGcsBucket`). |

---

### **2. Pytest Script**

This script uses `pytest` to test the data pipeline functions. It includes a fixture to manage the `SparkSession` lifecycle and mocks the BigQuery write operation to ensure the tests are isolated and can run without actual GCP credentials.

```python
# test_data_pipeline.py

import pytest
import logging
from unittest.mock import patch, MagicMock

from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, DateType, Row
)
from pyspark.sql.utils import AnalysisException

# Import the functions from the main script
# Assuming the script is saved as 'data_pipeline.py'
from data_pipeline import (
    create_spark_session,
    read_json_data,
    transform_data,
    write_to_bigquery
)

# Configure logging for tests
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


@pytest.fixture(scope="session")
def spark_session():
    """
    Pytest fixture to create a SparkSession for the test suite.
    This is session-scoped to avoid the overhead of creating a session for each test.
    """
    spark = SparkSession.builder \
        .appName("Pytest-GCP-DataProc-Tests") \
        .master("local[2]") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()
    logging.info("Test Spark session created.")
    yield spark
    spark.stop()
    logging.info("Test Spark session stopped.")


def are_dfs_equal(df1, df2):
    """Helper function to compare two Spark DataFrames."""
    if df1.schema != df2.schema:
        return False
    if df1.count() != df2.count():
        return False
    # Sort and collect to ensure order doesn't affect comparison
    return sorted(df1.collect()) == sorted(df2.collect())


class TestDataPipeline:
    """Groups all unit tests for the data pipeline."""

    def test_create_spark_session(self):
        """
        TC-001: Test that create_spark_session returns a SparkSession instance.
        """
        session = None
        try:
            session = create_spark_session(app_name="TestSession")
            assert isinstance(session, SparkSession)
            assert session.sparkContext.appName == "TestSession"
        finally:
            if session:
                session.stop()

    def test_read_json_data_happy_path(self, spark_session, tmp_path):
        """
        TC-002: Test reading a valid JSON file.
        """
        json_content = """
        {"member_id": "101", "personal_info": {"first_name": "John", "last_name": "Doe", "date_of_birth": "1990-05-15"}, "contact": {"email": "john.doe@example.com", "phone": "123-456-7890"}, "subscription": {"plan": "premium", "start_date": "2023-01-10", "status": "active"}}
        """
        json_file = tmp_path / "data.json"
        json_file.write_text(json_content)

        df = read_json_data(spark_session, str(json_file))
        assert df.count() == 1
        assert df.columns == ["member_id", "personal_info", "contact", "subscription"]
        first_row = df.first()
        assert first_row["member_id"] == "101"
        assert first_row["personal_info"]["first_name"] == "John"

    def test_read_json_data_empty_file(self, spark_session, tmp_path):
        """
        TC-003: Test reading an empty JSON file.
        """
        json_file = tmp_path / "empty.json"
        json_file.write_text("")

        df = read_json_data(spark_session, str(json_file))
        assert df.count() == 0
        assert "member_id" in df.columns

    def test_read_json_data_missing_fields(self, spark_session, tmp_path):
        """
        TC-004: Test reading JSON with missing nested fields.
        """
        json_content = """
        {"member_id": "102", "personal_info": {"first_name": "Jane", "last_name": "Smith"}, "contact": null}
        """
        json_file = tmp_path / "missing.json"
        json_file.write_text(json_content)

        df = read_json_data(spark_session, str(json_file))
        first_row = df.first()
        assert df.count() == 1
        assert first_row["member_id"] == "102"
        assert first_row["personal_info"]["date_of_birth"] is None
        assert first_row["contact"] is None

    def test_read_json_data_invalid_path(self, spark_session):
        """
        TC-005: Test reading from a non-existent path.
        """
        with pytest.raises(AnalysisException):
            read_json_data(spark_session, "non_existent_path/data.json")

    def test_transform_data_happy_path(self, spark_session):
        """
        TC-006: Test standard data transformation.
        """
        source_schema = StructType([
            StructField("member_id", StringType()),
            StructField("personal_info", StructType([StructField("first_name", StringType()), StructField("last_name", StringType()), StructField("date_of_birth", StringType())])),
            StructField("contact", StructType([StructField("email", StringType()), StructField("phone", StringType())])),
            StructField("subscription", StructType([StructField("plan", StringType()), StructField("start_date", StringType()), StructField("status", StringType())]))
        ])
        source_data = [Row(
            member_id="101",
            personal_info=Row(first_name="John", last_name="Doe", date_of_birth="1990-05-15"),
            contact=Row(email="john.doe@example.com", phone="123-456-7890"),
            subscription=Row(plan="premium", start_date="2023-01-10", status="active")
        )]
        source_df = spark_session.createDataFrame(source_data, source_schema)

        transformed_df = transform_data(source_df)

        expected_schema = StructType([
            StructField("user_id", StringType(), True), StructField("first_name", StringType(), True),
            StructField("last_name", StringType(), True), StructField("dob", DateType(), True),
            StructField("email_address", StringType(), True), StructField("phone_number", StringType(), True),
            StructField("subscription_plan", StringType(), True), StructField("subscription_start", DateType(), True),
            StructField("subscription_status", StringType(), False) # Not nullable due to when().otherwise()
        ])
        expected_data = [Row(
            user_id="101", first_name="John", last_name="Doe", dob=__import__("datetime").date(1990, 5, 15),
            email_address="john.doe@example.com", phone_number="123-456-7890",
            subscription_plan="premium", subscription_start=__import__("datetime").date(2023, 1, 10),
            subscription_status="active"
        )]
        expected_df = spark_session.createDataFrame(expected_data, expected_schema)

        assert are_dfs_equal(transformed_df, expected_df)

    def test_transform_data_empty_df(self, spark_session):
        """
        TC-007: Test transformation with an empty DataFrame.
        """
        source_schema = StructType([
            StructField("member_id", StringType()),
            StructField("personal_info", StructType([StructField("date_of_birth", StringType())])),
            StructField("subscription", StructType([StructField("status", StringType())]))
        ])
        # Create an empty DF with a subset of the schema
        empty_df = spark_session.createDataFrame([], source_schema)
        
        transformed_df = transform_data(empty_df)
        assert transformed_df.count() == 0
        assert "user_id" in transformed_df.columns
        assert "subscription_status" in transformed_df.columns

    def test_transform_data_null_status(self, spark_session):
        """
        TC-008: Test transformation where subscription_status is null.
        """
        source_data = [Row(
            member_id="102", personal_info=None, contact=None,
            subscription=Row(plan="basic", start_date="2022-11-01", status=None)
        )]
        source_df = spark_session.createDataFrame(source_data)
        
        transformed_df = transform_data(source_df)
        result_status = transformed_df.select("subscription_status").first()[0]
        assert result_status == "inactive"

    def test_transform_data_invalid_date(self, spark_session):
        """
        TC-009: Test transformation with invalid date formats.
        """
        source_data = [Row(
            member_id="103",
            personal_info=Row(first_name="Sam", last_name="Jones", date_of_birth="31-12-1995"), # Invalid format
            contact=None,
            subscription=Row(plan="free", start_date="not-a-date", status="active") # Invalid format
        )]
        source_df = spark_session.createDataFrame(source_data)

        transformed_df = transform_data(source_df)
        result_row = transformed_df.first()
        assert result_row["dob"] is None
        assert result_row["subscription_start"] is None

    def test_transform_data_null_nested_struct(self, spark_session):
        """
        TC-010: Test transformation with a null nested object.
        """
        source_data = [Row(
            member_id="104", personal_info=None, contact=None, subscription=None
        )]
        source_df = spark_session.createDataFrame(source_data)
        
        transformed_df = transform_data(source_df)
        result_row = transformed_df.first()
        assert result_row["first_name"] is None
        assert result_row["last_name"] is None
        assert result_row["email_address"] is None
        assert result_row["subscription_plan"] is None
        # Status becomes 'inactive' due to the null handling logic
        assert result_row["subscription_status"] == "inactive"

    @patch('data_pipeline.SparkSession', new_callable=MagicMock)
    def test_write_to_bigquery_mocked(self, mock_spark_session):
        """
        TC-011: Test that the BigQuery write operation is called correctly.
        This test mocks the DataFrameWriter to avoid actual GCP calls.
        """
        # Arrange: Create a mock DataFrame and its writer chain
        mock_df = MagicMock()
        mock_writer = MagicMock()
        mock_df.write.format.return_value = mock_writer
        mock_writer.option.return_value = mock_writer
        mock_writer.mode.return_value = mock_writer

        # Act: Call the function with test parameters
        project = "test-project"
        dataset = "test_dataset"
        table = "test_table"
        gcs_bucket = "test-bucket"
        write_to_bigquery(mock_df, project, dataset, table, gcs_bucket)

        # Assert: Verify that the methods were called with the correct arguments
        mock_df.write.format.assert_called_once_with("bigquery")
        mock_writer.option.assert_any_call("project", project)
        mock_writer.option.assert_any_call("temporaryGcsBucket", gcs_bucket)
        mock_writer.mode.assert_called_once_with("append")
        mock_writer.save.assert_called_once_with(f"{dataset}.{table}")

```

---

### **3. API Cost Calculation**

The cost is derived from the provided input script, as no tool is available to fetch real-time pricing data. The value is presented with full precision as requested.

```json
{
  "apiCost": 0.05
}
```