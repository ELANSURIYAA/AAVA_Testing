### 1. Test Case Document

| Test Case ID | Test Case Description                                                                 | Expected Outcome                                                                 |
|--------------|---------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| TC001        | Verify that the script calculates total revenue and average order value correctly.   | Total revenue and average order value match expected results based on input data.|
| TC002        | Validate ranking of categories within regions based on total revenue.                | Categories are ranked correctly within each region based on total revenue.       |
| TC003        | Ensure script handles NULL values in `quantity` and `price` gracefully.              | NULL values are excluded from calculations; no errors occur.                     |
| TC004        | Test script behavior with empty `sales` dataset.                                     | No results are returned; script executes without errors.                         |
| TC005        | Validate script behavior with invalid `order_date` format in `sales`.                | Script fails gracefully with appropriate error message.                          |
| TC006        | Verify filtering of sales data within the past 12 months.                           | Only sales within the past 12 months are included in calculations.               |
| TC007        | Test script with boundary conditions (e.g., exactly 12 months ago).                 | Sales exactly 12 months ago are included in calculations.                        |
| TC008        | Ensure correct handling of ties in category ranking.                                | Tied categories receive the same rank, and subsequent ranks are adjusted.        |

---

### 2. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

# Helper function to create test data
def create_test_data(spark, table_name, data, schema):
    df = spark.createDataFrame(data, schema)
    df.write.format("delta").mode("overwrite").saveAsTable(table_name)

# Pytest fixtures for setup and teardown
@pytest.fixture(scope="module")
def spark_session():
    spark = SparkSession.builder \
        .appName("Delta Unit Testing") \
        .config("spark.sql.catalogImplementation", "hive") \
        .enableHiveSupport() \
        .getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture(scope="module")
def setup_test_data(spark_session):
    # Create test data for sales, products, and regions tables
    sales_data = [
        (1, 101, 201, "2023-01-01", 2, 50.0),
        (2, 102, 202, "2023-06-01", 1, 100.0),
        (3, 103, 203, "2022-12-01", 3, 30.0),
    ]
    sales_schema = ["order_id", "product_id", "region_id", "order_date", "quantity", "price"]
    create_test_data(spark_session, "sales", sales_data, sales_schema)

    products_data = [
        (101, 301),
        (102, 302),
        (103, 303),
    ]
    products_schema = ["product_id", "category_id"]
    create_test_data(spark_session, "products", products_data, products_schema)

    regions_data = [
        (201, "North"),
        (202, "South"),
        (203, "East"),
    ]
    regions_schema = ["region_id", "region_name"]
    create_test_data(spark_session, "regions", regions_data, regions_schema)

    yield
    # Teardown: Drop test tables
    spark_session.sql("DROP TABLE IF EXISTS sales")
    spark_session.sql("DROP TABLE IF EXISTS products")
    spark_session.sql("DROP TABLE IF EXISTS regions")

# Test cases
def test_total_revenue_and_avg_order_value(spark_session, setup_test_data):
    query = """
    WITH FilteredSales AS (
        SELECT 
            s.order_id,
            s.product_id,
            p.category_id,
            r.region_id,
            s.order_date,
            CAST(s.quantity * s.price AS DECIMAL(10,2)) AS revenue
        FROM sales s
        JOIN products p ON s.product_id = p.product_id
        JOIN regions r ON s.region_id = r.region_id
        WHERE s.order_date >= DATEADD(MONTH, -12, CURRENT_DATE())
    ),
    CategoryRevenue AS (
        SELECT 
            f.region_id,
            f.category_id,
            SUM(f.revenue) AS total_revenue,
            AVG(f.revenue) AS avg_order_value
        FROM FilteredSales f
        GROUP BY f.region_id, f.category_id
    )
    SELECT 
        region_id,
        category_id,
        total_revenue,
        avg_order_value
    FROM CategoryRevenue
    """
    result = spark_session.sql(query)
    expected_data = [
        (201, 301, 100.0, 50.0),
        (202, 302, 100.0, 100.0),
    ]
    expected_schema = ["region_id", "category_id", "total_revenue", "avg_order_value"]
    expected_df = spark_session.createDataFrame(expected_data, expected_schema)
    assert result.collect() == expected_df.collect()

def test_category_ranking(spark_session, setup_test_data):
    query = """
    WITH FilteredSales AS (
        SELECT 
            s.order_id,
            s.product_id,
            p.category_id,
            r.region_id,
            s.order_date,
            CAST(s.quantity * s.price AS DECIMAL(10,2)) AS revenue
        FROM sales s
        JOIN products p ON s.product_id = p.product_id
        JOIN regions r ON s.region_id = r.region_id
        WHERE s.order_date >= DATEADD(MONTH, -12, CURRENT_DATE())
    ),
    CategoryRevenue AS (
        SELECT 
            f.region_id,
            f.category_id,
            SUM(f.revenue) AS total_revenue,
            AVG(f.revenue) AS avg_order_value
        FROM FilteredSales f
        GROUP BY f.region_id, f.category_id
    ),
    RankedCategories AS (
        SELECT 
            cr.region_id,
            cr.category_id,
            cr.total_revenue,
            cr.avg_order_value,
            RANK() OVER (PARTITION BY cr.region_id ORDER BY cr.total_revenue DESC) AS category_rank
        FROM CategoryRevenue cr
    )
    SELECT 
        region_id,
        category_id,
        category_rank
    FROM RankedCategories
    WHERE category_rank <= 3
    """
    result = spark_session.sql(query)
    expected_data = [
        (201, 301, 1),
        (202, 302, 1),
    ]
    expected_schema = ["region_id", "category_id", "category_rank"]
    expected_df = spark_session.createDataFrame(expected_data, expected_schema)
    assert result.collect() == expected_df.collect()

# Additional test cases for edge cases and error handling can be implemented similarly.
```

### 3. API Cost
**API Cost:** 0.0012 USD