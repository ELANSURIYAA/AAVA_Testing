**Test Case Document:**

1. **Test Case ID:** TC001  
   **Description:** Verify that the script correctly filters sales data for the last 12 months.  
   **Preconditions:** Sales data includes rows with `order_date` spanning more than 12 months.  
   **Test Steps:**  
      - Execute the `FilteredSales` CTE query.  
      - Check the `order_date` column values in the result.  
   **Expected Result:** The `FilteredSales` CTE should only include rows where `order_date` is within the last 12 months.  
   **Actual Result:** To be filled after execution.  
   **Pass/Fail Status:** To be filled after execution.

2. **Test Case ID:** TC002  
   **Description:** Validate the join logic between `sales`, `products`, and `regions` tables.  
   **Preconditions:** All tables contain valid data for joining.  
   **Test Steps:**  
      - Execute the `FilteredSales` CTE query.  
      - Check the resulting dataset for valid `category_id` and `region_id` for each `order_id`.  
   **Expected Result:** The resulting dataset should have valid `category_id` and `region_id` for each `order_id`.  
   **Actual Result:** To be filled after execution.  
   **Pass/Fail Status:** To be filled after execution.

3. **Test Case ID:** TC003  
   **Description:** Check the calculation of `revenue` in the `FilteredSales` CTE.  
   **Preconditions:** Sales data includes rows with valid `quantity` and `price` values.  
   **Test Steps:**  
      - Execute the `FilteredSales` CTE query.  
      - Validate the `revenue` column values against expected calculations.  
   **Expected Result:** The `revenue` column should correctly compute as `quantity * price` for each row.  
   **Actual Result:** To be filled after execution.  
   **Pass/Fail Status:** To be filled after execution.

4. **Test Case ID:** TC004  
   **Description:** Validate the aggregation logic in the `CategoryRevenue` CTE.  
   **Preconditions:** Filtered sales data includes rows with valid `revenue` values.  
   **Test Steps:**  
      - Execute the `CategoryRevenue` CTE query.  
      - Validate the `total_revenue` and `avg_order_value` columns against expected calculations.  
   **Expected Result:** The `total_revenue` and `avg_order_value` columns should correctly compute as `SUM(revenue)` and `AVG(revenue)` grouped by `region_id` and `category_id`.  
   **Actual Result:** To be filled after execution.  
   **Pass/Fail Status:** To be filled after execution.

5. **Test Case ID:** TC005  
   **Description:** Verify the ranking logic in the `RankedCategories` CTE.  
   **Preconditions:** Category revenue data includes rows with varying `total_revenue` values.  
   **Test Steps:**  
      - Execute the `RankedCategories` CTE query.  
      - Validate the `category_rank` column values against expected rankings.  
   **Expected Result:** Categories within each region should be ranked correctly based on `total_revenue` in descending order.  
   **Actual Result:** To be filled after execution.  
   **Pass/Fail Status:** To be filled after execution.

6. **Test Case ID:** TC006  
   **Description:** Ensure that only the top 3 categories per region are selected in the final output.  
   **Preconditions:** Ranked categories data includes rows with varying `category_rank` values.  
   **Test Steps:**  
      - Execute the final query.  
      - Validate the result dataset for rows where `category_rank <= 3`.  
   **Expected Result:** The final result should only include rows where `category_rank <= 3`.  
   **Actual Result:** To be filled after execution.  
   **Pass/Fail Status:** To be filled after execution.

---

**Pytest Script:**

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from datetime import datetime, timedelta

@pytest.fixture(scope="module")
def spark():
    return SparkSession.builder \
        .appName("DeltaUnitTests") \
        .master("local[*]") \
        .getOrCreate()

@pytest.fixture
def sales_data(spark):
    data = [
        (1, 101, 201, datetime.now() - timedelta(days=30), 2, 50.0),
        (2, 102, 202, datetime.now() - timedelta(days=400), 1, 100.0),
        (3, 103, 203, datetime.now() - timedelta(days=10), 5, 20.0),
    ]
    schema = ["order_id", "product_id", "region_id", "order_date", "quantity", "price"]
    return spark.createDataFrame(data, schema)

@pytest.fixture
def products_data(spark):
    data = [
        (101, 301),
        (102, 302),
        (103, 303),
    ]
    schema = ["product_id", "category_id"]
    return spark.createDataFrame(data, schema)

@pytest.fixture
def regions_data(spark):
    data = [
        (201, "North"),
        (202, "South"),
        (203, "East"),
    ]
    schema = ["region_id", "region_name"]
    return spark.createDataFrame(data, schema)

def test_filtered_sales(spark, sales_data, products_data, regions_data):
    sales_data.createOrReplaceTempView("sales")
    products_data.createOrReplaceTempView("products")
    regions_data.createOrReplaceTempView("regions")
    
    query = """
    WITH FilteredSales AS (
        SELECT 
            s.order_id,
            s.product_id,
            p.category_id,
            r.region_id,
            s.order_date,
            CAST(s.quantity * s.price AS DECIMAL(10,2)) AS revenue
        FROM sales s
        JOIN products p ON s.product_id = p.product_id
        JOIN regions r ON s.region_id = r.region_id
        WHERE s.order_date >= DATE_ADD(CURRENT_DATE(), -365)
    )
    SELECT * FROM FilteredSales
    """
    result = spark.sql(query)
    assert result.count() == 2  # Only 2 rows should be within the last 12 months

def test_category_revenue(spark, sales_data, products_data, regions_data):
    sales_data.createOrReplaceTempView("sales")
    products_data.createOrReplaceTempView("products")
    regions_data.createOrReplaceTempView("regions")
    
    query = """
    WITH FilteredSales AS (
        SELECT 
            s.order_id,
            s.product_id,
            p.category_id,
            r.region_id,
            s.order_date,
            CAST(s.quantity * s.price AS DECIMAL(10,2)) AS revenue
        FROM sales s
        JOIN products p ON s.product_id = p.product_id
        JOIN regions r ON s.region_id = r.region_id
        WHERE s.order_date >= DATE_ADD(CURRENT_DATE(), -365)
    ),
    CategoryRevenue AS (
        SELECT 
            f.region_id,
            f.category_id,
            SUM(f.revenue) AS total_revenue,
            AVG(f.revenue) AS avg_order_value
        FROM FilteredSales f
        GROUP BY f.region_id, f.category_id
    )
    SELECT * FROM CategoryRevenue
    """
    result = spark.sql(query)
    assert result.count() == 3  # 3 categories should be present
    assert result.filter(col("region_id") == 201).count() == 1  # Validate specific region

# Additional test cases can be implemented similarly for other scenarios.

```

**API Cost:** 0.0012 USD