=============================================
Author:  Ascendion AAVA
Created on:   (Leave it empty)
Description:   Load summarized holding metrics from staging to fact table in Synapse DW
=============================================

**1. Workflow Overview**

The `LOAD_FACT_EXECUTIVE_SUMMARY` stored procedure is a data integration workflow that loads summarized holding metrics from the staging table (`STG_HOLDING_METRICS`) into the fact table (`FACT_EXECUTIVE_SUMMARY`). The workflow supports executive reporting and analytics by providing cleansed, validated, and integrated summary data. The key business objective is to maintain data quality while ensuring referential integrity through dimension table joins, applying business rules for data validation, and providing audit logging for monitoring purposes.

**2. Complexity Metrics**

| Metric | Description |
|--------|-------------|
| Number of Input Tables | 5 (STG_HOLDING_METRICS, DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT) |
| Number of Output Tables | 1 (FACT_EXECUTIVE_SUMMARY) |
| Variable Declarations | 2 (@v_row_count, @error_message) |
| Conditional Logic | 1 (CASE statement for income_amount validation) |
| Loop Constructs | 0 |
| Join Conditions | 4 (INNER JOINs with dimension tables) |
| Aggregations | 0 |
| Subqueries / CTEs | 0 |
| Procedural Calls | 0 |
| DML Operations | 1 (INSERT INTO statement) |
| Temporary Tables / Table Variables | 1 (#staging_metrics) |
| Transaction Handling | 0 |
| Error Handling Blocks | 0 (basic error handling with @error_message variable) |
| Complexity Score (0â€“100) | 25 |

**High-complexity areas:**
- Multiple dimension table joins requiring referential integrity validation
- Temporary table creation and management
- Business rule application for data quality (income_amount transformation)

**3. Syntax Differences**

Key T-SQL constructs in Synapse that require conversion to Fabric code equivalents:

- **DECLARE statements**: Variable declarations (@v_row_count, @error_message) must be replaced with pipeline parameters or system variables in Fabric
- **PRINT statements**: Logging statements need conversion to Fabric pipeline logging or monitoring activities
- **Temporary tables (#staging_metrics)**: T-SQL temp tables must be replaced with Dataflow staging entities or Lakehouse temporary tables
- **INSERT INTO ... SELECT ... JOIN**: Complex INSERT with joins requires conversion to Dataflow with multiple sources and join transformations
- **CASE WHEN logic**: Conditional transformations need implementation as Dataflow conditional column expressions
- **@@ROWCOUNT**: System variable for row count tracking must use Fabric activity output variables
- **IF OBJECT_ID ... DROP TABLE**: Object existence checks and cleanup require explicit Dataflow deletion activities
- **SET NOCOUNT ON**: Session settings are not applicable in Fabric declarative approach

**4. Manual Adjustments**

Components requiring manual implementation in Fabric code:

- **Temporary table management**: Replace T-SQL temporary table logic with Dataflow staging entities or Lakehouse temporary storage
- **Variable handling**: Convert procedural variables to pipeline parameters and system variables for state management
- **Logging and monitoring**: Implement custom logging using Fabric pipeline monitoring or dedicated monitoring tables instead of PRINT statements
- **Error handling**: Design error handling workflows using Fabric pipeline failure paths and exception handling activities
- **Join configuration**: Manually configure four dimension table joins in Dataflow UI with proper join conditions
- **Data quality validation**: Implement income_amount business rule as Dataflow conditional transformation
- **Audit trail**: Create audit logging mechanism using pipeline activity outputs and monitoring capabilities
- **Cleanup operations**: Add explicit cleanup activities for persistent staging entities if required

**5. Optimization Techniques**

Recommended Fabric code best practices and optimizations:

- **Modular design**: Break down the monolithic stored procedure into sequential Dataflow activities for better maintainability and parallel processing capabilities
- **Partitioning strategy**: Implement data partitioning in Dataflow sources for large tables to enable parallel processing and improve performance
- **Query pushdown**: Utilize Dataflow query pushdown capabilities to perform filtering and initial joins at the source level, reducing data movement
- **Incremental processing**: Implement watermarking or change data capture to process only new or modified records instead of full table scans
- **Resource optimization**: Configure appropriate Capacity Units (CU) allocation based on data volume and processing requirements
- **Pipeline monitoring**: Leverage built-in Fabric monitoring and alerting for comprehensive audit trails and error detection
- **Staging optimization**: Use Lakehouse temporary tables for large staging operations and Dataflow staging entities for smaller datasets

**Recommendation**: **Refactor** approach - retain the core business logic while restructuring for Fabric's declarative, activity-based architecture. The relatively low complexity score (25) and straightforward transformation logic make this suitable for systematic conversion rather than complete rebuild.

**6. API Cost Consumption**

apiCost: 0.0523 USD