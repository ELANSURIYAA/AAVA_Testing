=============================================
Author:  Ascendion AAVA
Date:   (Leave it empty)
Description:   Load Fact table with summarized holding metrics from staging data with business rule validation and referential integrity checks
=============================================

**1. Workflow Overview**

The Azure Synapse stored procedure `dbo.LOAD_FACT_EXECUTIVE_SUMMARY` is designed to populate the `FACT_EXECUTIVE_SUMMARY` fact table from the `STG_HOLDING_METRICS` staging table. The key business objective is **data integration and transformation** for executive reporting. The procedure performs data quality validation, applies business rules (such as handling negative or null income amounts), and ensures referential integrity by joining with four dimension tables (`DIM_DATE`, `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`). This supports executive summary reporting by providing accurate, validated, and enriched holding metrics data for business intelligence and regulatory reporting purposes.

**2. Complexity Metrics**

| Metric | Description |
|--------|-------------|
| Number of Input Tables | 5 (1 staging table + 4 dimension tables) |
| Number of Output Tables | 1 (FACT_EXECUTIVE_SUMMARY) |
| Variable Declarations | 2 variables (@v_row_count, @error_message) with moderate usage |
| Conditional Logic | 1 CASE statement for income_amount business rule validation |
| Loop Constructs | 0 (no WHILE or FOR loops present) |
| Join Conditions | 4 INNER JOINs to dimension tables for referential integrity |
| Aggregations | 0 (direct column mapping without aggregation operations) |
| Subqueries / CTEs | 0 in original, converted to 2 CTEs in Fabric code |
| Procedural Calls | 0 (no nested stored procedure invocations) |
| DML Operations | 1 INSERT operation with SELECT statement |
| Temporary Tables / Table Variables | 1 temporary table (#staging_metrics) |
| Transaction Handling | 0 (no explicit BEGIN TRAN, COMMIT, ROLLBACK) |
| Error Handling Blocks | 0 (no TRY...CATCH logic implemented) |
| Complexity Score (0â€“100) | 35 (moderate complexity due to multiple joins and procedural structure) |

**High-complexity areas identified:**
- Multiple dimension table joins requiring referential integrity validation
- Temporary table creation and management
- Procedural control flow with variable declarations and PRINT statements
- Business rule implementation through conditional logic

**3. Syntax Differences**

**T-SQL constructs requiring conversion:**
- **Variable Declarations**: `DECLARE @v_row_count INT = 0` and `DECLARE @error_message NVARCHAR(4000)` must be replaced with Fabric notebook variables or removed entirely
- **Procedural Structure**: `CREATE OR ALTER PROCEDURE...BEGIN...END` blocks need conversion to declarative SQL or Fabric notebook cells
- **Control Flow**: `SET NOCOUNT ON`, `PRINT` statements, and `@@ROWCOUNT` system variables have no direct Fabric equivalents
- **Temporary Tables**: `#staging_metrics` temporary table syntax needs conversion to CTEs or staging views
- **System Functions**: `OBJECT_ID('tempdb..#staging_metrics')` and `@@ROWCOUNT` require alternative approaches

**Required syntax changes:**
- Replace procedural wrapper with CTE-based declarative SQL
- Convert temporary table to CTE (`WITH staging_metrics AS`)
- Remove variable declarations and replace with logging mechanisms
- Transform PRINT statements to Python logging in Fabric notebooks
- Replace `@@ROWCOUNT` with alternative row counting methods

**Data type considerations:**
- All existing data types (INT, NVARCHAR) are compatible with Fabric
- No decimal precision adjustments required
- Date handling remains consistent between platforms

**4. Manual Adjustments**

**Components requiring manual implementation:**
- **Logging and Monitoring**: PRINT statements need replacement with Fabric notebook logging or pipeline monitoring
- **Error Handling**: Implement try-catch logic in Python cells or pipeline error handling
- **Row Count Auditing**: Replace `@@ROWCOUNT` with explicit COUNT queries or pipeline metrics
- **Execution Orchestration**: Manual setup of Fabric pipeline or notebook scheduling
- **Performance Monitoring**: Implement custom monitoring for execution time and resource usage

**External dependencies:**
- Validation of dimension table availability and structure in Fabric environment
- Confirmation of table permissions and access controls
- Setup of workspace security and service principal authentication
- Configuration of pipeline triggers and scheduling mechanisms

**Business rule validation requirements:**
- Verify income_amount CASE logic produces identical results
- Validate referential integrity enforcement through INNER JOINs
- Confirm data quality checks maintain same standards as original procedure
- Test edge cases for NULL and negative value handling

**5. Optimization Techniques**

**Fabric code best practices applied:**
- **Declarative SQL Structure**: Converted procedural logic to CTE-based declarative queries for better optimization
- **Simplified Logic Flow**: Eliminated temporary tables in favor of CTEs, reducing I/O operations and improving performance
- **Modular Query Design**: Separated staging data preparation and transformation logic into distinct CTEs for better readability and maintenance

**Performance optimization recommendations:**
- **Predicate Pushdown**: Leverage Fabric's query optimizer by placing filter conditions early in CTE chain
- **Join Optimization**: Maintain INNER JOINs for referential integrity while allowing Fabric to optimize join order
- **Partitioning Strategy**: Consider partitioning fact table by date_key for improved query performance on large datasets
- **Incremental Loading**: Implement date-based filtering to process only new or changed records

**Recommended approach: Refactor**
- Retain core business logic and data transformation rules
- Convert procedural constructs to declarative SQL patterns
- Maintain existing join strategies and business rules
- Enhance with Fabric-native logging and monitoring capabilities
- Preserve data lineage and audit trail functionality through pipeline metadata

**Additional optimizations:**
- Combine dimension lookups into single query execution plan
- Eliminate intermediate materializations through CTE chaining
- Implement column-level lineage tracking in Fabric workspace
- Consider implementing data quality checks as separate validation steps

**apiCost: 0.0523 USD**