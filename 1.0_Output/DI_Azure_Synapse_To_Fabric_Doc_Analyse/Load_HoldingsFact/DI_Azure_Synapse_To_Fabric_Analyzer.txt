=============================================
Author:  Ascendion AAVA
Date:   (Leave it empty)
Description:   Loads the FACT_EXECUTIVE_SUMMARY table with summarized holding metrics from staging, applying business rules and referential integrity checks.
=============================================

**1. Workflow Overview**

The Azure Synapse stored procedure `dbo.LOAD_FACT_EXECUTIVE_SUMMARY` is designed to populate a fact table with summarized holding metrics for executive reporting and analytics. The procedure supports regulatory compliance and business intelligence by extracting data from the staging table `STG_HOLDING_METRICS`, applying data quality validations, enforcing referential integrity through joins with four dimension tables (`DIM_DATE`, `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`), and loading the cleansed data into the `FACT_EXECUTIVE_SUMMARY` table. The key business objective is to provide a reliable, validated dataset for executive dashboards, regulatory reporting, and downstream analytics processes.

**2. Complexity Metrics**

| Metric | Description |
|--------|-------------|
| Number of Input Tables | 5 (STG_HOLDING_METRICS, DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT) |
| Number of Output Tables | 1 (FACT_EXECUTIVE_SUMMARY) |
| Variable Declarations | 2 variables (@v_row_count, @error_message) with moderate usage complexity |
| Conditional Logic | 1 CASE statement for income_amount validation |
| Loop Constructs | 0 (No WHILE or FOR loops present) |
| Join Conditions | 4 INNER JOINs connecting staging data to dimension tables |
| Aggregations | 0 (No SUM, COUNT, AVG operations) |
| Subqueries / CTEs | 0 (No subqueries or Common Table Expressions) |
| Procedural Calls | 0 (No stored procedure or function invocations) |
| DML Operations | 1 INSERT operation with SELECT |
| Temporary Tables / Table Variables | 1 temporary table (#staging_metrics) |
| Transaction Handling | 0 (No explicit BEGIN TRAN, COMMIT, ROLLBACK) |
| Error Handling Blocks | 0 (No TRY...CATCH logic) |
| Complexity Score (0â€“100) | 25 (Low to Moderate complexity) |

High-complexity areas include:
- Multiple dimension table joins requiring referential integrity validation
- Temporary table management and cleanup
- Business rule enforcement through conditional logic

**3. Syntax Differences**

Key T-SQL constructs in Synapse that require conversion to Fabric code equivalents:

- **Variable Declarations**: `DECLARE @v_row_count INT` must be replaced with Python variables in Fabric code
- **Procedural Structure**: `CREATE OR ALTER PROCEDURE...BEGIN...END` replaced with Python function definitions
- **Temporary Tables**: `#staging_metrics` temporary table replaced with Spark DataFrame caching mechanisms
- **Print Statements**: `PRINT` statements converted to Python `print()` functions
- **Row Count Capture**: `@@ROWCOUNT` replaced with DataFrame `.count()` method
- **Object Existence Check**: `OBJECT_ID('tempdb..#staging_metrics')` replaced with DataFrame unpersist operations
- **SQL Execution**: Direct T-SQL replaced with `spark.sql()` or DataFrame API operations
- **Table Operations**: `SELECT * INTO` replaced with DataFrame transformations and `.saveAsTable()` operations
- **SET NOCOUNT ON**: No direct equivalent needed in Fabric code as it's handled automatically

**4. Manual Adjustments**

Components requiring manual implementation in Fabric code:

- **Error Handling**: Original procedure lacks TRY...CATCH blocks, requiring manual addition of Python exception handling for production readiness
- **Transaction Management**: No explicit transaction handling in original code; may need manual implementation of checkpointing or rollback mechanisms in Fabric
- **Logging Framework**: Print statements need integration with Fabric's logging infrastructure for proper monitoring and alerting
- **Performance Optimization**: Manual tuning of Spark configurations, partitioning strategies, and caching mechanisms
- **Data Type Validation**: Ensure data type compatibility between Synapse and Fabric environments, particularly for DECIMAL precision and DATETIME formats
- **Connection Management**: Manual configuration of data source connections and authentication in Fabric workspace
- **Scheduling Integration**: Integration with Fabric pipeline orchestration to replace stored procedure execution triggers
- **Monitoring and Alerting**: Implementation of custom monitoring for job success/failure notifications

**5. Optimization Techniques**

Recommended Fabric code best practices for optimization:

- **Modular Query Design**: Break down the single INSERT statement into sequential CTEs for better readability and debugging capabilities
- **DataFrame Caching**: Implement strategic caching of frequently accessed dimension tables to reduce repeated reads
- **Predicate Pushdown**: Optimize joins by applying filters early in the transformation pipeline to reduce data movement
- **Partitioning Strategy**: Implement appropriate partitioning on date_key or institution_id for improved query performance
- **Broadcast Joins**: Use broadcast hints for smaller dimension tables to optimize join performance
- **Column Pruning**: Select only required columns early in the transformation to minimize memory usage
- **Coalesce Operations**: Optimize the number of output partitions to prevent small file problems

**Recommendation**: **Refactor** approach is suitable for this conversion. The logic is straightforward with standard joins and minimal business rules, making it ideal for retaining most of the original logic structure while adapting to Fabric code syntax. The procedural elements are minimal and can be easily converted to functional programming patterns without requiring a complete rebuild.

**6. API Cost Consumption**

apiCost: 0.0025 USD