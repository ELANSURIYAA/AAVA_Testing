=============================================
Author:  Ascendion AAVA
Created on:   (Leave it empty)
Description:   Loads the FACT_EXECUTIVE_SUMMARY table with summarized holding metrics from staging, applying business rules and ensuring referential integrity.
=============================================

## 1. Workflow Overview

The Azure Synapse stored procedure `dbo.LOAD_FACT_EXECUTIVE_SUMMARY` is designed to populate a fact table with aggregated holding metrics for executive reporting and analytics. The procedure extracts data from the staging table `STG_HOLDING_METRICS`, applies data quality validation and business rules, ensures referential integrity through dimension table joins, and loads the cleansed data into the `FACT_EXECUTIVE_SUMMARY` fact table. This workflow supports executive dashboards, regulatory compliance reporting, and downstream business intelligence applications by providing a reliable, validated dataset of holding metrics.

## 2. Complexity Metrics

| Metric | Description |
|--------|-------------|
| Number of Input Tables | 5 (STG_HOLDING_METRICS, DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT) |
| Number of Output Tables | 1 (FACT_EXECUTIVE_SUMMARY) |
| Variable Declarations | 2 variables (@v_row_count, @error_message) with moderate usage complexity |
| Conditional Logic | 1 CASE statement for income_amount validation |
| Loop Constructs | 0 (No WHILE or FOR loops present) |
| Join Conditions | 4 INNER JOINs to dimension tables |
| Aggregations | 0 (Direct field mapping without aggregation operations) |
| Subqueries / CTEs | 0 (No subqueries or Common Table Expressions) |
| Procedural Calls | 0 (No stored procedure or function invocations) |
| DML Operations | 1 INSERT operation with SELECT |
| Temporary Tables / Table Variables | 1 temporary table (#staging_metrics) |
| Transaction Handling | 0 (No explicit BEGIN TRAN, COMMIT, ROLLBACK statements) |
| Error Handling Blocks | 0 (No TRY...CATCH logic present) |
| Complexity Score (0â€“100) | 35 |

**High-complexity areas identified:**
- Multiple dimension table joins requiring referential integrity validation
- Temporary table creation and management
- Business rule application for data quality (income_amount validation)

## 3. Syntax Differences

**T-SQL Constructs requiring conversion:**
- **Variable Declarations**: `DECLARE @v_row_count INT` syntax needs to be replaced with Fabric code parameter handling
- **Procedural Structure**: `CREATE OR ALTER PROCEDURE` and `BEGIN...END` blocks must be converted to Fabric code notebook or pipeline structure
- **Temporary Tables**: `#staging_metrics` temporary table creation using `SELECT * INTO` needs conversion to Fabric code DataFrame operations
- **System Functions**: `@@ROWCOUNT` system variable requires replacement with Fabric code row counting methods
- **PRINT Statements**: Logging via `PRINT` statements need conversion to Fabric code logging mechanisms
- **Object Existence Checks**: `IF OBJECT_ID('tempdb..#staging_metrics') IS NOT NULL` requires Fabric code equivalent checks
- **SET NOCOUNT ON**: This T-SQL specific setting has no direct Fabric code equivalent

**Data Type Considerations:**
- All existing data types (INT, NVARCHAR) are compatible with Fabric code
- Date handling should remain consistent between platforms

## 4. Manual Adjustments

**Components requiring manual implementation:**
- **Logging Strategy**: Replace PRINT statements with Fabric code logging framework or notebook output cells
- **Error Handling**: Implement proper exception handling using try-catch blocks in Fabric code notebooks
- **Parameter Management**: Convert procedure variables to Fabric code notebook parameters or pipeline variables
- **Orchestration Integration**: Manual setup of Fabric code pipeline activities to replace stored procedure calls
- **Performance Monitoring**: Implement custom row count tracking and execution monitoring
- **Temporary Storage**: Replace temporary tables with DataFrame operations or temporary views in Fabric code
- **Business Rule Validation**: Verify that the income_amount CASE logic produces identical results in Fabric code environment

**External Dependencies:**
- Dimension table availability and schema consistency verification
- Source staging table data quality and format validation
- Target fact table schema alignment confirmation

## 5. Optimization Techniques

**Fabric Code Best Practices:**
- **Modular Design**: Break the single procedure into multiple Fabric code notebook cells for better maintainability and debugging
- **DataFrame Operations**: Replace temporary table operations with efficient DataFrame transformations using Spark SQL
- **Predicate Pushdown**: Optimize dimension table joins by applying filters early in the query execution
- **Caching Strategy**: Cache frequently accessed dimension tables in memory for improved join performance
- **Partitioning**: Consider partitioning the fact table by date_key for improved query performance
- **Incremental Loading**: Implement incremental data processing instead of full table operations where applicable
- **Resource Management**: Optimize Spark cluster configuration for the expected data volume and processing requirements

**Transformation Approach**: **Refactor** - The logic is straightforward enough to retain most of the existing structure while adapting to Fabric code syntax and best practices. The core business logic (joins and CASE statement) can be preserved with minimal changes.

**Performance Optimizations:**
- Combine the staging table creation and main INSERT operation into a single query
- Use broadcast joins for smaller dimension tables
- Implement column pruning to reduce data movement during joins

## 6. API Cost Consumption

```
apiCost: 0.0045 USD
```