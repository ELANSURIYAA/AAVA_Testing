=============================================
Author:  Ascendion AAVA
Date:   (Leave it empty)
Description:   Analysis and conversion of Azure Synapse LOAD_FACT_EXECUTIVE_SUMMARY stored procedure to Microsoft Fabric code
=============================================

**1. Workflow Overview**

The `LOAD_FACT_EXECUTIVE_SUMMARY` stored procedure is a data integration and transformation component that supports executive reporting and analytics for institutional holdings. The procedure performs the following key business functions:

- **Data Staging**: Copies all records from `STG_HOLDING_METRICS` to a temporary staging table for processing
- **Data Validation**: Ensures referential integrity by joining with four dimension tables (DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT)
- **Business Rule Application**: Applies data quality rules, specifically transforming income_amount values (setting to 0 if null or negative)
- **Fact Table Population**: Inserts validated and transformed records into the FACT_EXECUTIVE_SUMMARY table
- **Audit Logging**: Captures row counts for monitoring and audit purposes
- **Resource Cleanup**: Drops temporary tables after processing completion

The procedure supports executive reporting by aggregating and validating holding metrics while maintaining data quality and referential integrity standards.

**2. Complexity Metrics**

| Metric | Description |
|--------|-------------|
| Number of Input Tables | 5 (STG_HOLDING_METRICS + 4 dimension tables) |
| Number of Output Tables | 1 (FACT_EXECUTIVE_SUMMARY) |
| Variable Declarations | 2 variables (@v_row_count, @error_message) with moderate usage complexity |
| Conditional Logic | 1 CASE statement for income_amount transformation |
| Loop Constructs | 0 (no WHILE or FOR loops present) |
| Join Conditions | 4 INNER JOINs with dimension tables |
| Aggregations | 0 (no SUM, COUNT, AVG operations in main logic) |
| Subqueries / CTEs | 0 (straightforward SELECT with JOINs) |
| Procedural Calls | 0 (no nested stored procedure calls) |
| DML Operations | 2 (SELECT INTO for temp table, INSERT INTO for fact table) |
| Temporary Tables / Table Variables | 1 (#staging_metrics temporary table) |
| Transaction Handling | 0 (no explicit BEGIN TRAN, COMMIT, ROLLBACK) |
| Error Handling Blocks | 0 (no TRY...CATCH blocks, only PRINT statements) |
| Complexity Score (0â€“100) | 35 |

High-complexity areas identified:
- Multiple table joins requiring careful performance optimization in Fabric
- Temporary table usage that needs conversion to Fabric-appropriate constructs
- Referential integrity validation across four dimension tables

**3. Syntax Differences**

Key T-SQL constructs in Synapse that require conversion for Fabric code:

- **Temporary Tables**: `#staging_metrics` syntax must be replaced with Spark DataFrames or temporary views (`CREATE OR REPLACE TEMP VIEW`)
- **Variable Declarations**: `DECLARE @v_row_count INT` needs conversion to Python variables or Spark SQL variables
- **Row Count Capture**: `@@ROWCOUNT` system variable requires replacement with DataFrame `.count()` operations
- **Print Statements**: `PRINT` commands need conversion to Python `print()` or notebook logging mechanisms
- **Object Existence Checks**: `OBJECT_ID('tempdb..#staging_metrics')` requires conversion to Spark catalog operations
- **SELECT INTO**: `SELECT * INTO #staging_metrics` syntax needs replacement with DataFrame operations or `CREATE TABLE AS SELECT`
- **Procedural Structure**: `CREATE OR ALTER PROCEDURE` wrapper needs conversion to Fabric notebook cells or pipeline activities

Data type considerations:
- Most data types have direct equivalents between Synapse and Fabric
- Date handling may require attention for `date_key` and `date_value` mappings
- Numeric precision should be validated for financial amounts

**4. Manual Adjustments**

Components requiring manual implementation in Fabric code:

- **Temporary Table Conversion**: Replace `#staging_metrics` with Spark DataFrame or temporary view, requiring manual code restructuring
- **Variable Management**: Convert T-SQL variables to Python variables with appropriate scoping across notebook cells
- **Error Handling**: Implement try/except blocks in Python to replace basic PRINT-based error messaging
- **Audit Logging**: Develop custom logging mechanism using DataFrame operations or dedicated audit tables
- **Procedural Flow Control**: Convert stored procedure structure to sequential notebook cells or pipeline activities
- **Row Count Validation**: Implement custom row count capture and validation logic using Spark operations

External dependencies requiring validation:
- Verify dimension table availability and schema compatibility in Fabric environment
- Confirm staging table `STG_HOLDING_METRICS` structure matches source expectations
- Validate business rule logic for income_amount transformation maintains same behavior
- Ensure referential integrity constraints are properly enforced through join logic

**5. Optimization Techniques**

Recommended Fabric code best practices for optimization:

- **Partitioning Strategy**: Implement partitioning on `date_key` for the fact table to improve query performance and join efficiency
- **Broadcast Joins**: Configure smaller dimension tables (DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT) as broadcast joins to optimize performance
- **Temporary View Usage**: Replace temporary table with `CREATE OR REPLACE TEMP VIEW` to leverage Spark's lazy evaluation and memory management
- **Sequential CTE Approach**: Convert the staging logic into a series of CTEs to simplify the data flow and improve readability
- **Predicate Pushdown**: Structure joins to enable predicate pushdown optimization, particularly for date-based filtering
- **Column Pruning**: Select only required columns in intermediate steps to reduce memory usage and improve performance
- **Caching Strategy**: Cache intermediate DataFrames if the staging data will be reused multiple times

Recommended approach: **Refactor** - The existing logic is well-structured and can be retained with syntax conversions rather than requiring a complete rebuild. The straightforward join pattern and single transformation make it suitable for direct conversion to Fabric code while applying the optimization techniques mentioned above.

**6. API Cost Consumption**

apiCost: 0.0045 USD