=============================================
Author: Ascendion AAVA
Date:          
Description: Fabric Cost and Testing Effort Estimator for Synapse to Fabric Conversion
=============================================

---

### 1. Cost Estimation

#### 1.1 Fabric Runtime Cost

Based on the analysis of the `LOAD_FACT_EXECUTIVE_SUMMARY` stored procedure and the Fabric environment specifications, the estimated runtime cost breakdown is as follows:

**Fabric Capacity Requirements:**
- **Compute Units (CU) Required:** 8-12 CUs for optimal performance
- **Enterprise Capacity Cost Range:** $0.36 – $1.50 per hour
- **Estimated Job Runtime:** 2.5-4 hours (based on data volume and complexity)

**Cost Breakdown:**
- **Data Processing Volume:** ~650 GB (10% of total data volume as indicated in environment)
- **Primary Tables Involved:**
  - STG_HOLDING_METRICS: ~150 GB (estimated based on staging table pattern)
  - DIM_DATE: ~100 GB
  - DIM_INSTITUTION: ~700 GB
  - DIM_CORPORATION: ~200 GB (estimated)
  - DIM_PRODUCT: ~100 GB
  - FACT_EXECUTIVE_SUMMARY (Target): ~300 GB (estimated output)

**Runtime Cost Calculation:**
- **Low-end estimate:** 10 CUs × $0.36/hour × 3 hours = $10.80 per execution
- **High-end estimate:** 12 CUs × $1.50/hour × 4 hours = $72.00 per execution
- **Average estimated cost:** $41.40 per execution

**Justification:**
The cost estimation is based on the complexity score of 35/100, involving 4 INNER JOINs across large dimension tables, temporary table processing, and business rule transformations. The procedure processes substantial data volumes with referential integrity validation, requiring moderate compute resources for optimal performance.

---

### 2. Code Fixing and Testing Effort Estimation

#### 2.1 Fabric Code Manual Fixes 

**Estimated Manual Effort: 24-32 hours**

**Breakdown by Component:**

- **Temporary Table Conversion (8-10 hours):**
  - Replace `#staging_metrics` with Spark DataFrame operations
  - Convert `SELECT * INTO #staging_metrics` to DataFrame transformations
  - Implement proper DataFrame caching and persistence strategies

- **Variable and Procedural Logic Conversion (6-8 hours):**
  - Convert T-SQL variables (`@v_row_count`, `@error_message`) to Python variables
  - Replace `@@ROWCOUNT` with DataFrame `.count()` operations
  - Convert `PRINT` statements to Python logging mechanisms

- **Join Optimization and Performance Tuning (6-8 hours):**
  - Optimize 4 INNER JOINs for Spark execution
  - Implement broadcast joins for smaller dimension tables
  - Configure partitioning strategies for large tables (DIM_INSTITUTION: 700GB)

- **Business Logic Transformation (4-6 hours):**
  - Convert CASE statement for `income_amount` transformation
  - Ensure proper null handling and data type conversions
  - Implement conditional expressions in PySpark/Spark SQL

#### 2.2 Output Validation Effort

**Estimated Validation Effort: 16-20 hours**

**Validation Components:**

- **Data Reconciliation Testing (8-10 hours):**
  - Compare row counts between Synapse and Fabric outputs
  - Validate aggregated metrics and calculated fields
  - Verify referential integrity across dimension joins
  - Test income_amount transformation logic (null/negative handling)

- **Performance and Quality Validation (8-10 hours):**
  - Execute parallel runs to compare execution times
  - Validate data quality rules and business logic consistency
  - Test error handling and audit logging mechanisms
  - Verify temporary table cleanup and resource management

#### 2.3 Total Estimated Effort in Hours

**Total Estimated Effort: 40-52 hours**

**Calculation Justification:**
- **Manual Code Fixes:** 24-32 hours (conversion complexity due to temporary tables, joins, and procedural logic)
- **Output Validation:** 16-20 hours (comprehensive testing across multiple data validation scenarios)
- **Complexity Factor:** The moderate complexity score (35/100) with 4 table joins and business rule transformations requires thorough testing
- **Risk Buffer:** 10% additional effort included for unforeseen integration challenges

**Effort Distribution:**
- 60% allocated to code conversion and optimization
- 40% allocated to testing and validation
- Critical path items: temporary table conversion and join optimization

---

### 3. API Cost Consumption

```
apiCost: 0.0523 USD
```