=============================================
Author: Ascendion AAVA
Created on: 
Description: Test case design and Pytest scripts for validating Synapse to Databricks Lakeflow pipeline conversion for FACT_EXECUTIVE_SUMMARY load
=============================================

**Test Case List:**

| Test Case ID | Test Case Description | Expected Outcome |
|--------------|----------------------|------------------|
| TC01 | Validate column expression mapping from Synapse to Lakeflow notebooks | All column-level transformations (including CASE for income_amount) match expected logic |
| TC02 | Validate aggregation logic mapping (groupBy/window functions) | Aggregated output consistent with Synapse results (none in this pipeline, but placeholder for future) |
| TC03 | Validate join strategy equivalence | Join outputs (INNER JOINs to dimension tables) are identical to Synapse logic |
| TC04 | Validate data type conversions between Synapse and Lakeflow | Schema matches expected PySpark/SQL types (e.g., DECIMAL → DoubleType, DATE → DateType) |
| TC05 | Validate null handling and case sensitivity | Nulls handled correctly (income_amount normalization), column names consistent |
| TC06 | Validate orchestration flow in Lakeflow | Notebooks execute in correct sequence with correct dependencies (staging, transform/load, audit/cleanup) |
| TC07 | Validate manual interventions and performance optimizations | Adjusted logic (e.g., temp table to temp view, print to logging) produces correct results and improved performance |

**Pytest Script for Each Test Case**

```python
import pytest
from pyspark.sql import SparkSession

@pytest.fixture(scope="module")
def spark():
    return SparkSession.builder.appName("Lakeflow_Test").getOrCreate()

def test_expression_mapping(spark):
    # Validate column-level transformations, especially income_amount normalization
    df_synapse = spark.read.parquet("testdata/synapse_fact_executive_summary.parquet")
    df_lakeflow = spark.read.parquet("testdata/lakeflow_fact_executive_summary.parquet")
    # Compare all columns except for possible audit/logging columns
    cols = [c for c in df_synapse.columns if c in df_lakeflow.columns]
    assert df_synapse.select(cols).collect() == df_lakeflow.select(cols).collect()

def test_aggregation_logic(spark):
    # Placeholder: No aggregation in current pipeline, but test for future extensibility
    assert True

def test_join_strategy_equivalence(spark):
    # Validate that joins to dimension tables produce identical results
    df_synapse = spark.read.parquet("testdata/synapse_fact_executive_summary.parquet")
    df_lakeflow = spark.read.parquet("testdata/lakeflow_fact_executive_summary.parquet")
    assert df_synapse.orderBy("date_key", "institution_id").collect() == \
           df_lakeflow.orderBy("date_key", "institution_id").collect()

def test_data_type_conversions(spark):
    # Validate schema alignment
    df_lakeflow = spark.read.parquet("testdata/lakeflow_fact_executive_summary.parquet")
    expected_schema = {
        "date_key": "int",
        "institution_id": "int",
        "corporation_id": "int",
        "product_id": "int",
        "a120_amount": "double",
        "a120_count": "int",
        "a30_to_59_amount": "double",
        "a30_to_59_count": "int",
        "a60_to_89_amount": "double",
        "a60_to_89_count": "int",
        "a90_to_119_amount": "double",
        "a90_to_119_count": "int",
        "charge_off_amount": "double",
        "charge_off_count": "int",
        "fraud_amount": "double",
        "fraud_count": "int",
        "income_amount": "double",
        "number_of_accounts": "int",
        "purchases_amount": "double",
        "purchases_count": "int"
    }
    for field in df_lakeflow.schema.fields:
        assert field.dataType.simpleString() == expected_schema[field.name]

def test_null_handling_and_case_sensitivity(spark):
    # Validate that income_amount nulls/negatives are set to 0, and column names are consistent
    df_lakeflow = spark.read.parquet("testdata/lakeflow_fact_executive_summary.parquet")
    assert df_lakeflow.filter("income_amount < 0").count() == 0
    assert df_lakeflow.filter("income_amount IS NULL").count() == 0
    # Column names should be lower_snake_case
    for col in df_lakeflow.columns:
        assert col == col.lower()

def test_orchestration_flow():
    # Validate that notebooks executed in correct order (staging, transform/load, audit/cleanup)
    # This can be checked via Lakeflow logs or notebook execution metadata
    executed_steps = ["01_stage_stg_holding_metrics.sql", "02_transform_and_load_fact_executive_summary.sql", "03_audit_logging_and_cleanup.py"]
    # Simulate reading execution log
    with open("testdata/lakeflow_execution_log.txt") as f:
        log = f.read()
    for step in executed_steps:
        assert step in log

def test_manual_interventions_and_performance():
    # Validate that temp table is replaced by temp view, print by logging, and cleanup is performed
    with open("testdata/lakeflow_execution_log.txt") as f:
        log = f.read()
    assert "CREATE OR REPLACE TEMP VIEW" in log
    assert "records inserted into FACT_EXECUTIVE_SUMMARY" in log
    assert "dropTempView" in log or "DROP VIEW" in log
```

**API Cost Estimation**

apiCost: 0.0537 USD