```
============================================= 
Author: Ascendion AAVA 
Created on: 
Description: Synapse to Databricks Lakeflow Conversion Test 
=============================================

**Test Case List:**

| Test Case ID | Test Case Description | Expected Outcome |
|--------------|----------------------|------------------|
| TC01 | Validate column expression mapping from Synapse to Lakeflow notebooks | All column-level transformations match expected logic |
| TC02 | Validate aggregation logic mapping (groupBy/window functions) | Aggregated output consistent with Synapse results |
| TC03 | Validate join strategy equivalence | Join outputs identical to Synapse logic |
| TC04 | Validate data type conversions between Synapse and Lakeflow | Schema matches expected PySpark/SQL types |
| TC05 | Validate null handling and case sensitivity | Nulls handled correctly and column names consistent |
| TC06 | Validate orchestration flow in Lakeflow | Notebooks execute in correct sequence with correct dependencies |
| TC07 | Validate manual interventions and performance optimizations | Adjusted logic produces correct results with improved performance |

**Pytest Script for Each Test Case**

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, DateType

@pytest.fixture(scope="module")
def spark():
    return SparkSession.builder.appName("Lakeflow_Test").getOrCreate()

def load_parquet(spark, path):
    return spark.read.parquet(path)

def test_expression_mapping(spark):
    # TC01: Validate column expression mapping
    df_synapse = load_parquet(spark, "test_data/synapse_expressions.parquet")
    df_lakeflow = load_parquet(spark, "test_data/lakeflow_expressions.parquet")
    # Compare all columns except those known to be handled differently (e.g., income_amount)
    cols = [col for col in df_synapse.columns if col in df_lakeflow.columns]
    assert df_synapse.select(cols).collect() == df_lakeflow.select(cols).collect()

def test_aggregation_logic(spark):
    # TC02: Validate aggregation logic mapping
    df_synapse = load_parquet(spark, "test_data/synapse_agg.parquet")
    df_lakeflow = load_parquet(spark, "test_data/lakeflow_agg.parquet")
    assert df_synapse.collect() == df_lakeflow.collect()

def test_join_strategy_equivalence(spark):
    # TC03: Validate join strategy equivalence
    df_synapse = load_parquet(spark, "test_data/synapse_joins.parquet")
    df_lakeflow = load_parquet(spark, "test_data/lakeflow_joins.parquet")
    assert df_synapse.collect() == df_lakeflow.collect()

def test_data_type_conversions(spark):
    # TC04: Validate data type conversions
    df_lakeflow = load_parquet(spark, "test_data/lakeflow_final.parquet")
    expected_schema = StructType([
        StructField("date_key", IntegerType(), True),
        StructField("institution_id", IntegerType(), True),
        StructField("corporation_id", IntegerType(), True),
        StructField("product_id", IntegerType(), True),
        StructField("a120_amount", DoubleType(), True),
        StructField("a120_count", IntegerType(), True),
        StructField("a30_to_59_amount", DoubleType(), True),
        StructField("a30_to_59_count", IntegerType(), True),
        StructField("a60_to_89_amount", DoubleType(), True),
        StructField("a60_to_89_count", IntegerType(), True),
        StructField("a90_to_119_amount", DoubleType(), True),
        StructField("a90_to_119_count", IntegerType(), True),
        StructField("charge_off_amount", DoubleType(), True),
        StructField("charge_off_count", IntegerType(), True),
        StructField("fraud_amount", DoubleType(), True),
        StructField("fraud_count", IntegerType(), True),
        StructField("income_amount", DoubleType(), True),
        StructField("number_of_accounts", IntegerType(), True),
        StructField("purchases_amount", DoubleType(), True),
        StructField("purchases_count", IntegerType(), True)
    ])
    assert df_lakeflow.schema == expected_schema

def test_null_handling_and_case_sensitivity(spark):
    # TC05: Validate null handling and case sensitivity
    df_lakeflow = load_parquet(spark, "test_data/lakeflow_final.parquet")
    # Check that income_amount is never null or negative
    assert df_lakeflow.filter("income_amount IS NULL OR income_amount < 0").count() == 0
    # Check column names are lower_case_with_underscores
    for col in df_lakeflow.columns:
        assert col == col.lower()

def test_orchestration_flow(spark):
    # TC06: Validate orchestration flow in Lakeflow
    # Simulate execution order by checking audit logs or output markers
    with open("test_data/orchestration_log.txt") as f:
        log = f.read()
    assert "*** LOAD_FACT_EXECUTIVE_SUMMARY completed successfully ***" in log
    # Check that temp views are dropped
    with open("test_data/catalog_state.txt") as f:
        catalog = f.read()
    assert "staging_metrics" not in catalog
    assert "executive_summary_joined" not in catalog

def test_manual_interventions_and_performance(spark):
    # TC07: Validate manual interventions and performance optimizations
    # Check for partitioning, caching, and broadcast join hints in Lakeflow code
    with open("lakeflow_code/fact_executive_summary_transform.py") as f:
        code = f.read()
    assert "broadcast" in code or "partitionBy" in code or "cache" in code
    # Validate output correctness after optimizations
    df_synapse = load_parquet(spark, "test_data/synapse_final.parquet")
    df_lakeflow = load_parquet(spark, "test_data/lakeflow_final.parquet")
    assert df_synapse.collect() == df_lakeflow.collect()
```

**API Cost Estimation**

apiCost: 0.0537 USD
```
