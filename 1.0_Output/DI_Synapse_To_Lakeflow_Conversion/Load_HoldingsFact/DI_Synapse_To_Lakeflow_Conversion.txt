```
=============================================
Author: Ascendion AAVA
Created on:   
Description:   Convert Azure Synapse stored procedures into equivalent Databricks Lakeflow pipelines
=============================================

Fully orchestrated Databricks Lakeflow pipeline converted from Azure Synapse stored procedures.  
All data read/write operations are implemented using standard table or view references across SQL, PySpark, and Python notebooks.  

---

## Databricks Lakeflow Pipeline: FACT_EXECUTIVE_SUMMARY Load

### 1. SQL Notebook: `01_stage_stg_holding_metrics.sql`

```sql
-- Databricks Lakeflow SQL Notebook
-- Step 1: Prepare staging data as a temporary view

CREATE OR REPLACE TEMP VIEW staging_metrics AS
SELECT *
FROM STG_HOLDING_METRICS;
```

---

### 2. SQL Notebook: `02_transform_and_load_fact_executive_summary.sql`

```sql
-- Databricks Lakeflow SQL Notebook
-- Step 2: Transform and load data into FACT_EXECUTIVE_SUMMARY

INSERT INTO FACT_EXECUTIVE_SUMMARY (
    date_key,
    institution_id,
    corporation_id,
    product_id,
    a120_amount,
    a120_count,
    a30_to_59_amount,
    a30_to_59_count,
    a60_to_89_amount,
    a60_to_89_count,
    a90_to_119_amount,
    a90_to_119_count,
    charge_off_amount,
    charge_off_count,
    fraud_amount,
    fraud_count,
    income_amount,
    number_of_accounts,
    purchases_amount,
    purchases_count
)
SELECT 
    dt.date_key,
    inst.institution_id,
    corp.corporation_id,
    prod.product_id,
    stg.a120_amount,
    stg.a120_count,
    stg.a30_to_59_amount,
    stg.a30_to_59_count,
    stg.a60_to_89_amount,
    stg.a60_to_89_count,
    stg.a90_to_119_amount,
    stg.a90_to_119_count,
    stg.charge_off_amount,
    stg.charge_off_count,
    stg.fraud_amount,
    stg.fraud_count,
    CASE 
        WHEN stg.income_amount IS NULL OR stg.income_amount < 0 THEN 0
        ELSE stg.income_amount
    END AS income_amount,
    stg.number_of_accounts,
    stg.purchases_amount,
    stg.purchases_count
FROM staging_metrics stg
INNER JOIN DIM_DATE dt ON dt.date_key = stg.date_value
INNER JOIN DIM_INSTITUTION inst ON inst.institution_id = stg.institution_id
INNER JOIN DIM_CORPORATION corp ON corp.corporation_id = stg.corporation_id
INNER JOIN DIM_PRODUCT prod ON prod.product_id = stg.product_id;
```

---

### 3. PySpark Notebook: `03_audit_logging_and_cleanup.py`

```python
# Databricks Lakeflow PySpark Notebook
# Step 3: Audit logging and cleanup

from pyspark.sql.functions import col

# Count inserted records for audit logging
fact_count = spark.sql("SELECT COUNT(*) AS row_count FROM FACT_EXECUTIVE_SUMMARY").collect()[0]['row_count']
print(f"{fact_count} records inserted into FACT_EXECUTIVE_SUMMARY.")

# Cleanup: Drop the temporary view if needed
spark.catalog.dropTempView("staging_metrics")
print("*** LOAD_FACT_EXECUTIVE_SUMMARY completed successfully ***")
```

---

### 4. Python Driver Notebook: `00_driver_orchestration.py`

```python
# Databricks Lakeflow Python Driver Notebook
# Orchestrates the execution of the pipeline

# Run staging SQL notebook
dbutils.notebook.run("01_stage_stg_holding_metrics.sql", 0)

# Run transformation and load SQL notebook
dbutils.notebook.run("02_transform_and_load_fact_executive_summary.sql", 0)

# Run audit logging and cleanup PySpark notebook
dbutils.notebook.run("03_audit_logging_and_cleanup.py", 0)
```

---

## Documentation & Best Practices

- **Variable Assignments:** All variable logic (e.g., row counts) is handled in Python/PySpark, not SQL variables.
- **Conditional Logic:** CASE statements are directly mapped in SQL; procedural IFs are handled in orchestration.
- **Joins & Aggregations:** All joins are performed in SQL using best practices; aggregations can be added as needed.
- **Insert/Update/Merge:** Uses Delta Lake `INSERT INTO` for efficient writes; can be refactored to `MERGE` for upserts.
- **Procedural Flow:** Each logical step is a separate notebook, orchestrated by the Python driver.
- **Audit Logging:** Row counts and status messages are printed for traceability.
- **Cleanup:** Temporary views are dropped after use to free resources.
- **Performance:** Use Delta tables, partitioning, and caching as needed for large data volumes.

---

## Quality Criteria

- Functional, modular notebooks with accurate business logic translation.
- Clear inline documentation and adherence to Databricks best practices.
- Proper handling of NULLs, data types, and performance optimization.
- End-to-end data flow replication from Synapse to Lakeflow.

---

## API Cost

apiCost: 0.0182 USD
```