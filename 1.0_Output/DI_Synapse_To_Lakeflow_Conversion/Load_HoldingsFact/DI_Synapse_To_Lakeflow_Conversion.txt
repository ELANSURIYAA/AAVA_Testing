=============================================
Author:        Ascendion AAVA
Created on:   
Description:   Convert Azure Synapse stored procedures into equivalent Databricks Lakeflow pipelines
=============================================

Fully orchestrated Databricks Lakeflow pipeline converted from Azure Synapse stored procedures.  
All data read/write operations are implemented using standard table or view references across SQL, PySpark, and Python notebooks.  

---

# Databricks Lakeflow Pipeline Conversion

## 1. Pipeline Overview

This Lakeflow pipeline replicates the logic of the Synapse stored procedure `dbo.LOAD_FACT_EXECUTIVE_SUMMARY`, loading the `FACT_EXECUTIVE_SUMMARY` Delta table from the staging table `STG_HOLDING_METRICS`. It performs data quality validation, applies business rules, and enforces referential integrity by joining with dimension tables (`DIM_DATE`, `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`). The pipeline is modular, consisting of:

- **SQL Notebook:** For staging and joining data.
- **PySpark Notebook:** For business rule transformations and handling procedural logic.
- **Python Driver Notebook:** For orchestration, audit logging, and scheduling.

---

## 2. Lakeflow Notebooks

### A. SQL Notebook: `fact_executive_summary_staging.sql`

```sql
-- Create temporary view for staging metrics
CREATE OR REPLACE TEMP VIEW staging_metrics AS
SELECT *
FROM STG_HOLDING_METRICS;

-- Join staging with dimension tables and apply business rules
CREATE OR REPLACE TEMP VIEW executive_summary_joined AS
SELECT 
    dt.date_key,
    inst.institution_id,
    corp.corporation_id,
    prod.product_id,
    stg.a120_amount,
    stg.a120_count,
    stg.a30_to_59_amount,
    stg.a30_to_59_count,
    stg.a60_to_89_amount,
    stg.a60_to_89_count,
    stg.a90_to_119_amount,
    stg.a90_to_119_count,
    stg.charge_off_amount,
    stg.charge_off_count,
    stg.fraud_amount,
    stg.fraud_count,
    CASE 
        WHEN stg.income_amount IS NULL OR stg.income_amount < 0 THEN 0
        ELSE stg.income_amount
    END AS income_amount,
    stg.number_of_accounts,
    stg.purchases_amount,
    stg.purchases_count
FROM staging_metrics stg
INNER JOIN DIM_DATE dt ON dt.date_key = stg.date_value
INNER JOIN DIM_INSTITUTION inst ON inst.institution_id = stg.institution_id
INNER JOIN DIM_CORPORATION corp ON corp.corporation_id = stg.corporation_id
INNER JOIN DIM_PRODUCT prod ON prod.product_id = stg.product_id;
```

---

### B. PySpark Notebook: `fact_executive_summary_transform.py`

```python
# Databricks notebook source
# Load joined data from temp view
executive_summary_df = spark.table("executive_summary_joined")

# Business rule: Ensure income_amount is non-negative (already handled in SQL, but double-check)
from pyspark.sql.functions import when, col

executive_summary_df = executive_summary_df.withColumn(
    "income_amount",
    when(col("income_amount").isNull() | (col("income_amount") < 0), 0).otherwise(col("income_amount"))
)

# Write to Delta table (FACT_EXECUTIVE_SUMMARY)
executive_summary_df.write.format("delta").mode("append").saveAsTable("FACT_EXECUTIVE_SUMMARY")

# Audit logging: Get row count
row_count = executive_summary_df.count()
print(f"{row_count} records inserted into FACT_EXECUTIVE_SUMMARY.")
```

---

### C. Python Driver Notebook: `fact_executive_summary_driver.py`

```python
# Databricks notebook source
# Orchestrate Lakeflow pipeline

# Step 1: Run SQL notebook for staging and joins
dbutils.notebook.run("fact_executive_summary_staging.sql", 60)

# Step 2: Run PySpark notebook for transformation and loading
dbutils.notebook.run("fact_executive_summary_transform.py", 60)

# Step 3: Audit logging and cleanup
print("*** LOAD_FACT_EXECUTIVE_SUMMARY completed successfully ***")

# Optional: Drop temp views if needed
spark.catalog.dropTempView("staging_metrics")
spark.catalog.dropTempView("executive_summary_joined")
```

---

## 3. Quality Criteria

- Functional, modular notebooks with accurate business logic translation.
- Clear inline documentation and adherence to Databricks best practices.
- Proper handling of NULLs, data types, and performance optimization.
- End-to-end data flow replication from Synapse to Lakeflow.

---

## 4. Performance Optimization

- Avoid unnecessary shuffles and wide transformations by using partitioning and caching where appropriate.
- Use broadcast joins for small dimension tables.
- Leverage Delta tables for efficient insert/update/merge operations.
- Clean up intermediate views/tables after processing.

---

## 5. API Cost

apiCost: 0.0182 USD