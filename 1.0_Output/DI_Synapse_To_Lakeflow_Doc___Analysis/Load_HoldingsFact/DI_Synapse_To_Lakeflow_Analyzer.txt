```
=============================================
Author:        Ascendion AVAA
Date:          
Description:   Load summarized holding metrics into the executive summary fact table in Synapse, ensuring data quality and referential integrity.
=============================================

# 1. Pipeline Overview

The `LOAD_FACT_EXECUTIVE_SUMMARY` stored procedure in Azure Synapse Analytics is designed to populate the `FACT_EXECUTIVE_SUMMARY` fact table with aggregated holding metrics sourced from the `STG_HOLDING_METRICS` staging table. The procedure enforces data quality, applies business rules, and ensures referential integrity by joining to dimension tables. This process supports regulatory reporting, executive dashboards, and downstream analytics by providing a cleansed, validated, and enriched summary of holdings data. Conceptually, this maps to Lakeflow layers as follows:
- **Ingest:** Extraction from `STG_HOLDING_METRICS` into a temporary staging table.
- **Transform:** Data validation, business rule enforcement, and enrichment via joins to dimension tables.
- **Publish:** Insert into `FACT_EXECUTIVE_SUMMARY` and audit logging.

# 2. Complexity Metrics

| Category                  | Measurement                                                                                 |
|---------------------------|--------------------------------------------------------------------------------------------|
| Number of Activities      | 1 (Stored Procedure)                                                                       |
| Source/Target Systems     | Source: `STG_HOLDING_METRICS`; Target: `FACT_EXECUTIVE_SUMMARY`; Reference: 4 dimension tables |
| Transformation Logic      | 1 (income_amount transformation via CASE statement)                                        |
| Parameters Used           | 2 variables (`@v_row_count`, `@error_message`); No parameters or triggers                  |
| Reusable Components       | None (no UDFs, templates, or reusable modules)                                             |
| Control Logic             | Conditional logic for income_amount; audit logging; temp table management                   |
| External Dependencies     | None (all tables within Synapse DW)                                                        |
| Performance Considerations| Use of temp table for staging; direct joins; no partitioning or parallelization            |
| Volume Handling           | Bulk insert from staging; no explicit batching or partitioning                             |
| Error Handling            | Audit logging via PRINT and row count; no explicit error handling or retry logic           |
| Overall Complexity Score  | 20                                                                                         |
| Conversion complexity rating | Low (Score: 20/100). The pipeline is straightforward: single stored procedure, direct mappings, simple transformation, no external dependencies. High-complexity areas: None. |

# 3. Syntax Differences

| Synapse SQL Construct                | Lakeflow Mapping (Databricks)                                          | Example from Pipeline                                      |
|--------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------|
| Stored Procedure (`CREATE PROCEDURE`)| Dataflow pipeline with transformation and load tasks                   | `LOAD_FACT_EXECUTIVE_SUMMARY` procedure                    |
| Temporary Table (`#staging_metrics`) | Intermediate Dataframe or temp view                                    | `SELECT * INTO #staging_metrics FROM STG_HOLDING_METRICS`  |
| CASE Statement                      | `when`/`otherwise` in Lakeflow SQL or Dataframe API                    | `CASE WHEN stg.income_amount IS NULL OR stg.income_amount < 0 THEN 0 ELSE stg.income_amount END` |
| INNER JOIN                          | `join` operation in Lakeflow SQL/Dataframe                             | Joins to DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT |
| PRINT/@@ROWCOUNT                    | Logging via Lakeflow event logs or notebook print statements           | `PRINT ...`, `SET @v_row_count = @@ROWCOUNT`               |
| DROP TABLE                          | Drop temp view/Dataframe or cleanup step                               | `DROP TABLE #staging_metrics`                              |

# 4. Manual Adjustments

- **Stored Procedure Migration:** Synapse stored procedures must be refactored as Lakeflow dataflows or notebooks; procedural logic (BEGIN...END, PRINT, variable declarations) must be rewritten using Lakeflow orchestration and transformation APIs.
- **Temporary Table Handling:** Replace `#staging_metrics` with intermediate Dataframes or temp views in Lakeflow.
- **Audit Logging:** Replace `PRINT` and `@@ROWCOUNT` with Lakeflow logging mechanisms (e.g., notebook logs, event logs).
- **Variable Declarations:** Replace SQL variables (`@v_row_count`, `@error_message`) with Python/Scala variables or Lakeflow workflow variables.
- **Cleanup:** Explicit temp table drops are not needed; manage Dataframes/views lifecycle in Lakeflow.
- **No triggers or parameters:** No migration required for triggers/parameters.

# 5. Optimization Techniques

- **Refactoring Recommendation:** The pipeline should be refactored, not rebuilt, as the logic is simple and direct. Use Lakeflow Dataframes for staging, apply transformations with `when`/`otherwise`, and leverage Lakeflow joins for referential integrity.
- **Performance:** Optimize joins by broadcasting small dimension tables, caching intermediate Dataframes, and using columnar storage for target tables.
- **Audit Logging:** Use Lakeflow's built-in logging and monitoring for row counts and process status.
- **Cleanup:** Rely on Dataframe garbage collection; avoid explicit drop statements.
- **Parallelization:** If processing large volumes, partition input Dataframes and leverage Lakeflow's parallel execution.

# 6. API Cost

apiCost: 0.00200000 USD
```