====================================================
Author:        AAVA
Date:          
Description:   Comprehensive documentation for the Data Warehousing Group Project Alteryx workflow, detailing ETL for dimension and fact tables.
====================================================

# Data Warehousing Group Project - Tables
---

## 1. Workflow Overview

This Alteryx workflow is designed to extract, transform, and load (ETL) data for a Data Warehousing project focused on language learning traces. The workflow processes raw CSV and text files, cleans and transforms the data, and loads it into a star-schema data warehouse with dimension and fact tables. The main business objective is to enable robust reporting and analytics on user learning traces, lexemes, and related linguistic features, providing actionable insights for educational product improvement.

**Business Problem Solved:**  
The workflow standardizes and structures disparate language learning trace data, making it suitable for advanced analytics and reporting. It ensures data quality, consistency, and referential integrity, supporting downstream BI and data science initiatives.

---

## 2. Tool Breakdown

### Input Data Tools
- **Input Data (learning_traces.13m.csv)**
  - **Purpose:** Loads raw learning trace data.
  - **Configuration:** Reads CSV with header, comma delimiter, encoding ISO-8859-1.
  - **Input Type:** CSV file.
  - **Output:** Raw learning trace records.

- **Input Data (lexeme_reference.txt)**
  - **Purpose:** Loads lexeme reference mappings.
  - **Configuration:** Reads flat file, custom columns.
  - **Input Type:** TXT file.
  - **Output:** Lexeme reference records.

### Data Preparation & Transformation Tools

- **Text to Columns**
  - **Purpose:** Splits delimited string fields into multiple columns.
  - **Configuration:** 
    - Tool 5: Splits `lexeme_string` by `<\`, 4 columns.
    - Tool 24: Splits `part_of_speech` by `+:`, 2 columns.
  - **Input:** String fields.
  - **Output:** Expanded columns for further processing.

- **Formula**
  - **Purpose:** Cleans and standardizes text fields.
  - **Key Configurations:**
    - Tool 20: Cleans `lexeme_string` (removes tags, symbols, applies title case).
    - Tool 31: Cleans `part_of_speech`, creates `lexeme_length`.
    - Tool 35: Cleans `morphological_components`.
    - Tool 36: Counts morphological components.
  - **Expressions:** Uses Replace, TitleCase, Length, CountWords.

- **Multi-Field Formula**
  - **Purpose:** Converts language abbreviations to full names.
  - **Configuration:** Maps 'en' to 'English', 'es' to 'Spanish', etc.
  - **Input:** Language code fields.
  - **Output:** Human-readable language names.

- **Find Replace**
  - **Purpose:** Replaces part-of-speech tags with full words.
  - **Configuration:** Maps abbreviations to descriptive names.

- **Select, Unique, RecordID, Union**
  - **Purpose:** Column selection, deduplication, record ID generation, combining streams.
  - **Configuration:** As needed for star schema construction.

- **Join**
  - **Purpose:** Combines data streams on keys (e.g., user IDs).
  - **Configuration:** Joins on `temporary_user_id`.

### Output Tools

- **Output Data (CSV)**
  - **Purpose:** Writes cleaned data to CSV for reference.
  - **Configuration:** Outputs to `CLEAN SAMPLE learning_traces.csv`.

- **Output Data (ODBC/SQL)**
  - **Purpose:** Loads dimension and fact tables to SQL database.
  - **Configuration:** DSN=Testing, table names as per schema, overwrite mode, executes post-load SQL for constraints.

---

## 3. Data Flow Breakdown

### Data Flow Diagram (Markdown)

```mermaid
graph TD
    A[learning_traces.13m.csv] --> B[Text to Columns: lexeme_string]
    B --> C[Formula: Clean lexeme_string]
    C --> D[Text to Columns: part_of_speech]
    D --> E[Formula: Clean part_of_speech, create lexeme_length]
    E --> F[Multi-Field Formula: Language Abbreviations]
    F --> G[Formula: Clean morphological_components]
    G --> H[Formula: Count morphological components]
    H --> I[Join: Add user, lexeme, lemma, language, pos IDs]
    I --> J[Select/Unique: Build Dimension Tables]
    J --> K[Output Data: SQL Dimension Tables]
    I --> L[Select: Build Fact Tables]
    L --> M[Output Data: SQL Fact Tables]
    I --> N[Output Data: Cleaned CSV]
    lexeme_reference.txt --> O[Find Replace: part_of_speech]
    O --> D
```

### Step-by-Step Data Flow

1. **Raw Data Load:**  
   - Loads `learning_traces.13m.csv` and `lexeme_reference.txt`.
2. **String Expansion:**  
   - Splits `lexeme_string` and `part_of_speech` into components.
3. **Cleaning & Transformation:**  
   - Cleans and standardizes key fields, creates derived columns (`lexeme_length`, `number_of_morphological_components`).
   - Converts language codes to full names.
4. **Reference Mapping:**  
   - Uses Find Replace to map part-of-speech tags.
5. **Dimension Table Preparation:**  
   - Selects, deduplicates, and assigns IDs for dimension tables (lemma, lexeme, language, pos, timestamp, user).
6. **Fact Table Preparation:**  
   - Joins dimension keys to create fact tables.
7. **Output:**  
   - Writes cleaned CSV and loads dimension/fact tables to SQL.

---

## 4. Data Transformations

- **String Cleaning:**  
  - Removes special tags (`<*sf>`, `>`, `@`) from `lexeme_string`.
  - Applies title case for consistency.

- **Part-of-Speech Mapping:**  
  - Replaces abbreviations (e.g., 'Cnj' â†’ 'Special conjunction').

- **Language Code Mapping:**  
  - Converts codes ('en', 'es', etc.) to full language names.

- **Derived Columns:**  
  - Calculates `lexeme_length` using string length.
  - Counts morphological components using word count.

- **ID Generation:**  
  - Assigns surrogate keys for dimensions.

- **Deduplication:**  
  - Ensures unique records for dimension tables.

- **Fact Table Construction:**  
  - Joins dimension keys to create star schema fact tables.

---

## 5. Technical Insights

- **Data Sources:**  
  - CSV: `learning_traces.13m.csv`
  - TXT: `lexeme_reference.txt`

- **Destinations:**  
  - SQL tables: `pos_dim`, `lexeme_dim`, `lemma_dim`, `language_dim`, `timestamp_dim`, `user_dim`, `lexeme_fact`, `learning_trace_fact`
  - CSV: `CLEAN SAMPLE learning_traces.csv`

- **Key Tables & Fields:**  
  - Dimension tables: IDs and descriptive fields.
  - Fact tables: Foreign keys to dimensions, measures.

- **Advanced Functions:**  
  - Custom formulas (Replace, TitleCase, Length, CountWords).
  - Multi-Field conditional logic for language mapping.

- **Macros:**  
  - No explicit macros detected.

- **SQL Post-Processing:**  
  - Alters columns, adds primary/foreign keys for referential integrity.

---

## 6. Technical Complexity

| Parameter                        | Value/Count | Notes                                                                 |
|-----------------------------------|-------------|-----------------------------------------------------------------------|
| Total Number of Tools             | ~40+        | Estimated from workflow structure                                     |
| Number of Unique Tool Types       | 13          | Input, Output, Formula, Multi-Field Formula, Text to Columns, etc.    |
| Number of Input & Output Tools    | 2 Inputs, 8 Outputs | 2 file inputs, 6 SQL outputs                                         |
| Number of Join-Type Tools         | 1           | Main Join on user/lexeme/lemma/pos                                   |
| Number of Formula/Expression Tools| 6           | Multiple Formula, Multi-Field Formula tools                           |
| Number of Branches (Parallel Streams) | 3+      | Separate streams for each dimension/fact                              |
| Tool Diversity                    | High        | Uses a wide range of Alteryx tools                                    |
| Macro Usage Count                 | 0           | No macros detected                                                    |
| Nested Macro Depth                | 0           | None detected                                                         |
| Analytic App Interface Count      | 0           | Not an analytic app                                                   |
| Custom SQL Query Count            | 6           | SQL post-processing for each table                                    |
| Local File Paths                  | 2           | Input/output files use local paths                                    |
| Non-Cloud-Compatible Tools        | 2           | File input/output not cloud compatible                                |
| Conditional Logic Count           | 3           | Language mapping, part-of-speech mapping, formula logic               |
| Data Sources                      | 2           | CSV, TXT                                                             |
| Joins and Blends                  | 1           | Main join for fact table creation                                     |
| Annotations                       | 10+         | Tool and step descriptions present                                    |
| Sensitive Data Usage              | Yes         | Encrypted password in ODBC string                                     |
| Overall Complexity Score          | 68          | Moderate-high: Multiple transformations, star schema, SQL integration |

---

## 7. Assumptions and Dependencies

- **Prerequisites:**
  - Input files (`learning_traces.13m.csv`, `lexeme_reference.txt`) must exist and match expected schema.
  - ODBC DSN 'Testing' must be configured and accessible.
  - Database user must have rights to create/overwrite tables.

- **Dependencies:**
  - Relies on Alteryx Designer and ODBC drivers.
  - Assumes consistent file encoding and delimiters.
  - Requires external SQL database for output.

- **Assumptions:**
  - Data files are complete and not corrupted.
  - All required columns are present and properly formatted.
  - User IDs and other keys are unique and consistent.

---

## 8. Key Outputs

- **Cleaned CSV:**  
  - `CLEAN SAMPLE learning_traces.csv` (for reference and validation).

- **Dimension Tables (SQL):**
  - `pos_dim`, `lexeme_dim`, `lemma_dim`, `language_dim`, `timestamp_dim`, `user_dim`
  - Used for descriptive attributes in reporting.

- **Fact Tables (SQL):**
  - `lexeme_fact`, `learning_trace_fact`
  - Used for analytical queries, aggregations, and BI.

- **Business Alignment:**  
  - Enables robust reporting on language learning traces.
  - Supports analytics on user behavior, linguistic features, and learning outcomes.

---

## 9. Error Handling and Logging

- **Error Handling:**
  - Deduplication steps prevent duplicate dimension records.
  - Data type conversions and cleaning formulas handle malformed data.
  - ODBC output tools configured to overwrite tables, ensuring schema consistency.

- **Logging:**
  - No explicit logging tool, but output files and SQL tables serve as checkpoints.
  - SQL errors (e.g., constraint violations) will surface during table creation.

- **Issue Management:**
  - Missing or invalid data is filtered or replaced with defaults (e.g., '_ERROR' for unknown languages).
  - Encrypted passwords in ODBC strings protect sensitive credentials.
  - Manual review of output files recommended for validation.

---

**End of Documentation**