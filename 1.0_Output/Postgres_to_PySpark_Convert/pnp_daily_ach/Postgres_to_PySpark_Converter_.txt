Converted PySpark Code
--------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Converted PySpark Code

This script converts the provided PostgreSQL query for daily achievements
into an equivalent PySpark code snippet.

API cost consumed for this call: $0.0020  # (example cost, modify as required)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum as _sum, count as _count, col, date_trunc

# Initialize the Spark session
spark = SparkSession.builder \
    .appName("DailyAchievements") \
    .getOrCreate()

# Load the data into a DataFrame (modify the path / data source as needed)
# For example, reading from a Parquet file or a table. Here we assume Parquet:
# pnpDF = spark.read.parquet("path_to_pnp_parquet_data")
# Alternatively, if using a JDBC connection:
# pnpDF = spark.read \
#             .format("jdbc") \
#             .option("url", "jdbc:postgresql://host:port/dbname") \
#             .option("dbtable", "pnp") \
#             .option("user", "username") \
#             .option("password", "password") \
#             .load()

# For the purpose of this conversion, we assume that 'pnpDF' is already defined.

# Filter data: achievements from 2023-01-01 onwards
filteredDF = pnpDF.filter(col("achievement_date") >= "2023-01-01")

# Truncate achievements to the day using date_trunc function
dailyDF = filteredDF.withColumn("day", date_trunc("day", col("achievement_date")))

# Aggregate data: sum of amount and count of records per day
aggDF = dailyDF.groupBy("day").agg(
    _sum("amount").alias("total_amount"),
    _count("*").alias("total_count")
)

# Order the result by day
resultDF = aggDF.orderBy("day")

# Show the result
resultDF.show()

# Stop the Spark session 
spark.stop()
--------------------------------------------------