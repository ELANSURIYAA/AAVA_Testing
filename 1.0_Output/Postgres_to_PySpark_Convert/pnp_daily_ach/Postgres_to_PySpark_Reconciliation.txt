{
  "Test Cases Document": [
    {
      "Test Case ID": "TC01_HappyPath",
      "Description": "Validate correct aggregation when provided with valid transactions and accounts DataFrames. This test ensures that the join, filtering, and aggregations produce the expected sum and average per (transaction_date, category) group.",
      "Input Data": {
        "transactions": [
          { "transaction_date": "2023-05-01", "status": "active", "account_id": 1, "category": "food", "amount": 100.0 },
          { "transaction_date": "2023-05-01", "status": "active", "account_id": 1, "category": "food", "amount": 200.0 },
          { "transaction_date": "2023-05-01", "status": "active", "account_id": 2, "category": "travel", "amount": 300.0 }
        ],
        "accounts": [
          { "id": 1, "account_name": "Account A" },
          { "id": 2, "account_name": "Account B" }
        ]
      },
      "Expected Output": "The aggregated DataFrame shows proper grouping by transaction_date and category with correct total_amount and average_amount calculations. For example, for category 'food' the total is 300.0 and average is 150.0; for 'travel' the total and average are both 300.0."
    },
    {
      "Test Case ID": "TC02_OnlyActiveTransactions",
      "Description": "Ensure only transactions with status 'active' are processed. Any transaction with a different status should be filtered out before joining and aggregation.",
      "Input Data": {
        "transactions": [
          { "transaction_date": "2023-06-15", "status": "active", "account_id": 1, "category": "entertainment", "amount": 120.0 },
          { "transaction_date": "2023-06-15", "status": "inactive", "account_id": 1, "category": "entertainment", "amount": 300.0 }
        ],
        "accounts": [
          { "id": 1, "account_name": "Account A" }
        ]
      },
      "Expected Output": "The output DataFrame contains only rows for transactions with status 'active', meaning only one transaction record should be present for category 'entertainment' with a total_amount of 120.0."
    },
    {
      "Test Case ID": "TC03_DateRangeFilter",
      "Description": "Verify that transactions falling outside the date range (2023-01-01 to 2023-12-31) are excluded.",
      "Input Data": {
        "transactions": [
          { "transaction_date": "2022-12-31", "status": "active", "account_id": 1, "category": "utilities", "amount": 80.0 },
          { "transaction_date": "2023-01-02", "status": "active", "account_id": 1, "category": "utilities", "amount": 120.0 }
        ],
        "accounts": [
          { "id": 1, "account_name": "Account A" }
        ]
      },
      "Expected Output": "Only the transaction with 'transaction_date' 2023-01-02 falls within the defined date range and is processed."
    },
    {
      "Test Case ID": "TC04_EmptyDataFrame",
      "Description": "Test the case where one or both of the input DataFrames (transactions or accounts) are empty.",
      "Input Data": {
        "transactions": [],
        "accounts": []
      },
      "Expected Output": "The final result is an empty DataFrame, with no errors produced during processing."
    },
    {
      "Test Case ID": "TC05_NullValues",
      "Description": "Check behavior when critical fields (e.g., transaction_date, amount) contain null values. The test will validate that such rows are either filtered out or handled such that aggregation does not crash.",
      "Input Data": {
        "transactions": [
          { "transaction_date": null, "status": "active", "account_id": 1, "category": "misc", "amount": 50.0 },
          { "transaction_date": "2023-07-07", "status": "active", "account_id": 1, "category": "misc", "amount": null },
          { "transaction_date": "2023-07-07", "status": "active", "account_id": 1, "category": "misc", "amount": 100.0 }
        ],
        "accounts": [
          { "id": 1, "account_name": "Account A" }
        ]
      },
      "Expected Output": "Only the valid row with non-null transaction_date and amount contributes to the aggregation so the expected output is aggregation for category 'misc' with total_amount 100.0."
    },
    {
      "Test Case ID": "TC06_JoinMismatch",
      "Description": "Validate the join operation when some transactions do not have a matching account in the accounts DataFrame.",
      "Input Data": {
        "transactions": [
          { "transaction_date": "2023-08-10", "status": "active", "account_id": 1, "category": "shopping", "amount": 220.0 },
          { "transaction_date": "2023-08-10", "status": "active", "account_id": 2, "category": "shopping", "amount": 180.0 }
        ],
        "accounts": [
          { "id": 1, "account_name": "Account A" }
        ]
      },
      "Expected Output": "Transactions with no matching account (i.e., account_id 2 in this case) are excluded. The final aggregate only includes data from account_id 1."
    },
    {
      "Test Case ID": "TC07_AggregationAccuracy",
      "Description": "Verify the accuracy of the aggregation functions by manually computing the expected sum and average values for a controlled dataset.",
      "Input Data": {
        "transactions": [
          { "transaction_date": "2023-09-01", "status": "active", "account_id": 1, "category": "health", "amount": 50.0 },
          { "transaction_date": "2023-09-01", "status": "active", "account_id": 1, "category": "health", "amount": 150.0 },
          { "transaction_date": "2023-09-01", "status": "active", "account_id": 1, "category": "health", "amount": 200.0 },
          { "transaction_date": "2023-09-01", "status": "active", "account_id": 1, "category": "health", "amount": 100.0 }
        ],
        "accounts": [
          { "id": 1, "account_name": "Account A" }
        ]
      },
      "Expected Output": "For category 'health', the total_amount should be 500.0 and the average_amount 125.0, exactly matching the manual computations."
    }
  ],
  "Pytest Script": "#!/usr/bin/env python3\n\"\"\"\nPySpark Unit Tests for Converted PySpark Code\n\nThis script performs unit tests for the following:\n- Filtering of active transactions\n- Date range filtering\n- DataFrame join between transactions and accounts\n- Aggregation into total and average amounts\n- Handling of empty datasets and null values\n\nAPI Cost Consumed for this conversion call: $0.0035\n\"\"\"\n\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, sum as _sum, avg as _avg\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n\n\n@pytest.fixture(scope=\"session\")\ndef spark():\n    spark_session = SparkSession.builder \\\n        .master(\"local[2]\") \\\n        .appName(\"PySparkTestSuite\") \\\n        .getOrCreate()\n    yield spark_session\n    spark_session.stop()\n\n\ndef create_transactions_df(spark, data):\n    schema = StructType([\n        StructField(\"transaction_date\", StringType(), True),\n        StructField(\"status\", StringType(), True),\n        StructField(\"account_id\", IntegerType(), True),\n        StructField(\"category\", StringType(), True),\n        StructField(\"amount\", DoubleType(), True)\n    ])\n    return spark.createDataFrame(data, schema)\n\n\ndef create_accounts_df(spark, data):\n    schema = StructType([\n        StructField(\"id\", IntegerType(), True),\n        StructField(\"account_name\", StringType(), True)\n    ])\n    return spark.createDataFrame(data, schema)\n\n\ndef process_data(spark, transactions_df, accounts_df):\n    # Convert transaction_date from string to date\n    transactions_df = transactions_df.withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\"))\n    \n    # Filter active transactions within date range\n    df_filtered = transactions_df.filter(\n        (col(\"status\") == \"active\") & \n        (col(\"transaction_date\") >= \"2023-01-01\") & \n        (col(\"transaction_date\") <= \"2023-12-31\")\n    )\n    \n    # Join with accounts on account_id and id\n    df_joined = df_filtered.join(accounts_df, transactions_df.account_id == accounts_df.id, how=\"inner\")\n    \n    # Aggregation: group by transaction_date and category, compute sum and average\n    df_aggregated = df_joined.groupBy(\"transaction_date\", \"category\").agg(\n        _sum(\"amount\").alias(\"total_amount\"),\n        _avg(\"amount\").alias(\"average_amount\")\n    )\n    \n    # Cache (if needed) and return the result\n    df_result = df_aggregated.cache()\n    return df_result\n\n\n# ---------------------------\n# Test Cases Implementation\n# ---------------------------\n\ndef test_happy_path(spark):\n    \"\"\" TC01_HappyPath: Validate aggregation with valid input data \"\"\"\n    transactions_data = [\n        (\"2023-05-01\", \"active\", 1, \"food\", 100.0),\n        (\"2023-05-01\", \"active\", 1, \"food\", 200.0),\n        (\"2023-05-01\", \"active\", 2, \"travel\", 300.0)\n    ]\n    accounts_data = [\n        (1, \"Account A\"),\n        (2, \"Account B\")\n    ]\n    transactions_df = create_transactions_df(spark, transactions_data)\n    accounts_df = create_accounts_df(spark, accounts_data)\n    result_df = process_data(spark, transactions_df, accounts_df)\n    \n    result = {row['category']: (row['total_amount'], row['average_amount']) for row in result_df.collect()}\n    # Expected: For category 'food': total=300, average=150; for 'travel': total=300, average=300\n    assert result.get(\"food\") == (300.0, 150.0)\n    assert result.get(\"travel\") == (300.0, 300.0)\n\n\ndef test_only_active_transactions(spark):\n    \"\"\" TC02_OnlyActiveTransactions: Ensure transactions with status != 'active' are filtered out \"\"\"\n    transactions_data = [\n        (\"2023-06-15\", \"active\", 1, \"entertainment\", 120.0),\n        (\"2023-06-15\", \"inactive\", 1, \"entertainment\", 300.0)\n    ]\n    accounts_data = [(1, \"Account A\")]\n    transactions_df = create_transactions_df(spark, transactions_data)\n    accounts_df = create_accounts_df(spark, accounts_data)\n    result_df = process_data(spark, transactions_df, accounts_df)\n    \n    results = result_df.collect()\n    # Only one active transaction should be present.\n    assert len(results) == 1\n    row = results[0]\n    assert row[\"category\"] == \"entertainment\"\n    assert row[\"total_amount\"] == 120.0\n\n\ndef test_date_range_filter(spark):\n    \"\"\" TC03_DateRangeFilter: Verify filtering of transactions by date range \"\"\"\n    transactions_data = [\n        (\"2022-12-31\", \"active\", 1, \"utilities\", 80.0),   # Outside date range\n        (\"2023-01-02\", \"active\", 1, \"utilities\", 120.0)      # Inside date range\n    ]\n    accounts_data = [(1, \"Account A\")]\n    transactions_df = create_transactions_df(spark, transactions_data)\n    accounts_df = create_accounts_df(spark, accounts_data)\n    result_df = process_data(spark, transactions_df, accounts_df)\n    \n    results = result_df.collect()\n    # Only the transaction with date '2023-01-02' should be included.\n    assert len(results) == 1\n    row = results[0]\n    assert row[\"category\"] == \"utilities\"\n    assert row[\"total_amount\"] == 120.0\n\n\ndef test_empty_dataframe(spark):\n    \"\"\" TC04_EmptyDataFrame: Test behavior with empty input DataFrames \"\"\"\n    transactions_data = []  # Empty transactions\n    accounts_data = []      # Empty accounts\n    transactions_df = create_transactions_df(spark, transactions_data)\n    accounts_df = create_accounts_df(spark, accounts_data)\n    result_df = process_data(spark, transactions_df, accounts_df)\n    \n    results = result_df.collect()\n    # Expecting an empty result\n    assert len(results) == 0\n\n\ndef test_null_values(spark):\n    \"\"\" TC05_NullValues: Test processing when critical fields contain null values \"\"\"\n    # A row with a null transaction_date or null amount should be handled appropriately\n    transactions_data = [\n        (None, \"active\", 1, \"misc\", 50.0),          # Null date\n        (\"2023-07-07\", \"active\", 1, \"misc\", None),    # Null amount\n        (\"2023-07-07\", \"active\", 1, \"misc\", 100.0)      # Valid row\n    ]\n    accounts_data = [(1, \"Account A\")]\n    transactions_df = create_transactions_df(spark, transactions_data)\n    accounts_df = create_accounts_df(spark, accounts_data)\n    result_df = process_data(spark, transactions_df, accounts_df)\n    \n    results = result_df.collect()\n    # Expect only the valid row to be aggregated.\n    if results:\n        row = results[0]\n        assert row[\"category\"] == \"misc\"\n        # Only the valid amount contributes to sum and average.\n        assert row[\"total_amount\"] == 100.0\n    else:\n        # In some implementations, rows with null values may be filtered out\n        assert True\n\n\ndef test_join_mismatch(spark):\n    \"\"\" TC06_JoinMismatch: Verify that transactions with no matching accounts are excluded after the join \"\"\"\n    transactions_data = [\n        (\"2023-08-10\", \"active\", 1, \"shopping\", 220.0),\n        (\"2023-08-10\", \"active\", 2, \"shopping\", 180.0)  # Account ID 2 will not be found\n    ]\n    accounts_data = [(1, \"Account A\")]\n    transactions_df = create_transactions_df(spark, transactions_data)\n    accounts_df = create_accounts_df(spark, accounts_data)\n    result_df = process_data(spark, transactions_df, accounts_df)\n    \n    results = result_df.collect()\n    # Only the transaction with account_id 1 should appear after the join.\n    assert len(results) == 1\n    row = results[0]\n    assert row[\"category\"] == \"shopping\"\n    assert row[\"total_amount\"] == 220.0\n\n\ndef test_aggregation_accuracy(spark):\n    \"\"\" TC07_AggregationAccuracy: Validate computed sum and average against expected values \"\"\"\n    transactions_data = [\n        (\"2023-09-01\", \"active\", 1, \"health\", 50.0),\n        (\"2023-09-01\", \"active\", 1, \"health\", 150.0),\n        (\"2023-09-01\", \"active\", 1, \"health\", 200.0),\n        (\"2023-09-01\", \"active\", 1, \"health\", 100.0)\n    ]\n    accounts_data = [(1, \"Account A\")]\n    transactions_df = create_transactions_df(spark, transactions_data)\n    accounts_df = create_accounts_df(spark, accounts_data)\n    result_df = process_data(spark, transactions_df, accounts_df)\n    \n    results = result_df.collect()\n    # Manual calculation: total = 500.0, average = 125.0\n    assert len(results) == 1\n    row = results[0]\n    assert row[\"category\"] == \"health\"\n    assert row[\"total_amount\"] == 500.0\n    assert row[\"average_amount\"] == 125.0\n\n# API Cost Consumed for this conversion call: $0.0035\n",
  "Total Cost": "$0.0035 (PostgreSQL to PySpark conversion) + $0.0035 (PyTest agent conversion) = $0.0070"
}