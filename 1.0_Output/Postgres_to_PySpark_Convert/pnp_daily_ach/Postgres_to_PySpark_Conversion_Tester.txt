--------------------------------------------------
Test Case List:
-----------------
TC1 - Happy Path Scenario:
  - Description: Provide a valid DataFrame with 'achievement_date' and 'amount' fields. The pipeline filters records from 2023-01-01 onward, truncates the date to the day, aggregates total amount and count per day, and orders by day.
  - Expected Outcome: A DataFrame with rows containing the day, total_amount (sum of amount), and total_count (record count) reflecting the valid input data.
  
TC2 - Empty DataFrame:
  - Description: Provide an empty DataFrame to the pipeline.
  - Expected Outcome: An empty DataFrame (no aggregation results).
  
TC3 - Null Values in 'amount' Column:
  - Description: Provide a DataFrame with valid 'achievement_date' but with null values in the 'amount' column. Here, we assume that null values are skipped during summation.
  - Expected Outcome: The aggregated DataFrame should correctly sum only non-null amounts and count records appropriately.
  
TC4 - Missing Required Column (Error Scenario):
  - Description: Provide a DataFrame that is missing the 'achievement_date' column.
  - Expected Outcome: The process should raise an exception (e.g., AnalysisException) due to the absence of the needed column.

--------------------------------------------------
Pytest Script (test_daily_achievements.py):
--------------------------------------------------
#!/usr/bin/env python3
"""
Pytest script for testing the PySpark daily achievements pipeline.
API cost consumed for this call: $0.0020
"""

import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, date_trunc, sum as _sum, count as _count
from pyspark.sql.utils import AnalysisException

# Fixture for SparkSession setup and teardown
@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("DailyAchievementsTest").getOrCreate()
    yield spark
    spark.stop()

# Helper function to simulate the transformation pipeline
def process_daily_achievements(pnpDF):
    # Filter data: achievements from 2023-01-01 onwards
    filteredDF = pnpDF.filter(col("achievement_date") >= "2023-01-01")
    
    # Truncate achievements to the day
    dailyDF = filteredDF.withColumn("day", date_trunc("day", col("achievement_date")))
    
    # Aggregate data: sum of amount and count of records per day
    aggDF = dailyDF.groupBy("day").agg(
        _sum("amount").alias("total_amount"),
        _count("*").alias("total_count")
    )
    
    # Order the result by day
    resultDF = aggDF.orderBy("day")
    return resultDF

# TC1: Happy Path Scenario
def test_happy_path(spark):
    """
    Test that valid data is correctly filtered, aggregated, and ordered.
    """
    data = [
        {"achievement_date": "2023-01-02 10:00:00", "amount": 100},
        {"achievement_date": "2023-01-02 15:30:00", "amount": 200},
        {"achievement_date": "2023-01-03 09:00:00", "amount": 150}
    ]
    # Create DataFrame
    df = spark.createDataFrame(data)
    
    resultDF = process_daily_achievements(df)
    
    # Collect results
    results = resultDF.collect()
    
    # Expected results:
    # For 2023-01-02: total_amount = 300, total_count = 2
    # For 2023-01-03: total_amount = 150, total_count = 1
    expected = {
        "2023-01-02": {"total_amount": 300, "total_count": 2},
        "2023-01-03": {"total_amount": 150, "total_count": 1},
    }
    
    # Verify each row
    for row in results:
        day_str = row["day"].strftime("%Y-%m-%d")
        assert day_str in expected
        assert row["total_amount"] == expected[day_str]["total_amount"]
        assert row["total_count"] == expected[day_str]["total_count"]

# TC2: Empty DataFrame
def test_empty_dataframe(spark):
    """
    Test that an empty DataFrame returns an empty result after processing.
    """
    # Create empty DataFrame with required schema
    schema = "achievement_date STRING, amount INT"
    empty_df = spark.createDataFrame([], schema)
    
    resultDF = process_daily_achievements(empty_df)
    
    # Collect results: should be empty list
    results = resultDF.collect()
    assert results == []

# TC3: Null Values in 'amount' Column
def test_null_values_in_amount(spark):
    """
    Test that the pipeline correctly processes records with null amounts.
    Nulls are skipped in summation.
    """
    data = [
        {"achievement_date": "2023-01-04 08:00:00", "amount": None},
        {"achievement_date": "2023-01-04 12:00:00", "amount": 50},
        {"achievement_date": "2023-01-04 16:00:00", "amount": None},
    ]
    df = spark.createDataFrame(data)
    
    resultDF = process_daily_achievements(df)
    results = resultDF.collect()
    
    # Expected: for 2023-01-04, total_amount should be 50 (None values skipped) and count should be 3.
    assert len(results) == 1
    row = results[0]
    day_str = row["day"].strftime("%Y-%m-%d")
    assert day_str == "2023-01-04"
    # Spark _sum function over nulls returns null if all values are null, otherwise it sums non-null values.
    # Here, sum should be 50.
    assert row["total_amount"] == 50
    assert row["total_count"] == 3

# TC4: Missing Required Column (Error Scenario)
def test_missing_achievement_date_column(spark):
    """
    Test that providing a DataFrame missing 'achievement_date' raises an AnalysisException.
    """
    # Create a DataFrame missing the 'achievement_date' column
    data = [
        {"timestamp": "2023-01-05 10:00:00", "amount": 100},
    ]
    schema = "timestamp STRING, amount INT"
    df = spark.createDataFrame(data, schema)
    
    with pytest.raises(Exception):
        # This call should raise an exception because 'achievement_date' column is not in the DataFrame.
        _ = process_daily_achievements(df)
--------------------------------------------------
(API cost consumed for this call: $0.0020)