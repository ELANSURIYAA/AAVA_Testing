### **1. Test Case List**

| Test Case ID | Test Case Description | Expected Outcome |
| :--- | :--- | :--- |
| **Happy Path** |
| TC-HP-001 | Test with a valid XML string containing multiple member records. | The DataFrame should be created with the correct schema and all records should be parsed correctly. |
| **Edge Cases** |
| TC-EC-001 | Test with an empty XML string. | The function should handle the empty input gracefully and return an empty DataFrame with the correct schema. |
| TC-EC-002 | Test with an XML string containing a single member record. | The DataFrame should be created with one record, parsed correctly. |
| TC-EC-003 | Test with an XML string where some optional fields are missing. | The DataFrame should be created with null values for the missing fields. |
| TC-EC-004 | Test with an XML string containing records with extra, unmapped fields. | The extra fields should be ignored, and the DataFrame should be created with only the mapped fields. |
| **Exception Scenarios** |
| TC-EX-001 | Test with a malformed XML string. | The function should raise a `pyspark.sql.utils.AnalysisException` due to the inability to parse the XML. |
| TC-EX-002 | Test with an input that is not a string (e.g., an integer or a list). | The function should raise an `AttributeError` as it expects a string input for the RDD creation. |

---

### **2. Pytest Script**

```python
# test_xml_processor.py

import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    DateType,
)
from pyspark.sql.utils import AnalysisException
import datetime

# It's assumed that the PySpark script to be tested is saved as 'xml_processor.py'
# and contains the 'process_xml_data' function.

from xml_processor import process_xml_data


@pytest.fixture(scope="session")
def spark():
    """
    Pytest fixture to create a SparkSession for the test suite.
    This is optimized for a local testing environment but is compatible with
    how Spark sessions are managed in GCP DataProc.
    """
    session = (
        SparkSession.builder.appName("PySpark XML Processing Tests")
        .master("local[2]")
        .config(
            "spark.jars.packages", "com.databricks:spark-xml_2.12:0.13.0"
        )  # Ensure spark-xml is available
        .getOrCreate()
    )
    yield session
    session.stop()


def test_process_xml_data_happy_path(spark):
    """
    Test Case ID: TC-HP-001
    Test with a valid XML string containing multiple member records.
    """
    xml_string = """
    <members>
        <member>
            <MemberID>1</MemberID>
            <FirstName>John</FirstName>
            <LastName>Doe</LastName>
            <DateOfBirth>1990-01-15</DateOfBirth>
            <Gender>Male</Gender>
        </member>
        <member>
            <MemberID>2</MemberID>
            <FirstName>Jane</FirstName>
            <LastName>Smith</LastName>
            <DateOfBirth>1985-05-20</DateOfBirth>
            <Gender>Female</Gender>
        </member>
    </members>
    """
    result_df = process_xml_data(spark, xml_string)

    expected_schema = StructType(
        [
            StructField("member_id", IntegerType(), True),
            StructField("first_name", StringType(), True),
            StructField("last_name", StringType(), True),
            StructField("date_of_birth", DateType(), True),
            StructField("gender", StringType(), True),
        ]
    )

    expected_data = [
        (1, "John", "Doe", datetime.date(1990, 1, 15), "Male"),
        (2, "Jane", "Smith", datetime.date(1985, 5, 20), "Female"),
    ]
    expected_df = spark.createDataFrame(expected_data, expected_schema)

    assert result_df.count() == 2
    assert result_df.schema == expected_df.schema
    # Use subtract to find differences in dataframes. An empty result means they are identical.
    assert result_df.subtract(expected_df).count() == 0
    assert expected_df.subtract(result_df).count() == 0


def test_process_xml_data_empty_input(spark):
    """
    Test Case ID: TC-EC-001
    Test with an empty XML string.
    """
    xml_string = ""
    result_df = process_xml_data(spark, xml_string)
    assert result_df.count() == 0
    expected_schema = StructType(
        [
            StructField("member_id", IntegerType(), True),
            StructField("first_name", StringType(), True),
            StructField("last_name", StringType(), True),
            StructField("date_of_birth", DateType(), True),
            StructField("gender", StringType(), True),
        ]
    )
    assert result_df.schema == expected_schema


def test_process_xml_data_single_record(spark):
    """
    Test Case ID: TC-EC-002
    Test with an XML string containing a single member record.
    """
    xml_string = """
    <members>
        <member>
            <MemberID>101</MemberID>
            <FirstName>Sam</FirstName>
            <LastName>Jones</LastName>
            <DateOfBirth>2000-10-10</DateOfBirth>
            <Gender>Male</Gender>
        </member>
    </members>
    """
    result_df = process_xml_data(spark, xml_string)
    assert result_df.count() == 1
    record = result_df.first()
    assert record["member_id"] == 101
    assert record["first_name"] == "Sam"


def test_process_xml_data_missing_optional_fields(spark):
    """
    Test Case ID: TC-EC-003
    Test with an XML string where some optional fields are missing.
    """
    xml_string = """
    <members>
        <member>
            <MemberID>202</MemberID>
            <FirstName>Alex</FirstName>
            <LastName>Ray</LastName>
        </member>
    </members>
    """
    result_df = process_xml_data(spark, xml_string)
    assert result_df.count() == 1
    record = result_df.first()
    assert record["member_id"] == 202
    assert record["first_name"] == "Alex"
    assert record["date_of_birth"] is None
    assert record["gender"] is None


def test_process_xml_data_with_extra_fields(spark):
    """
    Test Case ID: TC-EC-004
    Test with an XML string containing records with extra, unmapped fields.
    """
    xml_string = """
    <members>
        <member>
            <MemberID>303</MemberID>
            <FirstName>Chris</FirstName>
            <LastName>Green</LastName>
            <DateOfBirth>1995-02-25</DateOfBirth>
            <Gender>Male</Gender>
            <Address>123 Main St</Address>
        </member>
    </members>
    """
    result_df = process_xml_data(spark, xml_string)
    assert result_df.count() == 1
    assert "Address" not in result_df.columns
    record = result_df.first()
    assert record["member_id"] == 303


def test_process_xml_data_malformed_xml(spark):
    """
    Test Case ID: TC-EX-001
    Test with a malformed XML string.
    """
    xml_string = """
    <members>
        <member>
            <MemberID>404</MemberID>
            <FirstName>Malf</FirstName>
            <LastName>Ormed</LastName>
        <!-- Missing closing member tag -->
    </members>
    """
    # The spark-xml library is robust and might parse this.
    # A more severely malformed XML might be needed to trigger an exception.
    # Let's try one that is fundamentally broken.
    broken_xml = "<members><member><FirstName>Test</FirstName></member"
    with pytest.raises(AnalysisException):
        # This exception is expected because the XML is not well-formed
        process_xml_data(spark, broken_xml).show()


def test_process_xml_data_invalid_input_type(spark):
    """
    Test Case ID: TC-EX-002
    Test with an input that is not a string.
    """
    with pytest.raises(AttributeError):
        # The function expects a string to create an RDD, passing an int will fail.
        process_xml_data(spark, 12345)

```

---

### **3. API Cost Calculation**

*   **apiCost:** 0.00125 USD