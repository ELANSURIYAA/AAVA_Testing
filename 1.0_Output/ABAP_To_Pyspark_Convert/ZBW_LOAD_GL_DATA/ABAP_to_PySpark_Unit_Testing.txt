------------------------------
Test Case List:
------------------------------
1. Test Case ID: TC1_HappyPath_BelowThreshold
   - Description: Create a DataFrame with a value less than 50 (e.g., 30). The transformation should add a column 'message' with "Below threshold: 30".
   - Expected Outcome: The DataFrame returns one row with value 30 and message "Below threshold: 30".

2. Test Case ID: TC2_HappyPath_AboveThreshold
   - Description: Create a DataFrame with a value equal or above 50 (e.g., 100). The transformation should add a column 'message' with "Above threshold: 100".
   - Expected Outcome: The DataFrame returns one row with value 100 and message "Above threshold: 100".

3. Test Case ID: TC3_EmptyDataFrame
   - Description: Create an empty DataFrame with the column 'value'. The transformation should result in an empty DataFrame.
   - Expected Outcome: The transformed DataFrame is empty.

4. Test Case ID: TC4_NullValue
   - Description: Create a DataFrame with a row containing a null for 'value'. The transformation should handle the null gracefully (likely resulting in a null in the 'message' column).
   - Expected Outcome: The DataFrame returns one row with a null value and a null (or handled) message.

5. Test Case ID: TC5_InvalidDataType
   - Description: Create a DataFrame with an invalid data type (e.g., a string value) for 'value'. Since casting a string to string is allowed but concatenation may not produce a meaningful message, the test should check if an appropriate error is thrown or the result is handled properly.
   - Expected Outcome: Either the test asserts catching of an exception or validation that the result does not meet the expected numeric transformation.

------------------------------
Pytest Script:
------------------------------
#!/usr/bin/env python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col
from pyspark.sql.types import IntegerType, StringType, StructField, StructType

# Fixture: setup and teardown SparkSession
@pytest.fixture(scope="module")
def spark():
    spark_session = SparkSession.builder.appName("ZBW_LOAD_GL_DATA_Conversion_Test").getOrCreate()
    yield spark_session
    spark_session.stop()

def transform_dataframe(df):
    # Transformation logic similar to the given PySpark code:
    return df.withColumn("message", when(col("value") < 50, "Below threshold: " + col("value").cast("string"))
                         .otherwise("Above threshold: " + col("value").cast("string")))

# Test Case TC1: Happy Path - value less than 50
def test_below_threshold(spark):
    # Setup input DataFrame with a value less than 50
    data = [(30,)]
    schema = StructType([StructField("value", IntegerType(), True)])
    df = spark.createDataFrame(data, schema)
    
    # Apply transformation
    result_df = transform_dataframe(df)
    result = result_df.collect()
    
    # Expected message
    expected_message = "Below threshold: 30"
    
    # Assert that the result has the expected message
    assert result[0]["message"] == expected_message, f"Expected message {expected_message} but got {result[0]['message']}"

# Test Case TC2: Happy Path - value equal or above 50
def test_above_threshold(spark):
    # Setup input DataFrame with a value of 100 (>= 50)
    data = [(100,)]
    schema = StructType([StructField("value", IntegerType(), True)])
    df = spark.createDataFrame(data, schema)
    
    # Apply transformation
    result_df = transform_dataframe(df)
    result = result_df.collect()
    
    # Expected message
    expected_message = "Above threshold: 100"
    
    # Assert that the result has the expected message
    assert result[0]["message"] == expected_message, f"Expected message {expected_message} but got {result[0]['message']}"

# Test Case TC3: Edge Case - Empty DataFrame
def test_empty_dataframe(spark):
    # Setup an empty DataFrame with column 'value'
    schema = StructType([StructField("value", IntegerType(), True)])
    df = spark.createDataFrame([], schema)
    
    # Apply transformation
    result_df = transform_dataframe(df)
    
    # Assert that the DataFrame is empty
    assert result_df.rdd.isEmpty(), "Expected the DataFrame to be empty"

# Test Case TC4: Edge Case - Null value 
def test_null_value(spark):
    # Setup DataFrame with a null value for 'value'
    data = [(None,)]
    schema = StructType([StructField("value", IntegerType(), True)])
    df = spark.createDataFrame(data, schema)
    
    # Apply transformation
    result_df = transform_dataframe(df)
    result = result_df.collect()
    
    # For null, the when condition should not match and cast should result in a null.
    assert result[0]["message"] is None, f"Expected message to be None when value is null, but got {result[0]['message']}"

# Test Case TC5: Error Handling - Invalid data type (string instead of integer)
def test_invalid_data_type(spark):
    # Setup DataFrame with an invalid data type (string)
    data = [("invalid",)]
    schema = StructType([StructField("value", StringType(), True)])
    df = spark.createDataFrame(data, schema)
    
    # Since the transformation logic expects numerical comparison,
    # attempting to perform 'value' < 50 may result in an exception.
    with pytest.raises(Exception):
        # This invocation is expected to raise an exception as invalid comparison is performed.
        transform_dataframe(df).collect()

# Cost consumed by the API for this call: 0.001 units