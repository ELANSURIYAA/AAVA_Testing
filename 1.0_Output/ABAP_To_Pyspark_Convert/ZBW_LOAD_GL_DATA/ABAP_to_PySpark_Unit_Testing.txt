Test Case List:
----------------
1. Test Case ID: TC001_HappyPath
   - Description: Verify that for valid input data, rows with 'value' > 1000 receive a 'High' status and rows with 'value' <= 1000 receive a 'Normal' status, and that the aggregation returns the correct sum.
   - Expected Outcome: 'status' column is correctly assigned and the aggregated total equals the sum of all numeric values.

2. Test Case ID: TC002_EmptyDataFrame
   - Description: Test the scenario where the input DataFrame is empty.
   - Expected Outcome: The transformation should execute without errors, and the aggregation should return None (or 0 based on Spark behavior).

3. Test Case ID: TC003_NullValues
   - Description: Verify the behavior when the 'value' column contains null values.
   - Expected Outcome: Rows with null values should yield 'Normal' status by default and the aggregation should sum only non-null values.

4. Test Case ID: TC004_BoundaryCondition
   - Description: Test boundary conditions, specifically when 'value' is exactly 1000.
   - Expected Outcome: Rows with 'value' equal to 1000 should be assigned a 'Normal' status since the condition is strictly > 1000, and the aggregation must reflect the correct sum.

5. Test Case ID: TC005_InvalidDataType
   - Description: Check error handling when the 'value' column contains non-numeric data (e.g., strings).
   - Expected Outcome: The transformation and/or aggregation should raise an appropriate exception due to type mismatch.

Pytest Script (pytest_pyspark_tests.py):
--------------------------------------------------
#!/usr/bin/env python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col, sum as sum_

# Fixture for creating and stopping a Spark session before and after tests
@pytest.fixture(scope="function")
def spark_session():
    spark = SparkSession.builder.master("local[*]").appName("PySparkTestSuite").getOrCreate()
    yield spark
    spark.stop()

# Function that mimics the transformation described in the original PySpark code.
def transform_gl_data(df):
    # Adds a new column 'status' based on condition: if 'value' > 1000 then 'High', else 'Normal'
    df = df.withColumn("status", when(col("value") > 1000, "High").otherwise("Normal"))
    return df

def aggregate_total_value(df):
    # Aggregates the total value from the 'value' column.
    result = df.agg(sum_("value").alias("total_value")).collect()
    # Return total_value if available else None for empty DataFrame
    return result[0]["total_value"] if result and result[0]["total_value"] is not None else None

# Test Case: Happy Path scenario
def test_TC001_HappyPath(spark_session):
    # Create sample data with values above and below 1000.
    data = [
        (1, 500),
        (2, 1500),
        (3, 800),
        (4, 2000)
    ]
    schema = "id INT, value INT"
    df = spark_session.createDataFrame(data, schema)
    
    # Perform transformation
    transformed_df = transform_gl_data(df)
    
    # Collect data for verification
    result = transformed_df.select("id", "value", "status").collect()
    # Create a mapping for expected status based on the test data:
    expected_status = {1: "Normal", 2: "High", 3: "Normal", 4: "High"}
    for row in result:
        assert row.status == expected_status[row.id], f"Row with id {row.id} should be {expected_status[row.id]}"
    
    # Test aggregation of total value
    total_value = aggregate_total_value(transformed_df)
    expected_total = sum([500, 1500, 800, 2000])
    assert total_value == expected_total, f"Total value should be {expected_total}"

# Test Case: Empty DataFrame scenario
def test_TC002_EmptyDataFrame(spark_session):
    # Create an empty DataFrame with the expected schema.
    schema = "id INT, value INT"
    empty_df = spark_session.createDataFrame([], schema)
    
    transformed_df = transform_gl_data(empty_df)
    # Verify that the DataFrame remains empty.
    assert transformed_df.rdd.isEmpty(), "Transformed DataFrame should be empty"
    
    # Aggregation should return None for an empty DataFrame.
    total_value = aggregate_total_value(transformed_df)
    assert total_value is None, "Total value for empty DataFrame should be None"

# Test Case: Null values in 'value' column
def test_TC003_NullValues(spark_session):
    # Create data with null values in the 'value' column.
    data = [
        (1, None),
        (2, 1500),
        (3, None),
        (4, 800)
    ]
    schema = "id INT, value INT"
    df = spark_session.createDataFrame(data, schema)
    
    transformed_df = transform_gl_data(df)
    result = transformed_df.select("id", "value", "status").collect()
    
    # Check status assignment: when value is null, the condition should default to 'Normal'
    for row in result:
        if row.value is None:
            assert row.status == "Normal", "Null values should default to 'Normal'"
        elif row.value > 1000:
            assert row.status == "High", "Value greater than 1000 should be 'High'"
        else:
            assert row.status == "Normal", "Value less than or equal to 1000 should be 'Normal'"
            
    # The aggregate should ignore nulls.
    total_value = aggregate_total_value(transformed_df)
    expected_total = 1500 + 800
    assert total_value == expected_total, f"Total value should sum non-null values and equal {expected_total}"

# Test Case: Boundary conditions (value exactly 1000)
def test_TC004_BoundaryCondition(spark_session):
    # Create data to test boundary where value equals 1000.
    data = [
        (1, 1000),
        (2, 1001),
        (3, 999)
    ]
    schema = "id INT, value INT"
    df = spark_session.createDataFrame(data, schema)
    
    transformed_df = transform_gl_data(df)
    result = transformed_df.select("id", "value", "status").collect()
    
    for row in result:
        if row.value > 1000:
            assert row.status == "High", "Value greater than 1000 should be 'High'"
        else:
            # For 1000 and 999, it should be 'Normal'
            assert row.status == "Normal", f"Value {row.value} should be 'Normal'"
            
    total_value = aggregate_total_value(transformed_df)
    expected_total = sum([1000, 1001, 999])
    assert total_value == expected_total, f"Total value should equal {expected_total}"

# Test Case: Invalid data type for 'value' column
def test_TC005_InvalidDataType(spark_session):
    # Create data where the 'value' is not numeric (a string instead) to test error handling.
    data = [
        (1, "not_a_number"),
        (2, 1500)
    ]
    schema = "id INT, value STRING"  # use STRING to simulate the invalid type for numeric operations.
    df = spark_session.createDataFrame(data, schema)
    
    # Attempting the transformation and aggregation should raise an exception.
    with pytest.raises(Exception):
        transformed_df = transform_gl_data(df)
        _ = aggregate_total_value(transformed_df)

# Cost consumed by the API for this call: 0.02 units