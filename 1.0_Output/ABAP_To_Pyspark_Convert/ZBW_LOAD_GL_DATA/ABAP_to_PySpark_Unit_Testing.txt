-------------------------------------------------
Test Case List:
-------------------------------------------------
• Test case ID: TC_001
  - Description: Validate that for records with amount > 1000 the new column "status" is set to "HIGH".
  - Expected Outcome: All rows with amount > 1000 have "status" equal to "HIGH", and rows with amount <= 1000 have "status" equal to "NORMAL".

• Test case ID: TC_002
  - Description: Validate that for records with amount <= 1000 the new column "status" is set to "NORMAL".
  - Expected Outcome: Rows with amount <= 1000 return "NORMAL" in the "status" column.

• Test case ID: TC_003
  - Description: Validate that an empty DataFrame returns an empty DataFrame with the same schema after transformation.
  - Expected Outcome: The transformation should run and yield an empty DataFrame with identical schema including the "status" column.

• Test case ID: TC_004
  - Description: Validate that when the "amount" column contains null values, the transformation handles them correctly (e.g., outputs "NORMAL" for null or handles them appropriately).
  - Expected Outcome: Rows with null "amount" are assigned "NORMAL" in the "status" column (given when() condition defaults to otherwise()).

• Test case ID: TC_005
  - Description: Validate that the DataFrame schema remains consistent after the transformation.
  - Expected Outcome: The output DataFrame’s schema must include all original columns plus the "status" column with string type.

-------------------------------------------------
Pytest Script:
-------------------------------------------------
#!/usr/bin/env python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# Define the data transformation function (as in the original PySpark code)
def transform_gl_dataframe(df):
    """
    Transforms the input DataFrame by adding a new column 'status'.
    If amount > 1000 -> 'HIGH', otherwise 'NORMAL'.
    """
    return df.withColumn("status", when(col("amount") > 1000, "HIGH").otherwise("NORMAL"))

@pytest.fixture(scope="session")
def spark():
    spark_session = SparkSession.builder.master("local[2]").appName("PySparkTest").getOrCreate()
    yield spark_session
    spark_session.stop()

@pytest.fixture
def sample_schema():
    return StructType([
        StructField("id", IntegerType(), True),
        StructField("amount", DoubleType(), True),
        StructField("description", StringType(), True)
    ])

@pytest.fixture
def sample_data(sample_schema):
    # Data for happy path testing
    data = [
        (1, 1500.0, "Payment A"),   # amount > 1000 => HIGH
        (2, 500.0, "Payment B"),    # amount <= 1000 => NORMAL
        (3, 1000.0, "Payment C")    # boundary value => NORMAL
    ]
    return data

def test_transformation_status_values(spark, sample_schema, sample_data):
    """
    TC_001 & TC_002:
    Validate that the transformation correctly assigns 'HIGH' if amount > 1000,
    and 'NORMAL' otherwise.
    """
    df = spark.createDataFrame(sample_data, schema=sample_schema)
    df_transformed = transform_gl_dataframe(df)
    result = {row.id: row.status for row in df_transformed.collect()}
    # Expected: id 1 -> HIGH, others -> NORMAL
    assert result[1] == "HIGH"
    assert result[2] == "NORMAL"
    assert result[3] == "NORMAL"

def test_empty_dataframe(spark, sample_schema):
    """
    TC_003:
    Validate that an empty DataFrame returns an empty DataFrame with the same schema after transformation.
    """
    empty_rdd = spark.sparkContext.emptyRDD()
    df_empty = spark.createDataFrame(empty_rdd, schema=sample_schema)
    df_transformed = transform_gl_dataframe(df_empty)
    # Check that there are no rows and the new column exists in schema
    assert df_transformed.count() == 0
    schema_fields = [field.name for field in df_transformed.schema.fields]
    assert "status" in schema_fields

def test_null_amount_handling(spark, sample_schema):
    """
    TC_004:
    Validate that when the 'amount' column contains null values, it defaults to 'NORMAL' (per otherwise clause).
    """
    data_with_null = [
        (1, None, "Payment Null")
    ]
    df = spark.createDataFrame(data_with_null, schema=sample_schema)
    df_transformed = transform_gl_dataframe(df)
    row = df_transformed.collect()[0]
    # When amount is null, the when clause condition is not met and "NORMAL" should be set.
    assert row.status == "NORMAL"

def test_schema_preservation(spark, sample_schema, sample_data):
    """
    TC_005:
    Validate that the DataFrame schema remains consistent after transformation,
    ensuring that the new 'status' column is added with the correct data type.
    """
    df = spark.createDataFrame(sample_data, schema=sample_schema)
    df_transformed = transform_gl_dataframe(df)
    # Check that original fields are present and new field 'status' is added.
    expected_fields = set(["id", "amount", "description", "status"])
    actual_fields = set(field.name for field in df_transformed.schema.fields)
    assert expected_fields == actual_fields
    # Validate that "status" field is of StringType
    status_field = [field for field in df_transformed.schema.fields if field.name == "status"][0]
    assert isinstance(status_field.dataType, StringType)

# Optional test for incorrect data types (if applicable)
def test_incorrect_data_type(spark, sample_schema):
    """
    Additional Edge Case:
    Validate that providing non-numeric data in 'amount' column causes an exception
    or is handled correctly (depending on implementation specifics).
    Here, we simulate the scenario by casting and capturing the error.
    """
    data_with_error = [
        (1, "not_a_number", "Payment Error")
    ]
    # Modify schema to treat amount as StringType improperly
    error_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    df = spark.createDataFrame(data_with_error, schema=error_schema)
    # Try to cast "amount" to double and then transform
    from pyspark.sql.functions import col
    df_casted = df.withColumn("amount", col("amount").cast("double"))
    df_transformed = transform_gl_dataframe(df_casted)
    # Since "not_a_number" cannot be cast, amount becomes null, thus "NORMAL"
    row = df_transformed.collect()[0]
    assert row.status == "NORMAL"

# API call cost consumed: $0.005

-------------------------------------------------
End of Final Answer.