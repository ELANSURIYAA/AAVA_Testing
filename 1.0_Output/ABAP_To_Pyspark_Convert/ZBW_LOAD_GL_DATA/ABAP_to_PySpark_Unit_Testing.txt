------------------------------------------------------------
Test Case List:
------------------------------------------------------------
TC001 - Happy Path Test:
   - Description: Input a valid CSV row in a text DataFrame with exactly 7 comma-separated values. Verify that the output DataFrame returns one row with correctly mapped columns ("bukrs", "fiscyear", "costcenter", "gl_account", "amount" cast to float, "currency", "posting_date").
   - Expected Outcome: Final DataFrame contains one row with each field correctly extracted and "amount" as a float.

TC002 - Insufficient Fields Test:
   - Description: Input a row with fewer than 7 fields. Verify that such rows are filtered out.
   - Expected Outcome: Final DataFrame is empty.

TC003 - Excess Fields Test:
   - Description: Input a row with more than 7 fields. Verify that such rows are filtered out.
   - Expected Outcome: Final DataFrame is empty.

TC004 - Amount Casting Test:
   - Description: Validate that the "amount" field is properly cast from string to float. Provide a valid numeric string and check its type in the final output.
   - Expected Outcome: "amount" column value is of float type and correctly converted.

TC005 - Empty File Test:
   - Description: Provide an empty DataFrame (simulating an empty file) and verify that the process does not fail and returns an empty DataFrame.
   - Expected Outcome: Final DataFrame is empty.

------------------------------------------------------------
Pytest Script (pytest_pyspark_tests.py):
------------------------------------------------------------
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Pytest script for testing the PySpark conversion code which mimics the functionality of the original ABAP code.
This script includes:
 - Setup/teardown of SparkSession.
 - Unit tests for valid input, insufficient columns, excess columns, amount casting, and empty input.
 - Comments explaining each test case.
"""

import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, size
from pyspark.sql.types import FloatType

# Fixture for SparkSession
@pytest.fixture(scope="module")
def spark():
    spark_session = SparkSession.builder \
        .appName("PySpark Unit Tests") \
        .master("local[2]") \
        .getOrCreate()
    yield spark_session
    spark_session.stop()

def transform_data(raw_df):
    """
    Function to simulate the transformation logic from the provided PySpark code.
    This includes splitting the text, filtering by field count, mapping columns and casting 'amount'.
    """
    split_df = raw_df.withColumn("fields", split(col("value"), ","))
    # Filter rows with exactly 7 fields
    filtered_df = split_df.filter(size(col("fields")) == 7)
    # Map fields to columns
    final_df = filtered_df.select(
        col("fields").getItem(0).alias("bukrs"),
        col("fields").getItem(1).alias("fiscyear"),
        col("fields").getItem(2).alias("costcenter"),
        col("fields").getItem(3).alias("gl_account"),
        col("fields").getItem(4).alias("amount"),
        col("fields").getItem(5).alias("currency"),
        col("fields").getItem(6).alias("posting_date")
    )
    final_df = final_df.withColumn("amount", col("amount").cast(FloatType()))
    return final_df

def test_happy_path(spark):
    """
    TC001 - Happy Path
    Create a DataFrame with one valid CSV row (exactly 7 fields) and verify proper mapping and type casting.
    """
    data = [("1000,2021,CC01,4000,123.45,USD,2021-12-31", )]
    raw_df = spark.createDataFrame(data, ["value"])
    result_df = transform_data(raw_df)
    
    result = result_df.collect()
    assert len(result) == 1, "Expected one row in the final DataFrame"
    row = result[0]
    # Verify each field mapping
    assert row["bukrs"] == "1000"
    assert row["fiscyear"] == "2021"
    assert row["costcenter"] == "CC01"
    assert row["gl_account"] == "4000"
    # Verify that amount is cast to float
    assert isinstance(row["amount"], float)
    assert row["amount"] == 123.45
    assert row["currency"] == "USD"
    assert row["posting_date"] == "2021-12-31"

def test_insufficient_fields(spark):
    """
    TC002 - Insufficient Fields
    Test with a row that has fewer than 7 comma-separated fields.
    The expected behavior is to filter out such rows so that the result is an empty DataFrame.
    """
    data = [("1000,2021,CC01,4000,123.45,USD", )]  # Only 6 fields
    raw_df = spark.createDataFrame(data, ["value"])
    result_df = transform_data(raw_df)
    
    result = result_df.collect()
    assert len(result) == 0, "Expected no rows in the final DataFrame for insufficient fields"

def test_excess_fields(spark):
    """
    TC003 - Excess Fields
    Test with a row that has more than 7 comma-separated fields.
    The expected behavior is to filter out such rows.
    """
    data = [("1000,2021,CC01,4000,123.45,USD,2021-12-31,ExtraField", )]  # 8 fields
    raw_df = spark.createDataFrame(data, ["value"])
    result_df = transform_data(raw_df)
    
    result = result_df.collect()
    assert len(result) == 0, "Expected no rows in the final DataFrame for excess fields"

def test_amount_casting(spark):
    """
    TC004 - Amount Casting Test
    This test ensures that the 'amount' field is correctly cast to a float.
    """
    data = [("1000,2021,CC01,4000,456.78,USD,2021-12-31", )]
    raw_df = spark.createDataFrame(data, ["value"])
    result_df = transform_data(raw_df)
    
    result = result_df.collect()
    assert len(result) == 1, "Expected one row after transformation"
    row = result[0]
    assert isinstance(row["amount"], float), "The amount field should be of float type"
    assert row["amount"] == 456.78, "The converted float value of amount does not match expected value"

def test_empty_file(spark):
    """
    TC005 - Empty File Test
    Test with an empty DataFrame (simulating an empty file) and ensure that the transformation handles it gracefully.
    """
    # Create an empty DataFrame with the same schema as the input text file
    empty_rdd = spark.sparkContext.emptyRDD()
    raw_df = spark.createDataFrame(empty_rdd, ["value"])
    result_df = transform_data(raw_df)
    
    result = result_df.collect()
    assert len(result) == 0, "Expected an empty DataFrame for an empty input"

# API cost consumed for this call: 1 unit
------------------------------------------------------------