--------------------------------------------------
Test Case List:
1. TC1 (Happy Path):
   - Description: Validate that for a valid CSV input, the code correctly reads the file, caches the DataFrame, applies any necessary transformations, and writes the output as a parquet file without error.
   - Expected Outcome: The output parquet file exists and its row count equals the input row count.
2. TC2 (Read Error):
   - Description: Simulate an error when reading the CSV file (e.g., by monkeypatching the Spark CSV reader to throw an exception). This verifies that the error is caught, an error message is printed, and the application exits.
   - Expected Outcome: The process prints an error message and exits (raising a SystemExit exception).
3. TC3 (Write Error):
   - Description: Simulate an error during the write operation (e.g., by monkeypatching the DataFrame write method to raise an exception). This verifies that the error is handled properly upon write failure.
   - Expected Outcome: The process prints an error message and exits (raising a SystemExit exception).
4. TC4 (Empty DataFrame):
   - Description: Provide an empty CSV file input and verify that the output parquet dataset is created without error and is empty.
   - Expected Outcome: The output parquet exists and contains zero rows.
5. TC5 (Data Integrity):
   - Description: Provide a sample CSV with multiple rows and verify that the output parquet file contains the same data (i.e., the number of rows and content of each row match the input).
   - Expected Outcome: The row count in the output exactly equals the row count in the input, and the data remains unchanged.

--------------------------------------------------
Pytest Script for the Test Cases:

#!/usr/bin/env python
# test_gl_data_load.py

import os
import sys
import tempfile
import pytest
from pyspark.sql import SparkSession
from unittest.mock import patch

# Import the main function from the module containing the PySpark code.
# Assumption: The PySpark code is in a file named "gl_data_load.py" and main() is modified to accept input_path and output_path as optional parameters.
# If not, you may need to refactor the code accordingly for testability.
from gl_data_load import main

# Fixture for SparkSession setup and teardown.
@pytest.fixture(scope="module")
def spark_session():
    spark = SparkSession.builder.master("local[1]").appName("PySparkUnitTest").getOrCreate()
    yield spark
    spark.stop()

# Test Case 1: Happy Path
def test_happy_path(spark_session):
    # Prepare a valid CSV input with header and sample data
    input_data = ("bukrs,fiscyear,costcenter,gl_account,amount,currency,posting_date\n"
                  "1000,2023,CC01,GL01,1000,USD,2023-01-01\n"
                  "2000,2023,CC02,GL02,2000,EUR,2023-01-02\n")
    with tempfile.NamedTemporaryFile(mode='w', suffix=".csv", delete=False) as temp_input:
        temp_input.write(input_data)
        temp_input_path = temp_input.name

    output_dir = tempfile.mkdtemp()

    # Run main with temporary input and output paths.
    # Note: main is assumed to accept input_path and output_path as parameters.
    main(input_path=temp_input_path, output_path=output_dir)

    # Validate that output parquet file(s) exist.
    output_files = os.listdir(output_dir)
    assert any("parquet" in file for file in output_files), "Parquet output files not found."

    # Read the output parquet and check row count.
    df_out = spark_session.read.parquet(output_dir)
    assert df_out.count() == 2, "Row count mismatch between input and output."

    # Cleanup temporary file.
    os.remove(temp_input_path)

# Test Case 2: Read Error
def test_read_error():
    # Using patch to simulate a read error by throwing Exception on csv() call.
    with patch("pyspark.sql.DataFrameReader.csv", side_effect=Exception("Intentional Read Error")):
        with pytest.raises(SystemExit):
            main(input_path="non_existent_input.csv", output_path="some_output_dir")

# Test Case 3: Write Error
def test_write_error(spark_session):
    # Prepare a valid CSV input.
    input_data = ("bukrs,fiscyear,costcenter,gl_account,amount,currency,posting_date\n"
                  "1000,2023,CC01,GL01,1000,USD,2023-01-01\n")
    with tempfile.NamedTemporaryFile(mode='w', suffix=".csv", delete=False) as temp_input:
        temp_input.write(input_data)
        temp_input_path = temp_input.name

    # Patch the DataFrameWriter.parquet method to raise an exception to mimic write failure.
    with patch("pyspark.sql.DataFrameWriter.parquet", side_effect=Exception("Intentional Write Error")):
        with pytest.raises(SystemExit):
            main(input_path=temp_input_path, output_path="dummy_output_dir")

    os.remove(temp_input_path)

# Test Case 4: Empty DataFrame
def test_empty_dataframe(spark_session):
    # Create an empty CSV file (only header, or completely empty).
    input_data = ""
    with tempfile.NamedTemporaryFile(mode='w', suffix=".csv", delete=False) as temp_input:
        temp_input.write(input_data)
        temp_input_path = temp_input.name

    output_dir = tempfile.mkdtemp()

    # Run the main function.
    main(input_path=temp_input_path, output_path=output_dir)

    # Check that output parquet exists.
    output_files = os.listdir(output_dir)
    assert any("parquet" in file for file in output_files), "Parquet output files not found for empty input."

    # Verify that the output parquet is empty.
    df_out = spark_session.read.parquet(output_dir)
    assert df_out.count() == 0, "Expected zero rows for empty input."

    os.remove(temp_input_path)

# Test Case 5: Data Integrity
def test_data_integrity(spark_session):
    # Prepare a CSV input with multiple rows.
    input_data = ("bukrs,fiscyear,costcenter,gl_account,amount,currency,posting_date\n"
                  "1000,2023,CC01,GL01,1000,USD,2023-01-01\n"
                  "2000,2023,CC02,GL02,2000,EUR,2023-01-02\n")
    with tempfile.NamedTemporaryFile(mode='w', suffix=".csv", delete=False) as temp_input:
        temp_input.write(input_data)
        temp_input_path = temp_input.name

    output_dir = tempfile.mkdtemp()

    # Run main function.
    main(input_path=temp_input_path, output_path=output_dir)

    # Validate that output parquet file(s) exist.
    output_files = os.listdir(output_dir)
    assert any("parquet" in file for file in output_files), "Parquet output files not found."

    # Load the data from parquet.
    df_out = spark_session.read.parquet(output_dir)
    # Load original CSV data for comparison.
    df_in = spark_session.read.csv(temp_input_path, header=True, inferSchema=True)

    # Check row count integrity.
    assert df_out.count() == df_in.count(), "Row count mismatch between input and output."

    # Check data integrity by sorting and comparing all rows.
    out_rows = sorted(df_out.collect(), key=lambda r: tuple(r))
    in_rows = sorted(df_in.collect(), key=lambda r: tuple(r))
    assert out_rows == in_rows, "Data content mismatch between input and output."

    os.remove(temp_input_path)

# Note:
# The API cost consumed in running the original PySpark code is 0.002 USD.
--------------------------------------------------
Cost Consumed: 0.002 USD