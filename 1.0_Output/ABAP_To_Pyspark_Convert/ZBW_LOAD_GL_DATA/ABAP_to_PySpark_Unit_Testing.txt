Test Case List:
1. Test Case ID: TC001
   - Description: Test with a mix of valid lines and lines containing '*ERROR*'.
   - Expected Outcome: Only valid lines are counted; rows with '*ERROR*' are excluded.
2. Test Case ID: TC002
   - Description: Test with all valid lines.
   - Expected Outcome: All lines are counted.
3. Test Case ID: TC003
   - Description: Test with all lines containing '*ERROR*'.
   - Expected Outcome: The count is zero since all lines are excluded.
4. Test Case ID: TC004
   - Description: Test with an empty input.
   - Expected Outcome: The count is zero as there are no lines to process.
5. Test Case ID: TC005
   - Description: Test with a DataFrame missing the 'value' column.
   - Expected Outcome: An AnalysisException is raised indicating the missing column.

Pytest Script:
------------------------------------------------------------
#!/usr/bin/env python
# test_gl_data_processing.py

import pytest
from pyspark.sql import SparkSession
from pyspark.sql.utils import AnalysisException

# Fixture to create and tear down the SparkSession for each test module
@pytest.fixture(scope="module")
def spark():
    spark_session = SparkSession.builder.master("local[*]").appName("PySparkTest").getOrCreate()
    yield spark_session
    spark_session.stop()

# Helper function to replicate the PySpark job logic from the given script
def filter_and_count_valid_entries(df):
    # Ensure the DataFrame has the expected column 'value'
    if 'value' not in df.columns:
        raise AnalysisException("The required column 'value' is missing.")
    # Filter out rows containing '*ERROR*' and count the remaining rows
    valid_df = df.filter(~df['value'].contains('*ERROR*'))
    return valid_df.count()

# Test cases:

def test_mixed_lines(spark):
    """
    TC001: Test with a mix of valid lines and lines containing '*ERROR*'.
    Expected Outcome: Only valid lines are counted.
    """
    data = [
        ("valid line 1",),
        ("*ERROR* line",),
        ("valid line 2",)
    ]
    df = spark.createDataFrame(data, ["value"])
    result = filter_and_count_valid_entries(df)
    assert result == 2  # Expect 2 valid lines

def test_all_valid_lines(spark):
    """
    TC002: Test with all valid lines.
    Expected Outcome: All lines are counted.
    """
    data = [
        ("valid line 1",),
        ("valid line 2",),
        ("valid line 3",)
    ]
    df = spark.createDataFrame(data, ["value"])
    result = filter_and_count_valid_entries(df)
    assert result == 3  # All 3 lines are valid

def test_all_error_lines(spark):
    """
    TC003: Test with all lines containing '*ERROR*'.
    Expected Outcome: Count is zero since all lines are excluded.
    """
    data = [
        ("*ERROR* line 1",),
        ("*ERROR* line 2",),
        ("*ERROR* line 3",)
    ]
    df = spark.createDataFrame(data, ["value"])
    result = filter_and_count_valid_entries(df)
    assert result == 0  # No valid lines

def test_empty_input(spark):
    """
    TC004: Test with an empty input.
    Expected Outcome: Count is zero as there are no lines to process.
    """
    data = []
    df = spark.createDataFrame(data, ["value"])
    result = filter_and_count_valid_entries(df)
    assert result == 0  # No lines present

def test_missing_value_column(spark):
    """
    TC005: Test with a DataFrame missing the 'value' column.
    Expected Outcome: An AnalysisException is raised due to the missing column.
    """
    data = [
        ("valid line 1",),
        ("*ERROR* line",)
    ]
    # Creating DataFrame with a different column name
    df = spark.createDataFrame(data, ["different_column"])
    with pytest.raises(AnalysisException):
        filter_and_count_valid_entries(df)

# Cost consumed by the API for this call: 0.002 USD
------------------------------------------------------------