Test Case List:
1. Test Case ID: TC1_HappyPath
   - Description: Provide a valid, non-empty DataFrame containing GL data with correct column names and data types.
   - Expected Outcome: The transformation function processes the data correctly with expected transformations (e.g., proper data type conversion, derived columns generated) and returns a DataFrame with the same or appropriately altered number of rows.
2. Test Case ID: TC2_EmptyDataFrame
   - Description: Provide an empty DataFrame (no records) as input.
   - Expected Outcome: The transformation function should return an empty DataFrame without errors.
3. Test Case ID: TC3_NullValues
   - Description: Provide a DataFrame that contains null values in critical columns.
   - Expected Outcome: The transformation should handle the null values gracefully (e.g., assign default values or skip processing) and the output DataFrame should either contain the fixed values or note the missing data as per business logic.
4. Test Case ID: TC4_InvalidDataTypes
   - Description: Provide a DataFrame with columns having invalid data types or unexpected string values in place of numbers.
   - Expected Outcome: The transformation function should either convert the values correctly if possible or raise an informative exception regarding data type incompatibilities.
5. Test Case ID: TC5_UDFApplication
   - Description: Verify that any user-defined functions (UDFs) used in the transformation are applied correctly.
   - Expected Outcome: The UDF should generate the expected output (e.g., a new derived column) with correct calculations based on the input rows.
6. Test Case ID: TC6_WindowFunctionOrAggregation
   - Description: Test a scenario that involves grouping or windowing (if applicable) over GL data.
   - Expected Outcome: The output should show correctly aggregated metrics or window function results matching the test criteria.

Pytest Script:
------------------------------------------------------------
#!/usr/bin/env python
"""
Pytest Script for testing PySpark GL Data transformation.
This script includes several test cases to validate the transformation logic.
Assumption: The module under test (e.g., gl_transformer.py) includes a function called process_gl_data(spark, input_df)
which performs the main data transformation that was converted from ABAP.
"""

import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# Assuming the transformation functionality is in a module named gl_transformer:
# from gl_transformer import process_gl_data

# For demonstration purposes, we define a dummy transformation function.
# In real scenario, the function would reside in the converted module.
def process_gl_data(spark, input_df):
    """
    Dummy transformation: Assume we convert a string numeric column "amount" to double,
    fill null description with a default value 'N/A', and compute a new column "amount_taxed" = amount * 1.2.
    """
    from pyspark.sql.types import DoubleType

    # Convert the "amount" column to double if it is not already.
    df = input_df.withColumn("amount", F.col("amount").cast(DoubleType()))
    
    # Fill nulls in "description" column.
    df = df.fillna({"description": "N/A"})
    
    # Apply a transformation to compute a new column.
    df = df.withColumn("amount_taxed", F.col("amount") * 1.2)
    return df

# Fixture to create a SparkSession
@pytest.fixture(scope="session")
def spark():
    spark_session = SparkSession.builder\
        .master("local[*]")\
        .appName("gl_transformation_tests")\
        .getOrCreate()
    yield spark_session
    spark_session.stop()

# Helper function to compare DataFrame content (order insensitive)
def assert_df_equality(df1, df2):
    data1 = sorted([tuple(row) for row in df1.collect()])
    data2 = sorted([tuple(row) for row in df2.collect()])
    assert data1 == data2

# Test case TC1_HappyPath: Valid data transformation
def test_happy_path(spark):
    """
    TC1_HappyPath:
    Input: Valid DataFrame with GL columns: id, amount, description.
    Expected: 'amount' is cast to double, null descriptions handled, and new column "amount_taxed" is computed.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    data = [
        ("1", "100", "Payment"),
        ("2", "200", "Invoice")
    ]
    input_df = spark.createDataFrame(data, schema)

    result_df = process_gl_data(spark, input_df)

    # Expected data: amounts converted to double and new computed column "amount_taxed"
    expected_data = [
        ("1", 100.0, "Payment", 120.0),
        ("2", 200.0, "Invoice", 240.0)
    ]
    expected_schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("description", StringType(), True),
        StructField("amount_taxed", DoubleType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, expected_schema)

    assert_df_equality(result_df, expected_df)

# Test case TC2_EmptyDataFrame: Processing an empty DataFrame
def test_empty_dataframe(spark):
    """
    TC2_EmptyDataFrame:
    Input: Empty DataFrame with proper schema.
    Expected: The transformation returns an empty DataFrame.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    input_df = spark.createDataFrame([], schema)

    result_df = process_gl_data(spark, input_df)

    assert result_df.count() == 0

# Test case TC3_NullValues: Handling nulls in critical columns
def test_null_values(spark):
    """
    TC3_NullValues:
    Input: DataFrame with null in 'description' and possibly in 'amount'
    Expected: Null 'description' values are replaced with default "N/A" and 'amount' conversion yields null if non-convertible.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    data = [
        ("1", "150", None),   # Null description, valid amount
        ("2", None, "Invoice") # Null amount, valid description
    ]
    input_df = spark.createDataFrame(data, schema)

    result_df = process_gl_data(spark, input_df)
    # Check that description null becomes "N/A".
    results = result_df.collect()
    for row in results:
        if row.id == "1":
            assert row.description == "N/A"
            # amount taxed should be computed as 150 * 1.2
            assert row.amount == 150.0
            assert row.amount_taxed == 180.0
        if row.id == "2":
            # For a null amount, cast will yield null and amount_taxed will be null.
            assert row.amount is None
            assert row.amount_taxed is None

# Test case TC4_InvalidDataTypes: Input with non-numeric string for 'amount'
def test_invalid_datatype(spark):
    """
    TC4_InvalidDataTypes:
    Input: DataFrame with non-numeric string in 'amount' column.
    Expected: The cast to double results in a null for that row's 'amount' and 'amount_taxed'.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    data = [
        ("1", "not_a_number", "ErrorCase")
    ]
    input_df = spark.createDataFrame(data, schema)

    result_df = process_gl_data(spark, input_df)
    row = result_df.collect()[0]
    # Expect that amount cannot be cast, so results in None
    assert row.amount is None
    assert row.amount_taxed is None

# Test case TC5_UDFApplication: Verify UDF or calculated column correctness
def test_udf_application(spark):
    """
    TC5_UDFApplication:
    Input: DataFrame with a valid numeric 'amount' and description field.
    Expected: New derived column "amount_taxed" is computed correctly.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    data = [
        ("1", "50", "Test")
    ]
    input_df = spark.createDataFrame(data, schema)

    result_df = process_gl_data(spark, input_df)
    row = result_df.collect()[0]
    # 50 * 1.2 should equal 60.0
    assert row.amount == 50.0
    assert row.amount_taxed == 60.0

# Test case TC6_WindowFunctionOrAggregation: Sample Test for grouping or window functions
def test_window_function_aggregation(spark):
    """
    TC6_WindowFunctionOrAggregation:
    Input: DataFrame with multiple records to simulate an aggregation scenario.
    Expected: After performing group by or window operations, aggregated results match the expected outcome.
    Note: This test simulates a scenario if grouping is applied.
    """
    # For demonstration, we simulate a grouping aggregation
    schema = StructType([
        StructField("group", StringType(), True),
        StructField("amount", StringType(), True)
    ])
    data = [
        ("A", "100"),
        ("A", "200"),
        ("B", "300")
    ]
    input_df = spark.createDataFrame(data, schema)

    # Perform aggregation similar to what might be in the transformation process
    df_transformed = input_df.withColumn("amount", F.col("amount").cast(DoubleType()))
    agg_df = df_transformed.groupBy("group").agg(F.sum("amount").alias("total_amount"))

    # Expected aggregated result
    expected_data = [
        ("A", 300.0),
        ("B", 300.0)
    ]
    expected_schema = StructType([
        StructField("group", StringType(), True),
        StructField("total_amount", DoubleType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, expected_schema)

    assert_df_equality(agg_df, expected_df)

# API Cost Consumed: $0.00

if __name__ == "__main__":
    # Allow the script to be run directly for manual testing
    import sys
    sys.exit(pytest.main(["-v"]))
------------------------------------------------------------

API Cost Consumed: $0.00