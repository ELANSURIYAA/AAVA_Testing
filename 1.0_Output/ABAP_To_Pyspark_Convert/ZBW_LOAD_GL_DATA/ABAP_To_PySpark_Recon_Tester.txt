------------------------------------------------------------
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Comprehensive Python Script for ABAP-to-PySpark Migration Validation

This script automates the validation process by:
1. Reading and simulating execution of an ABAP SQL code file ("ZBW_LOAD_GL_DATA.txt").
2. Exporting simulated ABAP outputs to CSV, then converting to Parquet.
3. Executing the provided PySpark transformation code on the generated CSV.
4. Comparing the outputs of ABAP and PySpark executions (row count and column-level validation).
5. Generating a detailed JSON reconciliation report and logging progress.

The script handles:
 - Secure I/O operations (no hard-coded credentials, simulation only).
 - Performance considerations and error handling.
 - Various edge cases such as schema mismatches, null values, and data type conversions.
 - Real-time logs is written using Pythonâ€™s logging module.
 - Designed to run automatically in a scheduled environment.
 
Ensure that the necessary dependencies (pandas, pyspark) are installed in your environment.
"""

import os
import csv
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.types import FloatType
from pyspark.sql.functions import col, split, size
import logging
import json

# Configure logging for real-time execution tracking
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def execute_abap_code(abap_file_path, output_csv_path, output_parquet_path):
    """
    Simulates the execution of ABAP code.
    Reads the given ABAP file and produces simulated output data.
    Exports the data as CSV and converts it to Parquet format.
    
    Parameters:
    - abap_file_path (str): Path to the ABAP SQL code file.
    - output_csv_path (str): Path to output CSV.
    - output_parquet_path (str): Path to output Parquet file.
    """
    try:
        logging.info("Simulating ABAP code execution...")
        # Read the ABAP SQL file content for logging purposes (simulate processing)
        with open(abap_file_path, 'r') as file:
            abap_code = file.read()
        logging.info("ABAP code loaded for simulation.")

        # Simulated ABAP output data (in practice, this would be the result of executing ABAP SQL)
        simulated_data = [
            ["1000", "2023", "CC001", "GL001", "1000.50", "USD", "2023-10-01"],
            ["1000", "2023", "CC002", "GL002", "2000.75", "USD", "2023-10-02"]
        ]
        
        # Write simulated data to CSV file
        logging.info(f"Writing simulated ABAP output to CSV: {output_csv_path}")
        with open(output_csv_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            # Write header row
            writer.writerow(["bukrs", "fiscyear", "costcenter", "gl_account", "amount", "currency", "posting_date"])
            # Write the output rows
            writer.writerows(simulated_data)
            
        # Convert the CSV data to Parquet using pandas
        logging.info(f"Converting CSV to Parquet: {output_parquet_path}")
        df = pd.read_csv(output_csv_path)
        df.to_parquet(output_parquet_path, index=False)
        logging.info("ABAP output successfully converted to Parquet.")

    except Exception as e:
        logging.error(f"Error during ABAP execution simulation: {e}")
        raise

def execute_pyspark_code(input_file_path, output_parquet_path):
    """
    Executes the provided PySpark conversion code.
    Reads CSV data from the specified input file,
    applies transformations to mimic the ABAP logic,
    and writes the resulting output as a Parquet file.
    
    Parameters:
    - input_file_path (str): Path to the CSV file (produced by ABAP simulation).
    - output_parquet_path (str): Path for the PySpark output in Parquet format.
    """
    try:
        logging.info("Executing PySpark code for conversion...")
        # Create a Spark session
        spark = SparkSession.builder.appName("ZBW_LOAD_GL_DATA Conversion").getOrCreate()

        # Read the CSV data as text lines (simulate as if read as text file)
        raw_df = spark.read.text(input_file_path)

        # Split each line by comma to mimic ABAP SPLIT operation
        split_df = raw_df.withColumn("fields", split(col("value"), ","))

        # Filter rows that have exactly 7 fields (exact schema expected)
        filtered_df = split_df.filter(size(col("fields")) == 7)

        # Map the fields to columns with expected naming conventions
        final_df = filtered_df.select(
            col("fields").getItem(0).alias("bukrs"),
            col("fields").getItem(1).alias("fiscyear"),
            col("fields").getItem(2).alias("costcenter"),
            col("fields").getItem(3).alias("gl_account"),
            col("fields").getItem(4).alias("amount"),
            col("fields").getItem(5).alias("currency"),
            col("fields").getItem(6).alias("posting_date")
        )

        # Cast the 'amount' column to float for numeric consistency
        final_df = final_df.withColumn("amount", col("amount").cast(FloatType()))

        # Write the PySpark dataframe output to Parquet format
        logging.info(f"Writing PySpark conversion output to Parquet: {output_parquet_path}")
        final_df.write.parquet(output_parquet_path, mode='overwrite')

        # Optionally view the result; for production remove or replace with logging.
        final_df.show()

        # Stop the Spark session
        spark.stop()
        logging.info("PySpark execution completed successfully.")

    except Exception as e:
        logging.error(f"Error during PySpark execution: {e}")
        raise

def compare_outputs(abap_parquet_path, pyspark_parquet_path, report_path):
    """
    Compares the outputs generated from the ABAP simulation and the PySpark execution.
    Performs row count validation and column-by-column comparisons, generating a detailed
    reconciliation report saved as JSON.
    
    Parameters:
    - abap_parquet_path (str): Path to ABAP output in Parquet format.
    - pyspark_parquet_path (str): Path to PySpark output in Parquet format.
    - report_path (str): Path to output reconciliation report (JSON format).
    """
    try:
        logging.info("Comparing ABAP and PySpark outputs...")

        # Load Parquet files into Pandas DataFrames for flexible comparisons
        abap_df = pd.read_parquet(abap_parquet_path)
        pyspark_df = pd.read_parquet(pyspark_parquet_path)

        # Validate row count match
        abap_row_count = len(abap_df)
        pyspark_row_count = len(pyspark_df)
        row_count_match = (abap_row_count == pyspark_row_count)

        # Validate each column: check for type conversion, null handling, and mismatches
        column_comparisons = []
        for column in abap_df.columns:
            if column in pyspark_df.columns:
                # For comparison, fill NaN with empty strings and convert to string
                abap_values = abap_df[column].fillna("").astype(str)
                pyspark_values = pyspark_df[column].fillna("").astype(str)

                # If both series contain exactly same data, flag as match else note mismatches
                match = abap_values.equals(pyspark_values)
                mismatches = []
                if not match:
                    # Identify mismatched rows if possible (for simplicity, list values from ABAP not in PySpark)
                    mismatches = abap_values[~abap_values.isin(pyspark_values)].tolist()

                column_comparisons.append({
                    "column": column,
                    "match": match,
                    "mismatches": mismatches
                })
            else:
                column_comparisons.append({
                    "column": column,
                    "match": False,
                    "mismatches": "Column missing in PySpark output"
                })

        # Form the reconciliation report structure
        report = {
            "row_count_match": row_count_match,
            "abap_row_count": abap_row_count,
            "pyspark_row_count": pyspark_row_count,
            "column_comparisons": column_comparisons
        }

        # Write the reconciliation report to a JSON file for external parsing
        logging.info(f"Writing reconciliation report to: {report_path}")
        with open(report_path, 'w') as report_file:
            json.dump(report, report_file, indent=4)
        logging.info("Reconciliation report generated successfully.")

    except Exception as e:
        logging.error(f"Error during output comparison: {e}")
        raise

if __name__ == "__main__":
    # Define file paths for ABAP input and outputs
    abap_file_path = "ZBW_LOAD_GL_DATA.txt"      # ABAP SQL Code file
    abap_output_csv = "abap_output.csv"            # Simulated ABAP output in CSV
    abap_output_parquet = "abap_output.parquet"    # Converted Parquet file from ABAP output
    pyspark_output_parquet = "pyspark_output.parquet"  # PySpark execution output in Parquet
    report_path = "reconciliation_report.json"     # Final JSON reconciliation report

    try:
        # Step 1: Simulate executing ABAP code and exporting the output
        execute_abap_code(abap_file_path, abap_output_csv, abap_output_parquet)
        
        # Step 2: Execute the provided PySpark code to transform the CSV content
        execute_pyspark_code(abap_output_csv, pyspark_output_parquet)
        
        # Step 3: Compare ABAP and PySpark outputs to validate the migration
        compare_outputs(abap_output_parquet, pyspark_output_parquet, report_path)
        
        logging.info("ABAP-to-PySpark migration validation process completed successfully.")

    except Exception as e:
        logging.error(f"Migration validation process failed: {e}")
------------------------------------------------------------
API Cost for this call: 1 unit