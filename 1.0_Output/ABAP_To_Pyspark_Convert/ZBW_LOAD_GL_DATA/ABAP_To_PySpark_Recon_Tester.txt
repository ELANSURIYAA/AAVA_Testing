#!/usr/bin/env python3
"""
ABAP-to-PySpark Migration Validation Script
--------------------------------------------------
This script automates the process of validating a migration from ABAP to PySpark.
It performs the following steps:
1. Parses and executes the provided ABAP SQL code on an SAP system.
2. Exports the resulting data tables to CSV and converts them to Parquet files.
3. Transfers the Parquet files to a distributed storage system (S3/HDFS/Data Lake).
4. Sets up a PySpark session and runs the provided PySpark code.
5. Compares the outputs from the ABAP execution and PySpark execution.
6. Generates a detailed reconciliation report that includes:
   - Match status (MATCH, PARTIAL MATCH, NO MATCH),
   - Row count differences,
   - Column-level discrepancies, and
   - Samples of mismatched records.
7. Implements robust error handling, secure connections, and logging.
8. Designed to run automatically in scheduled environments.
--------------------------------------------------
Before running:
  • Ensure that SAP connection credentials and storage credentials are set via environment variables or a secure vault.
  • Install required packages: pyspark, boto3 (if using S3), pyrfc (if using SAP RFC), pandas, pyarrow.
--------------------------------------------------
"""

import os
import sys
import json
import logging
import datetime
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col

# -------------------------------------------------------------------------
# Configure logging for real-time execution logs.
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

# -------------------------------------------------------------------------
# Placeholder functions for external system connections.
def connect_to_abap_system():
    """
    Establish connection to the ABAP system.
    This function should connect using SAP RFC SDK or pyrfc.
    Credentials and connection parameters must be securely obtained.
    """
    try:
        # For demonstration, we simulate the connection
        logging.info("Connecting to the ABAP system...")
        # sap_conn = pyrfc.Connection(user=os.getenv("SAP_USER"),
        #                             passwd=os.getenv("SAP_PASS"),
        #                             ashost=os.getenv("SAP_ASHOST"),
        #                             sysnr=os.getenv("SAP_SYSNR"),
        #                             client=os.getenv("SAP_CLIENT"))
        sap_conn = "SIMULATED_ABAP_CONNECTION"
        logging.info("Connected to ABAP system.")
        return sap_conn
    except Exception as e:
        logging.error("Failed to connect to ABAP system: %s", e)
        raise

def execute_abap_code(sap_conn, abap_code):
    """
    Execute provided ABAP SQL based code on the ABAP system,
    and retrieve the output dataset.
    
    For simulation, we assume that the ABAP code returns a dataset as a pandas DataFrame.
    """
    try:
        logging.info("Executing ABAP code ...")
        # In practice: result = sap_conn.call('RFC_READ_TABLE', QUERY_PARAMETERS)
        # Here we simulate a result dataset.
        data = {
            "id": [1, 2, 3],
            "amount": [1500.0, 500.0, 1000.0],
            "description": ["Payment A", "Payment B", "Payment C"]
        }
        df_abap = pd.DataFrame(data)
        logging.info("ABAP execution completed. Retrieved %d rows.", len(df_abap))
        return df_abap
    except Exception as e:
        logging.error("Error during ABAP code execution: %s", e)
        raise

def export_dataframe_to_parquet(df, table_name, output_dir="output_files"):
    """
    Exports a pandas DataFrame to a Parquet file.
    The file is named as table_name_timestamp.parquet.
    """
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{table_name}_{timestamp}.parquet"
        file_path = os.path.join(output_dir, filename)
        table = pa.Table.from_pandas(df)
        pq.write_table(table, file_path)
        logging.info("Data exported to Parquet: %s", file_path)
        return file_path
    except Exception as e:
        logging.error("Error exporting data to Parquet: %s", e)
        raise

def upload_to_distributed_storage(file_path, storage_location):
    """
    Transfers the file to a distributed storage system (e.g., S3, HDFS, Data Lake).
    For demonstration, we assume a local simulation.
    In a real implementation, use boto3 for S3 or a PyArrow Hadoop filesystem for HDFS.
    """
    try:
        logging.info("Uploading %s to distributed storage at %s", file_path, storage_location)
        # Simulate file transfer:
        # For S3, you might do:
        # s3_client.upload_file(file_path, bucket_name, destination_path)
        # For simulation, we copy file to a "storage" folder.
        storage_dir = os.path.join("distributed_storage", storage_location)
        if not os.path.exists(storage_dir):
            os.makedirs(storage_dir)
        destination = os.path.join(storage_dir, os.path.basename(file_path))
        with open(file_path, "rb") as src, open(destination, "wb") as dst:
            dst.write(src.read())
        logging.info("File successfully uploaded to %s", destination)
        return destination
    except Exception as e:
        logging.error("Error uploading file to storage: %s", e)
        raise

def initialize_pyspark_session(app_name="GLDataProcessing"):
    """
    Creates and returns a PySpark session.
    """
    try:
        logging.info("Initializing PySpark session...")
        spark = SparkSession.builder.appName(app_name).getOrCreate()
        logging.info("PySpark session initialized.")
        return spark
    except Exception as e:
        logging.error("Failed to initialize PySpark session: %s", e)
        raise

def run_pyspark_transformation(spark, storage_path, table_name="zbw_gl_data"):
    """
    Reads data from the distributed storage and runs the PySpark transformation.
    This function simulates:
      - Reading Parquet file from storage.
      - Applying the transformation (adding 'status' column).
      - Returning the transformed Spark DataFrame.
    """
    try:
        logging.info("Reading data from Parquet file stored at: %s", storage_path)
        df_gl = spark.read.parquet(storage_path)
        # Transformation: if amount > 1000 then 'HIGH', else 'NORMAL'
        df_gl_updated = df_gl.withColumn(
            "status",
            when(col("amount") > 1000, "HIGH").otherwise("NORMAL")
        )
        df_gl_updated.cache()
        logging.info("PySpark data transformation completed.")
        # Optionally show the data:
        df_gl_updated.show(10)
        return df_gl_updated
    except Exception as e:
        logging.error("Error in PySpark transformation: %s", e)
        raise

def compare_datasets(df_abap, df_pyspark):
    """
    Compare the datasets from ABAP and PySpark execution.
    Performs:
      - Row count validation.
      - Column-wise data validation.
      - Computes match percentages.
    
    Returns a dictionary report detailing the match status, differences, and examples.
    """
    logging.info("Starting dataset comparison...")
    report = {
        "row_count_abap": len(df_abap),
        "row_count_pyspark": df_pyspark.count(),
        "columns": {},
        "overall_match": "MATCH",
        "mismatched_rows": []
    }
    
    # Validate row count
    if report["row_count_abap"] != report["row_count_pyspark"]:
        report["overall_match"] = "PARTIAL MATCH"
        report["row_count_difference"] = report["row_count_abap"] - report["row_count_pyspark"]
    
    # Convert PySpark DataFrame to Pandas
    df_pyspark_pd = df_pyspark.toPandas()
    
    # Ensure both dataframes have same columns
    common_columns = set(df_abap.columns).intersection(set(df_pyspark_pd.columns))
    # Compare each column
    for col_name in common_columns:
        abap_values = df_abap[col_name].fillna("NULL").tolist()
        pyspark_values = df_pyspark_pd[col_name].fillna("NULL").tolist()
        match = (abap_values == pyspark_values)
        report["columns"][col_name] = "MATCH" if match else "NO MATCH"
        if not match:
            report["overall_match"] = "PARTIAL MATCH"
            # store sample differences
            samples = []
            for i, (a, b) in enumerate(zip(abap_values, pyspark_values)):
                if a != b:
                    samples.append({ "row": i, "ABAP": a, "PySpark": b })
                if len(samples) >= 5:  # limit sample size
                    break
            report["mismatched_rows"].append({col_name: samples})
    
    logging.info("Dataset comparison completed.")
    return report

def generate_reconciliation_report(report, output_file="reconciliation_report.json"):
    """
    Saves the reconciliation report as a JSON file and logs the summary.
    """
    try:
        with open(output_file, "w") as f:
            json.dump(report, f, indent=4)
        logging.info("Reconciliation report generated at: %s", output_file)
    except Exception as e:
        logging.error("Error writing reconciliation report: %s", e)
        raise

def main():
    try:
        # ------------------------- INPUTS -------------------------
        # ABAP SQL code is typically read from an input file.
        abap_file = "ZBW_LOAD_GL_DATA.txt"
        if not os.path.isfile(abap_file):
            logging.error("ABAP SQL file '%s' not found.", abap_file)
            sys.exit(1)
        with open(abap_file, "r") as f:
            abap_code = f.read()
        
        # Converted PySpark code is provided as context.
        # Here we assume that the actual transformation logic is integrated below.
        # (Refer to the transformation logic inside run_pyspark_transformation function.)
        
        # ------------------------- STEP 1: EXECUTE ABAP CODE -------------------------
        sap_conn = connect_to_abap_system()
        df_abap = execute_abap_code(sap_conn, abap_code)
        # (In a real scenario, the output can include multiple tables.
        # For this demo, we assume one table named "zbw_gl_data".)
        
        # ------------------------- STEP 2: EXPORT & TRANSFORM DATA -------------------------
        parquet_file = export_dataframe_to_parquet(df_abap, table_name="zbw_gl_data")
        
        # ------------------------- STEP 3: TRANSFER TO DISTRIBUTED STORAGE -------------------------
        # The storage location/path could be derived from config or env variables.
        storage_location = "gl_data_validations"
        storage_file_path = upload_to_distributed_storage(parquet_file, storage_location)
        
        # ------------------------- STEP 4: INITIALIZE PYSPARK ENVIRONMENT -------------------------
        spark = initialize_pyspark_session()
        
        # ------------------------- STEP 5: RUN PYSPARK TRANSFORMATION -------------------------
        df_pyspark = run_pyspark_transformation(spark, storage_file_path)
        
        # ------------------------- STEP 6: COMPARE RESULTS -------------------------
        reconciliation_report = compare_datasets(df_abap, df_pyspark)
        
        # ------------------------- STEP 7: REPORTING -------------------------
        generate_reconciliation_report(reconciliation_report)
        
        # Log final summary
        logging.info("Migration validation completed. Overall match status: %s", reconciliation_report.get("overall_match"))
        
        # Stop Spark session
        spark.stop()
        
    except Exception as e:
        logging.error("Overall migration validation failed: %s", e)
        sys.exit(1)

if __name__ == '__main__':
    main()

# API call cost consumed: $0.005