--------------------------------------------------
#!/usr/bin/env python3
"""
ABAP-to-PySpark Migration Validation Script
This script automates the entire process of validating an ABAP-to-PySpark migration.
It performs the following major steps:
 1. Execute ABAP code (simulated by reading a provided ABAP SQL file).
 2. Export the resulting dataset (simulate by writing CSV from dummy data) and convert it to Parquet.
 3. Upload the Parquet files to a simulated distributed storage (local folder simulating HDFS/S3).
 4. Execute the converted PySpark code which is assumed to be provided in a separate file.
 5. Compare outputs between the ABAP execution and the PySpark execution.
 6. Generate a reconciliation report with details (row counts, column discrepancies, sample mismatches).
 
Security, error handling and detailed logging are implemented. This script is designed to run in
automated/scheduled environments.

Usage:
    python migration_validation.py --abap_file ZBW_LOAD_GL_DATA.txt --pyspark_file converted_pyspark_code.py

NOTE: The actual connections to SAP systems or distributed storage are simulated in this script.
Make sure to update file paths and connection details as necessary.
"""

import os
import sys
import argparse
import logging
import traceback
import datetime
import json
import shutil

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Configure detailed logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Simulated functions for ABAP execution and export

def execute_abap_code(abap_file):
    """
    Simulate execution of ABAP code.
    Here, we simply read the file and assume that it generates output data.
    In actual production, this should connect to SAP using pyRFC and execute the code.
    """
    try:
        logger.info("Executing ABAP code from file: %s", abap_file)
        with open(abap_file, 'r') as f:
            abap_code = f.read()
        # Log the ABAP code (for debugging purposes only - remove sensitive content in production)
        logger.debug("ABAP Code: %s", abap_code)
        # Simulate output data from ABAP execution (normally, data is returned from SAP)
        # We simulate with sample data
        abap_output = [
            {"account": "A001", "amount": 1500, "posting_date": "2022-01-01"},
            {"account": "A002", "amount": 800, "posting_date": "2022-01-02"},
            {"account": "A001", "amount": 500, "posting_date": "2022-01-03"},
        ]
        logger.info("ABAP execution successful. Records retrieved: %d", len(abap_output))
        return abap_output
    except Exception as e:
        logger.error("Error executing ABAP code: %s", str(e))
        raise

def export_abap_output(abap_output, export_dir):
    """
    Export ABAP output data to CSV and convert it to Parquet format.
    This simulates the data export and file conversion steps.
    """
    try:
        if not os.path.exists(export_dir):
            os.makedirs(export_dir)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_file = os.path.join(export_dir, f"gl_data_{timestamp}.csv")
        parquet_file = os.path.join(export_dir, f"gl_data_{timestamp}.parquet")
        
        # Write CSV file
        logger.info("Exporting ABAP output data to CSV: %s", csv_file)
        with open(csv_file, 'w') as f:
            # Write header
            headers = ["account", "amount", "posting_date"]
            f.write(",".join(headers) + "\n")
            # Write rows
            for row in abap_output:
                f.write(",".join(str(row[h]) for h in headers) + "\n")
        
        # Convert CSV to Parquet using PySpark
        spark = SparkSession.builder.appName("ABAPExportToParquet").getOrCreate()
        schema = StructType([
            StructField("account", StringType(), True),
            StructField("amount", IntegerType(), True),
            StructField("posting_date", StringType(), True)
        ])
        df = spark.read.csv(csv_file, header=True, schema=schema)
        logger.info("Converting CSV file to Parquet: %s", parquet_file)
        df.write.mode("overwrite").parquet(parquet_file)
        spark.stop()
        
        logger.info("Export and conversion successful.")
        return csv_file, parquet_file
    except Exception as e:
        logger.error("Failed exporting ABAP output: %s", str(e))
        traceback.print_exc()
        raise

def transfer_to_distributed_storage(parquet_file, storage_dir):
    """
    Simulate transferring Parquet files to a distributed storage.
    In production, this could be a transfer to HDFS, S3, or another Data Lake.
    
    Here we simulate it by copying the file to a designated folder.
    """
    try:
        if not os.path.exists(storage_dir):
            os.makedirs(storage_dir)
        dest_file = os.path.join(storage_dir, os.path.basename(parquet_file))
        logger.info("Transferring Parquet file to distributed storage: %s", dest_file)
        shutil.copy(parquet_file, dest_file)
        logger.info("Transfer complete.")
        return dest_file
    except Exception as e:
        logger.error("Error transferring file to storage: %s", str(e))
        raise

def run_pyspark_job(pyspark_file, storage_parquet):
    """
    Execute the provided PySpark code.
    The converted PySpark code should load the data from the distributed storage,
    process it, and output a result DataFrame.
    This function uses the provided file (pyspark_file) and passes the parquet file path as input.
    
    NOTE: The pyspark_file is assumed to contain a main() function which accepts a data path.
    For simulation, we instead embed similar logic here.
    """
    try:
        logger.info("Starting PySpark job using converted code from: %s", pyspark_file)
        # Here we simulate by running equivalent PySpark transformations (as provided in the input)
        spark = SparkSession.builder.appName("ConvertedPySparkJob").getOrCreate()
        
        # Define schema corresponding to the ABAP structure (matching the export)
        schema = StructType([
            StructField("account", StringType(), True),
            StructField("amount", IntegerType(), True),
            StructField("posting_date", StringType(), True),
        ])
        # Load data from the transferred Parquet file
        df = spark.read.parquet(storage_parquet)
        logger.info("Loaded data from distributed storage.")
        
        # Apply filter and transformation logic analogous to the converted code
        filtered_df = df.filter(col("amount") > 0)
        processed_df = filtered_df.withColumn("status", when(col("amount") > 1000, "High").otherwise("Normal"))
        aggregated_df = processed_df.groupBy("account").sum("amount") \
            .withColumnRenamed("sum(amount)", "total_amount")
        
        # Simulate a join with a lookup table of account details (here we create dummy lookup data)
        account_schema = StructType([
            StructField("account", StringType(), True),
            StructField("account_description", StringType(), True)
        ])
        # In real scenario, account details file location should be provided
        account_data = [("A001", "Cash Account"), ("A002", "Revenue Account")]
        account_df = spark.createDataFrame(account_data, schema=account_schema)
        final_df = aggregated_df.join(account_df, on="account", how="left")
        # Persist final results to temporary storage for comparison
        result_path = os.path.join("tmp_results", "pyspark_output.parquet")
        final_df.write.mode("overwrite").parquet(result_path)
        logger.info("PySpark job completed. Results stored at: %s", result_path)
        spark.stop()
        return result_path
    except Exception as e:
        logger.error("Error during PySpark job execution: %s", str(e))
        traceback.print_exc()
        raise

def compare_outputs(abap_output_file, pyspark_output_path):
    """
    Compare outputs from ABAP execution (simulated) and PySpark execution.
    The comparison includes row counts and column-level validation.
    
    Returns a dictionary with detailed reconciliation metrics.
    """
    try:
        logger.info("Starting comparison of ABAP and PySpark outputs.")
        # Initialize SparkSession for reading outputs
        spark = SparkSession.builder.appName("OutputComparison").getOrCreate()
        # Read ABAP output exported CSV as DataFrame
        schema = StructType([
            StructField("account", StringType(), True),
            StructField("amount", IntegerType(), True),
            StructField("posting_date", StringType(), True)
        ])
        abap_df = spark.read.csv(abap_output_file, header=True, schema=schema)
        # Recreate ABAP transformation to match PySpark processing logic for an apples-to-apples comparison
        filtered_abap_df = abap_df.filter(col("amount") > 0)
        processed_abap_df = filtered_abap_df.withColumn("status", when(col("amount") > 1000, "High").otherwise("Normal"))
        aggregated_abap_df = processed_abap_df.groupBy("account").sum("amount") \
            .withColumnRenamed("sum(amount)", "total_amount")
        
        # For PySpark output, read the final result DataFrame from the Parquet output
        pyspark_df = spark.read.parquet(pyspark_output_path)
        
        # Compare row counts for each account
        abap_data = aggregated_abap_df.toPandas().set_index("account")
        pyspark_data = pyspark_df.toPandas().set_index("account")
        
        reconciliation = {}
        accounts = set(abap_data.index).union(set(pyspark_data.index))
        for acc in accounts:
            abap_val = abap_data.loc[acc]["total_amount"] if acc in abap_data.index else None
            pyspark_val = pyspark_data.loc[acc]["total_amount"] if acc in pyspark_data.index else None
            match = (abap_val == pyspark_val)
            reconciliation[acc] = {
                "abap_total_amount": abap_val,
                "pyspark_total_amount": pyspark_val,
                "match_status": "MATCH" if match else ("NO MATCH" if (abap_val is not None and pyspark_val is not None) else "PARTIAL MATCH")
            }
        
        spark.stop()
        logger.info("Comparison complete.")
        return reconciliation
    except Exception as e:
        logger.error("Error during output comparison: %s", str(e))
        traceback.print_exc()
        raise

def generate_report(reconciliation, report_file):
    """
    Generate a detailed reconciliation report as a JSON file.
    The report includes match status, differences, and a summary.
    """
    try:
        logger.info("Generating reconciliation report: %s", report_file)
        report = {
            "report_date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "summary": {
                "total_accounts_compared": len(reconciliation),
                "accounts_with_match": sum(1 for r in reconciliation.values() if r["match_status"] == "MATCH")
            },
            "account_details": reconciliation
        }
        with open(report_file, "w") as f:
            json.dump(report, f, indent=4)
        logger.info("Report generated successfully.")
        return report
    except Exception as e:
        logger.error("Error generating report: %s", str(e))
        raise

def main():
    parser = argparse.ArgumentParser(description="ABAP-to-PySpark Migration Validation Script")
    parser.add_argument("--abap_file", required=True, help="Path to the ABAP SQL code file (e.g., ZBW_LOAD_GL_DATA.txt)")
    parser.add_argument("--pyspark_file", required=True, help="Path to the converted PySpark code file")
    args = parser.parse_args()
    
    try:
        # Step 1: Execute ABAP code and retrieve output records
        abap_output = execute_abap_code(args.abap_file)
        
        # Step 2: Export ABAP output to CSV and convert to Parquet
        export_dir = "exported_files"
        csv_file, parquet_file = export_abap_output(abap_output, export_dir)
        
        # Step 3: Transfer Parquet file to distributed storage (simulate by copying locally)
        storage_dir = "distributed_storage"
        storage_parquet = transfer_to_distributed_storage(parquet_file, storage_dir)
        
        # Step 4: Execute the converted PySpark code against the transferred Parquet data
        pyspark_output_path = run_pyspark_job(args.pyspark_file, storage_parquet)
        
        # Step 5: Compare outputs between ABAP process and PySpark process
        reconciliation = compare_outputs(csv_file, pyspark_output_path)
        
        # Step 6: Generate a reconciliation report
        report_file = "reconciliation_report.json"
        report = generate_report(reconciliation, report_file)
        logger.info("Final Reconciliation Report:\n%s", json.dumps(report, indent=4))
        
    except Exception as e:
        logger.error("Migration validation failed: %s", str(e))
        sys.exit(1)

if __name__ == '__main__':
    main()

#
# -------------------------------------------------------------------------------
# Below is the complete pytest script for the provided test cases.
# Save this as test_gl_data_processing.py and run using pytest.
# -------------------------------------------------------------------------------

"""
#!/usr/bin/env python
import pytest
import os
import shutil
import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when
from pyspark.sql.types import StringType, IntegerType, StructType, StructField

# API cost consumed for this conversion call: 0.01 credits

@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder \
        .appName("TestGLDataLoad") \
        .master("local[*]") \
        .getOrCreate()
    yield spark
    spark.stop()

def process_gl_data(spark, gl_data, account_data):
    # Define schema as per original conversion
    schema = StructType([
        StructField("account", StringType(), True),
        StructField("amount", IntegerType(), True),
        StructField("posting_date", StringType(), True),
    ])
    
    # Create DataFrames from input data
    gl_df = spark.createDataFrame(gl_data, schema)
    
    # Filter out rows with amount <= 0 or null values in amount
    filtered_df = gl_df.filter((col("amount") > 0) & (col("amount").isNotNull()))
    
    # Add status column: "High" if amount > 1000 else "Normal"
    processed_df = filtered_df.withColumn("status", when(col("amount") > 1000, "High").otherwise("Normal"))
    
    # Aggregate: sum amounts per account
    aggregated_df = processed_df.groupBy("account").sum("amount") \
        .withColumnRenamed("sum(amount)", "total_amount")
    
    # Process account details DataFrame
    account_schema = StructType([
        StructField("account", StringType(), True),
        StructField("account_description", StringType(), True)
    ])
    acct_df = spark.createDataFrame(account_data, account_schema)
    
    # Join aggregated data with account details
    final_df = aggregated_df.join(acct_df, on="account", how="left")
    
    return final_df

# Test Case TC1_HappyPath
def test_happy_path(spark):
    gl_data = [
        ("A001", 1500, "2022-01-01"),
        ("A002", 800, "2022-01-02"),
        ("A001", 500, "2022-01-03")
    ]
    account_data = [
        ("A001", "Cash Account"),
        ("A002", "Revenue Account")
    ]
    result_df = process_gl_data(spark, gl_data, account_data)
    results = {row["account"]: (row["total_amount"], row.get("account_description")) for row in result_df.collect()}
    assert results["A001"][0] == 2000
    assert results["A001"][1] == "Cash Account"
    assert results["A002"][0] == 800
    assert results["A002"][1] == "Revenue Account"

# Test Case TC2_EmptyDataFrame
def test_empty_dataframe(spark):
    gl_data = []  # Empty GL data
    account_data = [
        ("A001", "Cash Account"),
        ("A002", "Revenue Account")
    ]
    result_df = process_gl_data(spark, gl_data, account_data)
    assert result_df.count() == 0

# Test Case TC3_NullValues
def test_null_values(spark):
    gl_data = [
        ("A001", None, "2022-01-01"),
        ("A002", 1200, "2022-01-02"),
        ("A003", -100, "2022-01-03"),
        ("A004", 0, "2022-01-04")
    ]
    account_data = [
        ("A002", "Revenue Account"),
        ("A003", "Expense Account"),
        ("A004", "Liability Account")
    ]
    result_df = process_gl_data(spark, gl_data, account_data)
    results = result_df.collect()
    assert len(results) == 1
    assert results[0]["account"] == "A002"
    assert results[0]["total_amount"] == 1200

# Test Case TC4_BoundaryAmounts
def test_boundary_amounts(spark):
    gl_data = [
        ("A001", 0, "2022-01-01"),
        ("A002", 1, "2022-01-02"),
        ("A003", 1000, "2022-01-03"),
        ("A004", 1001, "2022-01-04")
    ]
    account_data = [
        ("A002", "Revenue Account"),
        ("A003", "Expense Account"),
        ("A004", "Asset Account")
    ]
    result_df = process_gl_data(spark, gl_data, account_data)
    results = {row["account"]: row["total_amount"] for row in result_df.collect()}
    assert "A001" not in results
    assert results["A002"] == 1
    assert results["A003"] == 1000
    assert results["A004"] == 1001

# Test Case TC5_InvalidDataType
def test_invalid_data_type(spark):
    gl_data = [
        ("A001", "invalid_int", "2022-01-01")
    ]
    account_data = [
        ("A001", "Cash Account")
    ]
    with pytest.raises(Exception):
        process_gl_data(spark, gl_data, account_data)
        
"""
--------------------------------------------------

API cost for this particular API call: 0.01 credits