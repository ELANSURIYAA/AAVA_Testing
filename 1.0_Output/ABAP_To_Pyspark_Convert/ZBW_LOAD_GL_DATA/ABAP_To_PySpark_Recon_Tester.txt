#!/usr/bin/env python
"""
ABAP-to-PySpark Migration Validation Script
-------------------------------------------------
This script automates the end-to-end process to validate data migration from an SAP ABAP system to
a PySpark processing engine. It executes the following steps:
  1. Parse and “execute” the provided ABAP SQL code (simulated for demonstration purposes).
  2. Export the ABAP result dataset to CSV and convert it to Parquet format.
  3. Transfer the Parquet files to a distributed storage system (simulated by copying files).
  4. Execute the provided converted PySpark code that transforms the data.
  5. Compare the ABAP output and the PySpark output for validation:
         - Row count validation.
         - Column-wise data comparison.
         - Compute a match status based on a match percentage.
  6. Generate a detailed reconciliation report including:
         - Match Status (MATCH, NO MATCH, PARTIAL MATCH)
         - Row count differences.
         - Column discrepancies.
         - Sample mismatched records.
         - Overall summary.
  7. Log all steps and handle errors robustly.
-------------------------------------------------
Usage:
   python migration_validation.py --abap_file <path_to_abap_file> --pyspark_script <path_to_pyspark_script>
-------------------------------------------------
Notes:
   - This script simulates connections (SAP, distributed storage).
   - Do not hardcode credentials. Secure authentication methods must be used in production.
   - Designed to run in scheduled environments with detailed logging.
"""

import os
import sys
import shutil
import argparse
import logging
import datetime
import traceback

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

def setup_spark(app_name="MigrationValidationApp"):
    """Create and return a SparkSession."""
    spark = SparkSession.builder.appName(app_name).getOrCreate()
    return spark

def execute_abap_code(abap_file, spark):
    """
    Simulate execution of the ABAP code.
    In production, this would involve connecting to the SAP system using SAP RFC/pyRFC,
    executing the ABAP SQL code, and retrieving the data.
    For simulation purposes, we create a static DataFrame.
    """
    logging.info("Executing ABAP code from file: %s", abap_file)
    try:
        with open(abap_file, 'r') as f:
            abap_code = f.read()
        logging.debug("ABAP Code Content: %s", abap_code)
    except Exception as e:
        logging.error("Failed to read ABAP code file: %s", str(e))
        sys.exit(1)

    # Simulated output from executing the ABAP code.
    # In a real scenario, the output dataset would depend on the ABAP code execution.
    # Here we simulate with a couple of rows mimicking GL data.
    schema = StructType([
        StructField("bukrs", IntegerType(), True),
        StructField("fiscyear", IntegerType(), True),
        StructField("costcenter", StringType(), True),
        StructField("gl_account", StringType(), True),
        StructField("amount", IntegerType(), True),
        StructField("currency", StringType(), True),
        StructField("posting_date", StringType(), True)  # Using StringType for simplicity
    ])

    data = [
        (1000, 2023, "CC01", "GL01", 1000, "USD", "2023-01-01"),
        (2000, 2023, "CC02", "GL02", 2000, "EUR", "2023-01-02")
    ]
    abap_df = spark.createDataFrame(data, schema)
    abap_df.cache() # cache for performance if used multiple times
    logging.info("ABAP execution complete. Retrieved %d rows.", abap_df.count())
    return abap_df

def export_and_transform(abap_df, export_dir):
    """
    Export the ABAP result to CSV and convert it to Parquet format.
    The file is named using a useful convention: table_name_timestamp.parquet.
    """
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    table_name = "GL_DATA"
    csv_filename = os.path.join(export_dir, f"{table_name}_{timestamp}.csv")
    parquet_filename = os.path.join(export_dir, f"{table_name}_{timestamp}.parquet")

    try:
        logging.info("Exporting ABAP result to CSV: %s", csv_filename)
        abap_df.coalesce(1).write.mode("overwrite").option("header", True).csv(csv_filename)
    except Exception as e:
        logging.error("Error exporting ABAP output to CSV: %s", str(e))
        sys.exit(1)

    # Read CSV back into a DataFrame to convert to Parquet (simulate transformation using libraries like pandas/pyarrow if needed)
    try:
        # Sometimes CSV export writes into a directory with part files, we read that folder.
        df_csv = abap_df.sparkSession.read.option("header", True).csv(csv_filename)
        logging.info("Converting CSV to Parquet file: %s", parquet_filename)
        df_csv.write.mode("overwrite").parquet(parquet_filename)
    except Exception as e:
        logging.error("Error converting CSV to Parquet: %s", str(e))
        sys.exit(1)

    logging.info("Export and transformation complete. Parquet file created: %s", parquet_filename)
    return parquet_filename

def transfer_to_distributed_storage(local_parquet_file, storage_dir):
    """
    Simulate transferring the Parquet file to a distributed storage system.
    In production, this would use APIs (HDFS, S3, Data Lake SDK) with secure authentication.
    Here we simulate by copying the file.
    """
    if not os.path.exists(storage_dir):
        os.makedirs(storage_dir)
    try:
        dest_file = os.path.join(storage_dir, os.path.basename(local_parquet_file))
        logging.info("Transferring Parquet file to distributed storage: %s", dest_file)
        # For simulation, copy the file (or directory) to the target storage location.
        # Note: Parquet output from Spark may be a directory.
        if os.path.isdir(local_parquet_file):
            shutil.copytree(local_parquet_file, dest_file)
        else:
            shutil.copy2(local_parquet_file, dest_file)
    except Exception as e:
        logging.error("Error transferring file to distributed storage: %s", str(e))
        sys.exit(1)
    logging.info("Transfer complete.")
    return dest_file

def run_pyspark_job(pyspark_script, input_parquet, output_dir, spark):
    """
    Run the converted PySpark code.
    This function simulates executing the PySpark job.
    It reads the input Parquet file, performs transformations analogous to the provided converted code,
    and writes the output to the specified output directory.
    """
    logging.info("Executing PySpark job using script: %s", pyspark_script)
    try:
        # Read the input file; we assume the file is in CSV format if required,
        # but here we assume the input is in Parquet format.
        df = spark.read.parquet(input_parquet)
        df.cache()
        # Simulate any additional transformations that might be in the converted code.
        # For example, if the converted code was splitting, filtering, etc.
        # For demonstration, just perform a simple transformation, e.g., rename columns.
        df_transformed = df.withColumn("amount", col("amount"))
        # Write the resultant DataFrame as Parquet.
        logging.info("Writing PySpark job output to: %s", output_dir)
        df_transformed.write.mode("overwrite").parquet(output_dir)
    except Exception as e:
        logging.error("Error executing PySpark job: %s", str(e))
        traceback.print_exc()
        sys.exit(1)
    logging.info("PySpark job executed successfully.")
    return output_dir

def compare_results(spark, abap_parquet, pyspark_output_dir):
    """
    Compare the ABAP output and PySpark output.
    Validates row counts and column data.
    Returns a dictionary with detailed reconciliation report.
    """
    report = {}
    try:
        df_abap = spark.read.parquet(abap_parquet)
        df_pyspark = spark.read.parquet(pyspark_output_dir)
    except Exception as e:
        logging.error("Error reading Parquet files for comparison: %s", str(e))
        sys.exit(1)

    abap_count = df_abap.count()
    pyspark_count = df_pyspark.count()
    report['abap_row_count'] = abap_count
    report['pyspark_row_count'] = pyspark_count
    report['row_count_match'] = (abap_count == pyspark_count)

    # Compare column values
    # For simplicity, we assume both DataFrames have the same columns and ordering.
    abap_rows = sorted(df_abap.collect(), key=lambda r: tuple(r))
    pyspark_rows = sorted(df_pyspark.collect(), key=lambda r: tuple(r))
    data_match = (abap_rows == pyspark_rows)
    report['data_match'] = data_match

    # Compute match percentage (basic approach)
    total_rows = max(abap_count, pyspark_count)
    if total_rows > 0:
        matching_rows = sum(1 for a, p in zip(abap_rows, pyspark_rows) if a == p)
        match_percentage = (matching_rows / total_rows) * 100
    else:
        match_percentage = 100.0  # both are empty

    report['match_percentage'] = match_percentage

    # Determine match status
    if abap_count == pyspark_count and data_match:
        report['match_status'] = "MATCH"
    elif match_percentage > 80:
        report['match_status'] = "PARTIAL MATCH"
    else:
        report['match_status'] = "NO MATCH"

    # For detailed investigations, include sample mismatches
    mismatches = []
    for idx, (a, p) in enumerate(zip(abap_rows, pyspark_rows)):
        if a != p:
            mismatches.append({'row_index': idx, 'abap_row': str(a), 'pyspark_row': str(p)})
    report['mismatches'] = mismatches
    return report

def generate_report(report, report_file):
    """
    Write the reconciliation report to a file in a structured format.
    """
    try:
        with open(report_file, 'w') as f:
            f.write("ABAP-to-PySpark Migration Validation Report\n")
            f.write("Generated on: {}\n\n".format(datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")))
            f.write("ABAP Row Count: {}\n".format(report.get('abap_row_count')))
            f.write("PySpark Row Count: {}\n".format(report.get('pyspark_row_count')))
            f.write("Row Count Match: {}\n".format(report.get('row_count_match')))
            f.write("Data Match: {}\n".format(report.get('data_match')))
            f.write("Match Percentage: {:.2f}%\n".format(report.get('match_percentage')))
            f.write("Overall Match Status: {}\n\n".format(report.get('match_status')))
            if report.get('mismatches'):
                f.write("Sample Mismatches:\n")
                for mismatch in report.get('mismatches'):
                    f.write("Row Index {}: ABAP => {} | PySpark => {}\n".format(
                        mismatch['row_index'], mismatch['abap_row'], mismatch['pyspark_row']
                    ))
            else:
                f.write("No mismatches found.\n")
        logging.info("Reconciliation report generated at: %s", report_file)
    except Exception as e:
        logging.error("Error writing reconciliation report: %s", str(e))
        sys.exit(1)

def parse_arguments():
    """
    Parse command-line arguments.
    """
    parser = argparse.ArgumentParser(description='ABAP-to-PySpark Migration Validation')
    parser.add_argument('--abap_file', required=True, help='Path to the ABAP SQL code file (e.g., ZBW_LOAD_GL_DATA.txt)')
    parser.add_argument('--pyspark_script', required=True, help='Path to the converted PySpark code file')
    parser.add_argument('--temp_dir', default='./temp', help='Temporary directory for intermediate files')
    parser.add_argument('--storage_dir', default='./storage', help='Directory simulating distributed storage')
    parser.add_argument('--pyspark_output', default='./pyspark_output', help='Output directory for the PySpark job')
    parser.add_argument('--report_file', default='reconciliation_report.txt', help='File to write the reconciliation report')
    return parser.parse_args()

def main():
    args = parse_arguments()

    try:
        spark = setup_spark()
        if not os.path.exists(args.temp_dir):
            os.makedirs(args.temp_dir)
    except Exception as e:
        logging.error("Failed during Spark initialization: %s", str(e))
        sys.exit(1)

    # 1. Execute ABAP code and simulate data retrieval.
    abap_df = execute_abap_code(args.abap_file, spark)

    # 2. Export and transform ABAP output into Parquet.
    abap_parquet = export_and_transform(abap_df, args.temp_dir)

    # 3. Transfer the Parquet file to distributed storage.
    transferred_parquet = transfer_to_distributed_storage(abap_parquet, args.storage_dir)

    # 4. Execute the converted PySpark job.
    # For demonstration, we assume the converted PySpark script logic is executed in the function run_pyspark_job.
    # In production, you may 'exec' the code or import it as a module if designed appropriately.
    pyspark_output_dir = run_pyspark_job(args.pyspark_script, transferred_parquet, args.pyspark_output, spark)

    # 5. Compare the ABAP output and the PySpark job output.
    report = compare_results(spark, transferred_parquet, pyspark_output_dir)

    # 6. Generate a detailed reconciliation report for review.
    generate_report(report, args.report_file)

    logging.info("Migration validation process complete.")
    spark.stop()

if __name__ == "__main__":
    main()