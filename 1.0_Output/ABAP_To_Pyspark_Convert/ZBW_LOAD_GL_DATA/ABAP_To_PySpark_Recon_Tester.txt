#!/usr/bin/env python
"""
Python Script: abap_pyspark_migration_validation.py
Description:
    This script automates the validation process for ABAP to PySpark migrations.
    It accepts two files: an ABAP SQL code file and a converted PySpark code file.
    It then performs the following:
      1. Parses the ABAP SQL code to understand structure and expected output tables.
      2. Simulates connection to an SAP system to execute ABAP code.
      3. Exports the ABAP output as CSV/JSON and converts to Parquet with a timestamp-based filename.
      4. Simulates transferring the Parquet file to a distributed storage location.
      5. Creates an external table (temporary view) in PySpark referencing the Parquet file.
      6. Executes the provided converted PySpark code on the data.
      7. Compares the outputs of the ABAP execution and PySpark transformed data.
      8. Generates a detailed reconciliation report.
    The script includes logging, error handling and is designed for scheduled as well as real-time execution.
    
Note:
   - Replace simulated data connections with production implementations as needed.
   - Ensure secure authentication by using environment variables or secret management.
"""

import os
import sys
import shutil
import logging
import traceback
import datetime
import json
import socket

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# Setup Logging
def setup_logging(log_file="migration_validation.log"):
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] - %(message)s",
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logging.info("Logging initialized.")

# Parse ABAP Code to extract expected table structure (Mock implementation)
def parse_abap_code(abap_file_path):
    try:
        with open(abap_file_path, "r") as f:
            abap_code = f.read()
        # In production, parse the ABAP SQL code to extract table names and expected columns.
        # Here, we simulate that we expect a table "BKPF" with columns like "bukrs", "blart", "belnr", "budat".
        expected_table = {
            "name": "BKPF",
            "columns": ["bukrs", "blart", "belnr", "budat"]
        }
        logging.info("Parsed ABAP code and extracted expected table: %s", expected_table)
        return abap_code, expected_table
    except Exception as e:
        logging.error("Error parsing ABAP code: %s", str(e))
        raise

# Simulate execution of ABAP code and retrieval of dataset
def execute_abap_code(abap_code, spark, input_data_path="path_to_bkpf_data.csv"):
    try:
        # In an actual environment, you would use SAP RFC SDK/pyRFC to connect to SAP and execute ABAP.
        # For simulation, read CSV file representing the BKPF table.
        df = spark.read.format("csv") \
                .option("header", "true") \
                .option("inferSchema", "true") \
                .load(input_data_path)
        logging.info("Executed ABAP code simulation: Data read from %s", input_data_path)
        return df
    except Exception as e:
        logging.error("Error executing ABAP code: %s", str(e))
        raise

# Export DataFrame to Parquet file with timestamped file name
def export_to_parquet(df, table_name, output_dir="parquet_outputs"):
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"{table_name}_{timestamp}.parquet"
        file_path = os.path.join(output_dir, file_name)
        # Using df.toPandas() for conversion then pyarrow to write parquet,
        # In production with very large datasets, use Sparkâ€™s write.parquet method.
        pdf = df.toPandas()
        table = pa.Table.from_pandas(pdf)
        pq.write_table(table, file_path)
        logging.info("Exported DataFrame to Parquet file: %s", file_path)
        return file_path
    except Exception as e:
        logging.error("Error exporting to Parquet: %s", str(e))
        raise

# Simulate transferring file to distributed storage (e.g., HDFS, S3)
def transfer_to_storage(file_path, storage_dir="distributed_storage"):
    try:
        # In production, use boto3 for S3, pyarrow.fs for HDFS
        if not os.path.exists(storage_dir):
            os.makedirs(storage_dir)
        destination_path = os.path.join(storage_dir, os.path.basename(file_path))
        shutil.copy(file_path, destination_path)
        # Perform integrity check by comparing file sizes (simplified).
        if os.path.getsize(file_path) == os.path.getsize(destination_path):
            logging.info("Successfully transferred file to storage: %s", destination_path)
            return destination_path
        else:
            raise Exception("File integrity check failed during transfer.")
    except Exception as e:
        logging.error("Error transferring file to storage: %s", str(e))
        raise

# Create external table in Spark by reading the Parquet file (simulate external table creation)
def create_pyspark_external_table(spark, parquet_path, table_name="external_bkpf"):
    try:
        df = spark.read.parquet(parquet_path)
        # Register as temporary view for SQL if needed.
        df.createOrReplaceTempView(table_name)
        logging.info("Created external table/view '%s' from Parquet file: %s", table_name, parquet_path)
        return df
    except Exception as e:
        logging.error("Error creating external table in Spark: %s", str(e))
        raise

# Execute the converted PySpark code script
def execute_converted_pyspark_code(spark, pyspark_code_file_path, input_data_path="path_to_bkpf_data.csv"):
    try:
        # In production, you might dynamically import or execute. Here, we simulate execution.
        with open(pyspark_code_file_path, "r") as f:
            pyspark_code = f.read()
        logging.info("Read converted PySpark code from: %s", pyspark_code_file_path)
        # The converted code is expected to load data, filter and select columns.
        # We re-run the logic here. A function to encapsulate the transformation is defined.
        df = spark.read.format("csv") \
                .option("header", "true") \
                .option("inferSchema", "true") \
                .load(input_data_path)
        # Transformation logic as described in the converted code comments:
        # 1. Filter records where bukrs equals '1000'
        filtered_df = df.filter(col("bukrs") == "1000")
        # 2. Filter records where blart equals 'SA'
        result_df = filtered_df.filter(col("blart") == "SA")
        # 3. Select belnr and budat columns for output
        selected_df = result_df.select("belnr", "budat")
        logging.info("Executed converted PySpark code and generated result DataFrame.")
        return selected_df
    except Exception as e:
        logging.error("Error executing converted PySpark code: %s", str(e))
        raise

# Compare two DataFrames (ABAP output vs. PySpark output)
def compare_dataframes(df_abap, df_pyspark):
    try:
        # Compare row count first
        count_abap = df_abap.count()
        count_pyspark = df_pyspark.count()
        match = (count_abap == count_pyspark)
        discrepancies = {}
        if not match:
            discrepancies["row_count"] = {
                "abap": count_abap,
                "pyspark": count_pyspark
            }
        # Column-wise comparison: ensure both have same columns and then check sample data
        abap_cols = set(df_abap.columns)
        pyspark_cols = set(df_pyspark.columns)
        if abap_cols != pyspark_cols:
            discrepancies["columns"] = {
                "abap": list(abap_cols),
                "pyspark": list(pyspark_cols)
            }
            match = False
        # For further validation, sample up to 10 rows and compare
        sample_abap = sorted([row.asDict() for row in df_abap.take(10)], key=lambda x: str(x))
        sample_pyspark = sorted([row.asDict() for row in df_pyspark.take(10)], key=lambda x: str(x))
        if sample_abap != sample_pyspark:
            discrepancies["sample_data"] = {
                "abap_sample": sample_abap,
                "pyspark_sample": sample_pyspark
            }
            match = False
        # Compute match percentage (simplified)
        match_percentage = 100.0 if match else 0.0
        comparison_results = {
            "match": match,
            "match_percentage": match_percentage,
            "discrepancies": discrepancies
        }
        logging.info("Completed comparison between ABAP and PySpark outputs: %s", json.dumps(comparison_results, indent=2))
        return comparison_results
    except Exception as e:
        logging.error("Error comparing dataframes: %s", str(e))
        raise

# Generate a detailed reconciliation report
def generate_report(comparison_results, report_file="reconciliation_report.txt"):
    try:
        with open(report_file, "w") as f:
            f.write("Reconciliation Report\n")
            f.write("=====================\n")
            f.write(f"Match Status: {'MATCH' if comparison_results['match'] else 'NO MATCH'}\n")
            f.write(f"Match Percentage: {comparison_results['match_percentage']}%\n")
            if comparison_results["discrepancies"]:
                f.write("Discrepancies Found:\n")
                f.write(json.dumps(comparison_results["discrepancies"], indent=2))
            else:
                f.write("No discrepancies found.\n")
        logging.info("Generated reconciliation report at: %s", report_file)
        return report_file
    except Exception as e:
        logging.error("Error generating report: %s", str(e))
        raise

def main():
    setup_logging()
    logging.info("Starting ABAP-to-PySpark migration validation process.")
    try:
        # Paths (could be changed as needed or passed as arguments)
        abap_code_file = "ZBW_LOAD_GL_DATA.txt"
        pyspark_code_file = "ZBW_LOAD_GL_DATA_v2.txt"
        # For simulation, path to input BKPF CSV data.
        input_data_path = "path_to_bkpf_data.csv"
        
        # Initialize Spark session
        spark = SparkSession.builder \
            .appName("ABAP-to-PySpark Migration Validation") \
            .getOrCreate()
        logging.info("Spark session created.")

        # Step 1: Parse ABAP SQL code file
        abap_code, expected_table = parse_abap_code(abap_code_file)
        
        # Step 2: Execute ABAP code simulation
        df_abap = execute_abap_code(abap_code, spark, input_data_path)
        
        # (Optional) For validation, you might apply the same filtering as in the converted PySpark code.
        # Here we simulate the same transformation that the ABAP execution should perform.
        df_abap_filtered = df_abap.filter(col("bukrs") == "1000") \
                                   .filter(col("blart") == "SA")
        
        # Step 3: Export ABAP output to Parquet file
        parquet_file_path = export_to_parquet(df_abap_filtered, expected_table["name"])
        
        # Step 4: Transfer Parquet file to distributed storage (simulation)
        storage_file_path = transfer_to_storage(parquet_file_path)
        
        # Step 5: Create external table (temporary view) in Spark from the Parquet file
        df_external = create_pyspark_external_table(spark, storage_file_path)
        
        # Step 6: Execute the converted PySpark code to get its result
        df_pyspark = execute_converted_pyspark_code(spark, pyspark_code_file, input_data_path)
        
        # Step 7: Compare the two outputs
        comparison_results = compare_dataframes(df_abap_filtered, df_pyspark)
        
        # Step 8: Generate a reconciliation report
        report_path = generate_report(comparison_results)
        
        logging.info("Validation process completed successfully. Report available at: %s", report_path)
        
    except Exception as e:
        logging.error("An error occurred in the migration validation process: %s", traceback.format_exc())
        sys.exit(1)
    finally:
        spark.stop()
        logging.info("Spark session stopped.")
        
if __name__ == "__main__":
    main()