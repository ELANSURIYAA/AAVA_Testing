#!/usr/bin/env python
"""
Comprehensive Python script for automating ABAP-to-PySpark migration validation.

This script:
  1. Reads ABAP SQL code from a file (e.g., "ZBW_LOAD_GL_DATA.txt").
  2. Simulates execution of the ABAP code on an SAP system (using a stub; in production, use pyRFC).
  3. Exports the resulting data into CSV and converts it to Parquet format.
  4. Transfers the Parquet files to a designated storage location (e.g., S3/HDFS; here we simulate via a local directory).
  5. Initializes a PySpark session and creates an external table from the uploaded Parquet file.
  6. Executes the provided converted PySpark code to transform the data (adding a "status" column based on the "betrag" field).
  7. Compares outputs from the ABAP execution and the PySpark transformation, computing row counts, column-level differences,
     and a match percentage for each table.
  8. Generates a reconciliation report with details on match status, discrepancies, and samples for further investigation.
  9. Implements robust error handling, secure connection placeholders, and detailed logging for auditability.

Assumptions and Placeholders:
  - ABAP execution is simulated here using a stub. In production, replace with actual SAP connection via pyRFC.
  - Distributed storage (S3, HDFS, or Data Lake) transfer is simulated via a designated output directory.
  - JDBC and Spark connection properties are placeholders and must be updated as per your secure environment.
  - The converted PySpark code is embedded in this script (i.e., the transformation logic is reproduced).

API Cost for this call: 0.005 USD (example cost)

To run:
  Ensure required libraries are installed (pandas, pyarrow, pyspark) and then execute this script.
"""

import os
import sys
import logging
import json
import pandas as pd
import pyarrow  # Ensure pyarrow is installed for parquet conversion
from datetime import datetime

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.utils import AnalysisException

# Configure logging for detailed real-time execution logs
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)

# --- Helper Functions ---

def read_abap_code(file_path):
    """
    Reads the ABAP SQL code from the given file.
    """
    try:
        with open(file_path, "r") as f:
            abap_code = f.read()
        logging.info("Successfully read ABAP code from %s", file_path)
        return abap_code
    except Exception as e:
        logging.error("Error reading ABAP code file", exc_info=True)
        raise

def simulate_abap_execution(abap_code, sap_conn_params=None):
    """
    Simulates execution of ABAP code.
    In production, this would connect to the SAP system (e.g., via pyRFC) and execute the ABAP SQL code.
    
    For simulation purposes, we create a dummy DataFrame that mimics an ABAP output.
    The DataFrame includes a 'betrag' column (numeric) which will be used for transformation.
    """
    try:
        # Placeholder: simulated ABAP output data.
        # In a real scenario, the ABAP SQL code will be parsed and executed, returning an output table.
        data = {
            "betrag": [1500.0, 800.0, None, 1200.0, 950.0],
            "doc_id": [101, 102, 103, 104, 105],
            "description": ["Entry A", "Entry B", "Entry C", "Entry D", "Entry E"]
        }
        df = pd.DataFrame(data)
        logging.info("Simulated ABAP execution completed; retrieved %d record(s).", len(df))
        return df
    except Exception as e:
        logging.error("Error during simulated ABAP execution", exc_info=True)
        raise

def export_df_to_parquet(df, table_name, output_dir="output"):
    """
    Exports the given Pandas DataFrame to CSV and then converts it to Parquet format.
    The filenames are based on the table name and current timestamp.
    Returns the path to the Parquet file.
    """
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = os.path.join(output_dir, f"{table_name}_{timestamp}.csv")
        parquet_filename = os.path.join(output_dir, f"{table_name}_{timestamp}.parquet")
        
        # Export to CSV first (for traceability)
        df.to_csv(csv_filename, index=False)
        logging.info("Exported DataFrame to CSV: %s", csv_filename)
        
        # Read CSV back and convert to Parquet
        df_read = pd.read_csv(csv_filename)
        df_read.to_parquet(parquet_filename, index=False)
        logging.info("Converted CSV to Parquet: %s", parquet_filename)
        return parquet_filename
    except Exception as e:
        logging.error("Error exporting DataFrame to Parquet", exc_info=True)
        raise

def upload_to_storage(parquet_filepath, storage_config=None):
    """
    Simulates transferring the Parquet file to a distributed storage system.
    In production, implement S3/HDFS/Data Lake APIs with secure authentication.
    
    Here we simulate by returning the same path (or copying to a designated 'storage' directory).
    """
    try:
        # For simplicity, simulate by copying to a "storage" folder.
        storage_dir = "storage"
        if not os.path.exists(storage_dir):
            os.makedirs(storage_dir)
        filename = os.path.basename(parquet_filepath)
        destination_path = os.path.join(storage_dir, filename)
        with open(parquet_filepath, "rb") as src, open(destination_path, "wb") as dst:
            dst.write(src.read())
        logging.info("Simulated file transfer to distributed storage: %s", destination_path)
        return destination_path
    except Exception as e:
        logging.error("Error uploading file to storage", exc_info=True)
        raise

def initialize_spark(app_name="GL_Data_Load_and_Transformation"):
    """
    Initializes and returns a SparkSession.
    """
    try:
        spark = SparkSession.builder.appName(app_name).getOrCreate()
        logging.info("Spark session initiated.")
        return spark
    except Exception as e:
        logging.error("Error initializing Spark session", exc_info=True)
        raise

def run_converted_pyspark_code(spark, jdbc_url, connection_properties, parquet_file):
    """
    Simulates the running of converted PySpark code for GL Data Processing.
    
    Instead of reading from a database, here we read from the Parquet file that was uploaded to storage.
    Then we perform the transformation: add a new column "status" based on the value of "betrag".
      - If betrag > 1000 -> status = 'HIGH'
      - Else -> status = 'LOW'
    
    Returns the transformed Spark DataFrame.
    """
    try:
        # Read the uploaded Parquet file as an external table
        df = spark.read.parquet(parquet_file)
        logging.info("Read Parquet file into Spark DataFrame with %d record(s).", df.count())
        
        # Transformation logic (same as the provided converted PySpark code)
        df_transformed = df.withColumn(
            "status",
            F.when(F.col("betrag") > 1000, F.lit("HIGH")).otherwise(F.lit("LOW"))
        )
        
        # Simulate writing back to target table by simply creating a temporary view
        df_transformed.createOrReplaceTempView("pyspark_output")
        logging.info("Applied PySpark transformation; created temporary table 'pyspark_output'.")
        return df_transformed
    except Exception as e:
        logging.error("Error in executing converted PySpark code", exc_info=True)
        raise

def compare_datasets(abap_df, pyspark_df):
    """
    Compares the ABAP DataFrame (converted to Spark DataFrame) and the PySpark transformed DataFrame.
    Performs:
      - Row count comparison.
      - Column-level comparison (for matching columns: betrag, status).
      - Computes a match percentage.
    
    Returns a reconciliation report as a dictionary.
    """
    try:
        # Convert ABAP pandas DataFrame to Spark DataFrame for uniform comparison
        spark = pyspark_df.sql_ctx.sparkSession
        abap_spark_df = spark.createDataFrame(abap_df)
        
        # Apply transformation on ABAP dataset using same logic for fair comparison
        abap_spark_df_transformed = abap_spark_df.withColumn(
            "status",
            F.when(F.col("betrag") > 1000, F.lit("HIGH")).otherwise(F.lit("LOW"))
        )
        
        abap_count = abap_spark_df_transformed.count()
        pyspark_count = pyspark_df.count()
        
        row_count_match = (abap_count == pyspark_count)
        
        # Do a join comparison based on unique key if available, here we use row order as a simulation.
        # For a more robust solution, a unique identifier (such as doc_id) must exist.
        abap_data = sorted(abap_spark_df_transformed.collect(), key=lambda row: row.__getitem__("doc_id") if "doc_id" in row.asDict() else 0)
        pyspark_data = sorted(pyspark_df.collect(), key=lambda row: row.__getitem__("doc_id") if "doc_id" in row.asDict() else 0)
        
        total_records = max(abap_count, pyspark_count)
        matching_records = 0
        discrepancies = []
        
        for i in range(total_records):
            try:
                abap_row = abap_data[i].asDict()
                pyspark_row = pyspark_data[i].asDict()
            except IndexError:
                # one of the datasets has fewer records
                discrepancies.append({"record": i, "abap": abap_data[i].asDict() if i < len(abap_data) else None,
                                       "pyspark": pyspark_data[i].asDict() if i < len(pyspark_data) else None,
                                       "error": "Row missing in one dataset"})
                continue
            # Compare each field in a column-wise fashion
            row_match = True
            row_diffs = {}
            for col in abap_row:
                # Only compare relevant columns (e.g., betrag and status)
                if col in ["betrag", "status"]:
                    if abap_row[col] != pyspark_row.get(col):
                        row_match = False
                        row_diffs[col] = {"abap": abap_row[col], "pyspark": pyspark_row.get(col)}
            if row_match:
                matching_records += 1
            else:
                discrepancies.append({"record": i, "differences": row_diffs})
        
        match_percentage = (matching_records / total_records) * 100 if total_records > 0 else 100.0
        
        report = {
            "row_count_match": row_count_match,
            "abap_row_count": abap_count,
            "pyspark_row_count": pyspark_count,
            "matching_records": matching_records,
            "total_records": total_records,
            "match_percentage": match_percentage,
            "discrepancies": discrepancies,
            "overall_status": "MATCH" if match_percentage == 100 else ("PARTIAL MATCH" if matching_records > 0 else "NO MATCH")
        }
        
        logging.info("Reconciliation report generated with match percentage: %.2f%%", match_percentage)
        return report
    except Exception as e:
        logging.error("Error during dataset comparison", exc_info=True)
        raise

def generate_report(report, report_path="reconciliation_report.json"):
    """
    Saves the reconciliation report as a JSON file for external parsing.
    """
    try:
        with open(report_path, "w") as f:
            json.dump(report, f, indent=4)
        logging.info("Reconciliation report saved: %s", report_path)
    except Exception as e:
        logging.error("Error saving reconciliation report", exc_info=True)
        raise

# --- Main Workflow ---
def main():
    logging.info("Starting the ABAP-to-PySpark Migration Validation process.")
    try:
        # 1. ANALYZE INPUTS:
        abap_file = "ZBW_LOAD_GL_DATA.txt"  # File containing ABAP SQL Code
        abap_code = read_abap_code(abap_file)
        
        # For converted PySpark code, we use the built-in transformation logic (see run_converted_pyspark_code)
        # and assume connection properties are set below:
        jdbc_url = "jdbc:your_database_url"  # Placeholder
        connection_properties = {
            "user": "your_username",
            "password": "your_password",
            "driver": "your_jdbc_driver"
        }
        
        # 2 & 3. EXECUTE ABAP CODE (SIMULATED) and RETRIEVE RESULT DATASET:
        abap_df = simulate_abap_execution(abap_code)
        
        # 4. EXPORT ABAP OUTPUT AND TRANSFORM (CSV -> Parquet):
        table_name = "gl_data"
        parquet_path = export_df_to_parquet(abap_df, table_name)
        
        # 5. TRANSFER PARQUET FILE TO DISTRIBUTED STORAGE (SIMULATED):
        storage_parquet_path = upload_to_storage(parquet_path)
        
        # 6. INITIALIZE PYSPARK ENVIRONMENT:
        spark = initialize_spark()
        
        # 7. EXECUTE CONVERTED PYSPARK CODE:
        pyspark_transformed_df = run_converted_pyspark_code(spark, jdbc_url, connection_properties, storage_parquet_path)
        
        # 8. VALIDATE & COMPARE OUTPUTS:
        reconciliation_report = compare_datasets(abap_df, pyspark_transformed_df)
        
        # 9. GENERATE DETAILED RECONCILIATION REPORT:
        report_file = "reconciliation_report.json"
        generate_report(reconciliation_report, report_file)
        
        logging.info("Migration validation process completed successfully.")
        logging.info("Final Reconciliation Report: %s", json.dumps(reconciliation_report, indent=4))
        
        # Clean up the Spark session
        spark.stop()
    except Exception as e:
        logging.error("Migration validation process failed", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()