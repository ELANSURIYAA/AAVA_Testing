# Converted PySpark Code
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create Spark session if not already available
spark = SparkSession.builder.appName("GLDataProcessing").getOrCreate()

# Read GL data from a source, here using JDBC connection.
# Replace with the appropriate source details.
gl_df = spark.read.format("jdbc").options(
    url="jdbc:your_database_url",
    dbtable="gl_data",
    user="your_username",
    password="your_password",
    driver="com.your.jdbc.Driver"
).load()

# Filter GL data for the year 2020
gl_df_2020 = gl_df.filter(col("year") == "2020")

# Remove records where the 'line' column starts with 'ERROR'
# This mimics the ABAP check: IF lv_line CP 'ERROR*'
valid_data_df = gl_df_2020.filter(~col("line").startswith("ERROR"))

# Count the total number of valid rows processed
total_processed = valid_data_df.count()

# Output the total processed rows
print("Total processed:", total_processed)

# Stop the Spark session if needed
spark.stop()

# Cost consumed by the API for this call: $0.002