-------------------------------------------------
# Converted PySpark Code (API Cost: 0.05 units)

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

def write_gl_data(df):
    """
    Simulates the ABAP function call 'Z_WRITE_GL_DATA'.
    In a real scenario, implement data persistence logic here.
    For demonstration, it prints the DataFrame content to the console.
    """
    df.show(truncate=False)

if __name__ == "__main__":
    # Initialize SparkSession
    spark = SparkSession.builder.appName("ZBW_LOAD_GL_DATA").getOrCreate()

    # Read GL entries from source table
    # This corresponds to the ABAP: SELECT * FROM zgl_source_table INTO TABLE @DATA(lt_gl_data).
    df = spark.table("zgl_source_table")

    # Check if the GL data is empty (similar to ABAP IS INITIAL check)
    if df.rdd.isEmpty():
        print("No GL data found.")
        spark.stop()
        exit(1)

    # Perform transformation logic:
    # For every row, multiply the gl_amount by 1.1, simulating the LOOP in ABAP that modifies each entry.
    df_transformed = df.withColumn("gl_amount", col("gl_amount") * 1.1)

    # Write out the transformed data by simulating the ABAP function call 'Z_WRITE_GL_DATA'
    write_gl_data(df_transformed)

    # Final output message as in the ABAP program
    print("GL Data loaded and processed successfully.")

    # Stop the Spark session
    spark.stop()