```
# Import necessary modules
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Load_GL_Data") \
    .getOrCreate()

# Read the gl_account table into a DataFrame.
# Here, assume that the connection details are set and the table is accessible via JDBC or a preloaded DataFrame.
# For the purpose of this transformation, we assume the data is in a DataFrame called df_gl_account.
# Replace the following line with the appropriate data source reading method.
df_gl_account = spark.read.format("jdbc") \
    .option("url", "jdbc:your_database_url") \
    .option("dbtable", "gl_account") \
    .option("user", "your_username") \
    .option("password", "your_password") \
    .load()

# Filter records where gl_type is 'ASSET'
df_assets = df_gl_account.filter(col("gl_type") == "ASSET")

# Count the number of assets
# This is equivalent to looping through each asset and incrementing a counter in ABAP.
asset_count = df_assets.count()

# Check if there are any assets and print appropriate message.
if asset_count > 0:
    print("Total assets:", asset_count)
else:
    print("No assets found.")

# Stop the Spark session after processing.
spark.stop()

# Cost consumed by the API for this call: [Insert API cost details here]
```