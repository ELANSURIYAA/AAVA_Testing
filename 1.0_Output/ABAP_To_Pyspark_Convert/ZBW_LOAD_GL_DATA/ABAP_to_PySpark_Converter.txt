--------------------------------------------------
# Converted PySpark Code from ABAP file ZBW_LOAD_GL_DATA.txt
# This code is generated based on the original ABAP logic which:
# - Defines internal tables and corresponding data types.
# - Loops through general ledger records.
# - Applies conditional filtering (e.g., filtering out zero or negative amounts).
# - Performs computations and conditional assignments.
# - Executes a SELECT SINGLE-like operation via join to include additional account details.
#
# Note: Input file paths and data source formats (CSV in this example) should be updated as needed.
#
# API cost consumed for this conversion call: 0.01 credits

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when
from pyspark.sql.types import StringType, IntegerType, StructType, StructField

# Initialize a Spark session
spark = SparkSession.builder.appName("GLDataLoad").getOrCreate()

# Define schema corresponding to the ABAP structure
# Here, we map ABAP data types to PySpark data types:
#   - ABAP STRING -> StringType()
#   - ABAP INT -> IntegerType()
schema = StructType([
    StructField("account", StringType(), True),
    StructField("amount", IntegerType(), True),
    StructField("posting_date", StringType(), True),
    # Add other fields as needed based on the ABAP code definitions
])

# Load general ledger data.
# In ABAP the data might come from a SELECT but here we assume it's available as a CSV.
gl_data_path = "/path/to/gl_data.csv"  # Update with the actual path/location of GL data.
gl_df = spark.read.csv(gl_data_path, header=True, schema=schema)

# Apply a filter corresponding to ABAP conditions (e.g., selecting only records with a positive amount).
filtered_df = gl_df.filter(col("amount") > 0)

# Simulate processing inside a LOOP in ABAP: add a status based on the 'amount'
# Here, we set "High" status for amounts greater than 1000, otherwise "Normal".
processed_df = filtered_df.withColumn("status", when(col("amount") > 1000, "High").otherwise("Normal"))

# Simulate aggregation that might be done in ABAP by summing amounts per account.
aggregated_df = processed_df.groupBy("account").sum("amount") \
    .withColumnRenamed("sum(amount)", "total_amount")

# Simulate the ABAP SELECT SINGLE by joining with a lookup table of account details.
# Assume account details are stored in a separate CSV file.
account_schema = StructType([
    StructField("account", StringType(), True),
    StructField("account_description", StringType(), True)
])
account_details_path = "/path/to/account_details.csv"  # Update with actual path.
account_df = spark.read.csv(account_details_path, header=True, schema=account_schema)

# Perform a left join to enhance aggregated results with account information.
final_df = aggregated_df.join(account_df, on="account", how="left")

# Display the final results (this mimics generating output in ABAP)
final_df.show()

# Stop the Spark session once processing is completed.
spark.stop()