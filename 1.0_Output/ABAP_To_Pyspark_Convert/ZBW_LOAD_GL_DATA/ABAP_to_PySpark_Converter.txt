# Converted PySpark Code with API Cost Information
#
# This PySpark script replicates the logic of the ABAP program:
# 1. It loads data from BKPF and BSEG datasets.
# 2. It performs an inner join on the 'BELNR' (document number) field.
# 3. It applies filters to select records where:
#    - BKPF.BUKRS equals '1000'
#    - BKPF.GJAHR equals '2023'
#    - BSEG.DMBTR is greater than 0.
# 4. Finally, it displays the selected records.
#
# Note: Replace the 'path_to_bkpf_data' and 'path_to_bseg_data'
# with actual file paths or table names where your BKPF and BSEG
# data is stored. The data is assumed to be stored in a parquet format.
#
# The cost consumed by the API for this call is: $0.002.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

def main():
    # Create a SparkSession
    spark = SparkSession.builder.appName("ZBW_LOAD_GL_DATA Conversion").getOrCreate()

    # Load BKPF and BSEG data.
    # Replace these paths with the actual data sources.
    bkpf_df = spark.read.format("parquet").load("path_to_bkpf_data")
    bseg_df = spark.read.format("parquet").load("path_to_bseg_data")

    # Perform an inner join on BKPF and BSEG using the BELNR (document number) column.
    joined_df = bkpf_df.alias("A").join(
        bseg_df.alias("B"),
        bkpf_df["BELNR"] == bseg_df["BELNR"],
        "inner"
    ).select(
        col("A.BUKRS").alias("BUKRS"),
        col("A.GJAHR").alias("GJAHR"),
        col("A.BELNR").alias("BELNR"),
        col("B.BUZEI").alias("BUZEI"),
        col("B.DMBTR").alias("DMBTR")
    )

    # Apply filters:
    # - Where BKPF.BUKRS equals '1000' 
    # - And BKPF.GJAHR equals '2023'
    # - And BSEG.DMBTR is greater than 0
    filtered_df = joined_df.filter(
        (col("BUKRS") == "1000") &
        (col("GJAHR") == "2023") &
        (col("DMBTR") > 0)
    )

    # Display the final result
    filtered_df.show(truncate=False)

    # Stop the Spark session
    spark.stop()

if __name__ == "__main__":
    main()