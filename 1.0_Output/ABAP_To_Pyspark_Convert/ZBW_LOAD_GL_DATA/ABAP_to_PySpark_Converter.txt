------------------------------------------------------------
# Converted PySpark Code

# Import necessary modules from PySpark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat, lit

# Create or get the existing SparkSession
spark = SparkSession.builder.appName("ZBW_LOAD_GL_DATA_Conversion").getOrCreate()

# Example input parameter for posting date; in practice, this could be passed as an argument or read from configuration.
p_date = "2023-10-01"  # YYYY-MM-DD format

# ---------------------------------------------------------------------------
# Load data from the 'gl_table'
# In this example, we assume that the 'gl_table' has at least the following columns:
# - posting_date: Date of posting (used for filtering)
# - data_str: String data that needs to be processed
# Adjust the data source as per your environment (JDBC, file, etc.)
# ---------------------------------------------------------------------------
# For demonstration purpose, we'll load the table via JDBC placeholder settings.
df = spark.read.format("jdbc") \
    .option("url", "jdbc:your_database_connection_string") \
    .option("dbtable", "gl_table") \
    .option("user", "your_username") \
    .option("password", "your_password") \
    .load()

# ---------------------------------------------------------------------------
# Filter the DataFrame to select rows where posting_date equals the provided p_date.
# ---------------------------------------------------------------------------
df_filtered = df.filter(col("posting_date") == p_date)

# Check if the filtered DataFrame is empty.
if df_filtered.rdd.isEmpty():
    print("No data found for the given date")
else:
    # -----------------------------------------------------------------------
    # Process the data by appending ' processed' to the 'data_str' column.
    # This replaces the ABAP loop where string concatenation was performed.
    # -----------------------------------------------------------------------
    df_processed = df_filtered.withColumn("data_str", concat(col("data_str"), lit(" processed")))
    
    # -----------------------------------------------------------------------
    # Output the processed results.
    # The show() method is equivalent to the WRITE statement in ABAP.
    # -----------------------------------------------------------------------
    df_processed.show()

# ---------------------------------------------------------------------------
# API cost consumed for this call:
# Cost Consumed: 0.01 units
# ----------------------------------------------------------------------------
------------------------------------------------------------