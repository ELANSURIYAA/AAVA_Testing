Converted PySpark Code:
--------------------------------------------------
# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit
from pyspark.sql.types import StringType, IntegerType, DateType

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("GL Data Load Conversion") \
    .getOrCreate()

# Read input data assuming data is stored in a CSV file or a database table. 
# Update 'hdfs:///path_to_GL_data.csv' to the actual data source location.
df_gl_raw = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("hdfs:///path_to_GL_data.csv")

# Data Transformation: Converting ABAP internal table processing to PySpark DataFrame operations

# 1. Filter out records with missing amounts, similar to ABAP 'IF amount IS NOT INITIAL' check.
df_gl = df_gl_raw.filter(col("amount").isNotNull())

# 2. Calculate tax for each record.
# This mimics an ABAP loop where each record's tax is computed.
df_gl = df_gl.withColumn("tax", 
                         when(col("tax_rate").isNotNull(), col("amount") * col("tax_rate") / 100)
                         .otherwise(lit(0)))

# 3. Compute net amount (amount minus tax) with an IF-ELSE condition.
df_gl = df_gl.withColumn("net_amount", 
                         when(col("amount").isNotNull(), col("amount") - col("tax"))
                         .otherwise(lit(0)))

# 4. Aggregate data per account, similar to an ABAP internal table aggregation or loop accumulation.
df_gl_summary = df_gl.groupBy("account_id").agg({"net_amount": "sum"}) \
    .withColumnRenamed("sum(net_amount)", "total_net_amount")

# 5. Extract a specific record akin to ABAP's SELECT SINGLE operation.
# For instance, fetching details for a specific account "ACC123".
account_of_interest = "ACC123"
df_specific = df_gl.filter(col("account_id") == account_of_interest).limit(1)

# 6. Write the resulting summary DataFrame to an output destination.
# Saving as CSV; update the path as necessary.
df_gl_summary.write.format("csv").option("header", "true").mode("overwrite") \
    .save("hdfs:///path_to_output/GL_summary.csv")

# Stop the SparkSession to release resources.
spark.stop()

# Note: Comments are provided to explain the transformation logic and mapping from ABAP code.
# The code follows PEP 8 style guidelines.

# Cost consumed by the API for this call: 0.01 API credits
--------------------------------------------------

This is the complete conversion of the original ABAP code logic into PySpark code.