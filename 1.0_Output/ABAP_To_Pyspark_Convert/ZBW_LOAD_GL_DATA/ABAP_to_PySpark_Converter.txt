------------------------------------------------------------
# Converted PySpark Code (Cost consumed by API for this call: $0.005)

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType

# Initialize Spark session
spark = SparkSession.builder \
    .appName("GL Data Loader") \
    .getOrCreate()

# ------------------------------------------------------------
# Step 1: Read the GL data from a text file.
# In ABAP, the code opens a dataset and reads line by line.
# In PySpark, we read the entire file into a DataFrame where each row represents a line.
# ------------------------------------------------------------

# Replace 'path/to/GL_DATA.DAT' with the actual file path where the GL_DATA is stored.
df_gl = spark.read.text("path/to/GL_DATA.DAT")

# ------------------------------------------------------------
# Step 2: Process the read data.
# In the ABAP code, only the lines that start with 'GL' are appended to the result.
# We use the DataFrame filter() to select only those rows that match the condition.
# ------------------------------------------------------------

# Define a UDF (User Defined Function) to check if a line starts with 'GL'
def starts_with_gl(line):
    if line is None:
        return False
    return line.startswith("GL")

# Register the UDF
starts_with_gl_udf = udf(lambda x: starts_with_gl(x), "boolean")

# Filter rows where the line starts with 'GL'
df_result = df_gl.filter(starts_with_gl_udf(col("value")))

# ------------------------------------------------------------
# Step 3: Show the final result.
# This mirrors the ABAP code that loops through the result array and writes each line.
# ------------------------------------------------------------

df_result.show(truncate=False)

# ------------------------------------------------------------
# Additional notes on conversion:
# - The ABAP file reading using OPEN DATASET and READ DATASET is replaced by spark.read.text().
# - The internal table processing (APPEND, LOOP AT) is replaced by the DataFrame transformation using filter().
# - This code follows PEP 8 style guidelines and intentionally contains inline comments explaining each step.
# ------------------------------------------------------------

# Stop the Spark session
spark.stop()
------------------------------------------------------------