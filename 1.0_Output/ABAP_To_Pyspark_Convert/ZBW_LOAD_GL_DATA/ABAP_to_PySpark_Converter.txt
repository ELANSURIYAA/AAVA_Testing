# Converted PySpark Code

from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col

# Initialize Spark session
spark = SparkSession.builder.appName("GLDataProcessing").getOrCreate()

# Read the GL data into a DataFrame.
# Adjust the data source as needed. Here, it is shown as a JDBC table read.
# If reading from a file, you might use spark.read.csv or spark.read.parquet.
df_gl = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:your_database_url") \
    .option("dbtable", "zbw_gl_data") \
    .option("user", "your_username") \
    .option("password", "your_password") \
    .load()

# Alternatively, if data is stored as a CSV file:
# df_gl = spark.read.option("header", "true").csv("path_to/zbw_gl_data.csv")

# Transform the DataFrame:
# Create a new column 'status' based on the condition: if amount > 1000 then 'HIGH' else 'NORMAL'
df_gl_updated = df_gl.withColumn(
    "status",
    when(col("amount") > 1000, "HIGH").otherwise("NORMAL")
)

# If you need to update/modify the DataFrame further, you can chain additional transformations here.
# For demonstration, we show the resulting DataFrame.
df_gl_updated.show()

# Optionally, write back the updated DataFrame to a target database or file system.
# Example of writing to a new table in the database:
# df_gl_updated.write \
#     .format("jdbc") \
#     .option("url", "jdbc:your_database_url") \
#     .option("dbtable", "zbw_gl_data_updated") \
#     .option("user", "your_username") \
#     .option("password", "your_password") \
#     .mode("overwrite") \
#     .save()

# Indicate processing is complete
print("GL Data processing complete.")

# Stop the Spark session
spark.stop()

# API call cost consumed: $0.005