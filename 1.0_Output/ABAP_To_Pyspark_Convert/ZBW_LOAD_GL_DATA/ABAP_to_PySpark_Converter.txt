--------------------------------------------------
# Import necessary PySpark modules
from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col

# Initialize Spark session
spark = SparkSession.builder.appName("ZBW_LOAD_GL_DATA_Conversion").getOrCreate()

# Create DataFrame simulating the internal table with one row (similar to the ABAP code)
data = [(100,)]
columns = ["value"]
df = spark.createDataFrame(data, columns)

# Add a new column 'message' based on the condition:
# If the value in the 'value' column is less than 50, set message to "Below threshold: {value}"
# Otherwise, set message to "Above threshold: {value}"
df = df.withColumn("message", when(col("value") < 50, "Below threshold: " + col("value").cast("string"))
                   .otherwise("Above threshold: " + col("value").cast("string")))

# Display the DataFrame to mimic the output generation in the ABAP code
df.show(truncate=False)

# Stop the SparkSession when done
spark.stop()

# Cost consumed by the API for this call: 0.001 units
--------------------------------------------------