-----------------------------------------------------
# Converted PySpark Code

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.appName("GL Data Processing").getOrCreate()

# File path for GL data as used in the original ABAP code (gl_data.txt)
file_path = "/path/to/gl_data.txt"  # Update this path as needed

# Load GL data from file into a DataFrame. Each line of the file becomes a row in the DataFrame.
df = spark.read.text(file_path)

# Filter out rows where the line contains the error pattern '*ERROR*'
# This is equivalent to the ABAP conditional check IF lv_item CP '*ERROR*'
valid_entries_df = df.filter(~col("value").contains("*ERROR*"))

# Count the total number of valid entries (lines that do not contain '*ERROR*')
total_valid_entries = valid_entries_df.count()

# Output the total valid entries, similar to ABAP WRITE statement
print("Total valid entries:", total_valid_entries)

# Stop the Spark session
spark.stop()

# Cost consumed by the API for this call: [Cost details would be determined by the API usage metrics]
-----------------------------------------------------
  
# Note: Replace "/path/to/gl_data.txt" with the actual path to the GL data file.
  
# This PySpark code follows PEP 8 style, uses DataFrame operations to process text files, filters out error lines,
# and counts valid entries, mapping ABAP logic to distributed Spark operations efficiently.