from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col, sum as sum_

# Initialize Spark session
spark = SparkSession.builder.appName("ZBW_LOAD_GL_DATA_Conversion").getOrCreate()

# Read data from the GL table source.
# This example assumes the data is stored in a Parquet file.
# Adjust the data source and format as needed.
gl_df = spark.read.format("parquet").load("/path/to/gl_table_parquet")

# Add a new column 'status' based on the condition:
# If the 'value' > 1000 then 'High', else 'Normal'
gl_df = gl_df.withColumn("status", when(col("value") > 1000, "High").otherwise("Normal"))

# Calculate the total value by aggregating the 'value' column.
total_value = gl_df.agg(sum_("value").alias("total_value")).collect()[0]["total_value"]

# Print the computed total value.
print("Total Value:", total_value)

# Stop the Spark session.
spark.stop()

# Cost consumed by the API for this call: 0.02 units