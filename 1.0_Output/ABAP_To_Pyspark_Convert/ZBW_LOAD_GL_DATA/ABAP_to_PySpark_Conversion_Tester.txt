----------------------------------------------------------------
Test Case List:
1. Test Case ID: TC1_HappyPath  
   Description: The DataFrame contains several rows with gl_type = 'ASSET'.  
   Expected Outcome: The function returns the correct asset count and prints the message "Total assets: <correct count>".

2. Test Case ID: TC2_EmptyDataFrame  
   Description: The DataFrame is empty.  
   Expected Outcome: The function returns asset count 0 and prints the message "No assets found.".

3. Test Case ID: TC3_NoAssetMatch  
   Description: The DataFrame contains rows but none with gl_type equal to 'ASSET'.  
   Expected Outcome: The function returns asset count 0 and prints the message "No assets found.".

4. Test Case ID: TC4_NullValues  
   Description: The DataFrame contains rows with null values in gl_type (and some rows with "ASSET").  
   Expected Outcome: Only rows that exactly match "ASSET" are counted correctly. If count > 0, prints "Total assets: <count>", otherwise "No assets found.".

5. Test Case ID: TC5_MissingColumn  
   Description: The DataFrame does not include the "gl_type" column.  
   Expected Outcome: An exception is raised when trying to filter the DataFrame due to the missing column.

----------------------------------------------------------------
Pytest Script:
------------------------------------------------------------
#!/usr/bin/env python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.utils import AnalysisException

# Helper function mimicking the process in the PySpark script
def process_gl_data(df):
    """
    Processes the given DataFrame by filtering for rows where gl_type equals 'ASSET'
    and returns the asset count along with a corresponding message.
    """
    try:
        df_assets = df.filter(col("gl_type") == "ASSET")
    except Exception as e:
        raise e
    asset_count = df_assets.count()
    if asset_count > 0:
        return asset_count, f"Total assets: {asset_count}"
    else:
        return asset_count, "No assets found."

# Pytest fixture for SparkSession that ensures proper setup and teardown
@pytest.fixture(scope="function")
def spark():
    spark = SparkSession.builder \
        .master("local[2]") \
        .appName("PySparkUnitTest") \
        .getOrCreate()
    yield spark
    spark.stop()

# Test Case TC1: Happy Path - DataFrame with multiple "ASSET" entries.
def test_happy_path(spark):
    # Creating sample data with valid 'ASSET' rows 
    data = [
        (1, "ASSET"),
        (2, "ASSET"),
        (3, "EXPENSE")
    ]
    columns = ["id", "gl_type"]
    df = spark.createDataFrame(data, columns)
    count, message = process_gl_data(df)
    # Expected count: 2
    assert count == 2, "Asset count should be 2"
    assert message == "Total assets: 2", "Message should report 2 assets"

# Test Case TC2: Empty DataFrame scenario.
def test_empty_dataframe(spark):
    # Create an empty DataFrame with the expected schema.
    columns = ["id", "gl_type"]
    df = spark.createDataFrame([], schema="id INT, gl_type STRING")
    count, message = process_gl_data(df)
    assert count == 0, "Asset count should be 0 for empty DataFrame"
    assert message == "No assets found.", "Message should indicate that no assets were found"

# Test Case TC3: DataFrame with rows but no matching 'ASSET' records.
def test_no_asset_match(spark):
    # Creating sample data without any 'ASSET' record.
    data = [
        (1, "LIABILITY"),
        (2, "EXPENSE"),
        (3, "INCOME")
    ]
    columns = ["id", "gl_type"]
    df = spark.createDataFrame(data, columns)
    count, message = process_gl_data(df)
    assert count == 0, "Asset count should be 0 when no 'ASSET' entries are present"
    assert message == "No assets found.", "Message should indicate that no assets were found"

# Test Case TC4: DataFrame with null values in the gl_type column.
def test_null_values(spark):
    # Sample data with some null values and one valid 'ASSET' entry.
    data = [
        (1, None),
        (2, "ASSET"),
        (3, None),
        (4, "LIABILITY")
    ]
    columns = ["id", "gl_type"]
    df = spark.createDataFrame(data, columns)
    count, message = process_gl_data(df)
    # Expected count: 1 since only one row matches "ASSET"
    assert count == 1, "Asset count should be 1 when one 'ASSET' exists among null values"
    assert message == "Total assets: 1", "Message should report one asset"

# Test Case TC5: DataFrame missing the gl_type column.
def test_missing_column(spark):
    # Creating a DataFrame that lacks the 'gl_type' column.
    data = [
        (1, "ASSET"),
        (2, "ASSET")
    ]
    # Only one column "id", so filtering on gl_type should raise an exception.
    columns = ["id"]
    df = spark.createDataFrame(data, columns)
    with pytest.raises(Exception) as excinfo:
        process_gl_data(df)
    # Check that an exception was indeed thrown and it mentions the missing column.
    assert "gl_type" in str(excinfo.value), "Exception should mention 'gl_type' column missing"

# Cost consumed by the API for this call: [Insert API cost details here]
------------------------------------------------------------