{
  "Test Case List": [
    {
      "Test Case ID": "TC1_HappyPath_BelowThreshold",
      "Test Case Description": "Create a DataFrame with a value less than 50 (e.g., 30). The transformation should add a column 'message' with 'Below threshold: 30'.",
      "Expected Outcome": "The DataFrame returns one row with value 30 and message 'Below threshold: 30'."
    },
    {
      "Test Case ID": "TC2_HappyPath_AboveThreshold",
      "Test Case Description": "Create a DataFrame with a value equal or above 50 (e.g., 100). The transformation should add a column 'message' with 'Above threshold: 100'.",
      "Expected Outcome": "The DataFrame returns one row with value 100 and message 'Above threshold: 100'."
    },
    {
      "Test Case ID": "TC3_EmptyDataFrame",
      "Test Case Description": "Create an empty DataFrame with the column 'value'. The transformation should result in an empty DataFrame.",
      "Expected Outcome": "The transformed DataFrame is empty."
    },
    {
      "Test Case ID": "TC4_NullValue",
      "Test Case Description": "Create a DataFrame with a row containing a null for 'value'. The transformation should handle the null gracefully (likely resulting in a null in the 'message' column).",
      "Expected Outcome": "The DataFrame returns one row with a null value and a null (or handled) message."
    },
    {
      "Test Case ID": "TC5_InvalidDataType",
      "Test Case Description": "Create a DataFrame with an invalid data type (e.g., a string value) for 'value'. Since casting a string to string is allowed but concatenation may not produce a meaningful message, the test should check if an appropriate error is thrown or the result is handled properly.",
      "Expected Outcome": "Either the test asserts catching of an exception or validation that the result does not meet the expected numeric transformation."
    }
  ],
  "Pytest Script": "#!/usr/bin/env python\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import when, col\nfrom pyspark.sql.types import IntegerType, StringType, StructField, StructType\n\n# Fixture: setup and teardown SparkSession\n@pytest.fixture(scope=\"module\")\ndef spark():\n    spark_session = SparkSession.builder.appName(\"ZBW_LOAD_GL_DATA_Conversion_Test\").getOrCreate()\n    yield spark_session\n    spark_session.stop()\n\n\ndef transform_dataframe(df):\n    # Transformation logic similar to the given PySpark code:\n    return df.withColumn(\"message\", when(col(\"value\") < 50, \"Below threshold: \" + col(\"value\").cast(\"string\"))\n                         .otherwise(\"Above threshold: \" + col(\"value\").cast(\"string\")))\n\n\n# Test Case TC1: Happy Path - value less than 50\n\ndef test_below_threshold(spark):\n    # Setup input DataFrame with a value less than 50\n    data = [(30,)]\n    schema = StructType([StructField(\"value\", IntegerType(), True)])\n    df = spark.createDataFrame(data, schema)\n    \n    # Apply transformation\n    result_df = transform_dataframe(df)\n    result = result_df.collect()\n    \n    # Expected message\n    expected_message = \"Below threshold: 30\"\n    \n    # Assert that the result has the expected message\n    assert result[0][\"message\"] == expected_message, f\"Expected message {expected_message} but got {result[0]['message']}\"\n\n\n# Test Case TC2: Happy Path - value equal or above 50\n\ndef test_above_threshold(spark):\n    # Setup input DataFrame with a value of 100 (>= 50)\n    data = [(100,)]\n    schema = StructType([StructField(\"value\", IntegerType(), True)])\n    df = spark.createDataFrame(data, schema)\n    \n    # Apply transformation\n    result_df = transform_dataframe(df)\n    result = result_df.collect()\n    \n    # Expected message\n    expected_message = \"Above threshold: 100\"\n    \n    # Assert that the result has the expected message\n    assert result[0][\"message\"] == expected_message, f\"Expected message {expected_message} but got {result[0]['message']}\"\n\n\n# Test Case TC3: Edge Case - Empty DataFrame\n\ndef test_empty_dataframe(spark):\n    # Setup an empty DataFrame with column 'value'\n    schema = StructType([StructField(\"value\", IntegerType(), True)])\n    df = spark.createDataFrame([], schema)\n    \n    # Apply transformation\n    result_df = transform_dataframe(df)\n    \n    # Assert that the DataFrame is empty\n    assert result_df.rdd.isEmpty(), \"Expected the DataFrame to be empty\"\n\n\n# Test Case TC4: Edge Case - Null value \n\ndef test_null_value(spark):\n    # Setup DataFrame with a null value for 'value'\n    data = [(None,)]\n    schema = StructType([StructField(\"value\", IntegerType(), True)])\n    df = spark.createDataFrame(data, schema)\n    \n    # Apply transformation\n    result_df = transform_dataframe(df)\n    result = result_df.collect()\n    \n    # For null, the when condition should not match and cast should result in a null value.\n    assert result[0][\"message\"] is None, f\"Expected message to be None when value is null, but got {result[0]['message']}\"\n\n\n# Test Case TC5: Error Handling - Invalid data type (string instead of integer)\n\ndef test_invalid_data_type(spark):\n    # Setup DataFrame with an invalid data type (string)\n    data = [(\"invalid\",)]\n    schema = StructType([StructField(\"value\", StringType(), True)])\n    df = spark.createDataFrame(data, schema)\n    \n    # Since the transformation logic expects numerical comparison,\n    # attempting to perform 'value' < 50 may result in an exception.\n    with pytest.raises(Exception):\n        # This invocation is expected to raise an exception as invalid comparison is performed.\n        transform_dataframe(df).collect()\n\n\n# Cost consumed by the API for this call: 0.001 units\n"
}