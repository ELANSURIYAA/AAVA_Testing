------------------------------------------------------------
Test Case List:
------------------------------------------------------------
• TC_001: Happy Path Scenario  
  - Description: Provide a DataFrame containing rows where posting_date equals the provided p_date and with valid 'data_str'. Expect the function to append ' processed' to 'data_str' for these rows.  
  - Expected Outcome: A DataFrame with the matching row(s) showing the updated 'data_str' column with ' processed' appended.
  
• TC_002: Empty DataFrame Scenario  
  - Description: Provide a DataFrame that, after filtering with p_date, results in an empty DataFrame.  
  - Expected Outcome: The function prints "No data found for the given date" and returns None.
  
• TC_003: Null Value in 'data_str'  
  - Description: Provide a DataFrame with a row where posting_date matches p_date but 'data_str' is null.  
  - Expected Outcome: The transformation should handle the null and result in a 'data_str' that is treated as an empty string appended with " processed". The expected result for that row is " processed" (or possibly "null processed" depending on Spark version/casting).
  
• TC_004: Filtering Validation with Mixed Data  
  - Description: Provide a DataFrame with multiple rows where only some rows have posting_date equal to p_date.  
  - Expected Outcome: Only the rows matching p_date are processed (appended with ' processed') while other rows remain unprocessed.
  
• TC_005: Error Handling Scenario - Invalid Data Type  
  - Description: Provide a DataFrame where the 'posting_date' column contains an invalid type (e.g., integers) making filtering invalid.  
  - Expected Outcome: An exception is raised during the filtering process due to type mismatch.

------------------------------------------------------------
Pytest Script:
------------------------------------------------------------
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Pytest module to test the PySpark processing logic converted from ABAP.
The tests include various scenarios to ensure correct data transformations.
"""

import pytest
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat, lit
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

def process_gl_data(df, p_date):
    """
    Mimics the behavior of the PySpark code:
    - Filters the DataFrame where 'posting_date' equals p_date.
    - If filtered DataFrame is empty, prints a message and returns None.
    - Otherwise, appends ' processed' to the 'data_str' column.
    """
    try:
        df_filtered = df.filter(col("posting_date") == p_date)
    except Exception as e:
        raise e

    if df_filtered.rdd.isEmpty():
        print("No data found for the given date")
        return None
    else:
        # Handling if data_str is null, casting it to string.
        df_processed = df_filtered.withColumn("data_str",
                                              concat(
                                                  col("data_str").cast("string"),
                                                  lit(" processed")
                                              ))
        return df_processed

@pytest.fixture(scope="module")
def spark():
    """
    Pytest fixture to create a SparkSession for testing.
    """
    spark_session = SparkSession.builder.appName("Test_GL_Data_Conversion").master("local[*]").getOrCreate()
    yield spark_session
    spark_session.stop()

def test_happy_path(spark):
    """
    TC_001: Happy path scenario.
    Provide a DataFrame with a matching posting_date and valid data_str.
    Verify that 'data_str' is appended correctly.
    """
    schema = StructType([
        StructField("posting_date", StringType(), True),
        StructField("data_str", StringType(), True)
    ])
    data = [("2023-10-01", "sample"), ("2023-10-02", "other")]
    df = spark.createDataFrame(data, schema)
    
    result_df = process_gl_data(df, "2023-10-01")
    # Collect result, should only include the row with p_date = "2023-10-01"
    result = result_df.collect()
    
    assert len(result) == 1
    assert result[0]["data_str"] == "sample processed"

def test_empty_dataframe(spark, capfd):
    """
    TC_002: Empty DataFrame scenario.
    Provide a DataFrame that does not contain any row matching p_date.
    Expect to see a printed message and function returns None.
    """
    schema = StructType([
        StructField("posting_date", StringType(), True),
        StructField("data_str", StringType(), True)
    ])
    data = [("2023-09-30", "sample")]
    df = spark.createDataFrame(data, schema)
    
    result = process_gl_data(df, "2023-10-01")
    # Capture printed output
    captured = capfd.readouterr().out
    assert "No data found for the given date" in captured
    assert result is None

def test_null_data_str(spark):
    """
    TC_003: Null value in 'data_str'.
    Provide a DataFrame with a row where data_str is null.
    The output for data_str should be ' processed' or 'null processed' based on casting.
    """
    schema = StructType([
        StructField("posting_date", StringType(), True),
        StructField("data_str", StringType(), True)
    ])
    data = [("2023-10-01", None)]
    df = spark.createDataFrame(data, schema)
    
    result_df = process_gl_data(df, "2023-10-01")
    result = result_df.collect()
    
    assert len(result) == 1
    # When data_str is null, concatenation after cast typically yields "null processed".
    # Adjust the expected value if your Spark configuration yields an empty string.
    expected = "null processed"
    assert result[0]["data_str"] == expected

def test_filtering_validation(spark):
    """
    TC_004: Filtering validation.
    Provide a DataFrame with multiple rows.
    Only the rows with posting_date equal to '2023-10-01' are processed.
    """
    schema = StructType([
        StructField("posting_date", StringType(), True),
        StructField("data_str", StringType(), True)
    ])
    data = [
        ("2023-10-01", "first"),
        ("2023-10-02", "second"),
        ("2023-10-01", "third")
    ]
    df = spark.createDataFrame(data, schema)
    
    result_df = process_gl_data(df, "2023-10-01")
    result = result_df.collect()
    
    # Expecting 2 rows processed with ' processed' appended
    assert len(result) == 2
    processed_values = [row["data_str"] for row in result]
    assert "first processed" in processed_values
    assert "third processed" in processed_values

def test_invalid_data_type(spark):
    """
    TC_005: Error handling scenario.
    Provide a DataFrame with an invalid type for 'posting_date' (integer instead of string)
    to trigger a type error during filtering.
    """
    schema = StructType([
        StructField("posting_date", IntegerType(), True),
        StructField("data_str", StringType(), True)
    ])
    data = [(20231001, "sample")]
    df = spark.createDataFrame(data, schema)
    
    # Since we are comparing an integer with a string value, this should raise an exception.
    with pytest.raises(Exception):
        process_gl_data(df, "2023-10-01")

# Cost Consumed for this API call: 0.01 units
------------------------------------------------------------