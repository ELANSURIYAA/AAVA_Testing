--------------------------------------------------
Test Case List:
----------------
1. Test Case ID: TC1_CSV_Load
   Test Case Description: Verify that the PySpark code correctly loads the CSV file with header and infers the schema, ensuring that all expected columns (account_type, company_code, and amount) are present.
   Expected Outcome: The loaded DataFrame contains columns "account_type", "company_code", and "amount" with the correct inferred data types.
   
2. Test Case ID: TC2_Filtering_Logic
   Test Case Description: Validate that the DataFrame filtering operation correctly retains only the rows with account_type equal to "GL" as dictated by the ABAP to PySpark conversion.
   Expected Outcome: The filtered DataFrame contains only rows where account_type is "GL".
   
3. Test Case ID: TC3_Aggregation
   Test Case Description: Confirm that the aggregation transformation groups data by "company_code" and calculates the correct count of GL entries and the correct total amount, mimicking the original ABAP summing and looping logic.
   Expected Outcome: For each company_code, the aggregated DataFrame shows the exact count of GL entries and the correct sum of the amounts.
   
4. Test Case ID: TC4_Transformation_Consistency
   Test Case Description: Verify that the overall data transformation pipeline (loading, filtering, and aggregating) correctly emulates the ABAP logic by ensuring data consistency and logical correctness.
   Expected Outcome: The final output DataFrame matches the expected structure (columns: company_code, gl_entry_count, total_amount) and values, ensuring the transformation adheres to the ABAP program behavior.
   
5. Test Case ID: TC5_Manual_Intervention_Check
   Test Case Description: Identify potential edge cases that may require manual intervention (e.g., performance optimizations for large datasets, handling of legacy constructs, or NULL values) even though these aren't explicitly executed by the code.
   Expected Outcome: Test scenarios should point out cases where additional performance tuning or error handling may be necessary. (In the test, provide a mock scenario and verify that the code gracefully handles it.)

--------------------------------------------------
Pytest Script for Each Test Case:
--------------------------------------------------
#!/usr/bin/env python
"""
PyTest Script: test_gl_data_load.py

This script uses pytest to verify the functionalities of the converted PySpark code for the
ABAP GL Data Load conversion. Each test corresponds to the test cases defined above.
Script operations include:
- Validating CSV load and schema inference.
- Filtering logic to mimic ABAP IF conditions.
- Aggregation to ensure count and summing operations are correctly implemented.
- Overall transformation consistency.
- A mock test for potential manual intervention scenarios.
--------------------------------------------------
Cost consumed by the API for this call: 0.005 USD
"""

import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Fixture to initialize a SparkSession for testing
@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder.master("local[*]").appName("TestGLDataLoad").getOrCreate()
    yield spark
    spark.stop()

# Fixture to provide sample test data simulating GL data
@pytest.fixture
def sample_data(spark):
    data = [
         {"account_type": "GL", "company_code": "C1", "amount": 100},
         {"account_type": "XX", "company_code": "C1", "amount": 50},
         {"account_type": "GL", "company_code": "C2", "amount": 200},
         {"account_type": "GL", "company_code": "C2", "amount": None}  # Testing potential NULL scenario
    ]
    return spark.createDataFrame(data)

# Test Case TC1_CSV_Load: Validate CSV load and schema inference
def test_csv_load(spark, sample_data):
    df = sample_data
    # Check that required columns exist in the DataFrame
    required_columns = {"account_type", "company_code", "amount"}
    assert required_columns.issubset(set(df.columns)), "Missing one or more required columns in the CSV load."

# Test Case TC2_Filtering_Logic: Validate filtering operation for account_type = "GL"
def test_filtering_logic(sample_data):
    df_filtered = sample_data.filter(F.col("account_type") == "GL")
    results = df_filtered.collect()
    # In our sample data, there are 3 rows with account_type = "GL" (even if one has NULL amount)
    assert len(results) == 3, f"Filtered DataFrame should have 3 rows, got {len(results)}"
    for row in results:
        assert row["account_type"] == "GL", "Filtered row does not have account_type 'GL'."

# Test Case TC3_Aggregation: Test grouping and aggregation correctness
def test_aggregation_logic(sample_data):
    df_filtered = sample_data.filter(F.col("account_type") == "GL")
    df_agg = df_filtered.groupBy("company_code").agg(
        F.count("*").alias("gl_entry_count"),
        F.sum("amount").alias("total_amount")
    )
    # Collect aggregated results into a dict for easy comparison
    results = {row["company_code"]: (row["gl_entry_count"], row["total_amount"]) for row in df_agg.collect()}
    # Expected aggregation:
    # For company C1: one row with amount 100
    # For company C2: two rows, one with 200 and one with NULL (sum should be 200)
    expected = {"C1": (1, 100), "C2": (2, 200)}
    assert results == expected, f"Aggregation results mismatch. Expected {expected}, got {results}"

# Test Case TC4_Transformation_Consistency: Check overall transformation pipeline consistency
def test_transformation_consistency(sample_data):
    df_filtered = sample_data.filter(F.col("account_type") == "GL")
    df_result = df_filtered.groupBy("company_code").agg(
        F.count("*").alias("gl_entry_count"),
        F.sum("amount").alias("total_amount")
    )
    expected_columns = {"company_code", "gl_entry_count", "total_amount"}
    assert set(df_result.columns) == expected_columns, \
           "The final DataFrame structure does not match the expected structure."

# Test Case TC5_Manual_Intervention_Check:
# This test simulates an edge case where incoming data might have irregularities (e.g., performance stress or NULL values)
def test_manual_intervention_scenario(sample_data):
    # Introduce an edge-case scenario with an unexpected data pattern
    # For example, a row with an unknown account_type that should be filtered out
    df = sample_data.union(sample_data.filter(F.col("account_type") != "GL"))
    df_filtered = df.filter(F.col("account_type") == "GL")
    # Even if extra rows are introduced, the filter should only pick valid GL rows
    results = df_filtered.collect()
    for row in results:
        assert row["account_type"] == "GL", "Manual intervention check failed: Found non 'GL' account_type."

# End of test script
--------------------------------------------------
Cost consumed by the API for this call: 0.005 USD
--------------------------------------------------