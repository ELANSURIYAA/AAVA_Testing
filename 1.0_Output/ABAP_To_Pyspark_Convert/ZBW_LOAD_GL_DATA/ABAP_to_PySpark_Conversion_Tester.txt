{
  "Test Case List": [
    {
      "Test Case ID": "TC001",
      "Test Case Description": "Test with a mix of valid lines and lines containing '*ERROR*'.",
      "Expected Outcome": "Only valid lines are counted; rows with '*ERROR*' are excluded."
    },
    {
      "Test Case ID": "TC002",
      "Test Case Description": "Test with all valid lines.",
      "Expected Outcome": "All lines are counted."
    },
    {
      "Test Case ID": "TC003",
      "Test Case Description": "Test with all lines containing '*ERROR*'.",
      "Expected Outcome": "The count is zero since all lines are excluded."
    },
    {
      "Test Case ID": "TC004",
      "Test Case Description": "Test with an empty input.",
      "Expected Outcome": "The count is zero as there are no lines to process."
    },
    {
      "Test Case ID": "TC005",
      "Test Case Description": "Test with a DataFrame missing the 'value' column.",
      "Expected Outcome": "An AnalysisException is raised indicating the missing column."
    }
  ],
  "Pytest Script": "#!/usr/bin/env python\n\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.utils import AnalysisException\n\n# Fixture to create and tear down the SparkSession for each test module\n@pytest.fixture(scope='module')\ndef spark():\n    spark_session = SparkSession.builder.master('local[*]').appName('PySparkTest').getOrCreate()\n    yield spark_session\n    spark_session.stop()\n\n# Helper function to replicate the PySpark job logic from the given script\n\ndef filter_and_count_valid_entries(df):\n    # Ensure the DataFrame has the expected column 'value'\n    if 'value' not in df.columns:\n        raise AnalysisException(\"The required column 'value' is missing.\")\n    # Filter out rows containing '*ERROR*' and count the remaining rows\n    valid_df = df.filter(~df['value'].contains('*ERROR*'))\n    return valid_df.count()\n\n# Test cases:\n\ndef test_mixed_lines(spark):\n    \"\"\"\n    TC001: Test with a mix of valid lines and lines containing '*ERROR*'.\n    Expected Outcome: Only valid lines are counted.\n    \"\"\"\n    data = [\n        (\"valid line 1\",),\n        (\"*ERROR* line\",),\n        (\"valid line 2\",)\n    ]\n    df = spark.createDataFrame(data, [\"value\"])\n    result = filter_and_count_valid_entries(df)\n    assert result == 2  # Expect 2 valid lines\n\n\ndef test_all_valid_lines(spark):\n    \"\"\"\n    TC002: Test with all valid lines.\n    Expected Outcome: All lines are counted.\n    \"\"\"\n    data = [\n        (\"valid line 1\",),\n        (\"valid line 2\",),\n        (\"valid line 3\",)\n    ]\n    df = spark.createDataFrame(data, [\"value\"])\n    result = filter_and_count_valid_entries(df)\n    assert result == 3  # All 3 lines are valid\n\n\ndef test_all_error_lines(spark):\n    \"\"\"\n    TC003: Test with all lines containing '*ERROR*'.\n    Expected Outcome: Count is zero since all lines are excluded.\n    \"\"\"\n    data = [\n        (\"*ERROR* line 1\",),\n        (\"*ERROR* line 2\",),\n        (\"*ERROR* line 3\",)\n    ]\n    df = spark.createDataFrame(data, [\"value\"])\n    result = filter_and_count_valid_entries(df)\n    assert result == 0  # No valid lines\n\n\ndef test_empty_input(spark):\n    \"\"\"\n    TC004: Test with an empty input.\n    Expected Outcome: Count is zero as there are no lines to process.\n    \"\"\"\n    data = []\n    df = spark.createDataFrame(data, [\"value\"])\n    result = filter_and_count_valid_entries(df)\n    assert result == 0  # No lines present\n\n\ndef test_missing_value_column(spark):\n    \"\"\"\n    TC005: Test with a DataFrame missing the 'value' column.\n    Expected Outcome: An AnalysisException is raised due to the missing column.\n    \"\"\"\n    data = [\n        (\"valid line 1\",),\n        (\"*ERROR* line\",)\n    ]\n    # Creating DataFrame with a different column name\n    df = spark.createDataFrame(data, [\"different_column\"])\n    with pytest.raises(AnalysisException):\n        filter_and_count_valid_entries(df)\n\n# Cost consumed by the API for this call: 0.002 USD\n"
}