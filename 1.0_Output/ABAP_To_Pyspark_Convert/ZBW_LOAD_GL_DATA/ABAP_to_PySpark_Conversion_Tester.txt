{
  "Test Case List": "------------------------------------------------------------\nTC001 - Happy Path Test:\n   - Description: Input a valid CSV row in a text DataFrame with exactly 7 comma-separated values. Verify that the output DataFrame returns one row with correctly mapped columns (\"bukrs\", \"fiscyear\", \"costcenter\", \"gl_account\", \"amount\" cast to float, \"currency\", \"posting_date\").\n   - Expected Outcome: Final DataFrame contains one row with each field correctly extracted and \"amount\" as a float.\n\nTC002 - Insufficient Fields Test:\n   - Description: Input a row with fewer than 7 fields. Verify that such rows are filtered out.\n   - Expected Outcome: Final DataFrame is empty.\n\nTC003 - Excess Fields Test:\n   - Description: Input a row with more than 7 fields. Verify that such rows are filtered out.\n   - Expected Outcome: Final DataFrame is empty.\n\nTC004 - Amount Casting Test:\n   - Description: Validate that the \"amount\" field is properly cast from string to float. Provide a valid numeric string and check its type in the final output.\n   - Expected Outcome: \"amount\" column value is of float type and correctly converted.\n\nTC005 - Empty File Test:\n   - Description: Provide an empty DataFrame (simulating an empty file) and verify that the process does not fail and returns an empty DataFrame.\n   - Expected Outcome: Final DataFrame is empty.\n------------------------------------------------------------",
  "Pytest Script": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nPytest script for testing the PySpark conversion code which mimics the functionality of the original ABAP code.\nThis script includes:\n - Setup/teardown of SparkSession.\n - Unit tests for valid input, insufficient columns, excess columns, amount casting, and empty input.\n - Comments explaining each test case.\n\nCost Calculation: API cost consumed for this analysis is 1 unit.\n\"\"\"\n\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, split, size\nfrom pyspark.sql.types import FloatType\n\n# Fixture for SparkSession\n@pytest.fixture(scope=\"module\")\ndef spark():\n    spark_session = SparkSession.builder \\\n        .appName(\"PySpark Unit Tests\") \\\n        .master(\"local[2]\") \\\n        .getOrCreate()\n    yield spark_session\n    spark_session.stop()\n\n\ndef transform_data(raw_df):\n    \"\"\"\n    Function to simulate the transformation logic from the provided PySpark code.\n    This includes splitting the text, filtering by field count, mapping columns and casting 'amount'.\n    \"\"\"\n    split_df = raw_df.withColumn(\"fields\", split(col(\"value\"), \",\"))\n    # Filter rows with exactly 7 fields\n    filtered_df = split_df.filter(size(col(\"fields\")) == 7)\n    # Map fields to columns\n    final_df = filtered_df.select(\n        col(\"fields\").getItem(0).alias(\"bukrs\"),\n        col(\"fields\").getItem(1).alias(\"fiscyear\"),\n        col(\"fields\").getItem(2).alias(\"costcenter\"),\n        col(\"fields\").getItem(3).alias(\"gl_account\"),\n        col(\"fields\").getItem(4).alias(\"amount\"),\n        col(\"fields\").getItem(5).alias(\"currency\"),\n        col(\"fields\").getItem(6).alias(\"posting_date\")\n    )\n    final_df = final_df.withColumn(\"amount\", col(\"amount\").cast(FloatType()))\n    return final_df\n\n\ndef test_happy_path(spark):\n    \"\"\"\n    TC001 - Happy Path\n    Create a DataFrame with one valid CSV row (exactly 7 fields) and verify proper mapping and type casting.\n    \"\"\"\n    data = [(\"1000,2021,CC01,4000,123.45,USD,2021-12-31\", )]\n    raw_df = spark.createDataFrame(data, [\"value\"])\n    result_df = transform_data(raw_df)\n    \n    result = result_df.collect()\n    assert len(result) == 1, \"Expected one row in the final DataFrame\"\n    row = result[0]\n    # Verify each field mapping\n    assert row[\"bukrs\"] == \"1000\"\n    assert row[\"fiscyear\"] == \"2021\"\n    assert row[\"costcenter\"] == \"CC01\"\n    assert row[\"gl_account\"] == \"4000\"\n    # Verify that amount is cast to float\n    assert isinstance(row[\"amount\"], float)\n    assert row[\"amount\"] == 123.45\n    assert row[\"currency\"] == \"USD\"\n    assert row[\"posting_date\"] == \"2021-12-31\"\n\n\ndef test_insufficient_fields(spark):\n    \"\"\"\n    TC002 - Insufficient Fields\n    Test with a row that has fewer than 7 comma-separated fields.\n    The expected behavior is to filter out such rows so that the result is an empty DataFrame.\n    \"\"\"\n    data = [(\"1000,2021,CC01,4000,123.45,USD\", )]  # Only 6 fields\n    raw_df = spark.createDataFrame(data, [\"value\"])\n    result_df = transform_data(raw_df)\n    \n    result = result_df.collect()\n    assert len(result) == 0, \"Expected no rows in the final DataFrame for insufficient fields\"\n\n\ndef test_excess_fields(spark):\n    \"\"\"\n    TC003 - Excess Fields\n    Test with a row that has more than 7 comma-separated fields.\n    The expected behavior is to filter out such rows.\n    \"\"\"\n    data = [(\"1000,2021,CC01,4000,123.45,USD,2021-12-31,ExtraField\", )]  # 8 fields\n    raw_df = spark.createDataFrame(data, [\"value\"])\n    result_df = transform_data(raw_df)\n    \n    result = result_df.collect()\n    assert len(result) == 0, \"Expected no rows in the final DataFrame for excess fields\"\n\n\ndef test_amount_casting(spark):\n    \"\"\"\n    TC004 - Amount Casting Test\n    This test ensures that the 'amount' field is correctly cast to a float.\n    \"\"\"\n    data = [(\"1000,2021,CC01,4000,456.78,USD,2021-12-31\", )]\n    raw_df = spark.createDataFrame(data, [\"value\"])\n    result_df = transform_data(raw_df)\n    \n    result = result_df.collect()\n    assert len(result) == 1, \"Expected one row after transformation\"\n    row = result[0]\n    assert isinstance(row[\"amount\"], float), \"The amount field should be of float type\"\n    assert row[\"amount\"] == 456.78, \"The converted float value of amount does not match expected value\"\n\n\ndef test_empty_file(spark):\n    \"\"\"\n    TC005 - Empty File Test\n    Test with an empty DataFrame (simulating an empty file) and ensure that the transformation handles it gracefully.\n    \"\"\"\n    # Create an empty DataFrame with the same schema as the input text file\n    empty_rdd = spark.sparkContext.emptyRDD()\n    raw_df = spark.createDataFrame(empty_rdd, [\"value\"])\n    result_df = transform_data(raw_df)\n    \n    result = result_df.collect()\n    assert len(result) == 0, \"Expected an empty DataFrame for an empty input\"\n\n# API cost consumed for this call: 1 unit\n"
}