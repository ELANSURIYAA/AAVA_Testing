Test Case List:
1. Test Case ID: TC1_HappyPath  
   - Test Case Description: Provide a valid, non-empty DataFrame containing GL data with correct column names and data types.  
   - Expected Outcome: The transformation function processes the data correctly with expected transformations (e.g., proper data type conversion, derived columns generated) and returns a DataFrame with the same or appropriately altered number of rows.

2. Test Case ID: TC2_EmptyDataFrame  
   - Test Case Description: Provide an empty DataFrame (no records) as input.  
   - Expected Outcome: The transformation function should return an empty DataFrame without errors.

3. Test Case ID: TC3_NullValues  
   - Test Case Description: Provide a DataFrame that contains null values in critical columns such as description or amount.  
   - Expected Outcome: The transformation should handle the null values gracefully (e.g., assign default values or skip processing) and the output DataFrame should either contain the fixed values or note the missing data as per business logic.

4. Test Case ID: TC4_InvalidDataTypes  
   - Test Case Description: Provide a DataFrame with columns having invalid data types or unexpected string values in place of numbers (e.g., non-numeric for amounts).  
   - Expected Outcome: The transformation function should either convert the values correctly if possible or raise an informative exception regarding data type incompatibilities (in our case, the casting should result in a null value for that conversion).

5. Test Case ID: TC5_UDFApplication  
   - Test Case Description: Verify that any user-defined functions (UDFs) or calculated columns used in the transformation are applied correctly.  
   - Expected Outcome: The UDF should generate the expected output (e.g., a new derived column, such as "amount_taxed") with correct calculations based on the input rows.

6. Test Case ID: TC6_WindowFunctionOrAggregation  
   - Test Case Description: Test a scenario that involves grouping or windowing over GL data, simulating aggregation of amounts by groups.  
   - Expected Outcome: After performing group by or window operations, aggregated results (e.g., sum of amounts by group) should match the expected outcome.

Pytest Script for Each Test Case:
------------------------------------------------------------
#!/usr/bin/env python
"""
Pytest Script for testing PySpark GL Data transformation.
This script includes several test cases to validate the transformation logic.
Assumption: The module under test (e.g., gl_transformer.py) includes a function called process_gl_data(spark, input_df)
which performs the main data transformation that was converted from ABAP.
"""

import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# For demonstration purposes, we define a dummy transformation function.
# In a real scenario, this function would reside in the converted module (e.g., gl_transformer.py).
def process_gl_data(spark, input_df):
    """
    Dummy transformation: 
    1. Convert a string numeric column "amount" to double.
    2. Fill null in "description" with default value 'N/A'.
    3. Compute a new column "amount_taxed" = amount * 1.2.
    """
    from pyspark.sql.types import DoubleType

    # Convert the "amount" column to double.
    df = input_df.withColumn("amount", F.col("amount").cast(DoubleType()))
    
    # Fill nulls in "description" column.
    df = df.fillna({"description": "N/A"})
    
    # Compute a new calculated column.
    df = df.withColumn("amount_taxed", F.col("amount") * 1.2)
    return df

# Fixture to create a SparkSession
@pytest.fixture(scope="session")
def spark():
    spark_session = SparkSession.builder\
        .master("local[*]")\
        .appName("gl_transformation_tests")\
        .getOrCreate()
    yield spark_session
    spark_session.stop()

# Helper function to compare DataFrame content (order insensitive)
def assert_df_equality(df1, df2):
    data1 = sorted([tuple(row) for row in df1.collect()])
    data2 = sorted([tuple(row) for row in df2.collect()])
    assert data1 == data2

# Test case TC1_HappyPath: Valid data transformation
def test_happy_path(spark):
    """
    TC1_HappyPath:
    Valid DataFrame with GL columns: id, amount, description.
    Expected: 'amount' is cast to double, null descriptions handled, and new column "amount_taxed" computed.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    data = [
        ("1", "100", "Payment"),
        ("2", "200", "Invoice")
    ]
    input_df = spark.createDataFrame(data, schema)

    result_df = process_gl_data(spark, input_df)

    # Expected data: amounts converted to double and new computed column "amount_taxed"
    expected_data = [
        ("1", 100.0, "Payment", 120.0),
        ("2", 200.0, "Invoice", 240.0)
    ]
    expected_schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("description", StringType(), True),
        StructField("amount_taxed", DoubleType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, expected_schema)

    assert_df_equality(result_df, expected_df)

# Test case TC2_EmptyDataFrame: Process an empty DataFrame
def test_empty_dataframe(spark):
    """
    TC2_EmptyDataFrame:
    Input: Empty DataFrame with a proper schema.
    Expected: Transformation returns an empty DataFrame.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    input_df = spark.createDataFrame([], schema)

    result_df = process_gl_data(spark, input_df)

    assert result_df.count() == 0

# Test case TC3_NullValues: Handling nulls in critical columns
def test_null_values(spark):
    """
    TC3_NullValues:
    Input: DataFrame with null in 'description' and possibly in 'amount'.
    Expected: 'description' null becomes "N/A" and for a null amount,
              casting results in null and therefore 'amount_taxed' is also null.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    data = [
        ("1", "150", None),   # Null description, valid amount
        ("2", None, "Invoice") # Null amount, valid description
    ]
    input_df = spark.createDataFrame(data, schema)

    result_df = process_gl_data(spark, input_df)
    results = result_df.collect()
    for row in results:
        if row.id == "1":
            assert row.description == "N/A"
            assert row.amount == 150.0
            assert row.amount_taxed == 180.0
        if row.id == "2":
            # For a null amount, cast yields null and hence amount_taxed remains null.
            assert row.amount is None
            assert row.amount_taxed is None

# Test case TC4_InvalidDataTypes: Input with non-numeric string for 'amount'
def test_invalid_datatype(spark):
    """
    TC4_InvalidDataTypes:
    Input: DataFrame with a non-numeric string in the 'amount' column.
    Expected: Casting to double results in a null value for 'amount' and accordingly 'amount_taxed' also becomes null.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    data = [
        ("1", "not_a_number", "ErrorCase")
    ]
    input_df = spark.createDataFrame(data, schema)

    result_df = process_gl_data(spark, input_df)
    row = result_df.collect()[0]
    # As the non-numeric string cannot be cast to double, we expect None.
    assert row.amount is None
    assert row.amount_taxed is None

# Test case TC5_UDFApplication: Verify correctness of UDF or calculated column
def test_udf_application(spark):
    """
    TC5_UDFApplication:
    Input: DataFrame with a valid numeric 'amount' and description.
    Expected: New column "amount_taxed" is computed correctly as amount * 1.2.
    """
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("description", StringType(), True)
    ])
    data = [
        ("1", "50", "Test")
    ]
    input_df = spark.createDataFrame(data, schema)

    result_df = process_gl_data(spark, input_df)
    row = result_df.collect()[0]
    # Expected: 50 * 1.2 equals 60.0.
    assert row.amount == 50.0
    assert row.amount_taxed == 60.0

# Test case TC6_WindowFunctionOrAggregation: Sample grouping/aggregation test for edge-case validation
def test_window_function_aggregation(spark):
    """
    TC6_WindowFunctionOrAggregation:
    Input: DataFrame with multiple records simulating aggregation after group by.
    Expected: Aggregated results (i.e., sum of amounts by group) match the expected outcome.
    """
    schema = StructType([
        StructField("group", StringType(), True),
        StructField("amount", StringType(), True)
    ])
    data = [
        ("A", "100"),
        ("A", "200"),
        ("B", "300")
    ]
    input_df = spark.createDataFrame(data, schema)

    # Simulate transformation: cast amount to double and aggregate by 'group'
    df_transformed = input_df.withColumn("amount", F.col("amount").cast(DoubleType()))
    agg_df = df_transformed.groupBy("group").agg(F.sum("amount").alias("total_amount"))

    # Expected aggregation: for group A sum is 300.0 and for group B sum is 300.0
    expected_data = [
        ("A", 300.0),
        ("B", 300.0)
    ]
    expected_schema = StructType([
        StructField("group", StringType(), True),
        StructField("total_amount", DoubleType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, expected_schema)

    assert_df_equality(agg_df, expected_df)

# API Cost Consumed for this call: $0.00

if __name__ == "__main__":
    import sys
    sys.exit(pytest.main(["-v"]))
------------------------------------------------------------

API Cost Consumed: $0.00