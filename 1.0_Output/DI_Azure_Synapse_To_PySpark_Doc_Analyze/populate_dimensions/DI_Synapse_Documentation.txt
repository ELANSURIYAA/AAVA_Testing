# Documentation: Synapse Stored Procedure `populate_dimensions`

---

## Metadata

| Property         | Value                        |
|------------------|-----------------------------|
| **Author**         | Ascendion AAVA              |
| **Date**           |                             |
| **Description**    | Loads and maintains dimension tables from staging data in Synapse DW |

---

## 1. Overview of Pipeline/Component

The `populate_dimensions` stored procedure in Azure Synapse is designed to load and maintain three core dimension tables—`DIM_INSTITUTION`, `DIM_CORPORATION`, and `DIM_PRODUCT`—from the staging table `STG_DIMENSION_DATA`. It ensures idempotent loads using SQL `MERGE` statements and applies business rules and data quality filters to exclude invalid, legacy, or test data. This process supports regulatory reporting, analytics, and downstream data enrichment.

---

## 2. Component Structure and Design

- **Stored Procedure:** `dbo.populate_dimensions`
- **Activities:**
  - **Source:** `dbo.STG_DIMENSION_DATA` (staging table)
  - **Transformation:** SQL `MERGE` statements with `SELECT DISTINCT` and business rule filters
  - **Sink:** `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT` (dimension tables)
- **Logical Grouping:**
  - Step 1: Populate `DIM_INSTITUTION`
  - Step 2: Populate `DIM_CORPORATION`
  - Step 3: Populate `DIM_PRODUCT`
- **Parameters/Variables:** None explicitly used in this procedure
- **Triggers:** Procedure is executed manually or via orchestrated pipeline
- **Connection Flow:** Each step reads from the staging table, applies filters, and merges into the target dimension table

---

## 3. Data Flow and Processing Logic

**Processed Datasets:**
- `STG_DIMENSION_DATA` (source)
- `DIM_INSTITUTION` (target)
- `DIM_CORPORATION` (target)
- `DIM_PRODUCT` (target)

**End-to-End Data Journey:**
1. Data is ingested into `STG_DIMENSION_DATA` from upstream sources.
2. The procedure reads distinct records from the staging table, applies filters, and merges them into the respective dimension tables.
3. Only new records are inserted; existing records are not updated or deleted.
4. Data quality and business rules are enforced at each step.

**Logical Steps:**
- **Filtering:** Exclude invalid IDs, legacy, and test data.
- **Deduplication:** Use `SELECT DISTINCT` to avoid duplicates.
- **Mapping:** Map source columns to target columns.
- **Business Rules:** Apply conditions such as non-null IDs, exclusion of test/legacy data.

---

## 4. Data Mapping (Lineage)

| Target Table Name | Target Column Name        | Source Table Name   | Source Column Name        | Remarks                                      |
|-------------------|--------------------------|---------------------|--------------------------|----------------------------------------------|
| DIM_INSTITUTION   | institution_id           | STG_DIMENSION_DATA  | institution_id           | 1:1 Mapping, filter: IS NOT NULL, LEN > 3    |
| DIM_INSTITUTION   | institution              | STG_DIMENSION_DATA  | institution              | 1:1 Mapping                                  |
| DIM_INSTITUTION   | institution_name         | STG_DIMENSION_DATA  | institution_name         | 1:1 Mapping                                  |
| DIM_CORPORATION   | corporation_id           | STG_DIMENSION_DATA  | corporation_id           | 1:1 Mapping, filter: NOT LIKE 'TEST%'        |
| DIM_CORPORATION   | corp_id                  | STG_DIMENSION_DATA  | corp_id                  | 1:1 Mapping                                  |
| DIM_CORPORATION   | corporation              | STG_DIMENSION_DATA  | corporation              | 1:1 Mapping                                  |
| DIM_CORPORATION   | corporation_name         | STG_DIMENSION_DATA  | corporation_name         | 1:1 Mapping, filter: NOT LIKE 'TEST%'        |
| DIM_CORPORATION   | sub_corporation          | STG_DIMENSION_DATA  | sub_corporation          | 1:1 Mapping                                  |
| DIM_CORPORATION   | sub_corporation_id       | STG_DIMENSION_DATA  | sub_corporation_id       | 1:1 Mapping                                  |
| DIM_CORPORATION   | sub_corporation_name     | STG_DIMENSION_DATA  | sub_corporation_name     | 1:1 Mapping                                  |
| DIM_CORPORATION   | master_corporation_dim_key | STG_DIMENSION_DATA | master_corporation_dim_key | 1:1 Mapping                               |
| DIM_CORPORATION   | association              | STG_DIMENSION_DATA  | association              | 1:1 Mapping                                  |
| DIM_CORPORATION   | bin                      | STG_DIMENSION_DATA  | bin                      | 1:1 Mapping                                  |
| DIM_CORPORATION   | company                  | STG_DIMENSION_DATA  | company                  | 1:1 Mapping                                  |
| DIM_CORPORATION   | company_id               | STG_DIMENSION_DATA  | company_id               | 1:1 Mapping                                  |
| DIM_CORPORATION   | company_name             | STG_DIMENSION_DATA  | company_name             | 1:1 Mapping                                  |
| DIM_PRODUCT       | product_id               | STG_DIMENSION_DATA  | product_id               | 1:1 Mapping, filter: NOT IN ('DEPRECATED', 'LEGACY') |
| DIM_PRODUCT       | product                  | STG_DIMENSION_DATA  | product                  | 1:1 Mapping                                  |
| DIM_PRODUCT       | product_description      | STG_DIMENSION_DATA  | product_description      | 1:1 Mapping                                  |
| DIM_PRODUCT       | sub_product              | STG_DIMENSION_DATA  | sub_product              | 1:1 Mapping                                  |
| DIM_PRODUCT       | processing_code          | STG_DIMENSION_DATA  | processing_code          | 1:1 Mapping                                  |
| DIM_PRODUCT       | processing_code_description | STG_DIMENSION_DATA | processing_code_description | 1:1 Mapping                              |
| DIM_PRODUCT       | processing_group         | STG_DIMENSION_DATA  | processing_group         | 1:1 Mapping, filter: NOT IN ('DEPRECATED', 'LEGACY') |

---

## 5. Transformation Logic

- **Deduplication:** `SELECT DISTINCT` ensures only unique records are considered for insertion.
- **Filtering:**
  - `DIM_INSTITUTION`: Only records with non-null `institution_id` and length > 3 are included.
  - `DIM_CORPORATION`: Excludes records where `corporation_name` starts with 'TEST'.
  - `DIM_PRODUCT`: Excludes records where `processing_group` is 'DEPRECATED' or 'LEGACY'.
- **MERGE Statement:** Ensures idempotency by inserting only records not already present in the target dimension table.
- **No UDFs or reusable templates** are used in this procedure.

---

## 6. Complexity Analysis

| Metric                              | Value/Details                                         |
|--------------------------------------|-------------------------------------------------------|
| Number of Pipeline Activities        | 1 (Stored Procedure with 3 main steps)                |
| Number of Dataflow Transformations   | 3 (one per dimension table)                           |
| SQL Scripts or Stored Procedures Used| 1 (this procedure)                                    |
| Joins Used                          | None (MERGE with ON condition, no explicit JOINs)     |
| Lookup Tables or Reference Data      | None                                                  |
| Parameters/Variables/Triggers        | 0                                                     |
| Number of Output Datasets            | 3 (DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT)     |
| Conditional Logic or if-else Flows   | 3 (filters in WHERE clauses for each dimension)        |
| External Dependencies                | STG_DIMENSION_DATA (staging table)                    |
| Overall Complexity Score             | 30                                                    |

---

## 7. Key Outputs

- **Tables Written:** `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`
- **Format:** Synapse SQL tables
- **Intended Use:** Data warehousing, regulatory reporting, analytics, and downstream enrichment

---

## API Cost

apiCost: 0.002 USD

---

**End of Documentation**