=============================================
Author:        Ascendion AVA
Created on:   
Description:   Populates and maintains dimension tables in Synapse DW from the staging layer using idempotent MERGE logic.
=============================================

## 1. Procedure Overview

This Azure Synapse SQL stored procedure (`dbo.populate_dimensions`) orchestrates the loading and maintenance of three core dimension tables—`DIM_INSTITUTION`, `DIM_CORPORATION`, and `DIM_PRODUCT`—from the staging table `STG_DIMENSION_DATA`. It applies business rules and data quality filters, and uses idempotent MERGE logic to ensure only new records are inserted. The procedure supports regulatory reporting, data warehousing, and analytics by keeping dimension tables current and accurate.  
**Number of mappings per workflow/session:** 3 (one per dimension table, all within a single procedure)

---

## 2. Complexity Metrics

| Metric                          | Value/Description                                                                                   |
|----------------------------------|-----------------------------------------------------------------------------------------------------|
| Number of Source Qualifiers      | 1 (STG_DIMENSION_DATA; SQL Server table)                                                            |
| Number of Transformations        | 3 (MERGE statements with WHERE clause filters)                                                      |
| Lookup Usage                     | 0 (No explicit lookup transformations)                                                              |
| Expression Logic                 | 3 (Simple WHERE clause filters: IS NOT NULL, LEN, UPPER, NOT LIKE, NOT IN)                         |
| Join Conditions                  | 3 (MERGE ON clause; equi-join on primary key for each dimension)                                   |
| Conditional Logic                | 3 (WHERE clause filters for each dimension)                                                         |
| Reusable Components              | 0 (No reusable transformations, mapplets, or sessions)                                              |
| Data Sources                     | 1 (SQL Server: STG_DIMENSION_DATA)                                                                  |
| Data Targets                     | 3 (SQL Server: DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT)                                      |
| Pre/Post SQL Logic               | 0 (No pre/post SQLs or external procedures)                                                         |
| Session/Workflow Controls        | 0 (No explicit workflow controls; logging via PRINT statements only)                                |
| DML Logic                        | 3 (MERGE statements for INSERT only; no UPDATE/DELETE/MERGE logic)                                 |
| Complexity Score (0–100)         | 20 (Low complexity; all logic is direct mapping and simple filters, matches DI_Azure_Synapse_Documentation) |

**High-complexity areas:**  
- None. All expressions are simple and direct. No nested logic, lookups, or branching.

---

## 3. Syntax Differences

- **MERGE Statement:**  
  - Azure Synapse SQL supports the MERGE statement for idempotent inserts. In PySpark, this must be implemented using DataFrame joins and conditional inserts (e.g., using `exceptAll` or anti-join logic).
- **Functions:**  
  - `LEN()` → PySpark equivalent is `length()` function.
  - `UPPER()` → PySpark equivalent is `upper()` function.
  - `NOT LIKE`, `NOT IN` → PySpark supports these via DataFrame filter expressions.
- **PRINT Statements:**  
  - PRINT for logging in SQL must be replaced with Python `print()` or Databricks logging utilities.
- **Data Types:**  
  - SQL Server types (e.g., VARCHAR, INT) may need to be mapped to Spark types (StringType, IntegerType).
- **Workflow/Control Logic:**  
  - No explicit workflow or transaction control in the procedure; all logic is linear.

---

## 4. Manual Adjustments

- **MERGE Logic:**  
  - Must be manually implemented in PySpark using join and anti-join logic to mimic "WHEN NOT MATCHED THEN INSERT".
- **External Dependencies:**  
  - None; all logic is contained within the procedure.
- **Business Logic Validation:**  
  - Filters (e.g., `LEN(institution_id) > 3`, `UPPER(corporation_name) NOT LIKE 'TEST%'`, `processing_group NOT IN ('DEPRECATED', 'LEGACY')`) should be validated post-conversion to ensure correctness.
- **Logging:**  
  - Replace SQL PRINT statements with appropriate Python/Databricks logging.
- **Idempotency:**  
  - Ensure PySpark logic does not introduce duplicates; validate anti-join logic carefully.

---

## 5. Optimization Techniques

- **Partitioning:**  
  - Partition input DataFrames by key columns (e.g., institution_id, corporation_id, product_id) for efficient joins and inserts.
- **Caching:**  
  - Cache intermediate DataFrames if reused across multiple dimension loads.
- **Broadcast Joins:**  
  - Use broadcast joins if dimension tables are small to optimize anti-join logic.
- **Pipeline Conversion:**  
  - Chain filters and joins into a single pipeline for each dimension load to minimize shuffling.
- **Window Functions:**  
  - Not required for this logic, but consider for future aggregations.
- **Refactor vs. Rebuild:**  
  - **Refactor**: Retain the original logic, as the procedure is simple and direct.
  - **Rebuild**: Only consider if future requirements introduce complexity (e.g., updates, deletes, or complex transformations).

---

## API Cost

apiCost: 0.007 USD