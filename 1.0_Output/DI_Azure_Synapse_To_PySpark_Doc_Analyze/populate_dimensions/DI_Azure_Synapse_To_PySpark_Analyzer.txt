=============================================
Author:        Ascendion AVA
Created on:   
Description:   Loads and maintains dimension tables (DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT) in Synapse DW from a staging layer using idempotent logic and business rules.
=============================================

---

## 1. Procedure Overview

The `populate_dimensions` stored procedure in Azure Synapse Analytics orchestrates the ETL workflow for three core dimension tables: `DIM_INSTITUTION`, `DIM_CORPORATION`, and `DIM_PRODUCT`. It extracts data from a single staging table (`STG_DIMENSION_DATA`), applies business rules and data quality filters, and loads only new, valid records into the dimension tables using SQL `MERGE` statements. This ensures idempotency and prevents duplicate inserts. The key business objective is to maintain accurate, clean, and up-to-date dimension data to support downstream analytics, regulatory reporting, and data enrichment.

- **Number of mappings per workflow/session:** 3 (one for each dimension table)
- **Key business objective:** Data integration, cleansing, and enrichment for analytics and reporting.

---

## 2. Complexity Metrics

| Metric                           | Value/Description                                                                 |
|-----------------------------------|----------------------------------------------------------------------------------|
| Number of Source Qualifiers       | 1 (STG_DIMENSION_DATA; SQL Server/Synapse SQL Table)                             |
| Number of Transformations         | 3 (MERGE with embedded filter logic per dimension)                               |
| Lookup Usage                      | 0 (No explicit lookups; all logic is inline)                                     |
| Expression Logic                  | 3 (Simple expressions: IS NOT NULL, LEN, NOT LIKE, NOT IN)                       |
| Join Conditions                   | 3 (MERGE ON clause per dimension; acts as a join for existence checks)           |
| Conditional Logic                 | 3 (WHERE clauses in source subqueries for each MERGE)                            |
| Reusable Components               | 0 (No reusable mapplets, transformations, or sessions)                           |
| Data Sources                      | 1 (STG_DIMENSION_DATA; SQL Server/Synapse SQL Table)                             |
| Data Targets                      | 3 (DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT; SQL Server/Synapse SQL Tables)|
| Pre/Post SQL Logic                | 0 (No explicit pre/post SQL blocks; only PRINT statements for logging)           |
| Session/Workflow Controls         | 0 (No decision tasks, command tasks, or event-based controls)                    |
| DML Logic                         | 3 (MERGE = INSERT logic only; no UPDATE/DELETE/MERGE for existing records)       |
| Data Source Types                 | SQL Server/Synapse SQL Table                                                     |
| Data Target Types                 | SQL Server/Synapse SQL Table                                                     |
| Complexity Score (0–100)          | 25 (Matches DI_Azure_Synapse_Documentation Complexity Score)                     |

**High-complexity areas:**
- None. The procedure is straightforward with simple filter logic and idempotent MERGE statements. No deeply nested expressions, multiple lookups, or branching logic.

---

## 3. Syntax Differences

- **MERGE Statements:**  
  - Azure Synapse SQL supports `MERGE` for idempotent inserts. In PySpark, this must be translated to a combination of DataFrame `join` (anti-join for new records) and `insertInto` or `write` operations.
- **Functions:**  
  - `LEN()` (Synapse) → `length()` (PySpark).
  - `UPPER()` (Synapse) → `upper()` (PySpark).
  - `NOT LIKE`/`NOT IN` (Synapse) → PySpark DataFrame filter expressions (e.g., `~col.like()`, `~col.isin()`).
- **PRINT Statements:**  
  - Used for logging in T-SQL; in PySpark, replace with `print()` or logging framework.
- **No direct equivalents:**  
  - No direct `MERGE` in PySpark DataFrame API; must use anti-join pattern.
- **Data Types:**  
  - Implicit type handling in SQL; explicit type casting may be required in PySpark (e.g., string length checks).

---

## 4. Manual Adjustments

- **MERGE to Anti-Join + Insert:**  
  - Manual translation of each `MERGE` to:  
    1. Left anti-join between staging and target dimension table on the key.
    2. Filter for business rules.
    3. Insert new records.
- **Logging:**  
  - Replace `PRINT` statements with `print()` or Spark logging.
- **No external dependencies:**  
  - All logic is inline; no external stored procedures, shell scripts, or pre/post SQL blocks.
- **Business rule validation:**  
  - Review and validate filter logic post-conversion to ensure business intent is preserved.
- **No reusable components:**  
  - All logic is procedure-local; consider refactoring for reusability in PySpark if needed.

---

## 5. Optimization Techniques

- **Partitioning:**  
  - Partition target dimension tables on primary key for efficient writes and anti-joins.
- **Broadcast Joins:**  
  - If dimension tables are small, use broadcast joins for anti-join logic.
- **Caching:**  
  - Cache staging DataFrame if reused across multiple dimension loads.
- **Pipeline Filters:**  
  - Chain filters before joins to minimize data shuffling.
- **Window Functions:**  
  - Not required here, but consider for deduplication if business logic evolves.
- **Refactor vs. Rebuild:**  
  - **Refactor** is recommended: The logic is straightforward and can be mapped 1:1 to PySpark with anti-join and insert patterns. No need for a full rebuild unless additional complexity or performance requirements arise.

---

## API Cost

apiCost: 0.002 USD