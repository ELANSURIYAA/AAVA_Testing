```
=============================================
Author:        Ascendion AVA
Created on:   
Description:   Loads and maintains dimension tables (institution, corporation, product) from staging data in Synapse DW using MERGE statements with business rule filters.
=============================================

## 1. Procedure Overview

The `populate_dimensions` stored procedure is designed to load and maintain three core dimension tables—`DIM_INSTITUTION`, `DIM_CORPORATION`, and `DIM_PRODUCT`—from the staging table `STG_DIMENSION_DATA` in Azure Synapse DW. It uses SQL `MERGE` statements to ensure idempotent loads and applies business rules and data quality filters to exclude invalid, legacy, or test data. This process supports key business objectives such as regulatory reporting, analytics, and downstream data enrichment.  
**Number of mappings per workflow/session:** 3 (one for each dimension table).

---

## 2. Complexity Metrics

| Metric                              | Value/Details                                         |
|--------------------------------------|-------------------------------------------------------|
| Number of Source Qualifiers          | 1 (STG_DIMENSION_DATA staging table)                  |
| Number of Transformations            | 3 (MERGE statements with SELECT DISTINCT and filters) |
| Lookup Usage                         | 0 (no lookup transformations)                         |
| Expression Logic                     | 3 (filter expressions: IS NOT NULL, LEN > 3, NOT LIKE, NOT IN) |
| Join Conditions                      | 3 (MERGE ON conditions, no explicit SQL JOINs)        |
| Conditional Logic                    | 3 (WHERE clauses for business rule filters)           |
| Reusable Components                  | 0 (no reusable transformations or mapplets)           |
| Data Sources                         | 1 (SQL Server/Synapse DW table: STG_DIMENSION_DATA)   |
| Data Targets                         | 3 (SQL Server/Synapse DW tables: DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT) |
| Pre/Post SQL Logic                   | 0 (no pre/post SQLs or external procedures)           |
| Session/Workflow Controls            | 0 (no explicit workflow/session controls)             |
| DML Logic                            | 3 (INSERT via MERGE, no UPDATE/DELETE/MERGE logic for existing records) |
| Complexity Score (0–100)             | 30 (matches DI_Azure_Synapse_Documentation)           |

**High-complexity areas:**  
- Filter logic in WHERE clauses (business rules for exclusion)
- Deduplication using SELECT DISTINCT
- MERGE statement for idempotency

---

## 3. Syntax Differences

- **MERGE Statement:**  
  Azure Synapse SQL uses the `MERGE` statement for idempotent inserts. In PySpark, this must be translated to DataFrame operations with anti-joins or Delta Lake `MERGE` (if using Delta tables).
- **LEN Function:**  
  `LEN(institution_id)` in SQL maps to `length(col("institution_id"))` in PySpark.
- **IS NOT NULL:**  
  SQL `IS NOT NULL` maps to PySpark `.filter(col("institution_id").isNotNull())`.
- **NOT LIKE / NOT IN:**  
  SQL `NOT LIKE 'TEST%'` and `NOT IN ('DEPRECATED', 'LEGACY')` map to PySpark string functions and `.isin()` negation.
- **SELECT DISTINCT:**  
  SQL `SELECT DISTINCT` maps to `.dropDuplicates()` in PySpark.
- **PRINT Statements:**  
  SQL `PRINT` statements for logging must be replaced with Python `print()` or logging.
- **No explicit workflow/control logic:**  
  SQL procedure control flow (BEGIN...END, PRINT) must be restructured as Python function or notebook cells.

---

## 4. Manual Adjustments

- **MERGE Logic:**  
  Must be manually implemented using PySpark DataFrame anti-join or Delta Lake `MERGE` if available.
- **Business Rule Filters:**  
  Each WHERE clause filter (length, NOT LIKE, NOT IN) must be manually translated to PySpark filter expressions.
- **Deduplication:**  
  Use `.dropDuplicates()` for SELECT DISTINCT logic.
- **No external dependencies:**  
  No pre/post SQLs, stored procedures, or shell scripts detected.
- **Logging:**  
  Replace SQL `PRINT` statements with Python logging or print.
- **Validation:**  
  Business logic for exclusion filters should be validated post-conversion to ensure correct mapping.

---

## 5. Optimization Techniques

- **Partitioning:**  
  Use DataFrame partitioning on dimension keys to optimize write performance.
- **Caching:**  
  Cache intermediate DataFrames if reused across steps.
- **Broadcast Joins:**  
  If dimension tables are small, use broadcast joins for anti-join logic in MERGE.
- **Pipeline Conversion:**  
  Chain filters and deduplication into a single pipeline for each dimension.
- **Window Functions:**  
  Use window functions for more complex deduplication or ranking if needed.
- **Refactor vs. Rebuild:**  
  Refactor recommended—retain most original logic, but leverage PySpark/Delta Lake features for MERGE and filter logic.  
  Rebuild only if additional optimizations (e.g., incremental loads, parallelism) are required in Databricks.

---

## API Cost

apiCost: 0.002 USD

---

**End of Documentation**
```