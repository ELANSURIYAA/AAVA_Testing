=============================================
Author:        Ascendion AVA
Created on:   
Description:   Analysis of Azure Synapse stored procedure for loading summarized holding metrics into FACT_EXECUTIVE_SUMMARY, supporting executive analytics and reporting.
=============================================

---

## 1. Procedure Overview

The `LOAD_FACT_EXECUTIVE_SUMMARY` stored procedure in Azure Synapse Analytics is designed to load the `FACT_EXECUTIVE_SUMMARY` fact table with aggregated holding metrics sourced from the `STG_HOLDING_METRICS` staging table. The process includes data quality checks, application of business rules (such as handling null or negative income amounts), and referential integrity enforcement by joining with dimension tables (`DIM_DATE`, `DIM_INSTITUTION`, `DIM_CORPORATION`, `DIM_PRODUCT`). This workflow supports the business objective of providing cleansed, validated, and enriched summary data for executive reporting and analytics.

- **Number of mappings per workflow/session:** 1 (single stored procedure handling end-to-end ETL for the fact table)

---

## 2. Complexity Metrics

| Metric                        | Value/Details                                                                                 |
|-------------------------------|---------------------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (STG_HOLDING_METRICS staging table)                                                       |
| Number of Transformations     | 2 (Business rule on income_amount, referential integrity via joins)                         |
| Lookup Usage                  | 4 (Dimension tables: DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT)               |
| Expression Logic              | 1 (CASE statement for income_amount)                                                        |
| Join Conditions               | 4 INNER JOINs (all normal joins for referential integrity)                                  |
| Conditional Logic             | 1 (CASE statement for income_amount)                                                        |
| Reusable Components           | 0 (No reusable transformations, mapplets, or sessions)                                      |
| Data Sources                  | 1 (SQL Server Synapse table: STG_HOLDING_METRICS)                                          |
| Data Targets                  | 1 (SQL Server Synapse table: FACT_EXECUTIVE_SUMMARY)                                       |
| Pre/Post SQL Logic            | 0 (No explicit pre/post SQLs or procedures in session context)                              |
| Session/Workflow Controls     | 0 (No decision tasks, command tasks, or event-based controls in code)                       |
| DML Logic                     | 1 INSERT operation (no UPDATE, DELETE, or MERGE)                                            |
| Data Source Types             | SQL Server Synapse (staging and dimension tables)                                           |
| Data Target Types             | SQL Server Synapse (fact table)                                                             |
| Complexity Score (0â€“100)      | 30 (matches DI_Azure_Synapse_Documentation Complexity Score)                                |

**High-complexity areas:**  
- Multiple lookups (4 dimension joins for referential integrity)
- Business rule transformation (income_amount logic)
- No deeply nested expressions, branching logic, or unstructured sources

---

## 3. Syntax Differences

- **Functions/Constructs without Direct PySpark Equivalents:**
  - `CASE WHEN ... THEN ... ELSE ... END`: Needs to be mapped to PySpark's `when` and `otherwise` functions.
  - Temporary tables (`#staging_metrics`): In PySpark, use DataFrame caching or temporary views instead.
  - `@@ROWCOUNT`: In PySpark, use `df.count()` after insert operations.
  - `PRINT`: Replace with logging or print statements in PySpark/Databricks notebooks.

- **Data Type Conversions:**
  - Ensure numeric and date types in Synapse map correctly to Spark SQL types (e.g., `INT`, `DECIMAL`, `DATE`).

- **Workflow/Control Logic:**
  - No explicit workflow constructs (like Router or Transaction Control) are present, but orchestration must be handled externally in Databricks (e.g., via notebooks or jobs).

---

## 4. Manual Adjustments

- **Components Requiring Manual Implementation:**
  - Temporary table logic (`#staging_metrics`): Must be replaced with DataFrame operations or Spark SQL temporary views.
  - Audit logging (`PRINT`, `@@ROWCOUNT`): Implement using logging frameworks or notebook output.
  - Error handling: The variable `@error_message` is reserved but not used; consider implementing robust error handling in PySpark.

- **External Dependencies:**
  - All dimension and staging tables must be accessible in Databricks (via JDBC, Delta Lake, or other connectors).
  - No pre/post SQLs, shell scripts, or external stored procedures referenced.

- **Business Logic Review:**
  - Validate the business rule for `income_amount` (null or negative to zero) post-conversion.
  - Ensure referential integrity logic is preserved in PySpark joins.

---

## 5. Optimization Techniques

- **Spark Best Practices:**
  - Use DataFrame caching or checkpointing for intermediate results (replacing temp tables).
  - Partition data on join keys (e.g., `date_key`, `institution_id`) to optimize join performance.
  - Use broadcast joins for small dimension tables to speed up lookups.
  - Chain filters and joins in a single pipeline to minimize shuffling.
  - Use Spark SQL window functions if aggregations become more complex in future enhancements.

- **Refactor or Rebuild Recommendation:**
  - **Refactor**: The logic is straightforward and can be directly mapped to PySpark DataFrame operations and SQL. Retain the original logic structure, but leverage Spark optimizations for performance.

---

**API Cost:**  
apiCost: 0.005 USD