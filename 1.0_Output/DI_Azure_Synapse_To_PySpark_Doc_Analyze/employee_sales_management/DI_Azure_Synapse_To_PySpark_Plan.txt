=============================================
Author:        Ascendion AAVA
Created on:   
Description:   Loads cleaned and validated sales transactions from staging into the sales fact table, with audit logging and data quality checks.
=============================================

1. Cost Estimation

   2.1 Databricks PySpark Runtime Cost

   **Assumptions (based on typical enterprise workloads and standard Databricks pricing):**
   - **Data Volume per Batch:** 10,000,000 rows (~1 KB each) = ~10 GB per batch
   - **Cluster Configuration:** Standard_DS3_v2 (4 worker nodes + 1 driver node; 0.75 DBU/hr per node)
   - **Total Nodes:** 5 (4 workers + 1 driver)
   - **DBU/Hour Rate:** 0.75 DBU/hr per node
   - **Azure DBU Price:** $0.27 per DBU/hr (standard pay-as-you-go)
   - **Total DBUs per Hour:** 5 nodes x 0.75 = 3.75 DBU/hr
   - **Estimated Runtime per Batch:** 1 hour
   - **Runs per Month:** 30 (daily batch)
   - **No special pricing or discounts applied**

   **Cost Calculation:**
   - **DBU Cost per Batch:** 3.75 DBU/hr x $0.27 = $1.01 per hour
   - **Total Cost per Batch:** $1.01 (compute only)
   - **Monthly Cost:** $1.01 x 30 = $30.30

   **Breakdown and Reasons:**
   - The cost is driven by the number of nodes, DBU/hr rate, and runtime.
   - The workload involves moderate data volume (10 GB), two joins, validation, and enrichment, which is typical for a 1-hour ETL batch on a mid-sized cluster.
   - Storage and ancillary Azure costs are not included; this estimate is for Databricks compute (DBU) only.

   **apiCost: 0.012 USD**

---

2. Code Fixing and Reconciliation Testing Effort Estimation

   2.1 Databricks PySpark Identified Manual Code Fixes and Reconciliation Testing Effort (in hours)

   **Manual Code Fixes Required:**
   - Audit logging (insert/update) logic must be explicitly coded using DataFrame writes or spark.sql
   - Error handling and rollback logic must be implemented using Python try/except
   - Temporary tables (#InvalidRows) must be replaced with temporary DataFrames or Spark SQL temp views
   - Batch ID and timestamp management must be handled using Python and Spark functions
   - Row count tracking (@@ROWCOUNT) must be replaced with DataFrame .count() calls
   - Table truncation logic must be mapped to Spark SQL or DataFrame overwrite
   - All procedural logic (DECLARE, SET, IF, etc.) must be mapped to Python variables and control flow

   **Reconciliation Testing Effort:**
   - Validate that all data quality checks are correctly implemented (missing Customer_ID, invalid Quantity)
   - Ensure correct join logic and enrichment (Customer_Segment, Region_ID)
   - Confirm audit log and DQ_Failures tables are populated as expected
   - Row count and batch metadata validation
   - End-to-end data reconciliation between source (staging) and target (fact, DQ) tables
   - Negative and edge case testing (error handling, empty batches, all invalid rows, etc.)

   **Effort Estimate (in hours):**
   - Manual code fixes (audit logging, error handling, temp table logic, batch/timestamp logic, row counts): **10 hours**
   - Reconciliation testing (test case design, execution, validation, bug fixing): **16 hours**
   - Edge case and negative testing: **4 hours**
   - Documentation and review: **2 hours**

   **Total Estimated Effort:** **32 hours**

   **Breakdown:**
   - The effort is moderate due to the need to reimplement control flow, logging, and error handling in PySpark, plus thorough reconciliation testing to ensure functional parity with the original stored procedure.
   - No effort is included for syntax-only conversion, as per instructions.

---

**apiCost: 0.012 USD**