=============================================
Author:        Ascendion AAVA
Created on:   
Description:   Effort and cost estimation for Databricks PySpark migration and testing of Azure Synapse stored procedure `dw.sp_load_sales_fact` – covers manual code fixes, reconciliation testing, and Databricks runtime cost analysis.
=============================================

1. Cost Estimation

   2.1 Databricks PySpark Runtime Cost

   **Assumptions:**
   - Data volume per batch: 1 million rows in `stg.Sales_Transactions`
   - Each row ~500 bytes (includes all columns and metadata)
   - Total data processed per batch: ~500 MB (source) + ~500 MB (dimension lookups) + ~500 MB (fact table write) + ~50 MB (DQ log) + ~10 MB (audit log) ≈ 1.6 GB per run (rounded up for intermediate steps, shuffles, and temp DataFrames)
   - Number of queries (distinct DataFrame actions): 
     - Read staging (1)
     - Data quality filter (2)
     - Delete invalid rows (1)
     - Join with customer (1)
     - Join with date (1)
     - Write to fact table (1)
     - Write to DQ log (1)
     - Write to audit log (2: start/end)
     - Truncate staging (1)
     - Total: ~10 actions per batch
   - Databricks pricing (as of 2024, for AWS/Azure Standard DBU, on-demand): 
     - 1 DBU (Databricks Unit) = $0.15 USD/hour (Standard, interactive cluster)
     - Typical small ETL job: 2 nodes x 4 DBUs = 8 DBUs/hour
     - Estimated runtime for this job: 10 minutes (0.167 hours) per batch

   **Calculation:**
   - DBUs consumed per run: 8 DBUs x 0.167 hours = 1.336 DBUs
   - Cost per run: 1.336 DBUs x $0.15 USD = $0.2004 USD
   - Storage and I/O cost (cloud storage, e.g., ADLS/S3): negligible for <2 GB per batch, typically < $0.01 USD per run
   - Total estimated cost per batch: ~$0.21 USD

   **Reasons:**
   - The job is compute-light, dominated by DataFrame transformations and small joins.
   - No large shuffles or aggregations; dimension tables are likely to be broadcast joins.
   - Audit and DQ log writes are small.
   - Cost can scale linearly with data volume and cluster size.

   **Summary Table:**
   | Resource                | Quantity         | Unit Cost (USD) | Subtotal (USD) |
   |-------------------------|------------------|-----------------|---------------|
   | Databricks DBU runtime  | 1.336 DBUs       | $0.15           | $0.2004       |
   | Storage/I/O             | ~2 GB            | $0.005          | $0.005        |
   | **Total (per batch)**   |                  |                 | **$0.21**     |

---

2. Code Fixing and Reconciliation Testing Effort Estimation

   2.1 Databricks PySpark Identified Manual Code Fixes and Reconciliation Testing Effort (in hours)

   **Manual Code Fixes (PySpark conversion):**
   - Replace T-SQL variables with Python variables (batch_id, timestamps, row counts): 0.5 hr
   - Implement audit logging (start/end, status, row counts) as DataFrame writes: 1 hr
   - Implement DQ failure logging (invalid rows DataFrame, write to DQ table): 0.5 hr
   - Replace temp table logic (`#InvalidRows`) with DataFrame: 0.5 hr
   - Implement error handling (try/except, update audit log on error): 0.5 hr
   - Implement transformation logic (joins, computed columns): 1 hr
   - Implement staging table truncate (overwrite or delete): 0.25 hr
   - Refactor all logic into a single PySpark script/notebook: 0.5 hr

   **Subtotal (manual code fixes):** 4.75 hours

   **Reconciliation Testing Effort:**
   - Unit test each transformation step (validation, joins, computed columns): 1 hr
   - Validate row counts (inserted, rejected) against source: 0.5 hr
   - Data reconciliation between Synapse output and PySpark output (fact table, DQ log): 1 hr
   - Audit log validation (start/end, status, messages): 0.5 hr
   - End-to-end test with sample data (including error scenario): 1 hr

   **Subtotal (testing):** 4 hours

   **Total Effort Estimate (manual code fixes + testing):** 8.75 hours

   **Breakdown Table:**
   | Task                                      | Estimated Hours |
   |-------------------------------------------|----------------|
   | Manual code fixes (PySpark conversion)    | 4.75           |
   | Reconciliation & testing                  | 4.00           |
   | **Total**                                | **8.75**       |

---

**API Cost:**  
apiCost: 0.0025 USD

---

**Summary:**

- **Databricks PySpark runtime cost per batch:** ~$0.21 USD (compute + storage)
- **Effort for manual code fixes and reconciliation testing:** ~8.75 hours
- **API cost for this estimation:** 0.0025 USD

---