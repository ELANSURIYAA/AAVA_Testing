=============================================
Author:        Ascendion AVA
Created on:   
Description:   Loads, validates, and enriches sales transaction data from staging to the sales fact table with audit and data quality logging.
=============================================

## 1. Procedure Overview

This document analyzes the Azure Synapse stored procedure `dw.sp_load_sales_fact`, which implements an ETL workflow to load sales transaction data from a staging table (`stg.Sales_Transactions`) into the data warehouse fact table (`dw.Fact_Sales`). The workflow performs data validation, enrichment via joins with dimension tables, audit logging, and records data quality failures. The key business objective is to ensure only clean, validated, and enriched sales data is loaded into the analytics-ready fact table, supporting enterprise reporting, analytics, and regulatory compliance.

**Number of mappings per workflow/session:** 1 (single stored procedure encapsulating the entire ETL workflow)

---

## 2. Complexity Metrics

| Metric                       | Value / Description                                                                                  |
|------------------------------|-----------------------------------------------------------------------------------------------------|
| Number of Source Qualifiers  | 1 (stg.Sales_Transactions; SQL Server)                                                              |
| Number of Transformations    | 4 (Validation, Join with Customer [SQL Server], Join with Date [SQL Server], Derived Column)        |
| Lookup Usage                 | 2 (dw.Dim_Customer, dw.Dim_Date; both as inner joins, equivalent to connected lookups)              |
| Expression Logic             | 2 (Total_Sales_Amount = Quantity * Unit_Price; Load_Timestamp = SYSDATETIME())                      |
| Join Conditions              | 2 (Inner Join: Customer_ID, Inner Join: Sales_Date = Date_Value)                                    |
| Conditional Logic            | 2 (Validation: Customer_ID IS NULL, Quantity <= 0; Error handling via TRY/CATCH)                    |
| Reusable Components          | 0 (no explicit reusable transformations, mapplets, or sessions)                                     |
| Data Sources                 | 1 (stg.Sales_Transactions; SQL Server table)                                                        |
| Data Targets                 | 3 (dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures; all SQL Server tables)                             |
| Pre/Post SQL Logic           | 2 (Audit Log insert/update, DQ Failure logging)                                                     |
| Session/Workflow Controls    | 1 (TRY/CATCH error handling, status updates in Audit Log)                                           |
| DML Logic                    | 4 (INSERT into Audit_Log, INSERT into Fact_Sales, DELETE from staging, TRUNCATE staging, INSERT into DQ_Failures, UPDATE Audit_Log) |
| Complexity Score (0–100)     | 55 (moderate complexity; multiple joins, validation, audit, and error handling)                     |

**High-complexity areas:**
- Nested control flow (TRY/CATCH with audit/error logging)
- Multiple lookups (joins with two dimension tables)
- Branching logic (validation and error handling)
- DML operations affecting multiple tables in a transactional context

---

## 3. Syntax Differences

- **Functions:**
    - `SYSDATETIME()` (Azure Synapse) → `current_timestamp()` (PySpark)
    - `NEWID()` (Azure Synapse) → `uuid.uuid4()` (PySpark, via Python)
    - `OBJECT_NAME(@@PROCID)` (Azure Synapse) → No direct equivalent; can be set as a string in PySpark
    - `@@ROWCOUNT` (Azure Synapse) → Use DataFrame `.count()` in PySpark after actions
    - `TRY/CATCH` (T-SQL) → `try/except` blocks in Python

- **Data Types:**
    - `UNIQUEIDENTIFIER` → `StringType()` in PySpark (UUID as string)
    - `DATETIME` → `TimestampType()` in PySpark
    - `NVARCHAR` → `StringType()` in PySpark

- **Temporary Tables:**
    - `#InvalidRows` (T-SQL temp table) → PySpark DataFrame, possibly persisted as a temp view

- **DML/Workflow:**
    - `INSERT INTO ... SELECT ...` → DataFrame `.write.insertInto()` or `.write.saveAsTable()` in PySpark
    - `DELETE ... FROM ... JOIN ...` → DataFrame anti-join or filter
    - `TRUNCATE TABLE` → Overwrite mode in DataFrame write, or explicit `.delete()` if supported

- **Audit and DQ Logging:**
    - Direct table inserts/updates in SQL → DataFrame writes in PySpark

---

## 4. Manual Adjustments

- **Manual Implementation Required:**
    - Audit logging logic (start/end, status, row counts) must be re-implemented using DataFrame operations and explicit writes to the audit table.
    - Error handling and transaction management: PySpark does not support SQL-style transactions; must use try/except and ensure idempotency.
    - Temporary tables (`#InvalidRows`) must be replaced with DataFrames and/or temp views.
    - Row count tracking (`@@ROWCOUNT`) must be explicitly calculated after DataFrame actions.
    - Procedure metadata (procedure name, batch ID) must be handled via variables in Python.
    - Any pipeline triggers or scheduling must be re-implemented using Databricks Jobs or Workflows.

- **External Dependencies:**
    - No explicit external scripts, but relies on existence of referenced tables and audit/DQ infrastructure.

- **Business Logic Review:**
    - Validation rules (Customer_ID, Quantity) should be reviewed for completeness.
    - Ensure data type compatibility and referential integrity in joins.
    - Confirm audit and DQ logging meets compliance and traceability requirements post-migration.

---

## 5. Optimization Techniques

- **Partitioning:** Use partitioning on large tables (e.g., by Sales_Date or Batch_ID) to optimize read/write performance.
- **Caching:** Cache dimension tables (`Dim_Customer`, `Dim_Date`) if reused in multiple joins.
- **Broadcast Joins:** Use broadcast joins for small dimension tables to avoid shuffles.
- **Pipeline Chaining:** Chain filters and joins in a single DataFrame pipeline to minimize intermediate writes.
- **Window Functions:** Use Spark SQL window functions to simplify aggregations if needed.
- **Error Handling:** Use Python `try/except` for robust error handling and ensure audit logs are updated in all cases.
- **Refactor vs. Rebuild:** Recommend **Refactor**—retain most of the original logic, but leverage Spark best practices for performance and scalability. Consider **Rebuild** only if business logic is to be significantly enhanced or simplified.

---

**API Cost:**  
apiCost: 0.018 USD