=============================================
Author:        Ascendion AVA
Created on:   
Description:   Loads cleaned and validated sales transactions from staging into the sales fact table, with audit logging and data quality checks.
=============================================

## 1. Procedure Overview

This document analyzes the migration of the Azure Synapse stored procedure `dw.sp_load_sales_fact` to Databricks PySpark. The procedure orchestrates the end-to-end ETL workflow for loading transactional sales data from a staging area into the enterprise data warehouse fact table, ensuring data quality, enrichment, audit logging, and robust error handling. The business objective is to deliver accurate, validated, and enriched sales data for downstream analytics and reporting, while maintaining traceability and compliance.

- **Number of mappings per workflow/session:** 1 (single ETL workflow encapsulated in the stored procedure)

---

## 2. Complexity Metrics

| Metric                            | Value/Details                                                                                  |
|------------------------------------|-----------------------------------------------------------------------------------------------|
| Number of Source Qualifiers        | 1 (stg.Sales_Transactions; SQL Server)                                                        |
| Number of Transformations          | 3 (Data quality filter, Join with Dim_Customer, Join with Dim_Date, Expression for calculation)|
| Lookup Usage                       | 2 (connected lookups: Dim_Customer, Dim_Date)                                                 |
| Expression Logic                   | 1 (Total_Sales_Amount = Quantity * Unit_Price)                                                |
| Join Conditions                    | 2 (INNER JOIN: Customer_ID, INNER JOIN: Sales_Date = Date_Value)                              |
| Conditional Logic                  | 2 (WHERE filters for DQ, TRY...CATCH for error handling)                                      |
| Reusable Components                | 0 (all logic is inline within the procedure)                                                  |
| Data Sources                       | 1 (stg.Sales_Transactions; SQL Server)                                                        |
| Data Targets                       | 3 (dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures; all SQL Server tables)                        |
| Pre/Post SQL Logic                 | 2 (Audit log insert/update, DQ failure logging)                                               |
| Session/Workflow Controls          | 1 (TRY...CATCH error handling, status tracking)                                               |
| DML Logic                          | 4 (INSERT, DELETE, TRUNCATE, UPDATE)                                                          |
| Complexity Score (0–100)           | 40 (matches DI_Azure_Synapse_Documentation Complexity Score)                                  |

**High-complexity areas:**
- Use of temporary tables for DQ failures
- Multiple lookups (customer, date)
- Branching logic for error handling and DQ rejection
- Audit and DQ logging with dynamic batch IDs and timestamps

---

## 3. Syntax Differences

- **Functions with no direct PySpark equivalent:**
  - `SYSDATETIME()` → Use `current_timestamp()` in PySpark.
  - `NEWID()` → Use `uuid.uuid4()` or `expr("uuid()")` in PySpark.
  - `@@ROWCOUNT` → Use DataFrame `.count()` after actions.
  - `OBJECT_NAME(@@PROCID)` → Hardcode or pass as parameter in PySpark.
  - `TRY...CATCH` → Use Python `try...except` blocks.
  - Table variables and temp tables (`#InvalidRows`) → Use DataFrames for intermediate results.
  - `TRUNCATE TABLE` → Use overwrite mode or explicit delete in PySpark.

- **Data type conversions:**
  - `DATETIME` (SQL Server) → `TimestampType` in Spark.
  - `UNIQUEIDENTIFIER` → `StringType` (UUID).
  - `NVARCHAR` → `StringType`.

- **Workflow/control logic:**
  - Audit logging and error handling must be implemented with DataFrame writes and Python exception handling.
  - Batch and session variables must be managed in Python context.

---

## 4. Manual Adjustments

- **Components requiring manual implementation:**
  - Audit log insert/update logic (must be written as DataFrame operations or SQL in PySpark).
  - DQ failure logging (requires DataFrame for rejected rows and write to DQ_Failures table).
  - Error handling and status updates (Python `try...except` with DataFrame writes).
  - Temporary tables (`#InvalidRows`) must be replaced with DataFrames.
  - Dynamic batch ID and timestamp assignment (use Python and Spark functions).
  - Truncating staging table (overwrite or explicit delete in Spark SQL).

- **External dependencies:**
  - All referenced tables must be accessible in Databricks (via JDBC, Delta, or external tables).
  - Any shell scripts or orchestration logic outside the procedure must be ported to Databricks Jobs or Workflows.

- **Business logic review:**
  - Validate that DQ checks and enrichment logic produce the same results in Spark as in SQL Server.
  - Confirm audit and error logs are written atomically and reliably in the new environment.

---

## 5. Optimization Techniques

- **Spark best practices:**
  - Partition DataFrames on `Batch_ID` or `Sales_Date` for parallelism and efficient writes.
  - Cache intermediate DataFrames if reused (e.g., filtered valid/invalid rows).
  - Use broadcast joins for small dimension tables (`Dim_Customer`, `Dim_Date`) to optimize join performance.
  - Chain filters and transformations in a single pipeline to avoid unnecessary shuffles.
  - Use Spark window functions if aggregations or row-level operations become complex.
  - Write to Delta Lake tables for ACID compliance and scalable data management.

- **Refactor or Rebuild:**
  - **Recommendation:** Refactor. The logic can be closely mapped to PySpark DataFrame operations, with optimization for Spark execution. Consider rebuilding only if there is a need to radically optimize or redesign the ETL flow for new business requirements.

---

**API Cost:** apiCost: 0.0025 USD