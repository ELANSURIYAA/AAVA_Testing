=============================================
Author:        Ascendion AVA
Created on:   
Description:   Analysis of Azure Synapse stored procedure `dw.sp_load_sales_fact` for migration to Databricks PySpark – covers ingestion, validation, transformation, audit, and DQ logging of sales transactions.
=============================================

## 1. Procedure Overview

The stored procedure `dw.sp_load_sales_fact` automates the end-to-end ETL process for sales transaction data, including data quality validation, enrichment via dimension lookups, transformation, and loading into a fact table. It logs audit information and data quality failures for traceability and compliance. This workflow supports business objectives such as regulatory reporting, analytics, and operational monitoring.  
**Number of mappings per workflow/session:** 1 (entire logic encapsulated in a single stored procedure/session).

---

## 2. Complexity Metrics

| Metric                        | Value / Type                                                                                                 |
|-------------------------------|-------------------------------------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (stg.Sales_Transactions) – SQL Server (Synapse SQL pool)                                                  |
| Number of Transformations     | 3 (Validation, Joins, Computed Columns)                                                                     |
| Lookup Usage                  | 2 (dw.Dim_Customer, dw.Dim_Date) – both as INNER JOINs (connected lookups)                                  |
| Expression Logic              | 2 (Total_Sales_Amount = Quantity * Unit_Price; conditional logic for validation)                            |
| Join Conditions               | 2 (INNER JOIN: Customer, Date)                                                                              |
| Conditional Logic             | 2 (Validation checks for Customer_ID and Quantity; error handling via TRY/CATCH)                            |
| Reusable Components           | 0 (all logic is inline, no reusable transformations or mapplets)                                            |
| Data Sources                  | 1 (stg.Sales_Transactions – SQL Server/Synapse SQL pool)                                                    |
| Data Targets                  | 3 (dw.Fact_Sales, dw.DQ_Failures, dw.Audit_Log – all SQL Server/Synapse SQL pool tables)                    |
| Pre/Post SQL Logic            | 2 (Audit log insert/update, DQ log insert; no external pre/post SQL scripts)                                |
| Session/Workflow Controls     | 1 (TRY/CATCH for error handling; no external workflow controls)                                             |
| DML Logic                     | 4 (INSERT into Audit_Log, Fact_Sales, DQ_Failures; DELETE from staging; TRUNCATE staging)                   |
| Complexity Score (0–100)      | 35 (matches DI_Azure_Synapse_Documentation)                                                                 |

**High-complexity areas:**
- Multiple lookups (dimension joins)
- Branching logic (validation, error handling)
- Audit and DQ logging
- Use of temporary tables for validation failures

---

## 3. Syntax Differences

- **Functions without direct PySpark equivalents:**
  - `NEWID()` → Use `uuid.uuid4()` or Spark’s `monotonically_increasing_id()` for unique batch IDs.
  - `SYSDATETIME()` → Use `current_timestamp()` in PySpark.
  - `OBJECT_NAME(@@PROCID)` → No direct equivalent; procedure name can be hardcoded or passed as a parameter in PySpark.
  - `@@ROWCOUNT` → Use DataFrame `.count()` after write operations.
  - `TRY/CATCH` blocks → Use Python `try/except` for error handling.
  - Temporary tables (`#InvalidRows`) → Use temporary DataFrames or Spark SQL temp views.
- **Data type conversions:**
  - `CAST(Sales_Date AS DATE)` → Use `to_date()` in PySpark.
  - `NVARCHAR`/`DATETIME` → Map to `StringType`/`TimestampType` in PySpark.
- **Workflow/Control logic:**
  - No direct equivalent for SQL transaction control; PySpark handles atomicity at DataFrame operation level.
  - Audit and DQ logging must be implemented as explicit DataFrame writes.

---

## 4. Manual Adjustments

- **Components requiring manual implementation:**
  - Audit logging logic (start/end, status, row counts) must be re-implemented using DataFrame operations and explicit writes.
  - DQ failure logging (invalid rows) requires DataFrame filtering and separate write.
  - Error handling (audit log update on failure) must be handled with Python `try/except` and DataFrame writes.
  - Temporary table logic (`#InvalidRows`) must be replaced with temporary DataFrames or views.
- **External dependencies:**
  - None in this procedure (no shell scripts, no external stored procedures).
- **Business logic review:**
  - Validation rules (e.g., Customer_ID null, Quantity <= 0) should be reviewed for completeness and correctness post-conversion.
  - Ensure audit and DQ logs are written atomically in case of errors.

---

## 5. Optimization Techniques

- **Spark best practices:**
  - Use DataFrame partitioning (e.g., by date or batch_id) to optimize large loads.
  - Cache DataFrames if reused across multiple steps (e.g., dimension lookups).
  - Use broadcast joins for small dimension tables (e.g., `dw.Dim_Customer`, `dw.Dim_Date`) to avoid shuffles.
  - Replace chain filters and joins with a single pipeline of DataFrame transformations.
  - Use Spark window functions if aggregations become more complex in future enhancements.
- **Refactor vs. Rebuild:**
  - **Recommendation:** Refactor. The logic is straightforward and can be mapped 1:1 to PySpark DataFrame operations. No need for a full rebuild unless future requirements introduce more complex transformations or performance bottlenecks.

---

**API Cost:**  
apiCost: 0.0025 USD