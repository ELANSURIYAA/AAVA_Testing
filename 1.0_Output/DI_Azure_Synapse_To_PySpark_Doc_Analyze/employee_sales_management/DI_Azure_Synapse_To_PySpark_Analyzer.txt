=============================================
Author:        Ascendion AVA
Created on:   
Description:   Analysis of Azure Synapse stored procedure for loading and validating sales fact data, to support conversion to Databricks PySpark.
=============================================

---

## 1. Procedure Overview

This document analyzes the Azure Synapse stored procedure `dw.sp_load_sales_fact`, which automates the ETL workflow for loading sales transactions from a staging table (`stg.Sales_Transactions`) into the data warehouse fact table (`dw.Fact_Sales`). The procedure performs data quality validation, transformation, audit logging, and error handling to ensure only clean, reliable data is loaded for downstream analytics and reporting.  
**Number of mappings per workflow/session:** 1 (single stored procedure encapsulating the full ETL logic for this workflow).

**Key business objective:**  
- Data integration and cleansing: Ensures only valid, joined, and enriched sales data is loaded to the warehouse, with full audit and data quality traceability.

---

## 2. Complexity Metrics

| Metric                        | Value / Type                                                                                  |
|-------------------------------|----------------------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (stg.Sales_Transactions; SQL Server)                                                       |
| Number of Transformations     | 3 (Validation [Expression/Filter], Joiner [2x], Expression [Computed Column])                |
| Lookup Usage                  | 2 (dw.Dim_Customer, dw.Dim_Date; SQL Server; both as inner joins)                            |
| Expression Logic              | 2 (Total_Sales_Amount = Quantity * Unit_Price; validation expressions for DQ)                |
| Join Conditions               | 2 (Inner Joins: Customer_ID, Sales_Date to Date_Value)                                       |
| Conditional Logic             | 2 (Validation filters, TRY/CATCH error handling)                                             |
| Reusable Components           | 0 (no reusable transformations or mapplets; all logic inline in procedure)                   |
| Data Sources                  | 1 (stg.Sales_Transactions; SQL Server table)                                                 |
| Data Targets                  | 3 (dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures; all SQL Server tables)                       |
| Pre/Post SQL Logic            | 2 (Audit logging at start/end, DQ failure logging)                                           |
| Session/Workflow Controls     | 1 (TRY/CATCH error handling, audit status updates)                                           |
| DML Logic                     | 4 (INSERT, DELETE, TRUNCATE, UPDATE)                                                         |
| Complexity Score (0–100)      | 40 (matches DI_Azure_Synapse_Documentation; moderate complexity due to validation, joins, audit) |

**High-complexity areas:**
- Nested validation logic (multiple DQ checks, temp table for invalids)
- Multiple lookups/joins (customer and date dimensions)
- Branching logic (TRY/CATCH for error handling)
- Audit and DQ logging (multiple targets, status management)

---

## 3. Syntax Differences

- **Functions without direct PySpark equivalents:**
  - `SYSDATETIME()` → Use `current_timestamp()` in PySpark.
  - `NEWID()` (uniqueidentifier) → Use `uuid.uuid4()` or `F.expr("uuid()")` in PySpark.
  - `OBJECT_NAME(@@PROCID)` → No direct equivalent; can be hardcoded or passed as a parameter in PySpark.
  - `@@ROWCOUNT` → In PySpark, use `.count()` on DataFrames after insert/filter actions.

- **Data type conversions:**
  - `UNIQUEIDENTIFIER` → Use `StringType` with UUIDs in PySpark.
  - `DATETIME` → Use `TimestampType` in PySpark.
  - `NVARCHAR` → Use `StringType` in PySpark.
  - `BIGINT` → Use `LongType` in PySpark.

- **Workflow/control logic:**
  - Temporary tables (`#InvalidRows`) → Use temporary DataFrames in PySpark.
  - TRY/CATCH error handling → Use Python try/except blocks.
  - DML operations (DELETE with JOIN, TRUNCATE) → Use DataFrame filters and overwrite modes.
  - Multi-statement transactional logic → PySpark is not transactional by default; must manage atomicity explicitly if needed.

---

## 4. Manual Adjustments

- **Components requiring manual implementation in PySpark:**
  - Audit logging: Manual DataFrame inserts/updates to `dw.Audit_Log`.
  - DQ failure logging: Manual DataFrame writes to `dw.DQ_Failures`.
  - Error handling: Python try/except with custom logging and rethrowing.
  - Row count tracking: Use DataFrame `.count()` before/after actions.
  - Batch ID generation: Use Python UUID library or Spark SQL `uuid()`.

- **External dependencies:**
  - None in the procedure itself, but expects upstream pipeline to load `stg.Sales_Transactions`.

- **Business logic to review/validate post-conversion:**
  - Validation logic for missing/invalid data (ensure PySpark logic matches SQL semantics).
  - Handling of audit and DQ logs (ensure atomicity and consistency).
  - Truncation/archival of staging data (ensure overwrite semantics are correct).
  - Error propagation for pipeline monitoring (ensure exceptions are surfaced as needed).

---

## 5. Optimization Techniques

- **Partitioning:** Partition staging and target tables/DataFrames by relevant keys (e.g., date, batch_id) to optimize read/write performance.
- **Caching:** Cache intermediate DataFrames (e.g., after validation, before joins) if reused multiple times in the workflow.
- **Broadcast joins:** Use `broadcast()` for small dimension tables (e.g., `dw.Dim_Customer`, `dw.Dim_Date`) to optimize join performance.
- **Pipeline chaining:** Combine filter, join, and transformation steps into a single DataFrame pipeline to minimize shuffles and I/O.
- **Window functions:** Use PySpark window functions for any future aggregations or deduplication, simplifying nested SQL logic.
- **Refactor vs. Rebuild:**  
  - **Refactor** is recommended: The logic can be mapped closely to PySpark DataFrame operations, with explicit handling for audit and DQ logging.  
  - **Rebuild** only if significant performance or maintainability improvements are required (e.g., if audit/DQ logging needs to be generalized across multiple pipelines).

---

**API Cost:** apiCost: 0.0025 USD