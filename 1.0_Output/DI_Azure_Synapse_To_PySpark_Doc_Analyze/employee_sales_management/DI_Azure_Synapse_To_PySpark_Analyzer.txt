=============================================
Author:        Ascendion AVA
Created on:   
Description:   Loads cleaned and validated sales transactions from staging into the sales fact table, with audit logging and data quality checks.
=============================================

# 1. Procedure Overview

This document analyzes the Azure Synapse stored procedure `dw.sp_load_sales_fact`, which automates the ETL workflow for loading, validating, and enriching sales transaction data from the staging table (`stg.Sales_Transactions`) into the enterprise data warehouse fact table (`dw.Fact_Sales`). The procedure supports the business objective of ensuring only high-quality, enriched sales data is loaded for accurate analytics and reporting. It also logs audit information and archives invalid records for data quality review.

- **Number of Mappings per Workflow/Session:** 1 (single stored procedure encapsulating the entire ETL workflow)

---

# 2. Complexity Metrics

| Metric                        | Value/Description                                                                                 |
|-------------------------------|--------------------------------------------------------------------------------------------------|
| Number of Source Qualifiers   | 1 (stg.Sales_Transactions; SQL Server table)                                                     |
| Number of Transformations     | 3 (Validation, Calculation, Enrichment via Joins)                                                |
| Lookup Usage                  | 2 (dw.Dim_Customer, dw.Dim_Date; both as SQL Server tables via INNER JOIN)                       |
| Expression Logic              | 2 (Total_Sales_Amount calculation, Load_Timestamp assignment)                                    |
| Join Conditions               | 2 INNER JOINs (Customer_ID, Sales_Date=Date_Value)                                               |
| Conditional Logic             | 2 (Validation WHERE clauses, TRY/CATCH error handling)                                           |
| Reusable Components           | 0 (all logic is inline within the procedure)                                                     |
| Data Sources                  | 3 (stg.Sales_Transactions, dw.Dim_Customer, dw.Dim_Date; all SQL Server tables)                 |
| Data Targets                  | 3 (dw.Fact_Sales, dw.Audit_Log, dw.DQ_Failures; all SQL Server tables)                          |
| Pre/Post SQL Logic            | 2 (Audit log insert/update, DQ_Failures insert)                                                  |
| Session/Workflow Controls     | 1 (TRY/CATCH block for error handling)                                                           |
| DML Logic                     | Frequent (INSERT, DELETE, TRUNCATE, UPDATE)                                                      |
| Complexity Score (0–100)      | 35 (moderate; matches DI_Azure_Synapse_Documentation score)                                      |

**High-Complexity Areas:**
- Nested expressions for audit and error handling
- Multiple lookups (joins with dimension tables)
- Branching logic via TRY/CATCH and validation WHERE clauses

---

# 3. Syntax Differences

- **Functions without Direct PySpark Equivalent:**
  - `SYSDATETIME()` → `current_timestamp()` in PySpark
  - `NEWID()` → `uuid.uuid4()` or `F.monotonically_increasing_id()` (for unique batch IDs)
  - `OBJECT_NAME(@@PROCID)` → Not directly available; can be set as a string variable in PySpark
  - `@@ROWCOUNT` → Use DataFrame `.count()` after write operations in PySpark
  - `TRY/CATCH` → Use Python `try/except` blocks

- **Data Type Conversions:**
  - `DATETIME` (SQL Server) → `TimestampType` (PySpark)
  - `UNIQUEIDENTIFIER` → `StringType` (PySpark, with UUIDs)
  - `NVARCHAR` → `StringType`
  - Temporary tables (`#InvalidRows`) → Temporary DataFrames or persisted tables in Spark

- **Workflow/Control Logic:**
  - SQL procedural constructs (DECLARE, SET, IF, etc.) → Python variables and control flow
  - Table truncation (`TRUNCATE TABLE`) → DataFrame overwrite with empty DataFrame or `spark.sql("TRUNCATE TABLE ...")` if using managed tables

---

# 4. Manual Adjustments

- **Components Requiring Manual Implementation:**
  - Audit logging logic (insert/update) must be explicitly coded using DataFrame writes or `spark.sql` statements
  - Error handling and rollback logic must be implemented using Python `try/except` and possibly Spark transactions (if supported)
  - Temporary tables (`#InvalidRows`) must be replaced with temporary DataFrames or Spark SQL temp views
  - Batch ID and timestamp management must be handled using Python and Spark functions
  - Row count tracking (`@@ROWCOUNT`) must be replaced with DataFrame `.count()` calls
  - Any orchestration (e.g., triggering from a pipeline) must be handled via Databricks Jobs or Workflows

- **External Dependencies:**
  - None detected; all tables are within the same SQL Server database context

- **Areas for Business Logic Review:**
  - Validation rules (e.g., what constitutes an invalid row)
  - Audit log schema and required fields
  - Error escalation and notification mechanisms

---

# 5. Optimization Techniques

- **Spark Best Practices:**
  - Use partitioning on large tables (e.g., by `Sales_Date` or `Region_ID`) to optimize joins and writes
  - Cache dimension DataFrames (`dw.Dim_Customer`, `dw.Dim_Date`) if reused multiple times
  - Use broadcast joins for small dimension tables to speed up enrichment
  - Chain filters and joins as a single pipeline to minimize shuffles and intermediate data
  - Use window functions for any future aggregations or deduplication
  - Write to Delta Lake tables for ACID compliance and efficient upserts

- **Refactor vs. Rebuild:**
  - **Recommendation:** Refactor. The logic is straightforward and can be mapped closely to PySpark DataFrame operations. No major redesign is required, but code should be modularized for maintainability and future extensibility.

---

**API Cost:**  
apiCost: 0.012 USD