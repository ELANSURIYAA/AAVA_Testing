# ====================================================
**Author:**        Ascendion AAVA  
**Date:**          <leave it blank>  
**Description:**   Loads cleaned and validated sales transactions from staging into the sales fact table, with audit logging and data quality checks.
# ====================================================

## 1. Overview of Pipeline/Component

This stored procedure (`dw.sp_load_sales_fact`) is designed to automate the ETL process for loading sales transaction data from a staging area (`stg.Sales_Transactions`) into the enterprise data warehouse's fact table (`dw.Fact_Sales`). It performs data quality checks, logs audit information, handles errors, and archives invalid records for further review. The procedure ensures only clean, validated, and enriched sales data is loaded, supporting accurate reporting and analytics.

## 2. Component Structure and Design

- **Component:** SQL Stored Procedure (`dw.sp_load_sales_fact`)
- **Key Activities:**
  - **Audit Logging:** Inserts start and end audit records into `dw.Audit_Log`.
  - **Data Quality Validation:** Identifies and removes invalid records (missing `Customer_ID`, non-positive `Quantity`).
  - **Transformation:** Calculates `Total_Sales_Amount`, enriches with `Region_ID` and `Customer_Segment`.
  - **Fact Table Load:** Inserts cleaned and transformed data into `dw.Fact_Sales`.
  - **Archival:** Invalid records are logged in `dw.DQ_Failures`.
  - **Cleanup:** Truncates staging table and drops temp tables.
  - **Error Handling:** Updates audit log on failure and rethrows error for pipeline monitoring.
- **Integration:** Designed for orchestration within Azure Synapse pipelines, can be triggered as a pipeline activity or scheduled job.

## 3. Data Flow and Processing Logic

**Processed Datasets:**
- `stg.Sales_Transactions` (staging/source)
- `dw.Fact_Sales` (target/fact)
- `dw.Dim_Customer` (dimension)
- `dw.Dim_Date` (dimension)
- `dw.Audit_Log` (audit)
- `dw.DQ_Failures` (DQ archive)

**Data Flow:**
1. **Start Audit:** Log batch start in `dw.Audit_Log`.
2. **Validation:** Identify invalid transactions (missing customer, invalid quantity) and store in `#InvalidRows`.
3. **Cleanse:** Remove invalid records from `stg.Sales_Transactions`.
4. **Transform & Enrich:** Join with `dw.Dim_Customer` and `dw.Dim_Date`, compute `Total_Sales_Amount`, add region and segment info.
5. **Load:** Insert transformed data into `dw.Fact_Sales`.
6. **Archive:** Log invalid transactions in `dw.DQ_Failures`.
7. **End Audit:** Update audit log with row counts and status.
8. **Cleanup:** Truncate staging, drop temp tables.

## 4. Data Mapping (Lineage)

| Target Table Name | Target Column Name   | Source Table Name     | Source Column Name   | Remarks                                                                 |
|-------------------|---------------------|-----------------------|---------------------|-------------------------------------------------------------------------|
| dw.Fact_Sales     | Transaction_ID      | stg.Sales_Transactions| Transaction_ID      | 1:1 Mapping                                                            |
| dw.Fact_Sales     | Customer_ID         | stg.Sales_Transactions| Customer_ID         | 1:1 Mapping (after validation)                                          |
| dw.Fact_Sales     | Product_ID          | stg.Sales_Transactions| Product_ID          | 1:1 Mapping                                                            |
| dw.Fact_Sales     | Sales_Date          | stg.Sales_Transactions| Sales_Date          | 1:1 Mapping                                                            |
| dw.Fact_Sales     | Quantity            | stg.Sales_Transactions| Quantity            | 1:1 Mapping (after validation)                                          |
| dw.Fact_Sales     | Unit_Price          | stg.Sales_Transactions| Unit_Price          | 1:1 Mapping                                                            |
| dw.Fact_Sales     | Total_Sales_Amount  | (calculated)          | Quantity, Unit_Price| Transformation: Quantity * Unit_Price                                   |
| dw.Fact_Sales     | Region_ID           | dw.Dim_Date           | Region_ID           | Enrichment: Join on Sales_Date = Date_Value                             |
| dw.Fact_Sales     | Customer_Segment    | dw.Dim_Customer       | Customer_Segment    | Enrichment: Join on Customer_ID                                         |
| dw.Fact_Sales     | Load_Timestamp      | (system)              | SYSDATETIME()       | Transformation: Load timestamp                                          |
| dw.Fact_Sales     | Batch_ID            | (procedure variable)  | @batch_id           | Transformation: Unique batch identifier                                 |
| dw.DQ_Failures    | Transaction_ID      | #InvalidRows          | Transaction_ID      | 1:1 Mapping (invalid records)                                           |
| dw.DQ_Failures    | Failure_Reason      | #InvalidRows          | Reason              | 1:1 Mapping                                                            |
| dw.DQ_Failures    | Logged_Timestamp    | (system)              | SYSDATETIME()       | Transformation: Failure log timestamp                                   |
| dw.DQ_Failures    | Batch_ID            | (procedure variable)  | @batch_id           | Transformation: Unique batch identifier                                 |

## 5. Transformation Logic

- **Data Quality Checks:**
  - Reject rows with `Customer_ID IS NULL`
  - Reject rows with `Quantity <= 0`
- **Total_Sales_Amount:**  
  - `Total_Sales_Amount = Quantity * Unit_Price`
- **Enrichment:**
  - Join with `dw.Dim_Customer` on `Customer_ID` to get `Customer_Segment`
  - Join with `dw.Dim_Date` on `CAST(Sales_Date AS DATE) = Date_Value` to get `Region_ID`
- **Audit Logging:**
  - Insert and update audit records with batch metadata, row counts, and status.
- **Archival:**
  - Log invalid transactions and reasons in `dw.DQ_Failures`.
- **Timestamps:**
  - Use `SYSDATETIME()` for all load and log timestamps.
- **Batch ID:**
  - Use `NEWID()` to generate a unique batch identifier per run.

## 6. Complexity Analysis

| Metric                              | Value/Description                                                                 |
|--------------------------------------|-----------------------------------------------------------------------------------|
| Number of Pipeline Activities        | 1 (Stored Procedure, can be a single pipeline activity)                           |
| Number of Dataflow Transformations   | 3 (Validation, Calculation, Enrichment via Joins)                                 |
| SQL Scripts or Stored Procedures Used| 1 (this procedure)                                                                |
| Joins Used                          | 2 (INNER JOIN: `dw.Dim_Customer`, `dw.Dim_Date`)                                 |
| Lookup Tables or Reference Data      | 2 (`dw.Dim_Customer`, `dw.Dim_Date`)                                             |
| Parameters/Variables/Triggers        | 7+ (batch_id, start_time, end_time, rows_inserted, rows_rejected, error_message, proc_name) |
| Number of Output Datasets            | 2 (`dw.Fact_Sales`, `dw.DQ_Failures`)                                            |
| Conditional Logic or if-else Flows   | 2 (TRY/CATCH for error handling, validation logic for row rejection)              |
| External Dependencies                | None (all tables within same database context)                                    |
| Overall Complexity Score             | 35                                                                                |

## 7. Key Outputs

- **Fact Table:** `dw.Fact_Sales`  
  - Contains validated, enriched sales transactions with calculated fields and batch metadata.
  - Intended for enterprise reporting, analytics, and downstream BI/ML workloads.
- **Data Quality Archive:** `dw.DQ_Failures`  
  - Stores invalid transactions with failure reasons for audit and remediation.
- **Audit Log:** `dw.Audit_Log`  
  - Tracks batch execution, row counts, and status for operational monitoring.

**Output Format:**  
- All outputs are written as SQL Server tables (row-based storage).  
- No files (Parquet, CSV, etc.) are generated by this procedure.

---

**API Cost:**  
apiCost: 0.012 USD