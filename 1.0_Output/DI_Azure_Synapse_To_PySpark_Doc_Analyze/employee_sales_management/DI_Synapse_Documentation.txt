# Azure Synapse Component Documentation

---
**Author:** Ascendion AAVA  
**Date:**  
**Description:** Loads cleaned and validated sales transaction data from staging into the sales fact table, with audit logging and data quality checks.
---

## 1. Overview of Pipeline/Component

This Azure Synapse stored procedure (`dw.sp_load_sales_fact`) is designed to automate the ETL process for loading sales transaction data from a staging area (`stg.Sales_Transactions`) into the enterprise data warehouse fact table (`dw.Fact_Sales`). The procedure performs data quality validation, transformation, audit logging, and error handling to ensure only clean, reliable data is loaded for downstream analytics, reporting, and regulatory compliance.

## 2. Component Structure and Design

- **Component:** SQL Stored Procedure (`dw.sp_load_sales_fact`)
- **Key Activities:**
  - **Audit Logging:** Inserts start and end audit records in `dw.Audit_Log`.
  - **Data Quality Validation:** Identifies and removes invalid records (missing customer IDs, invalid quantities) using a temp table (`#InvalidRows`).
  - **Data Transformation:** Joins staging data with dimension tables (`dw.Dim_Customer`, `dw.Dim_Date`), computes derived fields (e.g., `Total_Sales_Amount`), and loads into `dw.Fact_Sales`.
  - **Archival/Cleanup:** Truncates staging table post-load.
  - **DQ Failure Logging:** Logs validation failures to `dw.DQ_Failures`.
  - **Error Handling:** Updates audit log on failure, rethrows error for pipeline monitoring.

- **Integration:** This stored procedure is typically invoked as a **Stored Procedure Activity** within an Azure Synapse pipeline, following a **Copy Activity** that loads data into the staging table.

- **Parameters/Variables:** Uses internal variables for batch ID, timestamps, row counts, and error messages.

- **Connection Flow:**  
  1. Data lands in `stg.Sales_Transactions` (via upstream pipeline or copy activity).
  2. Procedure validates, transforms, and loads data into `dw.Fact_Sales`.
  3. Audit and DQ logs are updated for monitoring and traceability.

## 3. Data Flow and Processing Logic

- **Processed Datasets:**
  - `stg.Sales_Transactions` (source/staging)
  - `dw.Fact_Sales` (target/fact)
  - `dw.Dim_Customer` (dimension)
  - `dw.Dim_Date` (dimension)
  - `dw.Audit_Log` (audit)
  - `dw.DQ_Failures` (DQ log)

- **Data Flow:**
  1. **Source:** Data is ingested into `stg.Sales_Transactions`.
  2. **Validation:** Invalid rows (missing customer, invalid quantity) are identified and removed.
  3. **Transformation:** Valid rows are joined with customer and date dimensions, and `Total_Sales_Amount` is calculated.
  4. **Load:** Transformed data is inserted into `dw.Fact_Sales`.
  5. **Archival:** Staging table is truncated.
  6. **DQ Logging:** Invalid rows are logged in `dw.DQ_Failures`.
  7. **Audit Logging:** Start and end of process are logged in `dw.Audit_Log`.

## 4. Data Mapping (Lineage)

| Target Table Name | Target Column Name     | Source Table Name         | Source Column Name     | Remarks                                                                                  |
|-------------------|-----------------------|--------------------------|-----------------------|------------------------------------------------------------------------------------------|
| dw.Fact_Sales     | Transaction_ID        | stg.Sales_Transactions   | Transaction_ID        | 1:1 Mapping                                                                              |
| dw.Fact_Sales     | Customer_ID           | stg.Sales_Transactions   | Customer_ID           | 1:1 Mapping (validated for non-null)                                                     |
| dw.Fact_Sales     | Product_ID            | stg.Sales_Transactions   | Product_ID            | 1:1 Mapping                                                                              |
| dw.Fact_Sales     | Sales_Date            | stg.Sales_Transactions   | Sales_Date            | 1:1 Mapping                                                                              |
| dw.Fact_Sales     | Quantity              | stg.Sales_Transactions   | Quantity              | 1:1 Mapping (validated for > 0)                                                          |
| dw.Fact_Sales     | Unit_Price            | stg.Sales_Transactions   | Unit_Price            | 1:1 Mapping                                                                              |
| dw.Fact_Sales     | Total_Sales_Amount    | (computed)               | Quantity, Unit_Price  | Transformation: `Quantity * Unit_Price`                                                  |
| dw.Fact_Sales     | Region_ID             | dw.Dim_Date              | Region_ID             | Join: `stg.Sales_Transactions.Sales_Date` to `dw.Dim_Date.Date_Value`                    |
| dw.Fact_Sales     | Customer_Segment      | dw.Dim_Customer          | Customer_Segment      | Join: `stg.Sales_Transactions.Customer_ID` to `dw.Dim_Customer.Customer_ID`              |
| dw.Fact_Sales     | Load_Timestamp        | (system)                 | (system)              | Transformation: `SYSDATETIME()`                                                          |
| dw.Fact_Sales     | Batch_ID              | (procedure variable)     | (procedure variable)  | Transformation: `@batch_id` generated per execution                                      |

## 5. Transformation Logic

- **Data Quality Checks:**
  - Reject rows where `Customer_ID` is NULL.
  - Reject rows where `Quantity` <= 0.

- **Joins:**
  - `stg.Sales_Transactions.Customer_ID` → `dw.Dim_Customer.Customer_ID`
  - `CAST(stg.Sales_Transactions.Sales_Date AS DATE)` → `dw.Dim_Date.Date_Value`

- **Derived/Computed Columns:**
  - `Total_Sales_Amount = Quantity * Unit_Price`
  - `Load_Timestamp = SYSDATETIME()`
  - `Batch_ID = NEWID()` (per execution)

- **Audit and DQ Logging:**
  - Inserts and updates to `dw.Audit_Log` and `dw.DQ_Failures` for traceability.

- **Error Handling:**
  - Catches errors, logs failure status and message, and rethrows for pipeline monitoring.

## 6. Complexity Analysis

| Metric                              | Value                                               |
|--------------------------------------|-----------------------------------------------------|
| Number of Pipeline Activities        | 1 (Stored Procedure Activity, typically in pipeline)|
| Number of Dataflow Transformations   | 3 (Validation, Join, Computed Column)               |
| SQL Scripts or Stored Procedures Used| 1 (this procedure)                                  |
| Joins Used                          | 2 (Inner Joins: Customer, Date)                     |
| Lookup Tables or Reference Data      | 2 (`dw.Dim_Customer`, `dw.Dim_Date`)                |
| Parameters/Variables/Triggers        | 7 (batch_id, start_time, end_time, etc.)            |
| Number of Output Datasets           | 3 (`dw.Fact_Sales`, `dw.Audit_Log`, `dw.DQ_Failures`)|
| Conditional Logic or if-else Flows   | 2 (TRY/CATCH, validation logic)                     |
| External Dependencies                | 0 (all within Synapse SQL; expects upstream pipeline to load staging) |
| Overall Complexity Score             | 40                                                  |

## 7. Key Outputs

- **Fact Table:** `dw.Fact_Sales` (main output; used for analytics, reporting, ML, etc.)
- **Audit Log:** `dw.Audit_Log` (tracks ETL execution, row counts, status)
- **DQ Failures:** `dw.DQ_Failures` (captures rejected rows and reasons for data quality monitoring)
- **Format:** All outputs are in SQL tables (row-based, not file-based).
- **Intended Use:** Enables reliable, auditable sales analytics and downstream data consumption.

---

**API Cost:** apiCost: 0.0025 USD