# ====================================================
**Author:**        Ascendion AAVA  
**Date:**          <!-- leave it blank -->  
**Description:**   Loads cleaned and validated sales transaction data from staging into the sales fact table, with audit logging and data quality checks.
# ====================================================

## 1. Overview of Pipeline/Component

This Azure Synapse stored procedure (`dw.sp_load_sales_fact`) orchestrates the ETL process for loading sales transaction data from a staging table into the enterprise data warehouse's sales fact table. It enforces data quality by validating incoming records, logs audit information for traceability, and maintains a record of data quality failures. The procedure ensures only clean, enriched, and business-rule-compliant data is loaded into the analytics-ready fact table, supporting downstream reporting, regulatory, and analytics requirements.

## 2. Component Structure and Design

- **Component:** SQL Stored Procedure (`dw.sp_load_sales_fact`)
- **Key Activities:**
    - Audit logging at start and end of the process
    - Creation of a temporary table for invalid rows
    - Data quality checks (missing Customer ID, invalid Quantity)
    - Deletion of invalid records from staging
    - Data transformation and enrichment (joins with dimension tables, calculation of total sales amount)
    - Insertion into the fact table
    - Staging table truncation
    - Logging of data quality failures
    - Error handling with audit log updates

- **Logical Grouping:**
    - **Source:** `stg.Sales_Transactions` (staging table)
    - **Transformations:** Joins with `dw.Dim_Customer` and `dw.Dim_Date`, calculation of derived fields
    - **Audit & DQ Logging:** `dw.Audit_Log`, `dw.DQ_Failures`
    - **Target/Sink:** `dw.Fact_Sales` (fact table)

- **Parameters/Variables:** Batch ID, timestamps, row counters, error message, procedure name
- **Triggers:** Typically invoked by a Synapse pipeline activity (not shown in code)
- **Connection Flow:** Data flows from staging to fact table, with intermediate validation and enrichment steps.

## 3. Data Flow and Processing Logic

- **Processed Datasets:**
    - `stg.Sales_Transactions`
    - `dw.Dim_Customer`
    - `dw.Dim_Date`
    - `dw.Fact_Sales`
    - `dw.Audit_Log`
    - `dw.DQ_Failures`

- **Data Flow:**
    1. **Audit Log Start:** Insert a new batch record in `dw.Audit_Log`.
    2. **Validation:** Identify and log invalid records (missing Customer ID, Quantity <= 0) into a temp table.
    3. **Data Cleansing:** Remove invalid records from `stg.Sales_Transactions`.
    4. **Transformation & Enrichment:** Join cleaned staging data with customer and date dimensions; calculate `Total_Sales_Amount`.
    5. **Load:** Insert transformed records into `dw.Fact_Sales`.
    6. **Staging Cleanup:** Truncate staging table.
    7. **DQ Failure Logging:** Insert invalid records into `dw.DQ_Failures`.
    8. **Audit Log End:** Update batch record with row counts and status.
    9. **Error Handling:** On failure, update audit log with error message.

## 4. Data Mapping (Lineage)

| Target Table Name | Target Column Name     | Source Table Name      | Source Column Name      | Remarks                                                                 |
|-------------------|-----------------------|------------------------|------------------------|-------------------------------------------------------------------------|
| dw.Fact_Sales     | Transaction_ID        | stg.Sales_Transactions | Transaction_ID         | 1:1 Mapping                                                            |
| dw.Fact_Sales     | Customer_ID           | stg.Sales_Transactions | Customer_ID            | 1:1 Mapping (after validation, must not be NULL)                        |
| dw.Fact_Sales     | Product_ID            | stg.Sales_Transactions | Product_ID             | 1:1 Mapping                                                            |
| dw.Fact_Sales     | Sales_Date            | stg.Sales_Transactions | Sales_Date             | 1:1 Mapping                                                            |
| dw.Fact_Sales     | Quantity              | stg.Sales_Transactions | Quantity               | 1:1 Mapping (after validation, must be > 0)                             |
| dw.Fact_Sales     | Unit_Price            | stg.Sales_Transactions | Unit_Price             | 1:1 Mapping                                                            |
| dw.Fact_Sales     | Total_Sales_Amount    | (calculated)           | Quantity, Unit_Price   | Transformation: Quantity * Unit_Price                                   |
| dw.Fact_Sales     | Region_ID             | dw.Dim_Date            | Region_ID              | Enrichment: Join on Sales_Date = Date_Value                             |
| dw.Fact_Sales     | Customer_Segment      | dw.Dim_Customer        | Customer_Segment       | Enrichment: Join on Customer_ID                                         |
| dw.Fact_Sales     | Load_Timestamp        | (generated)            | (system)               | Transformation: SYSDATETIME()                                           |
| dw.Fact_Sales     | Batch_ID              | (generated)            | (procedure variable)   | Transformation: Unique batch ID per run                                 |

## 5. Transformation Logic

- **Total_Sales_Amount:**  
  `Quantity * Unit_Price` — computes the total sales value for each transaction.
- **Region_ID:**  
  Derived by joining `stg.Sales_Transactions.Sales_Date` (cast as DATE) to `dw.Dim_Date.Date_Value`.
- **Customer_Segment:**  
  Derived by joining `stg.Sales_Transactions.Customer_ID` to `dw.Dim_Customer.Customer_ID`.
- **Load_Timestamp:**  
  Set to `SYSDATETIME()` at load time.
- **Batch_ID:**  
  Set to a new unique identifier per procedure execution.
- **Validation Logic:**  
  - Reject rows where `Customer_ID IS NULL`
  - Reject rows where `Quantity <= 0`
- **Audit Logging:**  
  Inserts and updates to `dw.Audit_Log` track process status, row counts, and error messages.
- **DQ Failure Logging:**  
  Invalid rows and reasons are logged to `dw.DQ_Failures`.

## 6. Complexity Analysis

| Metric                                | Value / Description                                                                 |
|----------------------------------------|-------------------------------------------------------------------------------------|
| Number of Pipeline Activities          | 1 (Stored Procedure, typically invoked by a pipeline activity)                      |
| Number of Dataflow Transformations     | 3 (Validation, Join with Customer, Join with Date, Derived Column)                  |
| SQL Scripts or Stored Procedures Used  | 1 (this procedure)                                                                  |
| Joins Used                            | 2 (Inner Join: Customer, Inner Join: Date)                                          |
| Lookup Tables or Reference Data        | 2 (`dw.Dim_Customer`, `dw.Dim_Date`)                                                |
| Parameters/Variables/Triggers         | 7+ (Batch ID, timestamps, row counters, error message, proc name, etc.)             |
| Number of Output Datasets             | 3 (`dw.Fact_Sales`, `dw.Audit_Log`, `dw.DQ_Failures`)                               |
| Conditional Logic or if-else Flows    | 2 (Validation checks, Error handling)                                               |
| External Dependencies                 | None explicit; relies on Synapse SQL pools and referenced tables                    |
| Overall Complexity Score              | 55                                                                                  |

## 7. Key Outputs

- **Fact Table:**  
  `dw.Fact_Sales` — Contains validated, enriched, and transformed sales transactions for analytics and reporting.
- **Audit Log:**  
  `dw.Audit_Log` — Records batch execution metadata, row counts, status, and messages for traceability.
- **Data Quality Failures:**  
  `dw.DQ_Failures` — Captures invalid records and reasons for downstream review and remediation.

**Output Format:** All outputs are written as SQL tables within the Synapse dedicated SQL pool. Intended uses include business intelligence reporting, regulatory submissions, and as a trusted data source for downstream analytics or machine learning models.

---

**API Cost:**  
apiCost: 0.018 USD