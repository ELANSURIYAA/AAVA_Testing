=============================================
Author:        Ascendion AVA+
Date:   
Description:   Conversion plan and cost/effort estimation for migrating SSIS Package EDW_BC_Load_DimBillingAccount to PySpark on GCP Databricks.
=============================================

1. Cost Estimation

   2.1 PySpark Runtime Cost 

   - **Data Volume Processed:** 
     - DimBillingAccount: ~700 GB × 10% = 70 GB processed per run
     - Audit: ~50 GB × 10% = 5 GB
     - AuditReference: ~20 GB × 10% = 2 GB
     - ErrorLog: ~30 GB × 10% = 3 GB
     - **Total processed per run:** ~80 GB

   - **Databricks DBU Cost (GCP, Enterprise):** $0.15 - $0.75 per DBU/hour (using $0.22 as standard job cluster per input)
   - **Estimated Job Duration:** For a moderately complex ETL with multiple joins, lookups, and transformations on 80GB, a typical job would run for **1.5 to 2 hours** on a medium (8-16 vCPU) cluster.
   - **Typical DBU Consumption:** 8 DBUs/hour × 2 hours = 16 DBU-hours per run

   - **Compute Cost:** 16 DBU-hours × $0.22 = **$3.52 per run**
   - **Storage Cost:** 80 GB processed, but only incremental storage used. At $18.40/TB/month, incremental cost for 80GB is negligible per run.
   - **Total Estimated PySpark Runtime Cost per run:** **$3.52 USD**

---

2. Code Fixing and Testing Effort Estimation

   2.1 PySpark identified manual code fixes effort in hours covering the various temp tables, calculations

   - **Manual Conversion Areas (from analysis):**
     - Row count tracking (accumulators, DataFrame actions)
     - Event/error handling (try/except, logging)
     - Variable management (Python variables, accumulators)
     - Complex SQL extraction (multi-join, CTE to DataFrame logic)
     - Conditional split logic (when/otherwise)
     - Derived columns (concatenation, CASE to when/otherwise)
     - Data type conversions (SQL to Spark types)
     - Audit logging and parameterization
     - Data recon/testing (validation of business rules, counts, and mappings)

   - **Effort Estimation:**
     - Manual code fixes (PySpark logic for above): **18 hours**
     - Data reconciliation & testing (unit, integration, user acceptance): **10 hours**
     - **Total Effort:** **28 hours**

---

apiCost: 0.00000000 USD

---