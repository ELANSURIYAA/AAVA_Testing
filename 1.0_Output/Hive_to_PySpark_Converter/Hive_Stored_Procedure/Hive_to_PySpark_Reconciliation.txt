# Comprehensive Python Script for Hive to PySpark Migration and Validation

```python
import os
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum
from pyhive import hive
import pyarrow.parquet as pq
from datetime import datetime

# Environment variables for credentials
HIVE_HOST = os.getenv("HIVE_HOST")
HIVE_PORT = os.getenv("HIVE_PORT", 10000)
HIVE_USERNAME = os.getenv("HIVE_USERNAME")
HIVE_PASSWORD = os.getenv("HIVE_PASSWORD")
DATABRICKS_HOST = os.getenv("DATABRICKS_HOST")
DATABRICKS_TOKEN = os.getenv("DATABRICKS_TOKEN")
DATABRICKS_STORAGE_PATH = os.getenv("DATABRICKS_STORAGE_PATH")

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Hive to PySpark Migration Validation") \
    .getOrCreate()

def execute_hive_query(query):
    """Execute Hive query and return results."""
    conn = hive.Connection(host=HIVE_HOST, port=HIVE_PORT, username=HIVE_USERNAME, password=HIVE_PASSWORD)
    cursor = conn.cursor()
    cursor.execute(query)
    results = cursor.fetchall()
    conn.close()
    return results

def export_hive_table_to_parquet(table_name, output_path):
    """Export Hive table to Parquet format."""
    query = f"SELECT * FROM {table_name}"
    results = execute_hive_query(query)
    df = pd.DataFrame(results)
    parquet_file = os.path.join(output_path, f"{table_name}_{datetime.now().strftime('%Y%m%d%H%M%S')}.parquet")
    df.to_parquet(parquet_file, engine='pyarrow')
    return parquet_file

def transfer_to_databricks(parquet_files):
    """Transfer Parquet files to Databricks storage."""
    for file in parquet_files:
        # Assuming Databricks CLI or API is configured for file transfer
        os.system(f"databricks fs cp {file} {DATABRICKS_STORAGE_PATH}")

def create_external_table_in_pyspark(parquet_file, table_name):
    """Create external table in PySpark."""
    df = spark.read.parquet(parquet_file)
    df.createOrReplaceTempView(table_name)

def compare_tables(hive_table, pyspark_table):
    """Compare Hive table with PySpark table."""
    hive_results = execute_hive_query(f"SELECT * FROM {hive_table}")
    pyspark_results = spark.sql(f"SELECT * FROM {pyspark_table}").collect()
    hive_df = pd.DataFrame(hive_results)
    pyspark_df = pd.DataFrame(pyspark_results)
    comparison = hive_df.equals(pyspark_df)
    return comparison

def generate_report(comparisons):
    """Generate comparison report."""
    report = []
    for table, status in comparisons.items():
        report.append(f"Table: {table}, Match Status: {'MATCH' if status else 'NO MATCH'}")
    return "\n".join(report)

def main():
    # Hive SQL code
    hive_sql_code = """
    CREATE PROCEDURE process_sales_data(
        IN start_date STRING,
        IN end_date STRING
    )
    BEGIN
        DECLARE total_sales FLOAT;
    
        SET @dynamic_query = CONCAT(
            "INSERT INTO summary_table SELECT product_id, SUM(sales) AS total_sales ",
            "FROM sales_table WHERE sale_date BETWEEN '", start_date, "' AND '", end_date, "' ",
            "GROUP BY product_id"
        );
        EXECUTE IMMEDIATE @dynamic_query;
    
        CREATE TEMPORARY TABLE temp_sales_summary AS
        SELECT product_id, SUM(sales) AS total_sales
        FROM sales_table
        WHERE sale_date BETWEEN start_date AND end_date
        GROUP BY product_id;
    
        DECLARE cur CURSOR FOR SELECT product_id, total_sales FROM temp_sales_summary;
    
        OPEN cur;
    
        FETCH cur INTO product_id, total_sales;
        WHILE total_sales IS NOT NULL DO
            INSERT INTO detailed_sales_summary (product_id, total_sales)
            VALUES (product_id, total_sales);
            FETCH cur INTO product_id, total_sales;
        END WHILE;
    
        CLOSE cur;
    
        DROP TABLE temp_sales_summary;
    END;
    """

    # PySpark code
    def process_sales_data(start_date, end_date):
        sales_table = spark.read.format("parquet").load("path_to_sales_table")
        summary_table = sales_table.filter(
            (col("sale_date") >= start_date) & (col("sale_date") <= end_date)
        ).groupBy("product_id").agg(
            _sum("sales").alias("total_sales")
        )
        summary_table.write.format("parquet").mode("overwrite").save("path_to_summary_table")
        temp_sales_summary = sales_table.filter(
            (col("sale_date") >= start_date) & (col("sale_date") <= end_date)
        ).groupBy("product_id").agg(
            _sum("sales").alias("total_sales")
        )
        detailed_sales_summary = []
        for row in temp_sales_summary.collect():
            detailed_sales_summary.append((row["product_id"], row["total_sales"]))
        detailed_sales_summary_df = spark.createDataFrame(
            detailed_sales_summary, ["product_id", "total_sales"]
        )
        detailed_sales_summary_df.write.format("parquet").mode("overwrite").save("path_to_detailed_sales_summary")
        temp_sales_summary.unpersist()

    # Export Hive tables
    hive_tables = ["sales_table", "summary_table", "detailed_sales_summary"]
    parquet_files = [export_hive_table_to_parquet(table, "/tmp") for table in hive_tables]

    # Transfer to Databricks
    transfer_to_databricks(parquet_files)

    # Create external tables in PySpark
    for file, table in zip(parquet_files, hive_tables):
        create_external_table_in_pyspark(file, table)

    # Execute PySpark code
    process_sales_data("2023-01-01", "2023-01-31")

    # Compare tables
    comparisons = {table: compare_tables(table, table) for table in hive_tables}

    # Generate report
    report = generate_report(comparisons)
    print(report)

if __name__ == "__main__":
    main()
```