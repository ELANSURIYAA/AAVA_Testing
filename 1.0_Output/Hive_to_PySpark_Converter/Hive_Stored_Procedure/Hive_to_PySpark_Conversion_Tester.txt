Test Case List:

1. Test Case ID: TC001
   Test Case Description: Verify that the function filters and aggregates sales data correctly for a valid date range.
   Expected Outcome: The aggregated DataFrame should contain the correct total sales for each product ID within the specified date range.

2. Test Case ID: TC002
   Test Case Description: Test the function with an empty sales_table.
   Expected Outcome: The resulting summary and detailed sales summary tables should be empty.

3. Test Case ID: TC003
   Test Case Description: Test the function with null values in the sales column.
   Expected Outcome: Null values should not affect the aggregation, and the total sales should be calculated correctly.

4. Test Case ID: TC004
   Test Case Description: Verify that the temporary view is created and dropped correctly.
   Expected Outcome: The temporary view should exist during processing and be dropped after processing.

5. Test Case ID: TC005
   Test Case Description: Test the function with invalid date formats for start_date and end_date.
   Expected Outcome: The function should raise an appropriate exception.

6. Test Case ID: TC006
   Test Case Description: Test the function with boundary date conditions (e.g., start_date = end_date).
   Expected Outcome: The function should process sales data for the single day specified.

7. Test Case ID: TC007
   Test Case Description: Verify that data is written to the summary_table and detailed_sales_summary tables in the correct mode.
   Expected Outcome: Data should be written in "overwrite" mode for summary_table and "append" mode for detailed_sales_summary.

Pytest Script:

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.utils import AnalysisException
from process_sales_data import process_sales_data

@pytest.fixture(scope="module")
def spark():
    """Fixture for creating a SparkSession."""
    spark = SparkSession.builder \
        .appName("Test Process Sales Data") \
        .master("local[*]") \
        .enableHiveSupport() \
        .getOrCreate()
    yield spark
    spark.stop()

def test_filter_and_aggregate(spark):
    """Test filtering and aggregation of sales data."""
    # Create a sample sales_table
    sales_data = [
        ("2023-01-01", 1, 100.0),
        ("2023-01-02", 1, 200.0),
        ("2023-01-03", 2, 150.0),
    ]
    sales_df = spark.createDataFrame(sales_data, ["sale_date", "product_id", "sales"])
    sales_df.createOrReplaceTempView("sales_table")

    # Call the function
    process_sales_data(spark, "2023-01-01", "2023-01-02")

    # Verify the results
    result_df = spark.table("summary_table")
    expected_data = [(1, 300.0)]
    expected_df = spark.createDataFrame(expected_data, ["product_id", "total_sales"])
    assert result_df.collect() == expected_df.collect()

def test_empty_sales_table(spark):
    """Test with an empty sales_table."""
    # Create an empty sales_table
    sales_df = spark.createDataFrame([], ["sale_date", "product_id", "sales"])
    sales_df.createOrReplaceTempView("sales_table")

    # Call the function
    process_sales_data(spark, "2023-01-01", "2023-01-31")

    # Verify the results
    result_df = spark.table("summary_table")
    assert result_df.count() == 0

def test_null_sales_values(spark):
    """Test with null values in the sales column."""
    # Create a sales_table with null sales
    sales_data = [
        ("2023-01-01", 1, None),
        ("2023-01-02", 1, 200.0),
    ]
    sales_df = spark.createDataFrame(sales_data, ["sale_date", "product_id", "sales"])
    sales_df.createOrReplaceTempView("sales_table")

    # Call the function
    process_sales_data(spark, "2023-01-01", "2023-01-02")

    # Verify the results
    result_df = spark.table("summary_table")
    expected_data = [(1, 200.0)]
    expected_df = spark.createDataFrame(expected_data, ["product_id", "total_sales"])
    assert result_df.collect() == expected_df.collect()

def test_temporary_view(spark):
    """Verify temporary view creation and dropping."""
    # Create a sample sales_table
    sales_data = [
        ("2023-01-01", 1, 100.0),
    ]
    sales_df = spark.createDataFrame(sales_data, ["sale_date", "product_id", "sales"])
    sales_df.createOrReplaceTempView("sales_table")

    # Call the function
    process_sales_data(spark, "2023-01-01", "2023-01-01")

    # Verify the temporary view is dropped
    with pytest.raises(AnalysisException):
        spark.table("temp_sales_summary")

def test_invalid_date_format(spark):
    """Test with invalid date formats."""
    # Create a sample sales_table
    sales_data = [
        ("2023-01-01", 1, 100.0),
    ]
    sales_df = spark.createDataFrame(sales_data, ["sale_date", "product_id", "sales"])
    sales_df.createOrReplaceTempView("sales_table")

    # Call the function with invalid dates
    with pytest.raises(Exception):
        process_sales_data(spark, "invalid_date", "2023-01-01")

def test_boundary_date_conditions(spark):
    """Test with boundary date conditions."""
    # Create a sample sales_table
    sales_data = [
        ("2023-01-01", 1, 100.0),
    ]
    sales_df = spark.createDataFrame(sales_data, ["sale_date", "product_id", "sales"])
    sales_df.createOrReplaceTempView("sales_table")

    # Call the function
    process_sales_data(spark, "2023-01-01", "2023-01-01")

    # Verify the results
    result_df = spark.table("summary_table")
    expected_data = [(1, 100.0)]
    expected_df = spark.createDataFrame(expected_data, ["product_id", "total_sales"])
    assert result_df.collect() == expected_df.collect()

def test_write_modes(spark):
    """Verify write modes for summary_table and detailed_sales_summary."""
    # Create a sample sales_table
    sales_data = [
        ("2023-01-01", 1, 100.0),
    ]
    sales_df = spark.createDataFrame(sales_data, ["sale_date", "product_id", "sales"])
    sales_df.createOrReplaceTempView("sales_table")

    # Call the function
    process_sales_data(spark, "2023-01-01", "2023-01-01")

    # Verify data is written in the correct mode
    summary_df = spark.table("summary_table")
    detailed_df = spark.table("detailed_sales_summary")
    assert summary_df.count() > 0
    assert detailed_df.count() > 0
```

Cost consumed by the API for this call: Minimal cost for file listing, file reading, and delegation actions.