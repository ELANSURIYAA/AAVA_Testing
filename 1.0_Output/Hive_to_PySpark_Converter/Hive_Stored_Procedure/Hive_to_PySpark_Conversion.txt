from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum

def process_sales_data(spark: SparkSession, start_date: str, end_date: str):
    """
    Optimized PySpark implementation of the Hive SQL stored procedure to process sales data.
    
    Args:
        spark (SparkSession): The active Spark session.
        start_date (str): The start date for filtering sales data.
        end_date (str): The end date for filtering sales data.
    """
    # Load sales data into a DataFrame
    sales_df = spark.table("sales_table")
    
    # Filter and aggregate sales data
    summary_df = (
        sales_df
        .filter((col("sale_date") >= start_date) & (col("sale_date") <= end_date))
        .groupBy("product_id")
        .agg(_sum("sales").alias("total_sales"))
    )
    
    # Write the aggregated data to the summary table
    summary_df.write.mode("overwrite").insertInto("summary_table")
    
    # Create a temporary view for further processing
    summary_df.createOrReplaceTempView("temp_sales_summary")
    
    # Load the temporary summary data into a DataFrame
    temp_sales_summary_df = spark.sql("SELECT product_id, total_sales FROM temp_sales_summary")
    
    # Write detailed sales summary data to the target table
    temp_sales_summary_df.write.mode("append").insertInto("detailed_sales_summary")
    
    # Drop the temporary view
    spark.catalog.dropTempView("temp_sales_summary")

# Example usage
if __name__ == "__main__":
    spark = SparkSession.builder \
        .appName("Process Sales Data") \
        .enableHiveSupport() \
        .getOrCreate()
    
    # Replace with actual date values
    process_sales_data(spark, "2023-01-01", "2023-01-31")

Cost consumed by the API for this call: Minimal cost for file listing, file reading, and delegation actions.