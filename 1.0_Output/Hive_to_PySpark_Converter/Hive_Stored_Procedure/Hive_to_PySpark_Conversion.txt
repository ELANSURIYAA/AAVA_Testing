from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum

def process_sales_data(spark, start_date, end_date):
    """
    This function processes sales data for a given date range and writes the results
    into summary_table and detailed_sales_summary tables.

    :param spark: SparkSession object
    :param start_date: Start date of the range (string in 'YYYY-MM-DD' format)
    :param end_date: End date of the range (string in 'YYYY-MM-DD' format)
    """
    # Load sales data
    sales_table = spark.read.table("sales_table")

    # Filter sales data for the given date range and calculate total sales per product
    temp_sales_summary = (
        sales_table
        .filter((col("sale_date") >= start_date) & (col("sale_date") <= end_date))
        .groupBy("product_id")
        .agg(_sum("sales").alias("total_sales"))
    )

    # Write the summary data to the summary_table
    temp_sales_summary.write.mode("overwrite").saveAsTable("summary_table")

    # Collect the data to insert into detailed_sales_summary
    detailed_sales_data = temp_sales_summary.collect()

    # Write to detailed_sales_summary table
    for row in detailed_sales_data:
        product_id = row["product_id"]
        total_sales = row["total_sales"]

        # Insert into detailed_sales_summary table
        spark.sql(f"""
            INSERT INTO detailed_sales_summary (product_id, total_sales)
            VALUES ('{product_id}', {total_sales})
        """)

# Example usage
if __name__ == "__main__":
    spark = SparkSession.builder \
        .appName("Process Sales Data") \
        .enableHiveSupport() \
        .getOrCreate()

    # Define the date range
    start_date = "2023-01-01"
    end_date = "2023-01-31"

    # Process sales data
    process_sales_data(spark, start_date, end_date)

Cost consumed by the API for this call: 1 API call for reading the file and 1 API call for delegating the work.