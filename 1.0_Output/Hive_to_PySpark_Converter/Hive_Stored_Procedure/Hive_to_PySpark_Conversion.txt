from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Process Sales Data") \
    .getOrCreate()

def process_sales_data(start_date, end_date):
    # Step 1: Load sales_table data
    sales_table = spark.read.format("parquet").load("path_to_sales_table")

    # Step 2: Filter data based on date range and compute total sales
    summary_table = sales_table.filter(
        (col("sale_date") >= start_date) & (col("sale_date") <= end_date)
    ).groupBy("product_id").agg(
        _sum("sales").alias("total_sales")
    )

    # Step 3: Save summary_table to a persistent storage
    summary_table.write.format("parquet").mode("overwrite").save("path_to_summary_table")

    # Step 4: Create a temporary DataFrame for detailed sales summary
    temp_sales_summary = sales_table.filter(
        (col("sale_date") >= start_date) & (col("sale_date") <= end_date)
    ).groupBy("product_id").agg(
        _sum("sales").alias("total_sales")
    )

    # Step 5: Iterate through temp_sales_summary and insert into detailed_sales_summary
    detailed_sales_summary = []
    for row in temp_sales_summary.collect():
        detailed_sales_summary.append((row["product_id"], row["total_sales"]))

    # Convert the list to a DataFrame
    detailed_sales_summary_df = spark.createDataFrame(
        detailed_sales_summary, ["product_id", "total_sales"]
    )

    # Save detailed_sales_summary_df to a persistent storage
    detailed_sales_summary_df.write.format("parquet").mode("overwrite").save("path_to_detailed_sales_summary")

    # Step 6: Drop temporary DataFrame (no explicit drop needed in PySpark)
    temp_sales_summary.unpersist()

# Example usage
process_sales_data("2023-01-01", "2023-01-31")

# Stop Spark session
spark.stop()

# API Cost: 0.02 USD