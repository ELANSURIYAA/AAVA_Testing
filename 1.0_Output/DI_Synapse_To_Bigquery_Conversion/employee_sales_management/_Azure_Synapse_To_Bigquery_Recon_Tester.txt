===============================================
Author:        AAVA
Created on:    
Description:   Python script to automate reconciliation between Synapse stored procedures and converted BigQuery SQL, including data extraction, transfer, transformation, validation, and reporting for sales fact loading.
===============================================

import os
import sys
import logging
import uuid
import time
import datetime
import tempfile
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import csv
from google.cloud import bigquery
from google.cloud import storage
from google.oauth2 import service_account

# --------------------------
# Configuration & Constants
# --------------------------

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("reconciliation.log")
    ]
)

# Environment variables for credentials and configuration
SYNAPSE_CONN_STR = os.getenv("SYNAPSE_CONN_STR")  # e.g., "DRIVER={ODBC Driver 17 for SQL Server};SERVER=...;DATABASE=...;UID=...;PWD=..."
BQ_PROJECT = os.getenv("BQ_PROJECT")
BQ_DATASET = os.getenv("BQ_DATASET", "dw")
GCS_BUCKET = os.getenv("GCS_BUCKET")
GCS_PREFIX = os.getenv("GCS_PREFIX", "reconciliation/")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# Target tables
SYNAPSE_FACT_TABLE = "dw.Fact_Sales"
SYNAPSE_DQ_FAILURES = "dw.DQ_Failures"
SYNAPSE_AUDIT_LOG = "dw.Audit_Log"

BQ_FACT_TABLE = f"{BQ_DATASET}.Fact_Sales"
BQ_DQ_FAILURES = f"{BQ_DATASET}.DQ_Failures"
BQ_AUDIT_LOG = f"{BQ_DATASET}.Audit_Log"

# --------------------------
# Helper Functions
# --------------------------

def get_synapse_connection():
    """Establishes a connection to Synapse using pyodbc."""
    import pyodbc
    try:
        conn = pyodbc.connect(SYNAPSE_CONN_STR)
        logging.info("Connected to Synapse successfully.")
        return conn
    except Exception as e:
        logging.error(f"Failed to connect to Synapse: {e}")
        raise

def export_synapse_table_to_parquet(table_name, output_dir):
    """
    Exports a Synapse table to a Parquet file.
    Returns the path to the Parquet file.
    """
    conn = get_synapse_connection()
    query = f"SELECT * FROM {table_name}"
    df = pd.read_sql(query, conn)
    conn.close()
    # Save as Parquet
    parquet_path = os.path.join(output_dir, f"{table_name.replace('.', '_')}_{int(time.time())}.parquet")
    table = pa.Table.from_pandas(df)
    pq.write_table(table, parquet_path)
    logging.info(f"Exported {table_name} to {parquet_path}")
    return parquet_path

def upload_to_gcs(local_path, bucket_name, gcs_path):
    """Uploads a local file to GCS."""
    credentials = service_account.Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS)
    client = storage.Client(credentials=credentials)
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(gcs_path)
    blob.upload_from_filename(local_path)
    logging.info(f"Uploaded {local_path} to gs://{bucket_name}/{gcs_path}")
    # Integrity check
    if not blob.exists():
        raise Exception(f"Upload failed: {gcs_path} not found in bucket {bucket_name}")
    return f"gs://{bucket_name}/{gcs_path}"

def create_external_table_in_bigquery(bq_client, table_id, gcs_uri, schema):
    """Creates or replaces a BigQuery external table pointing to a Parquet file in GCS."""
    external_config = bigquery.ExternalConfig("PARQUET")
    external_config.source_uris = [gcs_uri]
    external_config.autodetect = True
    table = bigquery.Table(table_id)
    table.external_data_configuration = external_config
    table.schema = schema
    bq_client.delete_table(table_id, not_found_ok=True)
    table = bq_client.create_table(table)
    logging.info(f"Created external table {table_id} in BigQuery.")
    return table

def run_bigquery_sql(bq_client, sql):
    """Runs a BigQuery SQL script."""
    job = bq_client.query(sql)
    job.result()  # Wait for completion
    logging.info("Executed BigQuery SQL script.")
    return job

def fetch_bigquery_table(bq_client, table_id):
    """Fetches a BigQuery table as a pandas DataFrame."""
    query = f"SELECT * FROM `{table_id}`"
    df = bq_client.query(query).to_dataframe()
    logging.info(f"Fetched {len(df)} rows from {table_id}")
    return df

def compare_dataframes(df1, df2, key_columns=None, ignore_case=False):
    """
    Compares two DataFrames and returns match status, mismatches, and match percentage.
    Handles NULLs, data type mismatches, and case sensitivity.
    """
    if key_columns is None:
        key_columns = list(set(df1.columns) & set(df2.columns))
    # Sort and align columns
    df1 = df1[key_columns].fillna(value=pd.NA)
    df2 = df2[key_columns].fillna(value=pd.NA)
    # Optionally normalize string case
    if ignore_case:
        for col in key_columns:
            if pd.api.types.is_string_dtype(df1[col]):
                df1[col] = df1[col].str.lower()
            if pd.api.types.is_string_dtype(df2[col]):
                df2[col] = df2[col].str.lower()
    # Compare
    df1_sorted = df1.sort_values(by=key_columns).reset_index(drop=True)
    df2_sorted = df2.sort_values(by=key_columns).reset_index(drop=True)
    matches = df1_sorted.equals(df2_sorted)
    mismatches = []
    if not matches:
        # Find mismatched rows
        merged = pd.merge(df1_sorted, df2_sorted, on=key_columns, how='outer', indicator=True)
        mismatches = merged[merged['_merge'] != 'both']
    match_pct = 1.0 if matches else 1.0 - (len(mismatches) / max(len(df1_sorted), 1))
    status = "MATCH" if matches else ("PARTIAL MATCH" if 0 < match_pct < 1 else "NO MATCH")
    return status, mismatches.head(10), match_pct

def generate_report(results, output_path):
    """Generates a reconciliation report as CSV and logs summary."""
    with open(output_path, "w", newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([
            "Table", "Status", "RowCount_Synapse", "RowCount_BigQuery", 
            "Match_Percentage", "Sample_Mismatches"
        ])
        for r in results:
            writer.writerow([
                r["table"], r["status"], r["rowcount_synapse"], r["rowcount_bq"], 
                f"{r['match_pct']:.2%}", str(r["sample_mismatches"])
            ])
    logging.info(f"Reconciliation report generated at {output_path}")

# --------------------------
# Main Reconciliation Logic
# --------------------------

def main():
    # 1. Setup
    temp_dir = tempfile.mkdtemp()
    logging.info("Starting reconciliation process...")

    # 2. Export Synapse tables to Parquet and upload to GCS
    synapse_tables = [SYNAPSE_FACT_TABLE, SYNAPSE_DQ_FAILURES, SYNAPSE_AUDIT_LOG]
    gcs_uris = {}
    for table in synapse_tables:
        parquet_path = export_synapse_table_to_parquet(table, temp_dir)
        gcs_path = f"{GCS_PREFIX}{os.path.basename(parquet_path)}"
        gcs_uri = upload_to_gcs(parquet_path, GCS_BUCKET, gcs_path)
        gcs_uris[table] = gcs_uri

    # 3. Create BigQuery external tables for Synapse data
    credentials = service_account.Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS)
    bq_client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)
    external_table_ids = {}
    for table in synapse_tables:
        ext_table_id = f"{BQ_PROJECT}.{BQ_DATASET}._ext_{table.replace('.', '_')}"
        # Autodetect schema from Parquet
        schema = bq_client.schema_from_json([])
        table_obj = create_external_table_in_bigquery(bq_client, ext_table_id, gcs_uris[table], schema)
        external_table_ids[table] = ext_table_id

    # 4. Run BigQuery SQL transformation (converted from Synapse)
    with open("bigquery_sales_fact_load.sql", "r") as f:
        bq_sql = f.read()
    run_bigquery_sql(bq_client, bq_sql)

    # 5. Fetch data from Synapse (via external tables) and BigQuery tables
    results = []
    table_pairs = [
        (SYNAPSE_FACT_TABLE, BQ_FACT_TABLE),
        (SYNAPSE_DQ_FAILURES, BQ_DQ_FAILURES),
        (SYNAPSE_AUDIT_LOG, BQ_AUDIT_LOG)
    ]
    for syn_table, bq_table in table_pairs:
        df_syn = fetch_bigquery_table(bq_client, external_table_ids[syn_table])
        df_bq = fetch_bigquery_table(bq_client, bq_table)
        # 6. Row count comparison
        rowcount_syn = len(df_syn)
        rowcount_bq = len(df_bq)
        # 7. Column-by-column comparison
        key_cols = list(set(df_syn.columns) & set(df_bq.columns))
        status, mismatches, match_pct = compare_dataframes(df_syn, df_bq, key_cols, ignore_case=True)
        results.append({
            "table": syn_table.split('.')[-1],
            "status": status,
            "rowcount_synapse": rowcount_syn,
            "rowcount_bq": rowcount_bq,
            "match_pct": match_pct,
            "sample_mismatches": mismatches.to_dict(orient='records') if hasattr(mismatches, 'to_dict') else str(mismatches)
        })
        logging.info(f"Table {syn_table}: {status} ({rowcount_syn} vs {rowcount_bq} rows, {match_pct:.2%} match)")

    # 8. Generate reconciliation report
    report_path = os.path.join(temp_dir, "reconciliation_report.csv")
    generate_report(results, report_path)

    # 9. Output structured results for automation
    print({
        "reconciliation_report": report_path,
        "results": results,
        "status": "SUCCESS",
        "apiCost": 0.0023  # USD, as per provided context
    })

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.error(f"Reconciliation failed: {e}")
        print({
            "status": "FAILED",
            "error": str(e),
            "apiCost": 0.0023  # USD, as per provided context
        })
        sys.exit(1)

# --------------------------
# Notes:
# - This script expects the BigQuery SQL transformation (converted from Synapse) to be in 'bigquery_sales_fact_load.sql'.
# - All credentials and configuration must be set via environment variables.
# - The script logs all steps and generates a CSV reconciliation report.
# - Handles data type mismatches, NULLs, and case differences.
# - Can be run in automated environments (CI/CD, Airflow, etc.).
# - API cost for this execution: 0.0023 USD
# --------------------------

apiCost: 0.0023 USD