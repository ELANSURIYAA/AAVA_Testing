=============================================
Author:    AAVA
Created on:   
Description:   BigQuery SQL conversion of Azure Synapse stored procedure for sales fact loading with audit logging and data quality checks
=============================================

Test Case List:

| Test Case ID | Test Case Description                                                        | Expected Outcome                                                                                      |
|--------------|-----------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All staging rows valid, all joinable to Dim tables              | All rows loaded to Fact_Sales, Audit_Log shows correct counts, no DQ_Failures                        |
| TC02         | Missing Customer_ID in some staging rows                                    | Rows with NULL Customer_ID rejected, logged in DQ_Failures, not loaded to Fact_Sales                 |
| TC03         | Invalid Quantity (<=0) in some staging rows                                 | Rows with Quantity <=0 rejected, logged in DQ_Failures, not loaded to Fact_Sales                     |
| TC04         | Staging rows with Customer_ID not in Dim_Customer                           | Rows with missing dimension join not loaded to Fact_Sales, not in DQ_Failures (join failure only)    |
| TC05         | Staging rows with Sales_Date not in Dim_Date                                | Rows with missing date join not loaded to Fact_Sales, not in DQ_Failures (join failure only)         |
| TC06         | All staging rows invalid (all fail DQ checks)                               | No rows loaded to Fact_Sales, all rejected rows logged in DQ_Failures, Audit_Log shows 0 inserted    |
| TC07         | Empty staging table                                                         | No rows loaded, no DQ_Failures, Audit_Log shows 0 inserted/rejected                                  |
| TC08         | Staging table missing required columns                                      | Script fails, Audit_Log status is FAILED, error message is logged                                    |
| TC09         | Staging table contains unexpected data types (e.g., string in Quantity)     | Script fails, Audit_Log status is FAILED, error message is logged                                    |
| TC10         | Duplicate Transaction_IDs in staging                                        | Both rows processed, if valid, both loaded; if invalid, both rejected (depends on DQ/joins)          |

Pytest Script for Each Test Case


# ================================================
# Author: AAVA
# Created on:   
# Description: Pytest-based unit test suite for validating the BigQuery SQL conversion of Azure Synapse stored procedure for sales fact loading, including audit logging, data quality checks, and error handling.
# ================================================

import pytest
import pandas as pd
from sqlalchemy import create_engine, text
import uuid
import datetime

# Helper: Generate a unique batch_id for each test run
def generate_batch_id():
    return str(uuid.uuid4())

# Helper: Setup BigQuery test engine (replace with your BigQuery SQLAlchemy URI)
BQ_CONN_URI = "bigquery://your_project_id"

@pytest.fixture(scope="module")
def bq_engine():
    engine = create_engine(BQ_CONN_URI)
    yield engine

# Helper: Clean up all test tables before/after each test
def cleanup_tables(engine, batch_id):
    with engine.begin() as conn:
        conn.execute(text(f"DELETE FROM `dw.Fact_Sales` WHERE Batch_ID = '{batch_id}'"))
        conn.execute(text(f"DELETE FROM `dw.Audit_Log` WHERE Batch_ID = '{batch_id}'"))
        conn.execute(text(f"DELETE FROM `dw.DQ_Failures` WHERE Batch_ID = '{batch_id}'"))
        conn.execute(text(f"DROP TABLE IF EXISTS `dw._InvalidRows_{batch_id}`"))
        conn.execute(text("TRUNCATE TABLE `stg.Sales_Transactions`"))

# Helper: Insert test data into staging and dimension tables
def insert_test_data(engine, table, df):
    # Assumes df columns match table schema
    if df.empty:
        return
    # Convert DataFrame to list of dicts for insert
    records = df.to_dict(orient='records')
    cols = ', '.join([f"`{col}`" for col in df.columns])
    values = ', '.join([str(tuple(rec.values())) for rec in records])
    with engine.begin() as conn:
        for rec in records:
            placeholders = ', '.join([':{}'.format(k) for k in rec.keys()])
            sql = f"INSERT INTO `{table}` ({cols}) VALUES ({placeholders})"
            conn.execute(text(sql), **rec)

# Helper: Run the BigQuery script (simulate stored procedure execution)
def run_bq_load_sales_fact(engine, batch_id):
    # Replace the batch_id in the script with the test batch_id
    # For simplicity, assume the script is stored in a file 'bq_load_sales_fact.sql'
    with open('bq_load_sales_fact.sql', 'r') as f:
        script = f.read()
    script = script.replace('GENERATE_UUID()', f"'{batch_id}'")
    with engine.begin() as conn:
        conn.execute(text(script))

# Helper: Fetch table as DataFrame
def fetch_table(engine, table, batch_id=None):
    with engine.begin() as conn:
        if batch_id:
            df = pd.read_sql(f"SELECT * FROM `{table}` WHERE Batch_ID = '{batch_id}'", conn)
        else:
            df = pd.read_sql(f"SELECT * FROM `{table}`", conn)
    return df

# ========== TEST CASES ==========

def test_TC01_happy_path(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    # Insert valid staging data
    stg_df = pd.DataFrame([
        {'Transaction_ID': 1, 'Customer_ID': 100, 'Product_ID': 200, 'Sales_Date': '2024-05-01', 'Quantity': 2, 'Unit_Price': 10.0},
        {'Transaction_ID': 2, 'Customer_ID': 101, 'Product_ID': 201, 'Sales_Date': '2024-05-02', 'Quantity': 1, 'Unit_Price': 20.0},
    ])
    dim_customer_df = pd.DataFrame([
        {'Customer_ID': 100, 'Customer_Segment': 'Retail'},
        {'Customer_ID': 101, 'Customer_Segment': 'Wholesale'},
    ])
    dim_date_df = pd.DataFrame([
        {'Date_Value': '2024-05-01', 'Region_ID': 1},
        {'Date_Value': '2024-05-02', 'Region_ID': 2},
    ])
    insert_test_data(bq_engine, 'stg.Sales_Transactions', stg_df)
    insert_test_data(bq_engine, 'dw.Dim_Customer', dim_customer_df)
    insert_test_data(bq_engine, 'dw.Dim_Date', dim_date_df)
    run_bq_load_sales_fact(bq_engine, batch_id)
    fact_df = fetch_table(bq_engine, 'dw.Fact_Sales', batch_id)
    audit_df = fetch_table(bq_engine, 'dw.Audit_Log', batch_id)
    dq_df = fetch_table(bq_engine, 'dw.DQ_Failures', batch_id)
    assert len(fact_df) == 2
    assert audit_df.iloc[0]['Rows_Inserted'] == 2
    assert audit_df.iloc[0]['Rows_Rejected'] == 0
    assert len(dq_df) == 0

def test_TC02_missing_customer_id(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    stg_df = pd.DataFrame([
        {'Transaction_ID': 3, 'Customer_ID': None, 'Product_ID': 202, 'Sales_Date': '2024-05-03', 'Quantity': 1, 'Unit_Price': 15.0},
    ])
    dim_customer_df = pd.DataFrame([
        {'Customer_ID': 102, 'Customer_Segment': 'Retail'},
    ])
    dim_date_df = pd.DataFrame([
        {'Date_Value': '2024-05-03', 'Region_ID': 3},
    ])
    insert_test_data(bq_engine, 'stg.Sales_Transactions', stg_df)
    insert_test_data(bq_engine, 'dw.Dim_Customer', dim_customer_df)
    insert_test_data(bq_engine, 'dw.Dim_Date', dim_date_df)
    run_bq_load_sales_fact(bq_engine, batch_id)
    fact_df = fetch_table(bq_engine, 'dw.Fact_Sales', batch_id)
    dq_df = fetch_table(bq_engine, 'dw.DQ_Failures', batch_id)
    assert len(fact_df) == 0
    assert len(dq_df) == 1
    assert dq_df.iloc[0]['Failure_Reason'] == 'Missing CustomerID'

def test_TC03_invalid_quantity(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    stg_df = pd.DataFrame([
        {'Transaction_ID': 4, 'Customer_ID': 103, 'Product_ID': 203, 'Sales_Date': '2024-05-04', 'Quantity': 0, 'Unit_Price': 12.0},
    ])
    dim_customer_df = pd.DataFrame([
        {'Customer_ID': 103, 'Customer_Segment': 'Retail'},
    ])
    dim_date_df = pd.DataFrame([
        {'Date_Value': '2024-05-04', 'Region_ID': 4},
    ])
    insert_test_data(bq_engine, 'stg.Sales_Transactions', stg_df)
    insert_test_data(bq_engine, 'dw.Dim_Customer', dim_customer_df)
    insert_test_data(bq_engine, 'dw.Dim_Date', dim_date_df)
    run_bq_load_sales_fact(bq_engine, batch_id)
    fact_df = fetch_table(bq_engine, 'dw.Fact_Sales', batch_id)
    dq_df = fetch_table(bq_engine, 'dw.DQ_Failures', batch_id)
    assert len(fact_df) == 0
    assert len(dq_df) == 1
    assert dq_df.iloc[0]['Failure_Reason'] == 'Invalid Quantity'

def test_TC04_missing_dim_customer(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    stg_df = pd.DataFrame([
        {'Transaction_ID': 5, 'Customer_ID': 999, 'Product_ID': 204, 'Sales_Date': '2024-05-05', 'Quantity': 1, 'Unit_Price': 18.0},
    ])
    dim_customer_df = pd.DataFrame([
        {'Customer_ID': 100, 'Customer_Segment': 'Retail'},
    ])
    dim_date_df = pd.DataFrame([
        {'Date_Value': '2024-05-05', 'Region_ID': 5},
    ])
    insert_test_data(bq_engine, 'stg.Sales_Transactions', stg_df)
    insert_test_data(bq_engine, 'dw.Dim_Customer', dim_customer_df)
    insert_test_data(bq_engine, 'dw.Dim_Date', dim_date_df)
    run_bq_load_sales_fact(bq_engine, batch_id)
    fact_df = fetch_table(bq_engine, 'dw.Fact_Sales', batch_id)
    dq_df = fetch_table(bq_engine, 'dw.DQ_Failures', batch_id)
    assert len(fact_df) == 0
    assert len(dq_df) == 0  # Not a DQ failure, just a join miss

def test_TC05_missing_dim_date(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    stg_df = pd.DataFrame([
        {'Transaction_ID': 6, 'Customer_ID': 100, 'Product_ID': 205, 'Sales_Date': '2024-12-31', 'Quantity': 1, 'Unit_Price': 22.0},
    ])
    dim_customer_df = pd.DataFrame([
        {'Customer_ID': 100, 'Customer_Segment': 'Retail'},
    ])
    dim_date_df = pd.DataFrame([
        {'Date_Value': '2024-01-01', 'Region_ID': 6},
    ])
    insert_test_data(bq_engine, 'stg.Sales_Transactions', stg_df)
    insert_test_data(bq_engine, 'dw.Dim_Customer', dim_customer_df)
    insert_test_data(bq_engine, 'dw.Dim_Date', dim_date_df)
    run_bq_load_sales_fact(bq_engine, batch_id)
    fact_df = fetch_table(bq_engine, 'dw.Fact_Sales', batch_id)
    dq_df = fetch_table(bq_engine, 'dw.DQ_Failures', batch_id)
    assert len(fact_df) == 0
    assert len(dq_df) == 0  # Not a DQ failure, just a join miss

def test_TC06_all_invalid_rows(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    stg_df = pd.DataFrame([
        {'Transaction_ID': 7, 'Customer_ID': None, 'Product_ID': 206, 'Sales_Date': '2024-05-06', 'Quantity': -1, 'Unit_Price': 8.0},
        {'Transaction_ID': 8, 'Customer_ID': None, 'Product_ID': 207, 'Sales_Date': '2024-05-07', 'Quantity': 0, 'Unit_Price': 9.0},
    ])
    dim_customer_df = pd.DataFrame([
        {'Customer_ID': 106, 'Customer_Segment': 'Retail'},
    ])
    dim_date_df = pd.DataFrame([
        {'Date_Value': '2024-05-06', 'Region_ID': 7},
        {'Date_Value': '2024-05-07', 'Region_ID': 8},
    ])
    insert_test_data(bq_engine, 'stg.Sales_Transactions', stg_df)
    insert_test_data(bq_engine, 'dw.Dim_Customer', dim_customer_df)
    insert_test_data(bq_engine, 'dw.Dim_Date', dim_date_df)
    run_bq_load_sales_fact(bq_engine, batch_id)
    fact_df = fetch_table(bq_engine, 'dw.Fact_Sales', batch_id)
    dq_df = fetch_table(bq_engine, 'dw.DQ_Failures', batch_id)
    assert len(fact_df) == 0
    assert len(dq_df) == 2

def test_TC07_empty_staging(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    # No data inserted
    run_bq_load_sales_fact(bq_engine, batch_id)
    fact_df = fetch_table(bq_engine, 'dw.Fact_Sales', batch_id)
    dq_df = fetch_table(bq_engine, 'dw.DQ_Failures', batch_id)
    audit_df = fetch_table(bq_engine, 'dw.Audit_Log', batch_id)
    assert len(fact_df) == 0
    assert len(dq_df) == 0
    assert audit_df.iloc[0]['Rows_Inserted'] == 0
    assert audit_df.iloc[0]['Rows_Rejected'] == 0

def test_TC08_missing_columns(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    # Insert data missing required columns
    stg_df = pd.DataFrame([
        {'Transaction_ID': 9, 'Product_ID': 208, 'Sales_Date': '2024-05-08', 'Quantity': 1, 'Unit_Price': 10.0},
    ])  # Missing Customer_ID
    dim_customer_df = pd.DataFrame([
        {'Customer_ID': 108, 'Customer_Segment': 'Retail'},
    ])
    dim_date_df = pd.DataFrame([
        {'Date_Value': '2024-05-08', 'Region_ID': 9},
    ])
    insert_test_data(bq_engine, 'stg.Sales_Transactions', stg_df)
    insert_test_data(bq_engine, 'dw.Dim_Customer', dim_customer_df)
    insert_test_data(bq_engine, 'dw.Dim_Date', dim_date_df)
    with pytest.raises(Exception):
        run_bq_load_sales_fact(bq_engine, batch_id)
    audit_df = fetch_table(bq_engine, 'dw.Audit_Log', batch_id)
    assert audit_df.iloc[0]['Status'] == 'FAILED'

def test_TC09_invalid_data_type(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    # Insert data with invalid data type in Quantity
    stg_df = pd.DataFrame([
        {'Transaction_ID': 10, 'Customer_ID': 110, 'Product_ID': 209, 'Sales_Date': '2024-05-09', 'Quantity': 'bad', 'Unit_Price': 11.0},
    ])
    dim_customer_df = pd.DataFrame([
        {'Customer_ID': 110, 'Customer_Segment': 'Retail'},
    ])
    dim_date_df = pd.DataFrame([
        {'Date_Value': '2024-05-09', 'Region_ID': 10},
    ])
    insert_test_data(bq_engine, 'stg.Sales_Transactions', stg_df)
    insert_test_data(bq_engine, 'dw.Dim_Customer', dim_customer_df)
    insert_test_data(bq_engine, 'dw.Dim_Date', dim_date_df)
    with pytest.raises(Exception):
        run_bq_load_sales_fact(bq_engine, batch_id)
    audit_df = fetch_table(bq_engine, 'dw.Audit_Log', batch_id)
    assert audit_df.iloc[0]['Status'] == 'FAILED'

def test_TC10_duplicate_transaction_ids(bq_engine):
    batch_id = generate_batch_id()
    cleanup_tables(bq_engine, batch_id)
    stg_df = pd.DataFrame([
        {'Transaction_ID': 11, 'Customer_ID': 111, 'Product_ID': 210, 'Sales_Date': '2024-05-10', 'Quantity': 1, 'Unit_Price': 12.0},
        {'Transaction_ID': 11, 'Customer_ID': 111, 'Product_ID': 210, 'Sales_Date': '2024-05-10', 'Quantity': 2, 'Unit_Price': 12.0},
    ])
    dim_customer_df = pd.DataFrame([
        {'Customer_ID': 111, 'Customer_Segment': 'Retail'},
    ])
    dim_date_df = pd.DataFrame([
        {'Date_Value': '2024-05-10', 'Region_ID': 11},
    ])
    insert_test_data(bq_engine, 'stg.Sales_Transactions', stg_df)
    insert_test_data(bq_engine, 'dw.Dim_Customer', dim_customer_df)
    insert_test_data(bq_engine, 'dw.Dim_Date', dim_date_df)
    run_bq_load_sales_fact(bq_engine, batch_id)
    fact_df = fetch_table(bq_engine, 'dw.Fact_Sales', batch_id)
    audit_df = fetch_table(bq_engine, 'dw.Audit_Log', batch_id)
    # Both rows should be processed independently
    assert len(fact_df) == 2
    assert audit_df.iloc[0]['Rows_Inserted'] == 2



API Cost Estimation

apiCost: 0.0032 USD