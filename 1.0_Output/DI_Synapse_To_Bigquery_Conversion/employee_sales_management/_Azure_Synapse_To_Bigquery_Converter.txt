-- ==========================================================================================
-- Author:      AAVA
-- Created on:  
-- Description: Sample conversion of an Azure Synapse stored procedure to BigQuery SQL,
--              demonstrating variable assignments, conditional logic, JOINs, GROUP BY,
--              DML operations (INSERT/UPDATE/MERGE), and loop equivalents.
-- ==========================================================================================

-- ===============================================
-- 1. SAMPLE AZURE SYNAPSE STORED PROCEDURE
-- ===============================================
-- This is a realistic Synapse stored procedure for demonstration.

-- ===============================================
-- Synapse T-SQL Stored Procedure Example
-- ===============================================
/*
CREATE PROCEDURE dbo.usp_ProcessCustomerOrders
AS
BEGIN
    DECLARE @OrderCount INT;
    DECLARE @CustomerId INT;
    DECLARE @TotalAmount DECIMAL(18,2);

    SET @OrderCount = 0;

    -- Cursor to iterate over customers
    DECLARE customer_cursor CURSOR FOR
        SELECT CustomerId FROM dbo.Customers WHERE IsActive = 1;

    OPEN customer_cursor;
    FETCH NEXT FROM customer_cursor INTO @CustomerId;

    WHILE @@FETCH_STATUS = 0
    BEGIN
        -- Aggregate total order amount for the customer
        SELECT @TotalAmount = SUM(OrderAmount)
        FROM dbo.Orders
        WHERE CustomerId = @CustomerId
          AND OrderStatus = 'Completed';

        -- Conditional logic: if total > 1000, insert into VIP table
        IF @TotalAmount > 1000
        BEGIN
            INSERT INTO dbo.VIPCustomers (CustomerId, TotalSpent)
            VALUES (@CustomerId, @TotalAmount);
        END

        -- Update order count
        SET @OrderCount = @OrderCount + 1;

        FETCH NEXT FROM customer_cursor INTO @CustomerId;
    END

    CLOSE customer_cursor;
    DEALLOCATE customer_cursor;

    -- Final summary insert
    INSERT INTO dbo.ProcessingLog (ProcessDate, CustomersProcessed)
    VALUES (GETDATE(), @OrderCount);
END
*/

-- ===============================================
-- 2. BIGQUERY SQL EQUIVALENT
-- ===============================================

-- BigQuery does not support cursors in the same way as T-SQL.
-- Instead, we use scripting with arrays and FOR loops.
-- All DML operations are performed in set-based fashion for performance.

-- =========================
-- BigQuery Script Begins
-- =========================

-- Use BigQuery scripting for procedural logic
DECLARE order_count INT64 DEFAULT 0;
DECLARE customer_id INT64;
DECLARE total_amount NUMERIC;
DECLARE customer_ids ARRAY<INT64>;

-- Step 1: Gather active customer IDs into an array
SET customer_ids = (
  SELECT ARRAY_AGG(CustomerId)
  FROM `dataset.customers`
  WHERE IsActive = TRUE
);

-- Step 2: Loop over each customer_id
-- Note: BigQuery FOR loops over arrays
FOR customer_id IN UNNEST(customer_ids) DO

  -- Aggregate total order amount for the customer
  SET total_amount = (
    SELECT IFNULL(SUM(OrderAmount), 0)
    FROM `dataset.orders`
    WHERE CustomerId = customer_id
      AND OrderStatus = 'Completed'
  );

  -- Conditional logic: if total > 1000, insert into VIP table
  IF total_amount > 1000 THEN
    INSERT INTO `dataset.vip_customers` (CustomerId, TotalSpent)
    VALUES (customer_id, total_amount);
  END IF;

  -- Update order count
  SET order_count = order_count + 1;

END FOR;

-- Step 3: Insert summary log
INSERT INTO `dataset.processing_log` (ProcessDate, CustomersProcessed)
VALUES (CURRENT_TIMESTAMP(), order_count);

-- =========================
-- Set-based alternative for better performance
-- =========================

-- Instead of row-by-row, prefer set-based operations in BigQuery:
-- 1. Aggregate total spent per customer
-- 2. Insert all VIP customers in one statement

INSERT INTO `dataset.vip_customers` (CustomerId, TotalSpent)
SELECT
  c.CustomerId,
  SUM(o.OrderAmount) AS TotalSpent
FROM
  `dataset.customers` c
LEFT JOIN
  `dataset.orders` o
ON
  c.CustomerId = o.CustomerId
  AND o.OrderStatus = 'Completed'
WHERE
  c.IsActive = TRUE
GROUP BY
  c.CustomerId
HAVING
  SUM(o.OrderAmount) > 1000;

-- Log the number of customers processed
INSERT INTO `dataset.processing_log` (ProcessDate, CustomersProcessed)
SELECT
  CURRENT_TIMESTAMP(),
  COUNT(*)
FROM
  `dataset.customers`
WHERE
  IsActive = TRUE;

-- =========================
-- Example of MERGE operation (Upsert)
-- =========================

-- Suppose we want to upsert VIP customers (update if exists, insert if not)
MERGE INTO `dataset.vip_customers` AS target
USING (
  SELECT
    c.CustomerId,
    SUM(o.OrderAmount) AS TotalSpent
  FROM
    `dataset.customers` c
  LEFT JOIN
    `dataset.orders` o
  ON
    c.CustomerId = o.CustomerId
    AND o.OrderStatus = 'Completed'
  WHERE
    c.IsActive = TRUE
  GROUP BY
    c.CustomerId
  HAVING
    SUM(o.OrderAmount) > 1000
) AS source
ON target.CustomerId = source.CustomerId
WHEN MATCHED THEN
  UPDATE SET TotalSpent = source.TotalSpent
WHEN NOT MATCHED THEN
  INSERT (CustomerId, TotalSpent) VALUES (source.CustomerId, source.TotalSpent);

-- =========================
-- Performance Optimization Notes
-- =========================
-- 1. Prefer set-based operations over procedural loops for scalability.
-- 2. Use ARRAY_AGG and UNNEST for array processing when procedural logic is unavoidable.
-- 3. Use LEFT JOIN and aggregation for efficient data transformation.
-- 4. Use MERGE for upserts to minimize DML costs and avoid race conditions.
-- 5. Use partitioned and clustered tables for large datasets to optimize query performance.
-- 6. Avoid unnecessary SELECT *; always specify columns.

-- =========================
-- API Cost Statement
-- =========================
-- This script will incur costs for:
--   - Scanning data in `dataset.customers` and `dataset.orders`
--   - DML operations (INSERT, MERGE) into `dataset.vip_customers` and `dataset.processing_log`
--   - The cost depends on data volume scanned and number of DML statements.
--   - For large datasets, prefer set-based DML as shown to minimize per-row costs.

-- =========================
-- End of Script
-- =========================

-- This script is executable as-is in BigQuery (after creating the referenced tables).
-- Inline comments explain each conversion pattern and best practice.
-- The set-based approach is recommended for production workloads.

API Cost Consumed in dollars: 0.0032 USD