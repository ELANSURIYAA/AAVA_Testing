=============================================
Author:        AAVA
Created on:   
Description:   Load Sales Fact - Converted from Azure Synapse stored procedure to BigQuery
=============================================

-- This script implements the logic of the original Synapse stored procedure 'dw.sp_load_sales_fact'
-- using BigQuery Standard SQL, including variable management, audit logging, data quality checks,
-- transformation, fact loading, error handling, and audit updates.

-- ===========================
-- BEGIN: BigQuery SQL Script
-- ===========================

DECLARE batch_id STRING DEFAULT GENERATE_UUID();
DECLARE start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP();
DECLARE end_time TIMESTAMP;
DECLARE rows_inserted INT64 DEFAULT 0;
DECLARE rows_rejected INT64 DEFAULT 0;
DECLARE error_message STRING;
DECLARE proc_name STRING DEFAULT 'sp_load_sales_fact';

BEGIN
  -- Start Audit Log
  INSERT INTO `dw.Audit_Log`
    (Batch_ID, Procedure_Name, Start_Time, Status, Message)
  VALUES
    (batch_id, proc_name, start_time, 'STARTED', 'Sales Fact Load Initiated');

  -- Create temporary table for invalid rows
  CREATE TEMP TABLE InvalidRows AS
  SELECT Transaction_ID, 'Missing CustomerID' AS Reason
  FROM `stg.Sales_Transactions`
  WHERE Customer_ID IS NULL

  UNION ALL

  SELECT Transaction_ID, 'Invalid Quantity' AS Reason
  FROM `stg.Sales_Transactions`
  WHERE Quantity <= 0;

  -- Delete invalid rows from staging table
  -- BigQuery does not support DELETE with JOIN directly, so use WHERE IN
  DELETE FROM `stg.Sales_Transactions`
  WHERE Transaction_ID IN (SELECT Transaction_ID FROM InvalidRows);

  -- Count rejected rows
  SET rows_rejected = (
    SELECT COUNT(*) FROM InvalidRows
  );

  -- Data Transformation using CTE
  WITH transformed AS (
    SELECT
      s.Transaction_ID,
      s.Customer_ID,
      s.Product_ID,
      s.Sales_Date,
      s.Quantity,
      s.Unit_Price,
      s.Quantity * s.Unit_Price AS Total_Sales_Amount,
      d.Region_ID,
      c.Customer_Segment,
      CURRENT_TIMESTAMP() AS Load_Timestamp,
      batch_id AS Batch_ID
    FROM
      `stg.Sales_Transactions` s
      INNER JOIN `dw.Dim_Customer` c ON s.Customer_ID = c.Customer_ID
      INNER JOIN `dw.Dim_Date` d ON DATE(s.Sales_Date) = d.Date_Value
  )

  -- Insert into Fact Table
  INSERT INTO `dw.Fact_Sales`
    (Transaction_ID, Customer_ID, Product_ID, Sales_Date, Quantity, Unit_Price, Total_Sales_Amount, Region_ID, Customer_Segment, Load_Timestamp, Batch_ID)
  SELECT
    Transaction_ID, Customer_ID, Product_ID, Sales_Date, Quantity, Unit_Price, Total_Sales_Amount, Region_ID, Customer_Segment, Load_Timestamp, Batch_ID
  FROM
    transformed;

  -- Count inserted rows
  SET rows_inserted = (
    SELECT COUNT(*) FROM transformed
  );

  -- Truncate staging table
  TRUNCATE TABLE `stg.Sales_Transactions`;

  -- Insert DQ failures
  INSERT INTO `dw.DQ_Failures`
    (Transaction_ID, Failure_Reason, Logged_Timestamp, Batch_ID)
  SELECT
    Transaction_ID, Reason, CURRENT_TIMESTAMP(), batch_id
  FROM
    InvalidRows;

  -- End time
  SET end_time = CURRENT_TIMESTAMP();

  -- Update Audit Log as COMPLETED
  UPDATE `dw.Audit_Log`
  SET
    End_Time = end_time,
    Rows_Inserted = rows_inserted,
    Rows_Rejected = rows_rejected,
    Status = 'COMPLETED',
    Message = CONCAT('Inserted ', CAST(rows_inserted AS STRING), ' rows; Rejected ', CAST(rows_rejected AS STRING), ' rows.')
  WHERE
    Batch_ID = batch_id;

EXCEPTION WHEN ERROR THEN
  -- On error, update audit log as FAILED
  SET end_time = CURRENT_TIMESTAMP();
  SET error_message = @@error.message;
  UPDATE `dw.Audit_Log`
  SET
    End_Time = end_time,
    Status = 'FAILED',
    Message = error_message
  WHERE
    Batch_ID = batch_id;
  -- Optionally, re-raise error (BigQuery scripting will propagate error by default)
END;

-- ===========================
-- END: BigQuery SQL Script
-- ===========================

-- Notes:
-- - All table names are fully qualified with backticks.
-- - Temporary table InvalidRows is scoped to this script execution.
-- - Error handling uses EXCEPTION block (BigQuery scripting).
-- - Variables are declared and used in BigQuery scripting style.
-- - TRUNCATE TABLE is supported in BigQuery.
-- - Audit logging and DQ failure logging are preserved.
-- - The script is designed for execution in BigQuery scripting (e.g., via bq query --use_legacy_sql=false).

API Cost Consumed in dollars: 0.0023 USD