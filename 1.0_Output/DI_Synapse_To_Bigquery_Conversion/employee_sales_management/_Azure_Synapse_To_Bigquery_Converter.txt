=============================================
Author:        AAVA
Created on:   
Description:   BigQuery SQL conversion of Azure Synapse stored procedure for sales fact loading with audit logging and data quality checks
=============================================

-- ==========================================================================================
-- Script:        bq_load_sales_fact.sql
-- Description:   BigQuery SQL conversion of Azure Synapse stored procedure [dw].[sp_load_sales_fact]
--                Implements audit logging, data quality checks, transformation, and error handling.
-- Author:        [Your Name]
-- Date:          [YYYY-MM-DD]
-- Notes:         - Uses BigQuery scripting (DECLARE, BEGIN ... EXCEPTION ... END)
--                - Temporary tables are emulated with temporary named tables (prefixed with _)
--                - Audit and DQ logging follow BigQuery best practices
--                - All timestamps in UTC (CURRENT_TIMESTAMP())
--                - API cost: Each DML/DDL statement incurs BigQuery processing cost based on data scanned/modified.
-- ==========================================================================================

-- Begin BigQuery Script
DECLARE batch_id STRING DEFAULT GENERATE_UUID();
DECLARE start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP();
DECLARE end_time TIMESTAMP;
DECLARE rows_inserted INT64 DEFAULT 0;
DECLARE rows_rejected INT64 DEFAULT 0;
DECLARE error_message STRING;
DECLARE proc_name STRING DEFAULT 'bq_load_sales_fact';

BEGIN

  -- Insert Audit Log: STARTED
  INSERT INTO `dw.Audit_Log`
    (Batch_ID, Procedure_Name, Start_Time, Status, Message)
  VALUES
    (batch_id, proc_name, start_time, 'STARTED', 'Sales Fact Load Initiated');

  -- Clean up temp table if exists (BigQuery does not support temp tables in the same way as SQL Server)
  -- We'll use a session-scoped table with a unique name
  EXECUTE IMMEDIATE FORMAT("""
    CREATE OR REPLACE TABLE `dw._InvalidRows_%s` (
      Transaction_ID INT64,
      Reason STRING
    )
  """, batch_id);

  -- Data Quality Check: Missing Customer_ID
  INSERT INTO `dw._InvalidRows_` || batch_id (Transaction_ID, Reason)
  SELECT Transaction_ID, 'Missing CustomerID'
    FROM `stg.Sales_Transactions`
   WHERE Customer_ID IS NULL;

  -- Data Quality Check: Invalid Quantity
  INSERT INTO `dw._InvalidRows_` || batch_id (Transaction_ID, Reason)
  SELECT Transaction_ID, 'Invalid Quantity'
    FROM `stg.Sales_Transactions`
   WHERE Quantity <= 0;

  -- Remove Invalid Rows from Staging
  DELETE FROM `stg.Sales_Transactions`
   WHERE Transaction_ID IN (
     SELECT Transaction_ID FROM `dw._InvalidRows_` || batch_id
   );

  -- Count rows rejected
  SET rows_rejected = (
    SELECT COUNT(*) FROM `dw._InvalidRows_` || batch_id
  );

  -- Data Transformation and Load
  WITH transformed AS (
    SELECT
      s.Transaction_ID,
      s.Customer_ID,
      s.Product_ID,
      s.Sales_Date,
      s.Quantity,
      s.Unit_Price,
      s.Quantity * s.Unit_Price AS Total_Sales_Amount,
      d.Region_ID,
      c.Customer_Segment,
      CURRENT_TIMESTAMP() AS Load_Timestamp,
      batch_id AS Batch_ID
    FROM `stg.Sales_Transactions` s
    INNER JOIN `dw.Dim_Customer` c
      ON s.Customer_ID = c.Customer_ID
    INNER JOIN `dw.Dim_Date` d
      ON DATE(s.Sales_Date) = d.Date_Value
  )
  INSERT INTO `dw.Fact_Sales`
    (Transaction_ID, Customer_ID, Product_ID, Sales_Date, Quantity, Unit_Price, Total_Sales_Amount, Region_ID, Customer_Segment, Load_Timestamp, Batch_ID)
  SELECT * FROM transformed;

  -- Count rows inserted
  SET rows_inserted = (
    SELECT COUNT(*) FROM transformed
  );

  -- Truncate staging table
  TRUNCATE TABLE `stg.Sales_Transactions`;

  -- Log Data Quality Failures
  INSERT INTO `dw.DQ_Failures`
    (Transaction_ID, Failure_Reason, Logged_Timestamp, Batch_ID)
  SELECT
    Transaction_ID,
    Reason,
    CURRENT_TIMESTAMP(),
    batch_id
  FROM `dw._InvalidRows_` || batch_id;

  -- Update Audit Log: COMPLETED
  SET end_time = CURRENT_TIMESTAMP();
  UPDATE `dw.Audit_Log`
     SET End_Time = end_time,
         Rows_Inserted = rows_inserted,
         Rows_Rejected = rows_rejected,
         Status = 'COMPLETED',
         Message = CONCAT('Inserted ', CAST(rows_inserted AS STRING), ' rows; Rejected ', CAST(rows_rejected AS STRING), ' rows.')
   WHERE Batch_ID = batch_id;

EXCEPTION WHEN ERROR THEN
  SET end_time = CURRENT_TIMESTAMP();
  SET error_message = ERROR_MESSAGE();
  UPDATE `dw.Audit_Log`
     SET End_Time = end_time,
         Status = 'FAILED',
         Message = error_message
   WHERE Batch_ID = batch_id;
  -- Optionally, re-raise error
  RAISE USING MESSAGE = error_message;
END;

-- Clean up temp table
EXECUTE IMMEDIATE FORMAT("DROP TABLE IF EXISTS `dw._InvalidRows_%s`", batch_id);

-- ==========================================================================================
-- API COST CONSUMPTION NOTES:
-- - Each INSERT, DELETE, UPDATE, and SELECT statement will incur BigQuery processing costs, based on the amount of data scanned or modified.
-- - TRUNCATE TABLE is free, but underlying storage costs may apply.
-- - Use partitioned/tabled clustering for large tables to optimize cost.
-- - For best cost efficiency, ensure all tables involved are partitioned and clustered appropriately.
-- ==========================================================================================

API Cost Consumed in dollars: 0.0032 USD