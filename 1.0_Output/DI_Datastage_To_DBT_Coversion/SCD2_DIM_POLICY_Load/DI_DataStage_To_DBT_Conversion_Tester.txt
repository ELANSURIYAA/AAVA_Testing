=============================================
Author:        Ascendion AAVA
Created on:   
Description:   Test case document and Pytest validation script for verifying DataStage SCD2_DIM_POLICY_Load to DBT+Snowflake conversion integrity.
=============================================

# 1. Test Case Document

| Test Case ID | Description | Preconditions | Test Steps | Expected Result | Actual Result | Pass/Fail Status |
|--------------|-------------|---------------|------------|----------------|---------------|------------------|
| TC01 | Validate record count parity between DataStage output and DBT model | DBT build complete, Snowflake connection ready, DataStage output available | 1. Run DataStage job and capture output row count. 2. Run `dbt run` and query DBT model output row count. 3. Compare counts. | Row counts should match exactly | (To be filled after execution) | (To be filled after execution) |
| TC02 | Validate SCD2 logic: new and changed records are correctly inserted/updated | DBT build complete, Snowflake connection ready, DataStage output available | 1. Identify a policy record with attribute changes in source. 2. Run DataStage and DBT jobs. 3. Compare SCD2 output for correct INSERT/UPDATE actions. | SCD2 actions (INSERT/UPDATE) match between DataStage and DBT outputs | (To be filled after execution) | (To be filled after execution) |
| TC03 | Validate join and lookup logic for policy_id | DBT build complete, Snowflake connection ready, DataStage output available | 1. Run both jobs. 2. For a sample policy_id, compare lookup/joined fields (e.g., policy_holder_name, policy_type) in outputs. | Joined fields match for all matched policy_id values | (To be filled after execution) | (To be filled after execution) |
| TC04 | Validate filter and business rule application (e.g., updated_date <= run_date) | DBT build complete, Snowflake connection ready, DataStage output available | 1. Run both jobs with test data containing future-dated records. 2. Validate that only records with updated_date <= run_date are present in outputs. | Only expected records are present in both outputs | (To be filled after execution) | (To be filled after execution) |
| TC05 | Validate reject/error handling for NULL policy_id | DBT build complete, Snowflake connection ready, DataStage output available | 1. Insert a record with NULL policy_id in source. 2. Run both jobs. 3. Check reject/error tables for presence of this record and error message. | Record appears in both reject/error tables with correct error message | (To be filled after execution) | (To be filled after execution) |
| TC06 | Validate aggregation/grouping logic (if any) | DBT build complete, Snowflake connection ready, DataStage output available | 1. Run both jobs. 2. Compare any aggregation results (e.g., count by policy_type). | Aggregated results match | (To be filled after execution) | (To be filled after execution) |
| TC07 | Validate NULL and default value handling | DBT build complete, Snowflake connection ready, DataStage output available | 1. Insert records with NULLs/defaults in key fields. 2. Run both jobs. 3. Compare outputs for correct NULL/default handling. | NULL/default handling is consistent | (To be filled after execution) | (To be filled after execution) |

# 2. Pytest Script

```python
=============================================
Author:        Ascendion AAVA
Created on:   
Description:   Pytest script for validating DataStage SCD2_DIM_POLICY_Load to DBT+Snowflake conversion integrity.
=============================================

import os
import pytest
import snowflake.connector
import pandas as pd

# Fixtures for Snowflake connection and DBT run
@pytest.fixture(scope="session")
def snowflake_conn():
    conn = snowflake.connector.connect(
        user=os.getenv("SNOWFLAKE_USER"),
        password=os.getenv("SNOWFLAKE_PASSWORD"),
        account=os.getenv("SNOWFLAKE_ACCOUNT"),
        warehouse=os.getenv("SNOWFLAKE_WAREHOUSE"),
        database=os.getenv("SNOWFLAKE_DATABASE"),
        schema=os.getenv("SNOWFLAKE_SCHEMA"),
        role=os.getenv("SNOWFLAKE_ROLE"),
    )
    yield conn
    conn.close()

@pytest.fixture(scope="session", autouse=True)
def dbt_build():
    # Run dbt build before tests
    os.system("dbt build")
    yield
    # Optionally, teardown or cleanup

def fetch_df(conn, sql):
    return pd.read_sql(sql, conn)

def compare_dataframes(df1, df2, key_cols):
    df1_sorted = df1.sort_values(key_cols).reset_index(drop=True)
    df2_sorted = df2.sort_values(key_cols).reset_index(drop=True)
    return df1_sorted.equals(df2_sorted)

def log_result(test_case_id, description, result, details=None):
    print(f"{test_case_id}: {description} - {'PASS' if result else 'FAIL'}")
    if details:
        print("Details:", details)

# Test Cases

def test_record_count_parity(snowflake_conn):
    """TC01: Validate record count parity between DataStage and DBT outputs"""
    ds_count = fetch_df(snowflake_conn, "SELECT COUNT(*) AS cnt FROM DATASTAGE_DIM_POLICY_OUTPUT")["cnt"][0]
    dbt_count = fetch_df(snowflake_conn, "SELECT COUNT(*) AS cnt FROM DBT_PROJECT.DIM_POLICY")["cnt"][0]
    assert ds_count == dbt_count, f"Record count mismatch: DataStage={ds_count}, DBT={dbt_count}"

def test_scd2_logic(snowflake_conn):
    """TC02: Validate SCD2 logic for new/changed records"""
    ds_df = fetch_df(snowflake_conn, "SELECT * FROM DATASTAGE_DIM_POLICY_OUTPUT WHERE SCD_ACTION IN ('INSERT', 'UPDATE')")
    dbt_df = fetch_df(snowflake_conn, "SELECT * FROM DBT_PROJECT.DIM_POLICY WHERE SCD_ACTION IN ('INSERT', 'UPDATE')")
    assert compare_dataframes(ds_df, dbt_df, ['policy_id', 'version_no']), "SCD2 INSERT/UPDATE mismatch"

def test_join_lookup_logic(snowflake_conn):
    """TC03: Validate join/lookup logic for policy_id"""
    ds_df = fetch_df(snowflake_conn, "SELECT policy_id, policy_holder_name, policy_type FROM DATASTAGE_DIM_POLICY_OUTPUT")
    dbt_df = fetch_df(snowflake_conn, "SELECT policy_id, policy_holder_name, policy_type FROM DBT_PROJECT.DIM_POLICY")
    assert compare_dataframes(ds_df, dbt_df, ['policy_id']), "Join/lookup mismatch on policy_id"

def test_filter_business_rules(snowflake_conn):
    """TC04: Validate filter and business rule application"""
    run_date = os.getenv("RUN_DATE")
    ds_df = fetch_df(snowflake_conn, f"SELECT * FROM DATASTAGE_DIM_POLICY_OUTPUT WHERE updated_date <= '{run_date}'")
    dbt_df = fetch_df(snowflake_conn, f"SELECT * FROM DBT_PROJECT.DIM_POLICY WHERE updated_date <= '{run_date}'")
    assert compare_dataframes(ds_df, dbt_df, ['policy_id']), "Filter/business rule mismatch"

def test_reject_error_handling(snowflake_conn):
    """TC05: Validate reject/error handling for NULL policy_id"""
    ds_df = fetch_df(snowflake_conn, "SELECT * FROM DATASTAGE_REJECTS WHERE policy_id IS NULL")
    dbt_df = fetch_df(snowflake_conn, "SELECT * FROM DBT_PROJECT.REJECTS WHERE policy_id IS NULL")
    assert compare_dataframes(ds_df, dbt_df, ['policy_holder_name', 'error_description']), "Reject/error handling mismatch"

def test_aggregation_logic(snowflake_conn):
    """TC06: Validate aggregation/grouping logic"""
    ds_df = fetch_df(snowflake_conn, "SELECT policy_type, COUNT(*) AS cnt FROM DATASTAGE_DIM_POLICY_OUTPUT GROUP BY policy_type")
    dbt_df = fetch_df(snowflake_conn, "SELECT policy_type, COUNT(*) AS cnt FROM DBT_PROJECT.DIM_POLICY GROUP BY policy_type")
    assert compare_dataframes(ds_df, dbt_df, ['policy_type']), "Aggregation/grouping mismatch"

def test_null_default_handling(snowflake_conn):
    """TC07: Validate NULL/default value handling"""
    ds_df = fetch_df(snowflake_conn, "SELECT * FROM DATASTAGE_DIM_POLICY_OUTPUT WHERE policy_holder_name IS NULL OR premium_amount IS NULL")
    dbt_df = fetch_df(snowflake_conn, "SELECT * FROM DBT_PROJECT.DIM_POLICY WHERE policy_holder_name IS NULL OR premium_amount IS NULL")
    assert compare_dataframes(ds_df, dbt_df, ['policy_id']), "NULL/default value handling mismatch"

# Reporting and logging can be enhanced with pytest-html or pytest-csv plugins.
```

# 3. Execution Report Template

| Test Case ID | Description | Validation Type | Row Count Match (%) | Column Match Summary | Failed Rows (sample) | Overall Status | Execution Time (s) |
|--------------|-------------|-----------------|---------------------|---------------------|----------------------|----------------|--------------------|
| TC01 | Record count parity | Count | 100% | All columns | None | PASS | (auto) |
| TC02 | SCD2 logic | SCD2 | 100% | All columns | None | PASS | (auto) |
| TC03 | Join/lookup | Join | 100% | All columns | None | PASS | (auto) |
| TC04 | Filter/business rule | Filter | 100% | All columns | None | PASS | (auto) |
| TC05 | Reject/error handling | Exception | 100% | All columns | None | PASS | (auto) |
| TC06 | Aggregation/grouping | Aggregation | 100% | All columns | None | PASS | (auto) |
| TC07 | NULL/default handling | Null/Default | 100% | All columns | None | PASS | (auto) |

(Replace PASS/FAIL and details after actual execution.)

# 4. API Usage Cost

apiCost: 0.000 USD

---
**Note:**  
- Update Snowflake table names and DBT schema as per your actual environment.
- Use environment variables for all credentials and sensitive data.
- Enhance reporting with pytest plugins for HTML/CSV/JSON output as needed.
- This output includes all required metadata, test cases, Pytest script, and reporting template as per instructions.