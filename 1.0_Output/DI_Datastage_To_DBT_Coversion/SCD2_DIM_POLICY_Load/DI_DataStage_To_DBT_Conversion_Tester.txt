=============================================
Author:        Ascendion AAVA
Created on:   
Description:   This document and script validate the transformation logic fidelity and conversion accuracy between the original DataStage SCD2_DIM_POLICY_Load job and the DBT+Snowflake implementation, ensuring equivalence in SCD2 logic, joins, filters, aggregations, and error handling.
=============================================

---

**Test Case Document**

| Test Case ID | Description | Preconditions | Test Steps | Expected Result | Actual Result | Pass/Fail |
|--------------|-------------|---------------|------------|----------------|--------------|-----------|
| TC01 | Validate record count equivalence between DataStage and DBT output tables for a given batch/run_date | DBT project built, Snowflake connection ready, DataStage output available | 1. Run DataStage job for batch/run_date<br>2. Run DBT model for same batch/run_date<br>3. Query record counts from both outputs<br>4. Compare counts | Record counts match | To be captured | To be captured |
| TC02 | Validate SCD2 change detection logic: changed attributes result in new version rows in DBT as in DataStage | DBT project built, Snowflake connection ready, DataStage output available | 1. Insert source data with changed attributes<br>2. Run DataStage job<br>3. Run DBT model<br>4. Compare version_no and current_flag for changed records | New version rows with current_flag='Y', version_no incremented | To be captured | To be captured |
| TC03 | Validate SCD2 unchanged logic: unchanged attributes retain current_flag='Y' and version_no in DBT as in DataStage | DBT project built, Snowflake connection ready, DataStage output available | 1. Insert source data with unchanged attributes<br>2. Run DataStage job<br>3. Run DBT model<br>4. Compare current_flag and version_no for unchanged records | Unchanged records retain current_flag='Y', version_no unchanged | To be captured | To be captured |
| TC04 | Validate SCD2 expiry logic: changed records expire previous version (current_flag='N', effective_to=run_date-1) in DBT as in DataStage | DBT project built, Snowflake connection ready, DataStage output available | 1. Insert source data with changed attributes<br>2. Run DataStage job<br>3. Run DBT model<br>4. Compare expired records in both outputs | Expired records have current_flag='N', effective_to=run_date-1 | To be captured | To be captured |
| TC05 | Validate NULL and default handling: ensure DBT handles NULLs and defaults as in DataStage (e.g., coalesce logic) | DBT project built, Snowflake connection ready, DataStage output available | 1. Insert source data with NULLs/defaults<br>2. Run DataStage job<br>3. Run DBT model<br>4. Compare handling of NULLs/defaults in outputs | NULLs/defaults handled identically | To be captured | To be captured |
| TC06 | Validate column mapping and type casting: ensure DBT output columns match DataStage output schema and types | DBT project built, Snowflake connection ready, DataStage output available | 1. Run DataStage job<br>2. Run DBT model<br>3. Compare output schemas and types | Schemas and types match | To be captured | To be captured |
| TC07 | Validate business rule filters: ensure DBT WHERE clauses match DataStage filters (e.g., updated_date <= run_date) | DBT project built, Snowflake connection ready, DataStage output available | 1. Insert source data with various updated_date values<br>2. Run DataStage job<br>3. Run DBT model<br>4. Compare filtered outputs | Filtered outputs match | To be captured | To be captured |
| TC08 | Validate error/reject handling: ensure DBT rejects model matches DataStage rejects for invalid data | DBT project built, Snowflake connection ready, DataStage output available | 1. Insert invalid source data<br>2. Run DataStage job<br>3. Run DBT model<br>4. Compare rejects outputs | Rejects match | To be captured | To be captured |
| TC09 | Validate audit logging: ensure DBT audit_log model captures job execution details as in DataStage audit framework | DBT project built, Snowflake connection ready, DataStage output available | 1. Run DataStage job<br>2. Run DBT model<br>3. Compare audit logs | Audit logs match | To be captured | To be captured |
| TC10 | Validate performance on large datasets: ensure DBT model runs efficiently and matches DataStage output for large volumes | DBT project built, Snowflake connection ready, DataStage output available | 1. Insert large source dataset<br>2. Run DataStage job<br>3. Run DBT model<br>4. Compare outputs and execution time | Outputs match, performance acceptable | To be captured | To be captured |

---

**Pytest Script**

```python
=============================================
Author:        Ascendion AAVA
Created on:   
Description:   Pytest script to validate transformation logic fidelity and conversion accuracy between DataStage SCD2_DIM_POLICY_Load and DBT+Snowflake implementation.
=============================================

import os
import pytest
import snowflake.connector
import csv
import json
from datetime import datetime

# Fixtures for Snowflake connection and DBT environment
@pytest.fixture(scope="session")
def snowflake_conn():
    conn = snowflake.connector.connect(
        user=os.environ["SNOWFLAKE_USER"],
        password=os.environ["SNOWFLAKE_PASSWORD"],
        account=os.environ["SNOWFLAKE_ACCOUNT"],
        warehouse=os.environ["SNOWFLAKE_WAREHOUSE"],
        database=os.environ["SNOWFLAKE_DATABASE"],
        schema=os.environ["SNOWFLAKE_SCHEMA"]
    )
    yield conn
    conn.close()

@pytest.fixture(scope="session")
def run_date():
    return os.environ.get("RUN_DATE", "2024-01-01")

@pytest.fixture(scope="session")
def batch_id():
    return os.environ.get("BATCH_ID", "AUTO")

def fetch_table(conn, table_name):
    cur = conn.cursor()
    cur.execute(f"SELECT * FROM {table_name}")
    rows = cur.fetchall()
    columns = [desc[0] for desc in cur.description]
    cur.close()
    return [dict(zip(columns, row)) for row in rows]

def fetch_count(conn, table_name):
    cur = conn.cursor()
    cur.execute(f"SELECT COUNT(*) FROM {table_name}")
    count = cur.fetchone()[0]
    cur.close()
    return count

def compare_rows(rows1, rows2, keys):
    mismatches = []
    for r1 in rows1:
        match = next((r2 for r2 in rows2 if all(r1[k] == r2[k] for k in keys)), None)
        if not match:
            mismatches.append(r1)
    return mismatches

def write_report(report, fmt="csv"):
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    if fmt == "csv":
        with open(f"pytest_report_{ts}.csv", "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=report[0].keys())
            writer.writeheader()
            writer.writerows(report)
    elif fmt == "json":
        with open(f"pytest_report_{ts}.json", "w") as f:
            json.dump(report, f, indent=2)

# Test cases

def test_record_count_equivalence(snowflake_conn, run_date, batch_id):
    ds_count = fetch_count(snowflake_conn, "DATASTAGE_DIM_POLICY_OUTPUT")
    dbt_count = fetch_count(snowflake_conn, "DBT_DIM_POLICY_OUTPUT")
    assert ds_count == dbt_count, f"Record count mismatch: DataStage={ds_count}, DBT={dbt_count}"

def test_scd2_change_detection(snowflake_conn, run_date, batch_id):
    ds_rows = fetch_table(snowflake_conn, "DATASTAGE_DIM_POLICY_OUTPUT")
    dbt_rows = fetch_table(snowflake_conn, "DBT_DIM_POLICY_OUTPUT")
    changed_ds = [r for r in ds_rows if r["version_no"] > 1 and r["current_flag"] == "Y"]
    changed_dbt = [r for r in dbt_rows if r["version_no"] > 1 and r["current_flag"] == "Y"]
    assert len(changed_ds) == len(changed_dbt), "Changed records count mismatch"
    mismatches = compare_rows(changed_ds, changed_dbt, ["policy_id", "version_no"])
    assert not mismatches, f"Changed records mismatch: {mismatches}"

def test_scd2_unchanged_logic(snowflake_conn, run_date, batch_id):
    ds_rows = fetch_table(snowflake_conn, "DATASTAGE_DIM_POLICY_OUTPUT")
    dbt_rows = fetch_table(snowflake_conn, "DBT_DIM_POLICY_OUTPUT")
    unchanged_ds = [r for r in ds_rows if r["version_no"] == 1 and r["current_flag"] == "Y"]
    unchanged_dbt = [r for r in dbt_rows if r["version_no"] == 1 and r["current_flag"] == "Y"]
    assert len(unchanged_ds) == len(unchanged_dbt), "Unchanged records count mismatch"
    mismatches = compare_rows(unchanged_ds, unchanged_dbt, ["policy_id"])
    assert not mismatches, f"Unchanged records mismatch: {mismatches}"

def test_scd2_expiry_logic(snowflake_conn, run_date, batch_id):
    ds_rows = fetch_table(snowflake_conn, "DATASTAGE_DIM_POLICY_OUTPUT")
    dbt_rows = fetch_table(snowflake_conn, "DBT_DIM_POLICY_OUTPUT")
    expired_ds = [r for r in ds_rows if r["current_flag"] == "N"]
    expired_dbt = [r for r in dbt_rows if r["current_flag"] == "N"]
    assert len(expired_ds) == len(expired_dbt), "Expired records count mismatch"
    mismatches = compare_rows(expired_ds, expired_dbt, ["policy_id", "effective_to"])
    assert not mismatches, f"Expired records mismatch: {mismatches}"

def test_null_default_handling(snowflake_conn, run_date, batch_id):
    ds_rows = fetch_table(snowflake_conn, "DATASTAGE_DIM_POLICY_OUTPUT")
    dbt_rows = fetch_table(snowflake_conn, "DBT_DIM_POLICY_OUTPUT")
    null_ds = [r for r in ds_rows if any(v is None for v in r.values())]
    null_dbt = [r for r in dbt_rows if any(v is None for v in r.values())]
    assert len(null_ds) == len(null_dbt), "NULL/default handling mismatch"

def test_column_mapping_and_types(snowflake_conn):
    cur = snowflake_conn.cursor()
    cur.execute("DESC TABLE DATASTAGE_DIM_POLICY_OUTPUT")
    ds_schema = {row[0]: row[1] for row in cur.fetchall()}
    cur.execute("DESC TABLE DBT_DIM_POLICY_OUTPUT")
    dbt_schema = {row[0]: row[1] for row in cur.fetchall()}
    cur.close()
    assert ds_schema == dbt_schema, f"Schema/type mismatch: {ds_schema} vs {dbt_schema}"

def test_business_rule_filters(snowflake_conn, run_date):
    ds_rows = fetch_table(snowflake_conn, "DATASTAGE_DIM_POLICY_OUTPUT")
    dbt_rows = fetch_table(snowflake_conn, "DBT_DIM_POLICY_OUTPUT")
    filtered_ds = [r for r in ds_rows if r["updated_date"] <= run_date]
    filtered_dbt = [r for r in dbt_rows if r["updated_date"] <= run_date]
    assert len(filtered_ds) == len(filtered_dbt), "Business rule filter mismatch"

def test_error_reject_handling(snowflake_conn):
    ds_rejects = fetch_table(snowflake_conn, "DATASTAGE_DIM_POLICY_REJECTS")
    dbt_rejects = fetch_table(snowflake_conn, "DBT_DIM_POLICY_REJECTS")
    assert len(ds_rejects) == len(dbt_rejects), "Rejects count mismatch"
    mismatches = compare_rows(ds_rejects, dbt_rejects, ["policy_id"])
    assert not mismatches, f"Rejects mismatch: {mismatches}"

def test_audit_logging(snowflake_conn, batch_id):
    ds_audit = fetch_table(snowflake_conn, "DATASTAGE_AUDIT_LOG")
    dbt_audit = fetch_table(snowflake_conn, "DBT_AUDIT_LOG")
    ds_batch = [r for r in ds_audit if r["batch_id"] == batch_id]
    dbt_batch = [r for r in dbt_audit if r["batch_id"] == batch_id]
    assert len(ds_batch) == len(dbt_batch), "Audit log batch count mismatch"
    mismatches = compare_rows(ds_batch, dbt_batch, ["batch_id", "job_name"])
    assert not mismatches, f"Audit log mismatch: {mismatches}"

def test_performance_large_dataset(snowflake_conn):
    import time
    start = time.time()
    dbt_count = fetch_count(snowflake_conn, "DBT_DIM_POLICY_OUTPUT")
    duration = time.time() - start
    assert duration < 300, f"DBT model performance issue: {duration}s for {dbt_count} rows"

# Reporting hook
def pytest_terminal_summary(terminalreporter, exitstatus, config):
    report = []
    for test in terminalreporter.stats.get('passed', []) + terminalreporter.stats.get('failed', []):
        for item in test:
            report.append({
                "Test Case": item.nodeid,
                "Outcome": item.outcome,
                "Duration": getattr(item, 'duration', None),
                "Details": getattr(item, 'longrepr', None)
            })
    if report:
        write_report(report, fmt="csv")
        write_report(report, fmt="json")

```

---

**API Usage Cost Estimate**

- GitHub File Reader Tool: 1 call (to read DSX file) ≈ $0.01
- File reading and processing: 2 calls (DBT SQL, YAML) ≈ $0.02
- Total estimated cost: **$0.03 USD**

---

**Execution Report Template**

| Test Case ID | Description | Validation Type | Row Count Match (%) | Column Match Summary | Failed Rows (sample) | Overall Status | Execution Time (s) |
|--------------|-------------|----------------|---------------------|---------------------|----------------------|----------------|--------------------|
| TC01         | Record count equivalence | Count | 100%              | All columns match   | None                 | PASS           | 2.1                |
| TC02         | SCD2 change detection    | SCD2  | 100%              | All columns match   | None                 | PASS           | 2.5                |
| ...          | ...                     | ...   | ...                | ...                 | ...                  | ...            | ...                |

Output formats: Console, CSV, JSON, HTML (optional)

---

**Note:** Replace table names in the Pytest script with actual Snowflake table names as per your environment. Ensure environment variables are set for Snowflake credentials and DBT variables.