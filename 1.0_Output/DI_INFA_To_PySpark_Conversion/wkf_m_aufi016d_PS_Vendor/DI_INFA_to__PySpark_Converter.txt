```
=============================================
Author:        Ascendion AVA+
Date:   
Description:   PySpark ETL script to extract vendor master data from a GCS flat file, enrich with audit year and load timestamp, and write to a Delta table for GCP Dataproc.
=============================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, current_timestamp
import sys

# -------------------------------
# PARAMETERS (to be set at runtime or via Dataproc arguments)
# -------------------------------
# Example usage:
# spark-submit script.py <input_path> <output_path> <audit_year>
input_path = sys.argv[1]   # e.g., 'gs://your-bucket/input/PS_VENDOR_S4.txt'
output_path = sys.argv[2]  # e.g., 'gs://your-bucket/output/PS_VENDOR'
audit_year = int(sys.argv[3])  # e.g., 2024

# -------------------------------
# SPARK SESSION INIT
# -------------------------------
spark = SparkSession.builder \
    .appName("PS_VENDOR_ETL") \
    .getOrCreate()

# -------------------------------
# READ SOURCE FLAT FILE (DELIMITED BY ||, US-ASCII)
# -------------------------------
df_vendor = spark.read.format("csv") \
    .option("delimiter", "||") \
    .option("header", "true") \
    .option("encoding", "US-ASCII") \
    .option("inferSchema", "true") \
    .load(input_path)

# -------------------------------
# ENRICH WITH AUDIT YEAR AND LOAD TIMESTAMP
# -------------------------------
df_vendor_enriched = df_vendor \
    .withColumn("AUD_YR_NBR", lit(audit_year)) \
    .withColumn("LOAD_DT", current_timestamp())

# -------------------------------
# WRITE TO DELTA TABLE (OVERWRITE TO MIMIC TRUNCATE)
# -------------------------------
df_vendor_enriched.write.format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .save(output_path)

# -------------------------------
# END
# -------------------------------
print("API Cost Consumed in dollars: $X.XX")
```