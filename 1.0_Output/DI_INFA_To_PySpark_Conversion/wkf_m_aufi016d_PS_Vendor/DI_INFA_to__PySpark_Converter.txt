=============================================
Author:        Ascendion AVA+
Date:   
Description:   PySpark script to read PS_VENDOR_S4 flat file, apply field derivations, and write to PS_VENDOR Delta table on GCP Dataproc.
=============================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, current_timestamp
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# Initialize Spark session
spark = SparkSession.builder \
    .appName("PS_VENDOR_ETL") \
    .getOrCreate()

# Define schema for input flat file (matching Informatica source definition)
schema = StructType([
    StructField("AUD_YR_NBR", IntegerType(), False),
    StructField("SET_ID", StringType(), True),
    StructField("VENDOR_NBR_ID", StringType(), True),
    StructField("VENDOR_SHORT_NAME", StringType(), True),
    StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
    StructField("NAME_1", StringType(), True),
    StructField("NAME_2", StringType(), True),
    StructField("VENDOR_STAT_CD", StringType(), True),
    StructField("VENDOR_CLASS_CD", StringType(), True),
    StructField("REMIT_SET_ID", StringType(), True),
    StructField("REMIT_VENDOR_ID", StringType(), True),
    StructField("CORP_SET_ID", StringType(), True),
    StructField("CORP_VENDOR_ID", StringType(), True),
    StructField("ENTERED_BY_ID", StringType(), True),
    StructField("WTHD_SW_CD", StringType(), True),
    StructField("PRIM_VENDOR_ID", StringType(), True),
    StructField("LOAD_DT", StringType(), True),
    StructField("ACCOUNT_GROUP", StringType(), True)
])

# Path to input flat file in Google Cloud Storage (replace with actual path)
input_path = "gs://your-bucket/corp/SrcFiles/profrec_vendor.txt"

# Read flat file as DataFrame
df = spark.read \
    .format("csv") \
    .option("delimiter", "||") \
    .option("header", "false") \
    .option("nullValue", "*") \
    .schema(schema) \
    .load(input_path)

# Apply Expression transformation logic
# AUD_YR_NBR is set from a parameter (default 2019 if not provided)
AUDIT_YEAR = 2019  # Replace with dynamic parameter if needed

df_transformed = df.withColumn("AUD_YR_NBR", lit(AUDIT_YEAR)) \
    .withColumn("LOAD_DT", current_timestamp())

# Select and reorder columns as per target definition
df_final = df_transformed.select(
    "AUD_YR_NBR",
    "SET_ID",
    "VENDOR_NBR_ID",
    "VENDOR_SHORT_NAME",
    "VENDOR_SHORT_USR_NAME",
    "NAME_1",
    "NAME_2",
    "VENDOR_STAT_CD",
    "VENDOR_CLASS_CD",
    "REMIT_SET_ID",
    "REMIT_VENDOR_ID",
    "CORP_SET_ID",
    "CORP_VENDOR_ID",
    "ENTERED_BY_ID",
    "WTHD_SW_CD",
    "PRIM_VENDOR_ID",
    "LOAD_DT",
    "ACCOUNT_GROUP"
)

# Optional: Cache for performance if reused
df_final.cache()

# Path to output Delta table in Google Cloud Storage (replace with actual path)
output_path = "gs://your-bucket/delta/PS_VENDOR"

# Write to Delta table (overwrite mode, can be changed as needed)
df_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(output_path)

print("API Cost Consumed in dollars: $0.02")