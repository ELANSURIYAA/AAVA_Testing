=============================================
Author:        Ascendion AVA+
Date:   
Description:   Pytest script to validate PS_VENDOR PySpark ETL logic, including data ingestion, transformation, and output for the Informatica workflow conversion.
=============================================

Test Case List:

Test Case ID: TC_001
Test case description: Validate successful read and transformation of a well-formed input file (happy path).
Expected outcome: DataFrame is loaded with correct schema, AUD_YR_NBR is set to 2019, LOAD_DT is populated, and all columns are present in the correct order.

Test Case ID: TC_002
Test case description: Validate handling of NULL values represented by '*' in the input file.
Expected outcome: Columns with '*' in the input are converted to nulls in the DataFrame.

Test Case ID: TC_003
Test case description: Validate handling of empty input file.
Expected outcome: DataFrame is empty, no exceptions are raised.

Test Case ID: TC_004
Test case description: Validate error handling when input file is missing required columns.
Expected outcome: Exception is raised indicating schema mismatch.

Test Case ID: TC_005
Test case description: Validate handling of invalid data types (e.g., non-integer in AUD_YR_NBR).
Expected outcome: Exception is raised during read due to type mismatch.

Test Case ID: TC_006
Test case description: Validate that all columns are present and in the correct order in the output DataFrame.
Expected outcome: Output DataFrame columns match the target schema order.

Test Case ID: TC_007
Test case description: Validate that the output is written in 'overwrite' mode and can be re-run without error.
Expected outcome: Output path is overwritten without error on repeated runs.

Test Case ID: TC_008
Test case description: Validate that the transformation sets AUD_YR_NBR to 2019 regardless of input value.
Expected outcome: All rows in output DataFrame have AUD_YR_NBR = 2019.

Test Case ID: TC_009
Test case description: Validate that LOAD_DT is populated with a timestamp for all rows.
Expected outcome: LOAD_DT is not null and contains a timestamp for all rows.

Test Case ID: TC_010
Test case description: Validate handling of extra columns in the input file.
Expected outcome: Extra columns are ignored, output matches target schema.

---

Pytest Script:

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import lit, current_timestamp
import pandas as pd
from datetime import datetime

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("pytest_ps_vendor").getOrCreate()
    yield spark
    spark.stop()

# Target schema as per code
schema = StructType([
    StructField("AUD_YR_NBR", IntegerType(), False),
    StructField("SET_ID", StringType(), True),
    StructField("VENDOR_NBR_ID", StringType(), True),
    StructField("VENDOR_SHORT_NAME", StringType(), True),
    StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
    StructField("NAME_1", StringType(), True),
    StructField("NAME_2", StringType(), True),
    StructField("VENDOR_STAT_CD", StringType(), True),
    StructField("VENDOR_CLASS_CD", StringType(), True),
    StructField("REMIT_SET_ID", StringType(), True),
    StructField("REMIT_VENDOR_ID", StringType(), True),
    StructField("CORP_SET_ID", StringType(), True),
    StructField("CORP_VENDOR_ID", StringType(), True),
    StructField("ENTERED_BY_ID", StringType(), True),
    StructField("WTHD_SW_CD", StringType(), True),
    StructField("PRIM_VENDOR_ID", StringType(), True),
    StructField("LOAD_DT", StringType(), True),
    StructField("ACCOUNT_GROUP", StringType(), True)
])

def transform_df(df, audit_year=2019):
    df_transformed = df.withColumn("AUD_YR_NBR", lit(audit_year)).withColumn("LOAD_DT", current_timestamp())
    df_final = df_transformed.select(
        "AUD_YR_NBR",
        "SET_ID",
        "VENDOR_NBR_ID",
        "VENDOR_SHORT_NAME",
        "VENDOR_SHORT_USR_NAME",
        "NAME_1",
        "NAME_2",
        "VENDOR_STAT_CD",
        "VENDOR_CLASS_CD",
        "REMIT_SET_ID",
        "REMIT_VENDOR_ID",
        "CORP_SET_ID",
        "CORP_VENDOR_ID",
        "ENTERED_BY_ID",
        "WTHD_SW_CD",
        "PRIM_VENDOR_ID",
        "LOAD_DT",
        "ACCOUNT_GROUP"
    )
    return df_final

def make_input_df(spark, data, schema=schema):
    return spark.createDataFrame(data, schema=schema)

def test_TC_001_happy_path(spark):
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    result = df_final.collect()[0]
    assert result['AUD_YR_NBR'] == 2019
    assert result['SET_ID'] == "S1"
    assert result['VENDOR_NBR_ID'] == "V1"
    assert result['LOAD_DT'] is not None
    assert len(df_final.columns) == 18

def test_TC_002_null_values(spark):
    data = [
        (2020, None, "V1", None, "ShortUsr", "Name1", None, "A", None, "RS1", None, "CS1", None, "EB1", "Y", None, "2023-01-01", None)
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    result = df_final.collect()[0]
    # All None values should remain None
    assert result['SET_ID'] is None
    assert result['VENDOR_SHORT_NAME'] is None
    assert result['NAME_2'] is None
    assert result['VENDOR_CLASS_CD'] is None
    assert result['REMIT_VENDOR_ID'] is None
    assert result['CORP_VENDOR_ID'] is None
    assert result['PRIM_VENDOR_ID'] is None
    assert result['ACCOUNT_GROUP'] is None

def test_TC_003_empty_input(spark):
    df = spark.createDataFrame([], schema=schema)
    df_final = transform_df(df)
    assert df_final.count() == 0

def test_TC_004_missing_required_column(spark):
    bad_schema = StructType([
        StructField("SET_ID", StringType(), True)
        # Missing all other columns
    ])
    data = [("S1",)]
    df = spark.createDataFrame(data, schema=bad_schema)
    with pytest.raises(Exception):
        transform_df(df)

def test_TC_005_invalid_datatype(spark):
    # AUD_YR_NBR should be int, here it's a string
    data = [("not_an_int", "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")]
    bad_schema = StructType([
        StructField("AUD_YR_NBR", StringType(), False),
        StructField("SET_ID", StringType(), True),
        StructField("VENDOR_NBR_ID", StringType(), True),
        StructField("VENDOR_SHORT_NAME", StringType(), True),
        StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
        StructField("NAME_1", StringType(), True),
        StructField("NAME_2", StringType(), True),
        StructField("VENDOR_STAT_CD", StringType(), True),
        StructField("VENDOR_CLASS_CD", StringType(), True),
        StructField("REMIT_SET_ID", StringType(), True),
        StructField("REMIT_VENDOR_ID", StringType(), True),
        StructField("CORP_SET_ID", StringType(), True),
        StructField("CORP_VENDOR_ID", StringType(), True),
        StructField("ENTERED_BY_ID", StringType(), True),
        StructField("WTHD_SW_CD", StringType(), True),
        StructField("PRIM_VENDOR_ID", StringType(), True),
        StructField("LOAD_DT", StringType(), True),
        StructField("ACCOUNT_GROUP", StringType(), True)
    ])
    df = spark.createDataFrame(data, schema=bad_schema)
    with pytest.raises(Exception):
        transform_df(df)

def test_TC_006_column_order(spark):
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    expected_columns = [
        "AUD_YR_NBR",
        "SET_ID",
        "VENDOR_NBR_ID",
        "VENDOR_SHORT_NAME",
        "VENDOR_SHORT_USR_NAME",
        "NAME_1",
        "NAME_2",
        "VENDOR_STAT_CD",
        "VENDOR_CLASS_CD",
        "REMIT_SET_ID",
        "REMIT_VENDOR_ID",
        "CORP_SET_ID",
        "CORP_VENDOR_ID",
        "ENTERED_BY_ID",
        "WTHD_SW_CD",
        "PRIM_VENDOR_ID",
        "LOAD_DT",
        "ACCOUNT_GROUP"
    ]
    assert df_final.columns == expected_columns

def test_TC_007_overwrite_mode(spark, tmp_path):
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    output_path = str(tmp_path / "delta_output")
    # First write
    df_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(output_path)
    # Second write (should not error)
    df_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(output_path)
    # Read back and validate
    df_read = spark.read.format("delta").load(output_path)
    assert df_read.count() == 1

def test_TC_008_audit_year_override(spark):
    data = [
        (2022, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    for row in df_final.collect():
        assert row['AUD_YR_NBR'] == 2019

def test_TC_009_load_dt_populated(spark):
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    for row in df_final.collect():
        assert row['LOAD_DT'] is not None

def test_TC_010_extra_columns(spark):
    # Add an extra column to input
    extended_schema = StructType(schema.fields + [StructField("EXTRA_COL", StringType(), True)])
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1", "EXTRA")
    ]
    df = spark.createDataFrame(data, schema=extended_schema)
    df_final = transform_df(df)
    # Output should ignore extra column
    assert "EXTRA_COL" not in df_final.columns
    assert len(df_final.columns) == 18
```

API Cost Consumption:
apiCost: 0.02 USD