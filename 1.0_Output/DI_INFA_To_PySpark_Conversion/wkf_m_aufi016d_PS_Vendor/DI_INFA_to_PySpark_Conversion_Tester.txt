Description:
This document provides a comprehensive validation suite for the PySpark ETL migration of Informatica mapping 'wkf_aufi016d_PS_Vendor'/'m_aufi016d_PS_Vendor'. It covers transformation logic comparison, manual intervention recommendations, test case generation, and Pytest scripts to ensure the correctness of the PySpark code, focusing on data type conversions, null/edge case handling, schema alignment, and error scenarios.

Test Case List:

| Test Case ID | Description                                                                                       | Expected Outcome                                                                                          |
|--------------|---------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
| TC_001       | Happy Path: All columns present, valid types, valid LOAD_DT, no nulls                             | All columns mapped correctly, LOAD_DT cast to date, AUD_YR_NBR set, output matches expected DataFrame     |
| TC_002       | Edge: LOAD_DT contains invalid date string                                                        | LOAD_DT is set to null in output                                                                         |
| TC_003       | Edge: LOAD_DT is null                                                                             | LOAD_DT remains null in output                                                                           |
| TC_004       | Edge: Some string columns are null                                                                | Nulls are preserved in output                                                                            |
| TC_005       | Edge: Input DataFrame is empty                                                                    | Output DataFrame is empty, schema matches                                                                |
| TC_006       | Edge: Minimum and maximum boundary values for integer and string columns                          | Values are preserved and correctly cast                                                                  |
| TC_007       | Error: Missing required column (e.g., VENDOR_NBR_ID missing)                                      | Raises AnalysisException or similar error                                                                |
| TC_008       | Error: Wrong data type in integer column (e.g., AUD_YR_NBR as string in input)                    | AUD_YR_NBR is correctly cast to integer, or error if not possible                                        |
| TC_009       | Error: Additional unexpected columns in input                                                     | Extra columns are ignored, output schema matches target                                                  |
| TC_010       | Edge: All columns are null                                                                        | Output DataFrame contains all nulls, LOAD_DT remains null                                                |

Pytest Script:

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DateType
)
from pyspark.sql.utils import AnalysisException
from datetime import datetime

# Import the transformation function from the ETL script
# from your_etl_module import transform_vendor

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder \
        .master("local[1]") \
        .appName("pytest_PS_VENDOR") \
        .getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def audit_year():
    return 2019

def make_input_df(spark, data):
    schema = StructType([
        StructField("SET_ID", StringType(), True),
        StructField("VENDOR_NBR_ID", StringType(), True),
        StructField("VENDOR_SHORT_NAME", StringType(), True),
        StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
        StructField("NAME_1", StringType(), True),
        StructField("NAME_2", StringType(), True),
        StructField("VENDOR_STAT_CD", StringType(), True),
        StructField("VENDOR_CLASS_CD", StringType(), True),
        StructField("REMIT_SET_ID", StringType(), True),
        StructField("REMIT_VENDOR_ID", StringType(), True),
        StructField("CORP_SET_ID", StringType(), True),
        StructField("CORP_VENDOR_ID", StringType(), True),
        StructField("ENTERED_BY_ID", StringType(), True),
        StructField("WTHD_SW_CD", StringType(), True),
        StructField("PRIM_VENDOR_ID", StringType(), True),
        StructField("LOAD_DT", StringType(), True),
        StructField("ACCOUNT_GROUP", StringType(), True),
    ])
    return spark.createDataFrame(data, schema=schema)

def collect_as_dict(df):
    return [row.asDict() for row in df.collect()]

# TC_001: Happy Path
def test_happy_path(spark, audit_year):
    data = [(
        "SET1", "VEND123", "ShortName", "ShortUsr", "Name1", "Name2",
        "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "W", "PV1", "2023-01-15", "AG1"
    )]
    df_in = make_input_df(spark, data)
    from pyspark.sql.functions import lit
    from pyspark.sql.types import IntegerType, DateType
    # Import the function here to avoid circular import in pytest collection
    from your_etl_module import transform_vendor
    df_out = transform_vendor(df_in, audit_year)
    out = collect_as_dict(df_out)
    assert out[0]["AUD_YR_NBR"] == audit_year
    assert out[0]["SET_ID"] == "SET1"
    assert out[0]["VENDOR_NBR_ID"] == "VEND123"
    assert out[0]["LOAD_DT"] == datetime(2023, 1, 15).date()

# TC_002: Invalid LOAD_DT
def test_invalid_loaddt(spark, audit_year):
    data = [(
        "SET1", "VEND123", "ShortName", "ShortUsr", "Name1", "Name2",
        "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "W", "PV1", "invalid-date", "AG1"
    )]
    df_in = make_input_df(spark, data)
    from your_etl_module import transform_vendor
    df_out = transform_vendor(df_in, audit_year)
    out = collect_as_dict(df_out)
    assert out[0]["LOAD_DT"] is None

# TC_003: Null LOAD_DT
def test_null_loaddt(spark, audit_year):
    data = [(
        "SET1", "VEND123", "ShortName", "ShortUsr", "Name1", "Name2",
        "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "W", "PV1", None, "AG1"
    )]
    df_in = make_input_df(spark, data)
    from your_etl_module import transform_vendor
    df_out = transform_vendor(df_in, audit_year)
    out = collect_as_dict(df_out)
    assert out[0]["LOAD_DT"] is None

# TC_004: Null string columns
def test_null_string_columns(spark, audit_year):
    data = [(
        None, None, None, None, None, None,
        None, None, None, None, None, None, None, None, None, "2023-01-15", None
    )]
    df_in = make_input_df(spark, data)
    from your_etl_module import transform_vendor
    df_out = transform_vendor(df_in, audit_year)
    out = collect_as_dict(df_out)
    for col in [
        "SET_ID", "VENDOR_NBR_ID", "VENDOR_SHORT_NAME", "VENDOR_SHORT_USR_NAME", "NAME_1", "NAME_2",
        "VENDOR_STAT_CD", "VENDOR_CLASS_CD", "REMIT_SET_ID", "REMIT_VENDOR_ID", "CORP_SET_ID",
        "CORP_VENDOR_ID", "ENTERED_BY_ID", "WTHD_SW_CD", "PRIM_VENDOR_ID", "ACCOUNT_GROUP"
    ]:
        assert out[0][col] is None

# TC_005: Empty DataFrame
def test_empty_input(spark, audit_year):
    data = []
    df_in = make_input_df(spark, data)
    from your_etl_module import transform_vendor
    df_out = transform_vendor(df_in, audit_year)
    assert df_out.count() == 0
    assert set(df_out.columns) == {
        "AUD_YR_NBR", "SET_ID", "VENDOR_NBR_ID", "VENDOR_SHORT_NAME",
        "VENDOR_SHORT_USR_NAME", "NAME_1", "NAME_2", "VENDOR_STAT_CD",
        "VENDOR_CLASS_CD", "REMIT_SET_ID", "REMIT_VENDOR_ID", "CORP_SET_ID",
        "CORP_VENDOR_ID", "ENTERED_BY_ID", "WTHD_SW_CD", "PRIM_VENDOR_ID",
        "LOAD_DT", "ACCOUNT_GROUP"
    }

# TC_006: Boundary values
def test_boundary_values(spark, audit_year):
    data = [(
        "Z"*255, "9"*20, "A"*100, "B"*100, "C"*100, "D"*100,
        "E", "F", "G", "H", "I", "J", "K", "L", "M", "2022-12-31", "N"*50
    )]
    df_in = make_input_df(spark, data)
    from your_etl_module import transform_vendor
    df_out = transform_vendor(df_in, audit_year)
    out = collect_as_dict(df_out)
    assert out[0]["SET_ID"] == "Z"*255
    assert out[0]["VENDOR_NBR_ID"] == "9"*20
    assert out[0]["LOAD_DT"] == datetime(2022, 12, 31).date()

# TC_007: Missing required column
def test_missing_column(spark, audit_year):
    # Remove VENDOR_NBR_ID
    schema = StructType([
        StructField("SET_ID", StringType(), True),
        # StructField("VENDOR_NBR_ID", StringType(), True),  # Missing
        StructField("VENDOR_SHORT_NAME", StringType(), True),
        StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
        StructField("NAME_1", StringType(), True),
        StructField("NAME_2", StringType(), True),
        StructField("VENDOR_STAT_CD", StringType(), True),
        StructField("VENDOR_CLASS_CD", StringType(), True),
        StructField("REMIT_SET_ID", StringType(), True),
        StructField("REMIT_VENDOR_ID", StringType(), True),
        StructField("CORP_SET_ID", StringType(), True),
        StructField("CORP_VENDOR_ID", StringType(), True),
        StructField("ENTERED_BY_ID", StringType(), True),
        StructField("WTHD_SW_CD", StringType(), True),
        StructField("PRIM_VENDOR_ID", StringType(), True),
        StructField("LOAD_DT", StringType(), True),
        StructField("ACCOUNT_GROUP", StringType(), True),
    ])
    data = [("SET1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "W", "PV1", "2023-01-15", "AG1")]
    df_in = spark.createDataFrame(data, schema=schema)
    from your_etl_module import transform_vendor
    with pytest.raises(AnalysisException):
        transform_vendor(df_in, audit_year).collect()

# TC_008: Wrong data type in integer column
def test_wrong_type_aud_yr_nbr(spark, audit_year):
    # AUD_YR_NBR is always set by the function, so test input with string type for audit_year
    data = [(
        "SET1", "VEND123", "ShortName", "ShortUsr", "Name1", "Name2",
        "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "W", "PV1", "2023-01-15", "AG1"
    )]
    df_in = make_input_df(spark, data)
    from your_etl_module import transform_vendor
    df_out = transform_vendor(df_in, str(audit_year))  # Pass as string
    out = collect_as_dict(df_out)
    assert out[0]["AUD_YR_NBR"] == int(audit_year)

# TC_009: Extra columns in input
def test_extra_columns(spark, audit_year):
    schema = StructType([
        StructField("SET_ID", StringType(), True),
        StructField("VENDOR_NBR_ID", StringType(), True),
        StructField("VENDOR_SHORT_NAME", StringType(), True),
        StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
        StructField("NAME_1", StringType(), True),
        StructField("NAME_2", StringType(), True),
        StructField("VENDOR_STAT_CD", StringType(), True),
        StructField("VENDOR_CLASS_CD", StringType(), True),
        StructField("REMIT_SET_ID", StringType(), True),
        StructField("REMIT_VENDOR_ID", StringType(), True),
        StructField("CORP_SET_ID", StringType(), True),
        StructField("CORP_VENDOR_ID", StringType(), True),
        StructField("ENTERED_BY_ID", StringType(), True),
        StructField("WTHD_SW_CD", StringType(), True),
        StructField("PRIM_VENDOR_ID", StringType(), True),
        StructField("LOAD_DT", StringType(), True),
        StructField("ACCOUNT_GROUP", StringType(), True),
        StructField("EXTRA_COL", StringType(), True),
    ])
    data = [(
        "SET1", "VEND123", "ShortName", "ShortUsr", "Name1", "Name2",
        "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "W", "PV1", "2023-01-15", "AG1", "extra"
    )]
    df_in = spark.createDataFrame(data, schema=schema)
    from your_etl_module import transform_vendor
    df_out = transform_vendor(df_in, audit_year)
    assert "EXTRA_COL" not in df_out.columns

# TC_010: All columns null
def test_all_nulls(spark, audit_year):
    data = [(None,) * 17]
    df_in = make_input_df(spark, data)
    from your_etl_module import transform_vendor
    df_out = transform_vendor(df_in, audit_year)
    out = collect_as_dict(df_out)
    for v in out[0].values():
        if isinstance(v, int):
            continue  # AUD_YR_NBR is always set
        assert v is None

```