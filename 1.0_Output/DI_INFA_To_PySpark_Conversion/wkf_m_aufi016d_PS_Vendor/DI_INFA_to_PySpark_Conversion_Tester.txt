=============================================
Description: This document details the transformation of an Informatica ETL workflow for loading vendor master data from a flat file into an Oracle table (PS_VENDOR), with audit year and load timestamp enrichment, into its PySpark equivalent. It includes transformation change detection, recommended manual interventions, comprehensive test case generation, and Pytest scripts to validate the PySpark code.

1. Transformation Change Detection

- Expression Transformation Mapping:
  - Informatica’s Expression transformation sets `AUD_YR_NBR` to a mapping variable (`$$Audit_Year`, default 2019) and `LOAD_DT` to the session start time (`SESSSTARTTIME`). In PySpark, these are mapped to `withColumn("AUD_YR_NBR", lit(2019))` and `withColumn("LOAD_DT", current_timestamp())`.
- Aggregator Transformations:
  - None present in this mapping.
- Join Strategies:
  - None present; linear mapping with no joins.
- Data Type Transformations:
  - Informatica `number` → PySpark `IntegerType`
  - Informatica `string` → PySpark `StringType`
  - Informatica `date/time` → PySpark `TimestampType`
- Null Handling and Case Sensitivity Adjustments:
  - Informatica treats `*` as null in flat files; PySpark uses `.option("nullValue", "*")`.
  - Case sensitivity is preserved as column names are consistent.

2. Recommended Manual Interventions

- Parameterization of audit year and file paths (currently hardcoded in PySpark).
- Ensure overwrite mode in PySpark matches Informatica’s target truncation.
- Validate null handling for all columns.
- Confirm all columns are mapped 1:1 and in the correct order.
- No complex transformations, joins, or aggregations requiring UDFs.
- No string manipulations or format conversions beyond what is present.

3. Test Case Generation

Test Case List:

Test Case ID: TC_001
Test case description: Validate successful read and transformation of a well-formed input file (happy path).
Expected outcome: DataFrame is loaded with correct schema, AUD_YR_NBR is set to 2019, LOAD_DT is populated, and all columns are present in the correct order.

Test Case ID: TC_002
Test case description: Validate handling of NULL values represented by '*' in the input file.
Expected outcome: Columns with '*' in the input are converted to nulls in the DataFrame.

Test Case ID: TC_003
Test case description: Validate handling of empty input file.
Expected outcome: DataFrame is empty, no exceptions are raised.

Test Case ID: TC_004
Test case description: Validate error handling when input file is missing required columns.
Expected outcome: Exception is raised indicating schema mismatch.

Test Case ID: TC_005
Test case description: Validate handling of invalid data types (e.g., non-integer in AUD_YR_NBR).
Expected outcome: Exception is raised during read due to type mismatch.

Test Case ID: TC_006
Test case description: Validate that all columns are present and in the correct order in the output DataFrame.
Expected outcome: Output DataFrame columns match the target schema order.

Test Case ID: TC_007
Test case description: Validate that the output is written in 'overwrite' mode and can be re-run without error.
Expected outcome: Output path is overwritten without error on repeated runs.

Test Case ID: TC_008
Test case description: Validate that the transformation sets AUD_YR_NBR to 2019 regardless of input value.
Expected outcome: All rows in output DataFrame have AUD_YR_NBR = 2019.

Test Case ID: TC_009
Test case description: Validate that LOAD_DT is populated with a timestamp for all rows.
Expected outcome: LOAD_DT is not null and contains a timestamp for all rows.

Test Case ID: TC_010
Test case description: Validate handling of extra columns in the input file.
Expected outcome: Extra columns are ignored, output matches target schema.

---

2. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import lit, current_timestamp

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("pytest_ps_vendor").getOrCreate()
    yield spark
    spark.stop()

# Target schema as per code
schema = StructType([
    StructField("AUD_YR_NBR", IntegerType(), False),
    StructField("SET_ID", StringType(), True),
    StructField("VENDOR_NBR_ID", StringType(), True),
    StructField("VENDOR_SHORT_NAME", StringType(), True),
    StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
    StructField("NAME_1", StringType(), True),
    StructField("NAME_2", StringType(), True),
    StructField("VENDOR_STAT_CD", StringType(), True),
    StructField("VENDOR_CLASS_CD", StringType(), True),
    StructField("REMIT_SET_ID", StringType(), True),
    StructField("REMIT_VENDOR_ID", StringType(), True),
    StructField("CORP_SET_ID", StringType(), True),
    StructField("CORP_VENDOR_ID", StringType(), True),
    StructField("ENTERED_BY_ID", StringType(), True),
    StructField("WTHD_SW_CD", StringType(), True),
    StructField("PRIM_VENDOR_ID", StringType(), True),
    StructField("LOAD_DT", StringType(), True),
    StructField("ACCOUNT_GROUP", StringType(), True)
])

def transform_df(df, audit_year=2019):
    df_transformed = df.withColumn("AUD_YR_NBR", lit(audit_year)).withColumn("LOAD_DT", current_timestamp())
    df_final = df_transformed.select(
        "AUD_YR_NBR",
        "SET_ID",
        "VENDOR_NBR_ID",
        "VENDOR_SHORT_NAME",
        "VENDOR_SHORT_USR_NAME",
        "NAME_1",
        "NAME_2",
        "VENDOR_STAT_CD",
        "VENDOR_CLASS_CD",
        "REMIT_SET_ID",
        "REMIT_VENDOR_ID",
        "CORP_SET_ID",
        "CORP_VENDOR_ID",
        "ENTERED_BY_ID",
        "WTHD_SW_CD",
        "PRIM_VENDOR_ID",
        "LOAD_DT",
        "ACCOUNT_GROUP"
    )
    return df_final

def make_input_df(spark, data, schema=schema):
    return spark.createDataFrame(data, schema=schema)

def test_TC_001_happy_path(spark):
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    result = df_final.collect()[0]
    assert result['AUD_YR_NBR'] == 2019
    assert result['SET_ID'] == "S1"
    assert result['VENDOR_NBR_ID'] == "V1"
    assert result['LOAD_DT'] is not None
    assert len(df_final.columns) == 18

def test_TC_002_null_values(spark):
    data = [
        (2020, None, "V1", None, "ShortUsr", "Name1", None, "A", None, "RS1", None, "CS1", None, "EB1", "Y", None, "2023-01-01", None)
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    result = df_final.collect()[0]
    # All None values should remain None
    assert result['SET_ID'] is None
    assert result['VENDOR_SHORT_NAME'] is None
    assert result['NAME_2'] is None
    assert result['VENDOR_CLASS_CD'] is None
    assert result['REMIT_VENDOR_ID'] is None
    assert result['CORP_VENDOR_ID'] is None
    assert result['PRIM_VENDOR_ID'] is None
    assert result['ACCOUNT_GROUP'] is None

def test_TC_003_empty_input(spark):
    df = spark.createDataFrame([], schema=schema)
    df_final = transform_df(df)
    assert df_final.count() == 0

def test_TC_004_missing_required_column(spark):
    bad_schema = StructType([
        StructField("SET_ID", StringType(), True)
        # Missing all other columns
    ])
    data = [("S1",)]
    df = spark.createDataFrame(data, schema=bad_schema)
    with pytest.raises(Exception):
        transform_df(df)

def test_TC_005_invalid_datatype(spark):
    # AUD_YR_NBR should be int, here it's a string
    data = [("not_an_int", "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")]
    bad_schema = StructType([
        StructField("AUD_YR_NBR", StringType(), False),
        StructField("SET_ID", StringType(), True),
        StructField("VENDOR_NBR_ID", StringType(), True),
        StructField("VENDOR_SHORT_NAME", StringType(), True),
        StructField("VENDOR_SHORT_USR_NAME", StringType(), True),
        StructField("NAME_1", StringType(), True),
        StructField("NAME_2", StringType(), True),
        StructField("VENDOR_STAT_CD", StringType(), True),
        StructField("VENDOR_CLASS_CD", StringType(), True),
        StructField("REMIT_SET_ID", StringType(), True),
        StructField("REMIT_VENDOR_ID", StringType(), True),
        StructField("CORP_SET_ID", StringType(), True),
        StructField("CORP_VENDOR_ID", StringType(), True),
        StructField("ENTERED_BY_ID", StringType(), True),
        StructField("WTHD_SW_CD", StringType(), True),
        StructField("PRIM_VENDOR_ID", StringType(), True),
        StructField("LOAD_DT", StringType(), True),
        StructField("ACCOUNT_GROUP", StringType(), True)
    ])
    df = spark.createDataFrame(data, schema=bad_schema)
    with pytest.raises(Exception):
        transform_df(df)

def test_TC_006_column_order(spark):
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    expected_columns = [
        "AUD_YR_NBR",
        "SET_ID",
        "VENDOR_NBR_ID",
        "VENDOR_SHORT_NAME",
        "VENDOR_SHORT_USR_NAME",
        "NAME_1",
        "NAME_2",
        "VENDOR_STAT_CD",
        "VENDOR_CLASS_CD",
        "REMIT_SET_ID",
        "REMIT_VENDOR_ID",
        "CORP_SET_ID",
        "CORP_VENDOR_ID",
        "ENTERED_BY_ID",
        "WTHD_SW_CD",
        "PRIM_VENDOR_ID",
        "LOAD_DT",
        "ACCOUNT_GROUP"
    ]
    assert df_final.columns == expected_columns

def test_TC_007_overwrite_mode(spark, tmp_path):
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    output_path = str(tmp_path / "delta_output")
    # First write
    df_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(output_path)
    # Second write (should not error)
    df_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(output_path)
    # Read back and validate
    df_read = spark.read.format("delta").load(output_path)
    assert df_read.count() == 1

def test_TC_008_audit_year_override(spark):
    data = [
        (2022, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    for row in df_final.collect():
        assert row['AUD_YR_NBR'] == 2019

def test_TC_009_load_dt_populated(spark):
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1")
    ]
    df = make_input_df(spark, data)
    df_final = transform_df(df)
    for row in df_final.collect():
        assert row['LOAD_DT'] is not None

def test_TC_010_extra_columns(spark):
    # Add an extra column to input
    extended_schema = StructType(schema.fields + [StructField("EXTRA_COL", StringType(), True)])
    data = [
        (2020, "S1", "V1", "ShortName", "ShortUsr", "Name1", "Name2", "A", "B", "RS1", "RV1", "CS1", "CV1", "EB1", "Y", "PV1", "2023-01-01", "AG1", "EXTRA")
    ]
    df = spark.createDataFrame(data, schema=extended_schema)
    df_final = transform_df(df)
    # Output should ignore extra column
    assert "EXTRA_COL" not in df_final.columns
    assert len(df_final.columns) == 18
```