```python
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col
from typing import Optional
from datetime import date

def get_person_data_with_filters(
    spark: SparkSession,
    person_id: Optional[str] = None,
    gender: Optional[int] = None,
    race: Optional[str] = None,
    ethnicity: Optional[str] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    include_contact_info: bool = False,
    include_language: bool = False
) -> DataFrame:
    """
    PySpark implementation of the T-SQL GetPersonDataWithFilters procedure.
    
    This function reads data from the Person table and applies filters based on the
    provided parameters. It also conditionally includes additional columns based on
    the include_contact_info and include_language parameters.
    
    Args:
        spark: SparkSession instance
        person_id: Optional filter for PersonID
        gender: Optional filter for Gender (1 = Male, 0 = Female)
        race: Optional filter for Race
        ethnicity: Optional filter for Ethnicity
        start_date: Optional filter for minimum DOB
        end_date: Optional filter for maximum DOB
        include_contact_info: Whether to include contact information columns
        include_language: Whether to include language columns
        
    Returns:
        DataFrame with filtered Person data and selected columns
        
    Examples:
        >>> spark = SparkSession.builder.appName("PersonDataFilters").getOrCreate()
        >>> result = get_person_data_with_filters(
        ...     spark=spark,
        ...     gender=0,
        ...     ethnicity="Hispanic",
        ...     include_contact_info=True
        ... )
        >>> result.show()
    """
    try:
        # Validate input parameters
        if gender is not None and gender not in [0, 1]:
            raise ValueError("Gender must be 0 (Female) or 1 (Male)")
        
        # Define base columns that are always included
        base_columns = [
            "PersonID", "DOB", "Gender", "Race", "Ethnicity",
            "Insurance", "Provider", "SourceSystem", "IngestionTimestamp"
        ]
        
        # Add optional columns based on parameters
        selected_columns = base_columns.copy()
        
        if include_contact_info:
            selected_columns.extend(["Address", "ContactNumber", "EmailId"])
        
        if include_language:
            selected_columns.extend(["Spoken", "Written"])
        
        # Read data from the Person table
        person_df = spark.table("`dataswitch-poc.cvs.Person`")
        
        # Build filter conditions
        conditions = []
        
        if person_id is not None:
            conditions.append(col("PersonID") == person_id)
        
        if gender is not None:
            conditions.append(col("Gender") == gender)
        
        if race is not None:
            conditions.append(col("Race") == race)
        
        if ethnicity is not None:
            conditions.append(col("Ethnicity") == ethnicity)
        
        # Apply date range filters
        if start_date is not None and end_date is not None:
            conditions.append((col("DOB") >= start_date) & (col("DOB") <= end_date))
        elif start_date is not None:
            conditions.append(col("DOB") >= start_date)
        elif end_date is not None:
            conditions.append(col("DOB") <= end_date)
        
        # Apply all filters
        result_df = person_df
        for condition in conditions:
            result_df = result_df.filter(condition)
        
        # Select only the required columns
        result_df = result_df.select(*selected_columns)
        
        return result_df
        
    except Exception as e:
        import logging
        logging.error(f"Error in get_person_data_with_filters: {str(e)}")
        raise
```

This PySpark function replicates the functionality of the original T-SQL procedure `GetPersonDataWithFilters`. It:

1. Takes the same parameters as the original T-SQL procedure
2. Reads directly from the `dataswitch-poc.cvs.Person` table
3. Selects the base columns that are always included
4. Conditionally includes additional columns based on the `include_contact_info` and `include_language` parameters
5. Applies filters based on the provided parameters
6. Includes input validation and error handling
7. Uses optimized filter application by collecting all conditions and applying them efficiently

The function maintains the same functionality as the original T-SQL procedure while leveraging PySpark's distributed computing capabilities. It handles the dynamic SQL aspect of the original procedure by conditionally selecting columns and applying filters based on the provided parameters.