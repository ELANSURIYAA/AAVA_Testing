# PySpark Implementation of usp_GetPersonInfo

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
import logging
from typing import Optional, Dict, Any
import traceback

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("GetPersonInfo")

class GetPersonInfo:
    """
    PySpark implementation of the T-SQL stored procedure usp_GetPersonInfo
    with both Spark SQL and DataFrame API approaches.
    """
    
    def __init__(self, spark: SparkSession):
        """
        Initialize with a SparkSession
        
        Args:
            spark: Active SparkSession
        """
        self.spark = spark
        logger.info("GetPersonInfo initialized with SparkSession")
    
    def get_person_info_sql(self, 
                         person_id: Optional[str] = None,
                         gender: Optional[int] = None,
                         race: Optional[str] = None,
                         include_contact_info: bool = False,
                         include_language: bool = False) -> Dict[str, Any]:
        """
        Implementation using Spark SQL approach
        
        Args:
            person_id: Optional filter for PersonID
            gender: Optional filter for Gender
            race: Optional filter for Race
            include_contact_info: Whether to include contact information
            include_language: Whether to include language information
            
        Returns:
            Dictionary containing result DataFrame and execution status
        """
        try:
            logger.info(f"Executing get_person_info_sql with parameters: person_id={person_id}, "
                       f"gender={gender}, race={race}, include_contact_info={include_contact_info}, "
                       f"include_language={include_language}")
            
            # Build the dynamic SQL query
            select_clause = """
                SELECT p.PersonID, p.DOB, p.Gender, p.Race, p.Ethnicity, 
                       p.Insurance, p.Provider, p.SourceSystem, p.IngestionTimestamp
            """
            
            # Add optional contact info columns
            if include_contact_info:
                select_clause += ", c.Address, c.Phone, c.Email"
            
            # Add optional language columns
            if include_language:
                select_clause += ", l.Spoken, l.Written"
            
            # Base table
            from_clause = "FROM Person p"
            
            # Add joins based on parameters
            if include_contact_info:
                from_clause += " LEFT JOIN ContactInfo c ON p.PersonID = c.PersonID"
            
            if include_language:
                from_clause += " LEFT JOIN Language l ON p.PersonID = l.PersonID"
            
            # Add WHERE clause with dynamic conditions
            where_clause = "WHERE 1=1"
            
            # Create a dictionary to hold parameter values
            params = {}
            
            if person_id is not None:
                where_clause += " AND p.PersonID = :person_id"
                params["person_id"] = person_id
            
            if gender is not None:
                where_clause += " AND p.Gender = :gender"
                params["gender"] = gender
            
            if race is not None:
                where_clause += " AND p.Race = :race"
                params["race"] = race
            
            # Combine all parts of the query
            query = f"{select_clause} {from_clause} {where_clause}"
            
            logger.debug(f"Generated SQL query: {query}")
            logger.debug(f"With parameters: {params}")
            
            # Execute the query with parameters
            result_df = self.spark.sql(query, params)
            
            row_count = result_df.count()
            logger.info(f"Query executed successfully, returned {row_count} rows")
            
            return {
                "success": True,
                "data": result_df,
                "row_count": row_count,
                "message": f"Successfully retrieved {row_count} person records"
            }
            
        except Exception as e:
            error_msg = f"Error executing SQL query: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            
            return {
                "success": False,
                "data": None,
                "error": error_msg,
                "exception": e
            }
    
    def get_person_info_df(self, 
                        person_id: Optional[str] = None,
                        gender: Optional[int] = None,
                        race: Optional[str] = None,
                        include_contact_info: bool = False,
                        include_language: bool = False) -> Dict[str, Any]:
        """
        Implementation using DataFrame API approach
        
        Args:
            person_id: Optional filter for PersonID
            gender: Optional filter for Gender
            race: Optional filter for Race
            include_contact_info: Whether to include contact information
            include_language: Whether to include language information
            
        Returns:
            Dictionary containing result DataFrame and execution status
        """
        try:
            logger.info(f"Executing get_person_info_df with parameters: person_id={person_id}, "
                       f"gender={gender}, race={race}, include_contact_info={include_contact_info}, "
                       f"include_language={include_language}")
            
            # Start with the base person DataFrame
            person_df = self.spark.table("Person")
            result_df = person_df
            
            # Apply filters
            if person_id is not None:
                result_df = result_df.filter(col("PersonID") == person_id)
            
            if gender is not None:
                result_df = result_df.filter(col("Gender") == gender)
            
            if race is not None:
                result_df = result_df.filter(col("Race") == race)
            
            # Add joins if needed
            if include_contact_info:
                contact_df = self.spark.table("ContactInfo")
                result_df = result_df.join(
                    contact_df,
                    result_df["PersonID"] == contact_df["PersonID"],
                    "left"
                )
            
            if include_language:
                language_df = self.spark.table("Language")
                result_df = result_df.join(
                    language_df,
                    result_df["PersonID"] == language_df["PersonID"],
                    "left"
                )
            
            # Select only the columns we need
            columns_to_select = [
                "p.PersonID", "p.DOB", "p.Gender", "p.Race", "p.Ethnicity",
                "p.Insurance", "p.Provider", "p.SourceSystem", "p.IngestionTimestamp"
            ]
            
            if include_contact_info:
                columns_to_select.extend(["c.Address", "c.Phone", "c.Email"])
            
            if include_language:
                columns_to_select.extend(["l.Spoken", "l.Written"])
            
            # Convert column strings to actual column references
            select_cols = []
            for col_name in columns_to_select:
                parts = col_name.split(".")
                if len(parts) > 1:
                    table_alias, column = parts
                    select_cols.append(col(f"{column}").alias(f"{column}"))
                else:
                    select_cols.append(col(col_name))
            
            result_df = result_df.select(*select_cols)
            
            row_count = result_df.count()
            logger.info(f"DataFrame operation executed successfully, returned {row_count} rows")
            
            return {
                "success": True,
                "data": result_df,
                "row_count": row_count,
                "message": f"Successfully retrieved {row_count} person records"
            }
            
        except Exception as e:
            error_msg = f"Error executing DataFrame operations: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            
            return {
                "success": False,
                "data": None,
                "error": error_msg,
                "exception": e
            }

# Example usage:
def get_person_info(
    spark: SparkSession,
    person_id: Optional[str] = None,
    gender: Optional[int] = None,
    race: Optional[str] = None,
    include_contact_info: bool = False,
    include_language: bool = False
) -> Dict[str, Any]:
    """
    Wrapper function to call the class implementation
    
    Args:
        spark: SparkSession instance
        person_id: Optional filter for PersonID
        gender: Optional filter for Gender
        race: Optional filter for Race
        include_contact_info: Whether to include contact information
        include_language: Whether to include language information
        
    Returns:
        Dictionary containing result DataFrame and execution status
    """
    person_info = GetPersonInfo(spark)
    # Use DataFrame API implementation by default
    return person_info.get_person_info_df(
        person_id=person_id,
        gender=gender,
        race=race,
        include_contact_info=include_contact_info,
        include_language=include_language
    )

# Example of how to use the function:
if __name__ == "__main__":
    # Create a SparkSession
    spark = SparkSession.builder \
        .appName("PersonInfo") \
        .getOrCreate()
    
    # Call the function with parameters
    result = get_person_info(
        spark=spark,
        person_id="P001",
        include_contact_info=True,
        include_language=True
    )
    
    # Check if the query was successful
    if result["success"]:
        # Show the results
        result["data"].show()
        print(f"Retrieved {result['row_count']} records")
    else:
        # Handle the error
        print(f"Error: {result['error']}")
    
    # Clean up
    spark.stop()