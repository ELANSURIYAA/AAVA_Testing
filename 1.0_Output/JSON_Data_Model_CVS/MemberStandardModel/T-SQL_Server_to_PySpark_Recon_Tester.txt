```python
#!/usr/bin/env python3
"""
SQL Server to PySpark Migration Validation Script

This script provides an end-to-end solution for validating the migration of T-SQL stored procedures
to PySpark functions. It focuses specifically on validating the GetPersonDataWithFilters stored procedure
and its PySpark equivalent get_person_data_with_filters.

The script:
1. Executes the SQL Server stored procedure with various parameter combinations
2. Executes the equivalent PySpark function with the same parameters
3. Compares the results to ensure they match
4. Generates detailed validation reports

Author: Data Engineering Team
"""

import os
import sys
import logging
import argparse
import json
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta
from typing import Dict, List, Tuple, Any, Optional, Union
import pyodbc
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, when
import configparser
from pathlib import Path
import time
import uuid
import traceback

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("sql_to_spark_validation.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("SQLSparkValidator")

class DataReconciliation:
    """
    A class for reconciling data between SQL Server and PySpark dataframes.
    This utility helps in validating data consistency across systems during migration.
    """
    
    def __init__(self, sql_conn_string, spark, output_dir="./validation_reports"):
        """
        Initialize the DataReconciliation class.
        
        Args:
            sql_conn_string: Connection string for SQL Server
            spark: Active Spark session
            output_dir: Directory to store validation reports
        """
        self.spark = spark
        self.sql_conn_string = sql_conn_string
        self.output_dir = output_dir
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Create SQL connection
        try:
            self.sql_conn = pyodbc.connect(sql_conn_string)
            logger.info("Successfully connected to SQL Server")
        except Exception as e:
            logger.error(f"Failed to connect to SQL Server: {str(e)}")
            raise
    
    def execute_sql_proc(self, proc_name, params):
        """
        Execute a SQL Server stored procedure with parameters.
        
        Args:
            proc_name: Name of the stored procedure
            params: Dictionary of parameters
            
        Returns:
            pandas.DataFrame: Result of the stored procedure
        """
        try:
            cursor = self.sql_conn.cursor()
            
            # Build parameter string for the stored procedure
            param_str = ", ".join([f"@{key}=?" for key in params.keys()])
            sql = f"EXEC {proc_name} {param_str}"
            
            # Execute the stored procedure
            logger.info(f"Executing SQL stored procedure: {proc_name}")
            logger.debug(f"Parameters: {params}")
            
            cursor.execute(sql, list(params.values()))
            
            # Fetch results
            columns = [column[0] for column in cursor.description]
            results = cursor.fetchall()
            
            # Create DataFrame
            df = pd.DataFrame.from_records(results, columns=columns)
            logger.info(f"SQL query returned {len(df)} rows")
            
            return df
            
        except Exception as e:
            logger.error(f"Error executing stored procedure {proc_name}: {str(e)}")
            raise
        finally:
            if 'cursor' in locals():
                cursor.close()
    
    def execute_pyspark_func(self, func, params):
        """
        Execute a PySpark function with parameters.
        
        Args:
            func: PySpark function to execute
            params: Dictionary of parameters
            
        Returns:
            pyspark.sql.DataFrame: Result of the PySpark function
        """
        try:
            logger.info(f"Executing PySpark function: {func.__name__}")
            logger.debug(f"Parameters: {params}")
            
            # Add spark session to parameters
            params["spark"] = self.spark
            
            # Execute the function
            result = func(**params)
            
            logger.info(f"PySpark function returned {result.count()} rows")
            return result
            
        except Exception as e:
            logger.error(f"Error executing PySpark function {func.__name__}: {str(e)}")
            raise
    
    def convert_sql_params_to_pyspark(self, sql_params):
        """
        Convert SQL Server procedure parameters to PySpark function parameters.
        
        Args:
            sql_params: Dictionary of SQL Server parameters
            
        Returns:
            Dictionary of PySpark parameters
        """
        # Create a mapping between SQL and PySpark parameter names
        param_mapping = {
            "PersonID": "person_id",
            "Gender": "gender",
            "Race": "race",
            "Ethnicity": "ethnicity",
            "StartDate": "start_date",
            "EndDate": "end_date",
            "IncludeContactInfo": "include_contact_info",
            "IncludeLanguage": "include_language"
        }
        
        # Convert SQL parameters to PySpark parameters
        pyspark_params = {}
        for sql_param, value in sql_params.items():
            if sql_param in param_mapping:
                pyspark_param = param_mapping[sql_param]
                
                # Convert boolean parameters (bit to bool)
                if sql_param in ["IncludeContactInfo", "IncludeLanguage"]:
                    pyspark_params[pyspark_param] = bool(value)
                else:
                    pyspark_params[pyspark_param] = value
        
        return pyspark_params
    
    def compare_dataframes(self, sql_df, spark_df, key_columns):
        """
        Compare SQL and Spark dataframes for data reconciliation.
        
        Args:
            sql_df: Pandas DataFrame from SQL Server
            spark_df: Spark DataFrame from PySpark
            key_columns: List of columns to use as keys for comparison
            
        Returns:
            dict: Reconciliation results
        """
        try:
            logger.info("Starting dataframe comparison")
            
            # Convert Spark DataFrame to Pandas
            spark_pandas_df = spark_df.toPandas()
            
            # Normalize column names (convert to lowercase)
            sql_df.columns = [col.lower() for col in sql_df.columns]
            spark_pandas_df.columns = [col.lower() for col in spark_pandas_df.columns]
            
            # Convert key_columns to lowercase
            key_columns = [col.lower() for col in key_columns]
            
            # Compare row counts
            row_count_match = len(sql_df) == len(spark_pandas_df)
            
            # Find common columns for comparison
            sql_cols = set(sql_df.columns)
            spark_cols = set(spark_pandas_df.columns)
            common_cols = list(sql_cols.intersection(spark_cols))
            
            # Filter DataFrames to include only common columns
            sql_df_common = sql_df[common_cols].copy()
            spark_df_common = spark_pandas_df[common_cols].copy()
            
            # Sort both DataFrames by key columns to ensure proper comparison
            if all(col in common_cols for col in key_columns):
                sql_df_common.sort_values(key_columns, inplace=True)
                spark_df_common.sort_values(key_columns, inplace=True)
                sql_df_common.reset_index(drop=True, inplace=True)
                spark_df_common.reset_index(drop=True, inplace=True)
            
            # Compare data values
            if row_count_match:
                # Create a mask for differing rows
                diff_mask = ~(sql_df_common == spark_df_common).all(axis=1)
                
                # Count matches and mismatches
                matches = (~diff_mask).sum()
                mismatches = diff_mask.sum()
                
                # Calculate match percentage
                total_rows = len(sql_df_common)
                match_percentage = (matches / total_rows * 100) if total_rows > 0 else 0
                
                # Get mismatched keys
                mismatched_keys = []
                if mismatches > 0:
                    mismatched_df = sql_df_common[diff_mask]
                    for _, row in mismatched_df.iterrows():
                        key_dict = {key: row[key] for key in key_columns}
                        mismatched_keys.append(key_dict)
            else:
                # If row counts don't match, consider it a complete mismatch
                match_percentage = 0
                mismatched_keys = []
            
            # Create comparison results
            comparison_results = {
                "row_counts": {
                    "sql_count": len(sql_df),
                    "spark_count": len(spark_pandas_df),
                    "match": row_count_match
                },
                "column_comparison": {
                    "sql_columns": list(sql_cols),
                    "spark_columns": list(spark_cols),
                    "common_columns": common_cols,
                    "sql_only": list(sql_cols - spark_cols),
                    "spark_only": list(spark_cols - sql_cols)
                },
                "data_comparison": {
                    "match_percentage": match_percentage,
                    "mismatched_keys": mismatched_keys[:10]  # Limit to first 10
                }
            }
            
            logger.info(f"Comparison completed. Match percentage: {match_percentage:.2f}%")
            return comparison_results
            
        except Exception as e:
            logger.error(f"Error during dataframe comparison: {str(e)}")
            raise
    
    def reconcile(self, test_name, sql_proc_name, pyspark_func, params, key_columns):
        """
        Run the reconciliation process for a single test case.
        
        Args:
            test_name: Name of the test case
            sql_proc_name: Name of the SQL stored procedure
            pyspark_func: PySpark function to compare
            params: Dictionary of parameters
            key_columns: List of columns to use as keys for comparison
            
        Returns:
            dict: Reconciliation results
        """
        try:
            logger.info(f"Starting reconciliation for test: {test_name}")
            
            # Generate a unique run ID
            run_id = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            # Convert parameters for SQL and PySpark
            sql_params = params.copy()
            pyspark_params = self.convert_sql_params_to_pyspark(params)
            
            # Execute SQL stored procedure
            sql_df = self.execute_sql_proc(sql_proc_name, sql_params)
            
            # Execute PySpark function
            spark_df = self.execute_pyspark_func(pyspark_func, pyspark_params)
            
            # Compare results
            comparison_results = self.compare_dataframes(sql_df, spark_df, key_columns)
            
            # Save results to files
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_path = os.path.join(self.output_dir, f"{test_name}_{timestamp}")
            
            # Save SQL results
            sql_path = f"{base_path}_sql.csv"
            sql_df.to_csv(sql_path, index=False)
            
            # Save Spark results
            spark_path = f"{base_path}_spark.csv"
            spark_df.toPandas().to_csv(spark_path, index=False)
            
            # Save comparison results
            comparison_path = f"{base_path}_comparison.json"
            with open(comparison_path, 'w') as f:
                json.dump(comparison_results, f, indent=2, default=str)
            
            logger.info(f"Reconciliation completed for test: {test_name}")
            logger.info(f"Match percentage: {comparison_results['data_comparison']['match_percentage']:.2f}%")
            
            return comparison_results
            
        except Exception as e:
            logger.error(f"Error during reconciliation for test {test_name}: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    
    def close(self):
        """Close all connections and resources."""
        if hasattr(self, 'sql_conn'):
            self.sql_conn.close()
            logger.info("SQL Server connection closed")


def get_person_data_with_filters(
    spark: SparkSession,
    person_id: Optional[str] = None,
    gender: Optional[int] = None,
    race: Optional[str] = None,
    ethnicity: Optional[str] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    include_contact_info: bool = False,
    include_language: bool = False
) -> DataFrame:
    """
    PySpark implementation of the T-SQL GetPersonDataWithFilters procedure.
    
    This function reads data from the Person table and applies filters based on the
    provided parameters. It also conditionally includes additional columns based on
    the include_contact_info and include_language parameters.
    
    Args:
        spark: SparkSession instance
        person_id: Optional filter for PersonID
        gender: Optional filter for Gender (1 = Male, 0 = Female)
        race: Optional filter for Race
        ethnicity: Optional filter for Ethnicity
        start_date: Optional filter for minimum DOB
        end_date: Optional filter for maximum DOB
        include_contact_info: Whether to include contact information columns
        include_language: Whether to include language columns
        
    Returns:
        DataFrame with filtered Person data and selected columns
    """
    try:
        # Validate input parameters
        if gender is not None and gender not in [0, 1]:
            raise ValueError("Gender must be 0 (Female) or 1 (Male)")
        
        # Define base columns that are always included
        base_columns = [
            "PersonID", "DOB", "Gender", "Race", "Ethnicity",
            "Insurance", "Provider", "SourceSystem", "IngestionTimestamp"
        ]
        
        # Add optional columns based on parameters
        selected_columns = base_columns.copy()
        
        if include_contact_info:
            selected_columns.extend(["Address", "ContactNumber", "EmailId"])
        
        if include_language:
            selected_columns.extend(["Spoken", "Written"])
        
        # Read data from the Person table
        person_df = spark.table("`dataswitch-poc.cvs.Person`")
        
        # Build filter conditions
        conditions = []
        
        if person_id is not None:
            conditions.append(col("PersonID") == person_id)
        
        if gender is not None:
            conditions.append(col("Gender") == gender)
        
        if race is not None:
            conditions.append(col("Race") == race)
        
        if ethnicity is not None:
            conditions.append(col("Ethnicity") == ethnicity)
        
        # Apply date range filters
        if start_date is not None and end_date is not None:
            conditions.append((col("DOB") >= start_date) & (col("DOB") <= end_date))
        elif start_date is not None:
            conditions.append(col("DOB") >= start_date)
        elif end_date is not None:
            conditions.append(col("DOB") <= end_date)
        
        # Apply all filters
        result_df = person_df
        for condition in conditions:
            result_df = result_df.filter(condition)
        
        # Select only the required columns
        result_df = result_df.select(*selected_columns)
        
        return result_df
        
    except Exception as e:
        logger.error(f"Error in get_person_data_with_filters: {str(e)}")
        raise


def main():
    """Main function to run the validation process."""
    parser = argparse.ArgumentParser(description='Validate T-SQL to PySpark migration')
    parser.add_argument('--config', type=str, default='config.ini', help='Path to configuration file')
    parser.add_argument('--test-case', type=int, default=None, help='Specific test case to run (0-based index)')
    parser.add_argument('--all-tests', action='store_true', help='Run all test cases')
    args = parser.parse_args()
    
    # Load configuration
    config = configparser.ConfigParser()
    config.read(args.config)
    
    # Get SQL Server connection string
    sql_conn_string = config['SQL_SERVER']['connection_string']
    
    # Initialize Spark session
    spark = SparkSession.builder \
        .appName("T-SQL to PySpark Validation") \
        .config("spark.sql.legacy.timeParserPolicy", "LEGACY") \
        .getOrCreate()
    
    # Initialize DataReconciliation
    reconciliation = DataReconciliation(
        sql_conn_string=sql_conn_string,
        spark=spark,
        output_dir=config['VALIDATION']['output_dir']
    )
    
    try:
        # Define test cases
        test_cases = [
            {
                "name": "No filters - base columns only",
                "params": {
                    "PersonID": None,
                    "Gender": None,
                    "Race": None,
                    "Ethnicity": None,
                    "StartDate": None,
                    "EndDate": None,
                    "IncludeContactInfo": 0,
                    "IncludeLanguage": 0
                }
            },
            {
                "name": "Filter by gender (Male)",
                "params": {
                    "PersonID": None,
                    "Gender": 1,  # Male
                    "Race": None,
                    "Ethnicity": None,
                    "StartDate": None,
                    "EndDate": None,
                    "IncludeContactInfo": 0,
                    "IncludeLanguage": 0
                }
            },
            {
                "name": "Filter by gender (Female)",
                "params": {
                    "PersonID": None,
                    "Gender": 0,  # Female
                    "Race": None,
                    "Ethnicity": None,
                    "StartDate": None,
                    "EndDate": None,
                    "IncludeContactInfo": 0,
                    "IncludeLanguage": 0
                }
            },
            {
                "name": "Filter by race",
                "params": {
                    "PersonID": None,
                    "Gender": None,
                    "Race": "Asian",
                    "Ethnicity": None,
                    "StartDate": None,
                    "EndDate": None,
                    "IncludeContactInfo": 0,
                    "IncludeLanguage": 0
                }
            },
            {
                "name": "Filter by ethnicity",
                "params": {
                    "PersonID": None,
                    "Gender": None,
                    "Race": None,
                    "Ethnicity": "Hispanic",
                    "StartDate": None,
                    "EndDate": None,
                    "IncludeContactInfo": 0,
                    "IncludeLanguage": 0
                }
            },
            {
                "name": "Filter by date range",
                "params": {
                    "PersonID": None,
                    "Gender": None,
                    "Race": None,
                    "Ethnicity": None,
                    "StartDate": date(1980, 1, 1),
                    "EndDate": date(2000, 12, 31),
                    "IncludeContactInfo": 0,
                    "IncludeLanguage": 0
                }
            },
            {
                "name": "Include contact info only",
                "params": {
                    "PersonID": None,
                    "Gender": None,
                    "Race": None,
                    "Ethnicity": None,
                    "StartDate": None,
                    "EndDate": None,
                    "IncludeContactInfo": 1,
                    "IncludeLanguage": 0
                }
            },
            {
                "name": "Include language info only",
                "params": {
                    "PersonID": None,
                    "Gender": None,
                    "Race": None,
                    "Ethnicity": None,
                    "StartDate": None,
                    "EndDate": None,
                    "IncludeContactInfo": 0,
                    "IncludeLanguage": 1
                }
            },
            {
                "name": "Include both contact and language info",
                "params": {
                    "PersonID": None,
                    "Gender": None,
                    "Race": None,
                    "Ethnicity": None,
                    "StartDate": None,
                    "EndDate": None,
                    "IncludeContactInfo": 1,
                    "IncludeLanguage": 1
                }
            },
            {
                "name": "Complex filter - gender, race, and date range",
                "params": {
                    "PersonID": None,
                    "Gender": 1,  # Male
                    "Race": "Caucasian",
                    "Ethnicity": None,
                    "StartDate": date(1990, 1, 1),
                    "EndDate": date(2000, 12, 31),
                    "IncludeContactInfo": 1,
                    "IncludeLanguage": 0
                }
            },
            {
                "name": "Complex filter - gender, ethnicity, and all optional columns",
                "params": {
                    "PersonID": None,
                    "Gender": 0,  # Female
                    "Race": None,
                    "Ethnicity": "Hispanic",
                    "StartDate": None,
                    "EndDate": None,
                    "IncludeContactInfo": 1,
                    "IncludeLanguage": 1
                }
            }
        ]
        
        # Determine which test cases to run
        if args.test_case is not None:
            if args.test_case < 0 or args.test_case >= len(test_cases):
                logger.error(f"Invalid test case index: {args.test_case}. Must be between 0 and {len(test_cases)-1}")
                return
            selected_tests = [test_cases[args.test_case]]
        elif args.all_tests:
            selected_tests = test_cases
        else:
            # Default: run the first test case
            selected_tests = [test_cases[0]]
        
        # Run selected test cases
        results = []
        for test_case in selected_tests:
            logger.info(f"Running test case: {test_case['name']}")
            
            try:
                # Run reconciliation
                result = reconciliation.reconcile(
                    test_name=test_case["name"],
                    sql_proc_name="GetPersonDataWithFilters",
                    pyspark_func=get_person_data_with_filters,
                    params=test_case["params"],
                    key_columns=["PersonID"]
                )
                
                # Add to results
                results.append({
                    "test_name": test_case["name"],
                    "match_percentage": result["data_comparison"]["match_percentage"],
                    "row_count_match": result["row_counts"]["match"],
                    "sql_count": result["row_counts"]["sql_count"],
                    "spark_count": result["row_counts"]["spark_count"]
                })
                
            except Exception as e:
                logger.error(f"Error in test case {test_case['name']}: {str(e)}")
                results.append({
                    "test_name": test_case["name"],
                    "error": str(e)
                })
        
        # Generate summary report
        summary_path = os.path.join(
            config['VALIDATION']['output_dir'],
            f"summary_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        
        summary = {
            "timestamp": datetime.now().isoformat(),
            "total_tests": len(selected_tests),
            "successful_tests": sum(1 for r in results if "error" not in r),
            "perfect_matches": sum(1 for r in results if "match_percentage" in r and r["match_percentage"] == 100),
            "test_results": results
        }
        
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        logger.info(f"Validation completed. Summary report saved to: {summary_path}")
        
        # Print summary to console
        print("\n=== VALIDATION SUMMARY ===")
        print(f"Total tests: {summary['total_tests']}")
        print(f"Successful tests: {summary['successful_tests']}")
        print(f"Perfect matches: {summary['perfect_matches']}")
        print(f"Summary report: {summary_path}")
        print("=========================\n")
        
    finally:
        # Clean up resources
        reconciliation.close()
        spark.stop()


if __name__ == "__main__":
    main()
```

This comprehensive Python script handles the end-to-end process of validating the migration of the T-SQL stored procedure `GetPersonDataWithFilters` to its PySpark equivalent. The script:

1. Executes the SQL Server stored procedure with various parameter combinations
2. Executes the equivalent PySpark function with the same parameters
3. Compares the results to ensure they match
4. Generates detailed validation reports

Key features:
- Handles different parameter types and conversions between SQL Server and PySpark
- Provides detailed comparison of row counts, column presence, and data values
- Generates comprehensive reports for each test case and a summary report
- Includes robust error handling and logging
- Supports running individual test cases or all test cases
- Configurable through command-line arguments and a configuration file

The script can be easily extended to validate other stored procedures and their PySpark equivalents by adding new test cases and function implementations.