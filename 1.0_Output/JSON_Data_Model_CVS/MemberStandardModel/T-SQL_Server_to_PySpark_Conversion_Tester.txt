# Test Case Document and Pytest Script for SQL Server-to-PySpark Conversion Validation

## 1. Test Case Document

### Test Case ID: TC-001
**Description:** Basic Merge Functionality Test  
Validates that the PySpark implementation correctly updates existing records and inserts new ones in the Person table.

### Test Case ID: TC-002
**Description:** Contact Info Updates Test  
Verifies that the ContactInfo table is correctly updated with new and modified person data.

### Test Case ID: TC-003
**Description:** Language Table Updates Test  
Ensures that the Language table is correctly updated with new and modified language preferences.

### Test Case ID: TC-004
**Description:** Logging Functionality Test  
Validates that operations are correctly logged in the DataProcessingLog table.

### Test Case ID: TC-005
**Description:** Timestamp Updates Test  
Verifies that LastUpdated timestamps are correctly set during merge operations.

### Test Case ID: TC-006
**Description:** Phone Number Formatting Test  
Tests that phone numbers are correctly formatted when inserted into ContactInfo table.

### Test Case ID: TC-007
**Description:** Empty Staging Table Edge Case  
Tests behavior when the staging table contains no records.

### Test Case ID: TC-008
**Description:** NULL Values Handling Test  
Validates proper handling of NULL values in staging data.

### Test Case ID: TC-009
**Description:** Performance Test  
Measures execution time for processing a large dataset (1000 records).

### Test Case ID: TC-010
**Description:** Error Handling Test  
Verifies graceful handling of invalid data types.

### Test Case ID: TC-011
**Description:** Syntax Compatibility Test  
Ensures PySpark code doesn't use SQL Server specific syntax.

### Test Case ID: TC-012
**Description:** Data Integrity Constraints Test  
Validates that data integrity constraints are maintained (e.g., no duplicate PersonIDs).

### Test Case ID: TC-013
**Description:** SQL Server Results Comparison Test  
Compares PySpark results with expected SQL Server results.

## 2. Pytest Script

```python
import pytest
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, regexp_replace, current_timestamp
import pandas as pd
from datetime import datetime
from merge_person_data import merge_person_data  # Import the PySpark implementation

class TestMergePersonData:
    
    @pytest.fixture(scope="module")
    def spark(self):
        """Create a SparkSession for all tests."""
        spark = SparkSession.builder \
            .appName("MergePersonDataTest") \
            .master("local[*]") \
            .config("spark.sql.shuffle.partitions", "2") \
            .getOrCreate()
        yield spark
        spark.stop()
    
    @pytest.fixture(scope="function")
    def setup_tables(self, spark):
        """Set up test tables before each test."""
        # Create Person table
        person_data = [
            ("P12345", "1990-05-15", "123 Main St, NY", "123-456-7890", "john@example.com", 1, 
             "Caucasian", "Non-Hispanic", "English", "English", "BlueCross", "Dr. Smith", 
             "EMR_System_A", "2023-01-01 10:00:00", "Married", "Employed", "Bachelor", 
             "American", "Jane Doe - 555-1234", "2023-01-01 10:00:00"),
            ("P67890", "1985-10-20", "456 Oak Ave, CA", "987-654-3210", "mary@example.com", 0, 
             "Asian", "Non-Hispanic", "English", "English", "Aetna", "Dr. Johnson", 
             "EMR_System_B", "2023-01-02 11:00:00", "Single", "Employed", "Master", 
             "Canadian", "Bob Smith - 555-5678", "2023-01-02 11:00:00")
        ]
        
        person_schema = ["PersonID", "DOB", "Address", "ContactNumber", "EmailId", "Gender", 
                        "Race", "Ethnicity", "Spoken", "Written", "Insurance", "Provider", 
                        "SourceSystem", "IngestionTimestamp", "MaritalStatus", "EmploymentStatus", 
                        "EducationLevel", "Nationality", "EmergencyContact", "LastUpdated"]
        
        person_df = spark.createDataFrame(person_data, schema=person_schema)
        person_df.createOrReplaceTempView("Person")
        
        # Create ContactInfo table
        contact_data = [
            ("P12345", "123 Main St, NY", 1234567890, "john@example.com"),
            ("P67890", "456 Oak Ave, CA", 9876543210, "mary@example.com")
        ]
        
        contact_schema = ["PersonID", "Address", "Phone", "Email"]
        contact_df = spark.createDataFrame(contact_data, schema=contact_schema)
        contact_df.createOrReplaceTempView("ContactInfo")
        
        # Create Language table
        language_data = [
            ("P12345", "English", "English"),
            ("P67890", "English", "English")
        ]
        
        language_schema = ["PersonID", "Spoken", "Written"]
        language_df = spark.createDataFrame(language_data, schema=language_schema)
        language_df.createOrReplaceTempView("Language")
        
        # Create Staging table
        staging_data = [
            # Existing record with updates
            ("P12345", "1990-05-15", "123 Updated St, NY", "(123) 456-7890", "john.updated@example.com", 1, 
             "Caucasian", "Non-Hispanic", "Spanish", "Spanish", "BlueCross Premium", "Dr. Smith Jr.", 
             "EMR_System_A", "2023-02-01 10:00:00", "Divorced", "Self-Employed", "PhD", 
             "American", "Jane Doe - 555-9999", None),
            # New record
            ("P11111", "2000-01-01", "789 New St, TX", "555-123-4567", "new@example.com", 0, 
             "African American", "Hispanic", "Spanish", "English", "Medicare", "Dr. Brown", 
             "EMR_System_A", "2023-02-01 12:00:00", "Single", "Student", "High School", 
             "Mexican", "Parent - 555-8888", None)
        ]
        
        staging_df = spark.createDataFrame(staging_data, schema=person_schema)
        staging_df.createOrReplaceTempView("Staging_Person")
        
        # Create DataProcessingLog table
        log_schema = ["ProcessName", "TableName", "RecordsProcessed", "ProcessTimestamp"]
        log_df = spark.createDataFrame([], schema=log_schema)
        log_df.createOrReplaceTempView("DataProcessingLog")
        
        return {
            "person": person_df,
            "contact": contact_df,
            "language": language_df,
            "staging": staging_df,
            "log": log_df
        }
    
    def test_basic_merge_functionality(self, spark, setup_tables):
        """Test that the merge operation correctly updates existing records and inserts new ones."""
        # Execute the merge operation
        result = merge_person_data(spark, "EMR_System_A")
        
        # Verify the Person table has the correct number of records
        person_df = spark.table("Person")
        assert person_df.count() == 3, "Person table should have 3 records after merge"
        
        # Verify the updated record
        updated_record = person_df.filter(col("PersonID") == "P12345").first()
        assert updated_record["Address"] == "123 Updated St, NY"
        assert updated_record["EmailId"] == "john.updated@example.com"
        assert updated_record["Spoken"] == "Spanish"
        assert updated_record["Written"] == "Spanish"
        
        # Verify the new record
        new_record = person_df.filter(col("PersonID") == "P11111").first()
        assert new_record is not None, "New record should be inserted"
        assert new_record["Address"] == "789 New St, TX"
        
        # Verify the unchanged record remains
        unchanged_record = person_df.filter(col("PersonID") == "P67890").first()
        assert unchanged_record is not None, "Existing unchanged record should remain"
    
    def test_contact_info_updates(self, spark, setup_tables):
        """Test that ContactInfo table is correctly updated."""
        # Execute the merge operation
        result = merge_person_data(spark, "EMR_System_A")
        
        # Verify ContactInfo table updates
        contact_df = spark.table("ContactInfo")
        
        # Check new record
        new_contact = contact_df.filter(col("PersonID") == "P11111").first()
        assert new_contact is not None, "New contact record should be inserted"
        assert new_contact["Address"] == "789 New St, TX"
        assert new_contact["Email"] == "new@example.com"
        assert new_contact["Phone"] == 5551234567  # Converted from "555-123-4567"
    
    def test_language_updates(self, spark, setup_tables):
        """Test that Language table is correctly updated."""
        # Execute the merge operation
        result = merge_person_data(spark, "EMR_System_A")
        
        # Verify Language table updates
        language_df = spark.table("Language")
        
        # Check new record
        new_language = language_df.filter(col("PersonID") == "P11111").first()
        assert new_language is not None, "New language record should be inserted"
        assert new_language["Spoken"] == "Spanish"
        assert new_language["Written"] == "English"
    
    def test_logging_functionality(self, spark, setup_tables):
        """Test that operations are correctly logged."""
        # Execute the merge operation
        result = merge_person_data(spark, "EMR_System_A")
        
        # Verify logging
        log_df = spark.table("DataProcessingLog")
        
        # Should have at least one log entry
        assert log_df.count() >= 1, "Should have at least one log entry"
        
        # Check log entry details
        log_entry = log_df.first()
        assert log_entry["ProcessName"] == "MergePersonData"
        assert log_entry["TableName"] == "Person"
        assert log_entry["RecordsProcessed"] == 2  # Two records in staging for EMR_System_A
    
    def test_timestamp_updates(self, spark, setup_tables):
        """Test that LastUpdated timestamp is correctly set."""
        # Get current time before operation
        before_time = datetime.now()
        
        # Execute the merge operation
        result = merge_person_data(spark, "EMR_System_A")
        
        # Get current time after operation
        after_time = datetime.now()
        
        # Verify timestamps on updated records
        person_df = spark.table("Person")
        
        # Check updated record timestamp
        updated_record = person_df.filter(col("PersonID") == "P12345").first()
        last_updated = updated_record["LastUpdated"]
        
        # Convert to Python datetime for comparison
        last_updated_dt = last_updated.to_pydatetime() if hasattr(last_updated, 'to_pydatetime') else last_updated
        
        # Timestamp should be between before and after times
        assert before_time <= last_updated_dt <= after_time, "LastUpdated timestamp should be current"
        
        # Check new record timestamp
        new_record = person_df.filter(col("PersonID") == "P11111").first()
        new_last_updated = new_record["LastUpdated"]
        
        # Convert to Python datetime for comparison
        new_last_updated_dt = new_last_updated.to_pydatetime() if hasattr(new_last_updated, 'to_pydatetime') else new_last_updated
        
        # Timestamp should be between before and after times
        assert before_time <= new_last_updated_dt <= after_time, "LastUpdated timestamp should be current for new record"
    
    def test_phone_number_formatting(self, spark, setup_tables):
        """Test that phone numbers are correctly formatted when inserted into ContactInfo."""
        # Execute the merge operation
        result = merge_person_data(spark, "EMR_System_A")
        
        # Verify phone number formatting
        contact_df = spark.table("ContactInfo")
        
        # Check phone number format for new record
        new_contact = contact_df.filter(col("PersonID") == "P11111").first()
        assert new_contact["Phone"] == 5551234567, "Phone number should be converted from '555-123-4567' to 5551234567"
    
    def test_edge_case_empty_staging(self, spark, setup_tables):
        """Test behavior when staging table is empty for the specified source system."""
        # Execute the merge operation with a non-existent source system
        result = merge_person_data(spark, "NonExistentSystem")
        
        # Verify no changes to Person table
        person_df = spark.table("Person")
        assert person_df.count() == 2, "Person table should remain unchanged with 2 records"
        
        # Verify log entry shows zero records processed
        log_df = spark.table("DataProcessingLog")
        log_entry = log_df.filter(col("ProcessName") == "MergePersonData").first()
        assert log_entry["RecordsProcessed"] == 0, "Log should show zero records processed"
    
    def test_edge_case_null_values(self, spark, setup_tables):
        """Test handling of NULL values in staging data."""
        # Create staging data with NULL values
        null_staging_data = [
            ("P22222", None, None, None, None, None, 
             None, None, None, None, None, None, 
             "EMR_System_A", "2023-03-01 10:00:00", None, None, None, 
             None, None, None)
        ]
        
        null_staging_df = spark.createDataFrame(null_staging_data, schema=setup_tables["staging"].schema)
        null_staging_df.createOrReplaceTempView("Staging_Person_Null")
        
        # Combine with existing staging data
        spark.sql("""
            CREATE OR REPLACE TEMPORARY VIEW Staging_Person AS
            SELECT * FROM Staging_Person
            UNION ALL
            SELECT * FROM Staging_Person_Null
        """)
        
        # Execute the merge operation
        result = merge_person_data(spark, "EMR_System_A")
        
        # Verify the record with NULLs was inserted
        person_df = spark.table("Person")
        null_record = person_df.filter(col("PersonID") == "P22222").first()
        
        assert null_record is not None, "Record with NULL values should be inserted"
        assert null_record["DOB"] is None, "NULL DOB should be preserved"
        assert null_record["SourceSystem"] == "EMR_System_A", "Non-NULL values should be preserved"
    
    def test_performance(self, spark, setup_tables):
        """Test performance of the PySpark implementation."""
        # Create larger test dataset
        large_staging_data = []
        for i in range(1000):
            large_staging_data.append(
                (f"PL{i}", "2000-01-01", f"Address {i}", f"555-{i}", f"email{i}@example.com", i % 2, 
                 "Race", "Ethnicity", "English", "English", "Insurance", "Provider", 
                 "EMR_System_A", "2023-03-01 10:00:00", "Single", "Employed", "Bachelor", 
                 "Nationality", "Emergency Contact", None)
            )
        
        large_staging_df = spark.createDataFrame(large_staging_data, schema=setup_tables["staging"].schema)
        large_staging_df.createOrReplaceTempView("Staging_Person_Large")
        
        # Replace staging view with large dataset
        spark.sql("""
            CREATE OR REPLACE TEMPORARY VIEW Staging_Person AS
            SELECT * FROM Staging_Person_Large
        """)
        
        # Measure execution time
        start_time = time.time()
        result = merge_person_data(spark, "EMR_System_A")
        end_time = time.time()
        
        execution_time = end_time - start_time
        print(f"Execution time for 1000 records: {execution_time:.2f} seconds")
        
        # Verify all records were processed
        person_df = spark.table("Person")
        assert person_df.count() >= 1000, "All records should be processed"
        
        # Performance assertion - adjust threshold as needed
        assert execution_time < 60, "Merge operation should complete in under 60 seconds for 1000 records"
    
    def test_syntax_compatibility(self):
        """Test that the PySpark code doesn't use any SQL Server specific syntax."""
        # Inspect the PySpark function source code
        import inspect
        source_code = inspect.getsource(merge_person_data)
        
        # Check for SQL Server specific syntax
        sql_server_keywords = ["MERGE INTO", "OUTPUT", "INSERTED", "DELETED", "@@ROWCOUNT"]
        for keyword in sql_server_keywords:
            assert keyword not in source_code, f"SQL Server specific syntax '{keyword}' found in PySpark code"
    
    def test_data_integrity_constraints(self, spark, setup_tables):
        """Test that data integrity constraints are maintained."""
        # Add duplicate PersonID to staging
        duplicate_data = [
            ("P11111", "1995-05-15", "Duplicate Address", "555-9999", "dup@example.com", 1, 
             "Race", "Ethnicity", "English", "English", "Insurance", "Provider", 
             "EMR_System_A", "2023-03-01 10:00:00", "Single", "Employed", "Bachelor", 
             "Nationality", "Emergency Contact", None),
            ("P11111", "2000-01-01", "Another Address", "555-8888", "another@example.com", 0, 
             "Race", "Ethnicity", "Spanish", "Spanish", "Insurance", "Provider", 
             "EMR_System_A", "2023-03-01 11:00:00", "Married", "Unemployed", "Master", 
             "Nationality", "Emergency Contact", None)
        ]
        
        duplicate_df = spark.createDataFrame(duplicate_data, schema=setup_tables["staging"].schema)
        duplicate_df.createOrReplaceTempView("Staging_Person_Dup")
        
        # Replace staging view with duplicate dataset
        spark.sql("""
            CREATE OR REPLACE TEMPORARY VIEW Staging_Person AS
            SELECT * FROM Staging_Person_Dup
        """)
        
        # Execute the merge operation
        result = merge_person_data(spark, "EMR_System_A")
        
        # Verify only one record exists per PersonID
        person_df = spark.table("Person")
        duplicate_count = person_df.filter(col("PersonID") == "P11111").count()
        assert duplicate_count == 1, "Should only have one record per PersonID"
    
    def test_compare_with_sql_server_results(self, spark, setup_tables):
        """Compare PySpark results with expected SQL Server results."""
        # This test simulates the expected SQL Server results
        expected_results = [
            ("P12345", "1990-05-15", "123 Updated St, NY", "(123) 456-7890", "john.updated@example.com"),
            ("P67890", "1985-10-20", "456 Oak Ave, CA", "987-654-3210", "mary@example.com"),
            ("P11111", "2000-01-01", "789 New St, TX", "555-123-4567", "new@example.com")
        ]
        
        expected_df = spark.createDataFrame(
            expected_results, 
            ["PersonID", "DOB", "Address", "ContactNumber", "EmailId"]
        )
        
        # Execute the merge operation
        result = merge_person_data(spark, "EMR_System_A")
        
        # Get actual results
        actual_df = spark.table("Person").select("PersonID", "DOB", "Address", "ContactNumber", "EmailId")
        
        # Compare dataframes (ignoring order)
        expected_count = expected_df.count()
        actual_count = actual_df.count()
        assert expected_count == actual_count, f"Expected {expected_count} records, got {actual_count}"
        
        # Compare specific values
        for row in expected_results:
            person_id = row[0]
            actual_row = actual_df.filter(col("PersonID") == person_id).first()
            assert actual_row is not None, f"Person {person_id} not found in results"
            assert actual_row["Address"] == row[2], f"Address mismatch for {person_id}"
            assert actual_row["EmailId"] == row[4], f"Email mismatch for {person_id}"
```

## Running the Tests

To run these tests, save the script as `test_merge_person_data.py` and execute:

```bash
pytest -xvs test_merge_person_data.py
```

For performance tests only:

```bash
pytest -xvs test_merge_person_data.py::TestMergePersonData::test_performance
```

## Test Coverage Report

To generate a test coverage report:

```bash
pytest --cov=merge_person_data test_merge_person_data.py
```

## Implementation Notes

1. The test suite validates the key aspects of the SQL Server to PySpark conversion:
   - Proper implementation of MERGE logic using DataFrame operations
   - Correct handling of timestamps and data type conversions
   - Proper updating of related tables
   - Logging functionality
   - Performance characteristics

2. Manual interventions in the PySpark code that are tested include:
   - Breaking down the MERGE operation into component operations
   - Phone number formatting using regexp_replace
   - Handling of NULL values
   - Timestamp management

3. The tests ensure that the PySpark implementation maintains the same business logic and data integrity as the original T-SQL stored procedure.

**apiCost:** $0.00