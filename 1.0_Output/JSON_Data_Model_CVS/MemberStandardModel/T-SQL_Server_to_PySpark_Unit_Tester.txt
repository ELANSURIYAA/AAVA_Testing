# Comprehensive Unit Tests for get_person_data_with_filters Function

## Test Case List

| Test Case ID | Test Case Description | Expected Outcome |
|-------------|-------------|-----------------|
| TC001 | Basic functionality with no filters | Returns all person records with base columns |
| TC002 | Filter by person_id | Returns only the record matching the specified person_id |
| TC003 | Filter by gender (valid value 0) | Returns only records with gender=0 (Female) |
| TC004 | Filter by gender (valid value 1) | Returns only records with gender=1 (Male) |
| TC005 | Filter by invalid gender value | Raises ValueError with message "Gender must be 0 (Female) or 1 (Male)" |
| TC006 | Filter by race | Returns only records matching the specified race |
| TC007 | Filter by ethnicity | Returns only records matching the specified ethnicity |
| TC008 | Filter by start_date only | Returns only records with DOB >= start_date |
| TC009 | Filter by end_date only | Returns only records with DOB <= end_date |
| TC010 | Filter by date range (start_date and end_date) | Returns only records with DOB between start_date and end_date |
| TC011 | Include contact info columns | Returns base columns plus Address, ContactNumber, EmailId |
| TC012 | Include language columns | Returns base columns plus Spoken, Written |
| TC013 | Include both contact info and language columns | Returns all columns |
| TC014 | Multiple filters combined | Returns records matching all specified filters |
| TC015 | No matching records | Returns empty DataFrame with correct schema |
| TC016 | Handle exception when table doesn't exist | Raises Exception with appropriate message |

## Pytest Script

```python
import pytest
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, TimestampType
from datetime import datetime, date
from unittest.mock import patch, MagicMock

# Import the function to test - adjust the import path as needed
from person_data import get_person_data_with_filters

@pytest.fixture
def spark():
    """Create a SparkSession for testing."""
    return SparkSession.builder \
        .appName("UnitTestPersonFilters") \
        .master("local[1]") \
        .getOrCreate()

@pytest.fixture
def sample_data():
    """Create sample data for testing."""
    data = [
        # PersonID, DOB, Gender, Race, Ethnicity, Insurance, Provider, SourceSystem, IngestionTimestamp, Address, ContactNumber, EmailId, Spoken, Written
        ("P12345", date(1990, 5, 15), 1, "Caucasian", "Non-Hispanic", "BlueCross", "Dr. Smith", "System A", datetime(2023, 1, 1, 10, 0), "123 Main St", "555-1234", "john@example.com", "English", "English"),
        ("P23456", date(1985, 8, 22), 0, "Asian", "Non-Hispanic", "Aetna", "Dr. Johnson", "System B", datetime(2023, 1, 2, 11, 0), "456 Oak Ave", "555-5678", "mary@example.com", "English", "Chinese"),
        ("P34567", date(1978, 3, 10), 1, "African American", "Non-Hispanic", "UnitedHealth", "Dr. Brown", "System A", datetime(2023, 1, 3, 12, 0), "789 Pine St", "555-9012", "bob@example.com", "English", "English"),
        ("P45678", date(1995, 11, 30), 0, "Hispanic", "Hispanic", "Cigna", "Dr. Garcia", "System C", datetime(2023, 1, 4, 13, 0), "101 Elm St", "555-3456", "alice@example.com", "Spanish", "Spanish"),
    ]
    
    schema = StructType([
        StructField("PersonID", StringType(), False),
        StructField("DOB", DateType(), True),
        StructField("Gender", IntegerType(), True),
        StructField("Race", StringType(), True),
        StructField("Ethnicity", StringType(), True),
        StructField("Insurance", StringType(), True),
        StructField("Provider", StringType(), True),
        StructField("SourceSystem", StringType(), True),
        StructField("IngestionTimestamp", TimestampType(), True),
        StructField("Address", StringType(), True),
        StructField("ContactNumber", StringType(), True),
        StructField("EmailId", StringType(), True),
        StructField("Spoken", StringType(), True),
        StructField("Written", StringType(), True)
    ])
    
    return data, schema

@pytest.fixture
def mock_spark_table(spark, sample_data, monkeypatch):
    """
    Mocks the spark.table method to return test data specifically for 'dataswitch-poc.cvs.Person'.
    """
    data, schema = sample_data
    df = spark.createDataFrame(data, schema)
    
    # Mock the table method to specifically handle the 'dataswitch-poc.cvs.Person' table
    def mock_table(table_name):
        # Remove backticks if present in the table name for comparison
        clean_table_name = table_name.replace('`', '')
        
        if clean_table_name == 'dataswitch-poc.cvs.Person':
            return df
        else:
            # For other tables, raise an exception
            raise ValueError(f"Table {table_name} not found in test fixture")
    
    # Apply the mock to the spark session
    monkeypatch.setattr(spark, "table", mock_table)
    
    return spark

class TestGetPersonDataWithFilters:
    
    def test_basic_functionality_no_filters(self, mock_spark_table, sample_data):
        """TC001: Test basic functionality with no filters."""
        # Arrange
        data, _ = sample_data
        
        # Act
        result_df = get_person_data_with_filters(mock_spark_table)
        
        # Assert
        assert result_df.count() == len(data)
        # Check that only base columns are included
        assert "Address" not in result_df.columns
        assert "Spoken" not in result_df.columns
        
    def test_filter_by_person_id(self, mock_spark_table):
        """TC002: Test filtering by person_id."""
        # Arrange
        person_id = "P12345"
        
        # Act
        result_df = get_person_data_with_filters(mock_spark_table, person_id=person_id)
        
        # Assert
        assert result_df.count() == 1
        assert result_df.first()["PersonID"] == person_id
        
    def test_filter_by_gender_female(self, mock_spark_table):
        """TC003: Test filtering by gender=0 (Female)."""
        # Act
        result_df = get_person_data_with_filters(mock_spark_table, gender=0)
        
        # Assert
        assert result_df.count() == 2  # We have 2 female records in sample data
        # Check that all returned records have gender=0
        for row in result_df.collect():
            assert row["Gender"] == 0
            
    def test_filter_by_gender_male(self, mock_spark_table):
        """TC004: Test filtering by gender=1 (Male)."""
        # Act
        result_df = get_person_data_with_filters(mock_spark_table, gender=1)
        
        # Assert
        assert result_df.count() == 2  # We have 2 male records in sample data
        # Check that all returned records have gender=1
        for row in result_df.collect():
            assert row["Gender"] == 1
            
    def test_invalid_gender_value(self, mock_spark_table):
        """TC005: Test with invalid gender value."""
        # Act & Assert
        with pytest.raises(ValueError) as excinfo:
            get_person_data_with_filters(mock_spark_table, gender=2)
        
        # Check the error message
        assert "Gender must be 0 (Female) or 1 (Male)" in str(excinfo.value)
        
    def test_filter_by_race(self, mock_spark_table):
        """TC006: Test filtering by race."""
        # Act
        result_df = get_person_data_with_filters(mock_spark_table, race="Asian")
        
        # Assert
        assert result_df.count() == 1
        assert result_df.first()["Race"] == "Asian"
        
    def test_filter_by_ethnicity(self, mock_spark_table):
        """TC007: Test filtering by ethnicity."""
        # Act
        result_df = get_person_data_with_filters(mock_spark_table, ethnicity="Hispanic")
        
        # Assert
        assert result_df.count() == 1
        assert result_df.first()["Ethnicity"] == "Hispanic"
        
    def test_filter_by_start_date(self, mock_spark_table):
        """TC008: Test filtering by start_date."""
        # Act
        result_df = get_person_data_with_filters(
            mock_spark_table, 
            start_date=date(1990, 1, 1)
        )
        
        # Assert
        # Should return records with DOB >= 1990-01-01 (2 records in our sample)
        assert result_df.count() == 2
        for row in result_df.collect():
            assert row["DOB"] >= date(1990, 1, 1)
        
    def test_filter_by_end_date(self, mock_spark_table):
        """TC009: Test filtering by end_date."""
        # Act
        result_df = get_person_data_with_filters(
            mock_spark_table, 
            end_date=date(1990, 12, 31)
        )
        
        # Assert
        # Should return records with DOB <= 1990-12-31 (2 records in our sample)
        assert result_df.count() == 2
        for row in result_df.collect():
            assert row["DOB"] <= date(1990, 12, 31)
        
    def test_filter_by_date_range(self, mock_spark_table):
        """TC010: Test filtering by date range."""
        # Act
        result_df = get_person_data_with_filters(
            mock_spark_table, 
            start_date=date(1980, 1, 1),
            end_date=date(1990, 12, 31)
        )
        
        # Assert
        # Should return records with 1980-01-01 <= DOB <= 1990-12-31 (2 records)
        assert result_df.count() == 2
        for row in result_df.collect():
            assert row["DOB"] >= date(1980, 1, 1)
            assert row["DOB"] <= date(1990, 12, 31)
        
    def test_include_contact_info(self, mock_spark_table):
        """TC011: Test including contact info columns."""
        # Act
        result_df = get_person_data_with_filters(mock_spark_table, include_contact_info=True)
        
        # Assert
        # Check that contact info columns are included
        assert "Address" in result_df.columns
        assert "ContactNumber" in result_df.columns
        assert "EmailId" in result_df.columns
        # But language columns are not
        assert "Spoken" not in result_df.columns
        assert "Written" not in result_df.columns
        
    def test_include_language(self, mock_spark_table):
        """TC012: Test including language columns."""
        # Act
        result_df = get_person_data_with_filters(mock_spark_table, include_language=True)
        
        # Assert
        # Check that language columns are included
        assert "Spoken" in result_df.columns
        assert "Written" in result_df.columns
        # But contact info columns are not
        assert "Address" not in result_df.columns
        assert "ContactNumber" not in result_df.columns
        assert "EmailId" not in result_df.columns
        
    def test_include_all_optional_columns(self, mock_spark_table):
        """TC013: Test including both contact info and language columns."""
        # Act
        result_df = get_person_data_with_filters(
            mock_spark_table, 
            include_contact_info=True,
            include_language=True
        )
        
        # Assert
        # Check that both contact info and language columns are included
        assert "Address" in result_df.columns
        assert "ContactNumber" in result_df.columns
        assert "EmailId" in result_df.columns
        assert "Spoken" in result_df.columns
        assert "Written" in result_df.columns
        
    def test_multiple_filters(self, mock_spark_table):
        """TC014: Test multiple filters combined."""
        # Act
        result_df = get_person_data_with_filters(
            mock_spark_table,
            gender=0,
            ethnicity="Hispanic",
            start_date=date(1990, 1, 1)
        )
        
        # Assert
        assert result_df.count() == 1
        row = result_df.first()
        assert row["Gender"] == 0
        assert row["Ethnicity"] == "Hispanic"
        assert row["DOB"] >= date(1990, 1, 1)
        
    def test_no_matching_records(self, mock_spark_table):
        """TC015: Test with filters that result in no matching records."""
        # Act
        result_df = get_person_data_with_filters(
            mock_spark_table,
            race="Unknown Race"  # This race doesn't exist in our sample data
        )
        
        # Assert
        assert result_df.count() == 0
        # Schema should still be correct even with empty result
        assert "PersonID" in result_df.columns
        assert "DOB" in result_df.columns
        assert "Gender" in result_df.columns
        
    def test_table_not_found(self, spark):
        """TC016: Test handling of exception when table doesn't exist."""
        # Arrange - create a mock that raises an exception
        def mock_table_error(table_name):
            raise Exception("Table or view not found")
            
        spark.table = mock_table_error
        
        # Act & Assert
        with pytest.raises(Exception) as excinfo:
            get_person_data_with_filters(spark)
        
        # Check the error message
        assert "Table or view not found" in str(excinfo.value)

    # Additional edge case tests
    
    def test_null_values_in_data(self, mock_spark_table, sample_data, monkeypatch):
        """Test handling of NULL values in the data."""
        # Create data with NULL values
        data, schema = sample_data
        data_with_nulls = list(data)
        data_with_nulls.append(("P56789", None, None, None, None, "Medicare", "Dr. Lee", "System D", 
                               datetime(2023, 1, 5, 14, 0), "202 Cedar St", "555-7890", "sam@example.com", 
                               None, None))
        
        df_with_nulls = mock_spark_table.createDataFrame(data_with_nulls, schema)
        
        # Override the mock_table function to return our dataframe with nulls
        def mock_table_with_nulls(table_name):
            clean_table_name = table_name.replace('`', '')
            if clean_table_name == 'dataswitch-poc.cvs.Person':
                return df_with_nulls
            else:
                raise ValueError(f"Table {table_name} not found in test fixture")
        
        monkeypatch.setattr(mock_spark_table, "table", mock_table_with_nulls)
        
        # Test filtering on columns that contain nulls
        result_df = get_person_data_with_filters(mock_spark_table, gender=1)
        
        # Should only return records where gender=1 (nulls are excluded)
        assert result_df.count() == 2
        
    def test_empty_dataset(self, mock_spark_table, monkeypatch):
        """Test behavior with an empty dataset."""
        # Create an empty dataframe with the same schema
        empty_df = mock_spark_table.createDataFrame([], mock_spark_table.table("dataswitch-poc.cvs.Person").schema)
        
        # Override the mock_table function to return our empty dataframe
        def mock_empty_table(table_name):
            clean_table_name = table_name.replace('`', '')
            if clean_table_name == 'dataswitch-poc.cvs.Person':
                return empty_df
            else:
                raise ValueError(f"Table {table_name} not found in test fixture")
        
        monkeypatch.setattr(mock_spark_table, "table", mock_empty_table)
        
        # Test with an empty dataset
        result_df = get_person_data_with_filters(mock_spark_table)
        
        # Should return an empty dataframe with the correct schema
        assert result_df.count() == 0
        assert len(result_df.columns) > 0

if __name__ == "__main__":
    pytest.main(["-v"])
```

## Additional Test Considerations

1. **Performance Testing**: For large datasets, consider adding tests that verify the function's performance with different filter combinations.

2. **Integration Testing**: Test the function with actual Spark clusters and real data sources.

3. **Boundary Testing**: Add more tests for edge cases like:
   - Empty strings for string parameters
   - Minimum/maximum possible dates
   - Special characters in string parameters

4. **Negative Testing**: Additional tests for invalid inputs like:
   - Invalid date formats
   - None values for required parameters
   - Type mismatches (e.g., passing string for gender)

5. **Regression Testing**: Ensure that any future changes to the function don't break existing functionality.

The provided test suite covers the core functionality, input validation, and error handling of the `get_person_data_with_filters` function, ensuring it works correctly across various use cases.

## Notes on Implementation

1. The tests use pytest fixtures to set up test data and mock the Spark session.
2. The mock_spark_table fixture specifically mocks the spark.table method to return our test data when the function queries the "dataswitch-poc.cvs.Person" table.
3. Tests are organized by functionality, with separate tests for each filter type and combination.
4. Error handling tests verify that appropriate exceptions are raised for invalid inputs.
5. Additional edge case tests check behavior with NULL values and empty datasets.

This comprehensive test suite ensures that the `get_person_data_with_filters` function works correctly under all expected conditions and gracefully handles error cases.