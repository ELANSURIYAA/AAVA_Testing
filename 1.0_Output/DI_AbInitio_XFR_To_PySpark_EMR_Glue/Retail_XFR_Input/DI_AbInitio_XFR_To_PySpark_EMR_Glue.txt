from pyspark.sql import DataFrame
from pyspark.sql.functions import col, when, concat, expr, floor, lit, sum as spark_sum, count as spark_count
from pyspark.sql.types import DecimalType, DateType
from decimal import Decimal

def transform_cleanse_transform(df: DataFrame) -> DataFrame:
    return df.withColumn("txn_id", col("txn_id").cast(DecimalType(10, 0))) \
             .withColumn("store_id", col("store_id")) \
             .withColumn("txn_date", col("txn_date_str").cast(DateType())) \
             .withColumn("total_amount", (col("quantity_str").cast(DecimalType(10, 2)) * col("unit_price_str").cast(DecimalType(10, 2)))) \
             .withColumn("tax_amount", lit(0)) \
             .withColumn("final_bill", lit(0)) \
             .withColumn("loyalty_points", lit(0))

def transform_pricing_logic(df: DataFrame) -> DataFrame:
    tax_rate = lit(0.085)
    return df.withColumn("tax_amount", col("total_amount") * tax_rate) \
             .withColumn("final_bill", col("total_amount") + (col("total_amount") * tax_rate)) \
             .withColumn("loyalty_points", floor(col("total_amount") / lit(10)).cast(DecimalType(5, 0)))

def transform_rollup_logic(df: DataFrame) -> DataFrame:
    return df.groupBy("store_id", col("txn_date").alias("report_date")) \
             .agg(spark_sum("final_bill").alias("total_gross_sales"), \
                  spark_sum("tax_amount").alias("total_tax_collected"), \
                  spark_count("*").alias("total_transaction_count")) \
             .withColumn("newline", lit("\n"))