```
=============================================
Author:        Ascendion AVA+
Date:   
Description:   Pytest suite to validate PySpark DataFrame transformations for DimBillingAccount ETL logic, including joins, aggregations, filters, and error handling.
=============================================

# 1. Test Case List

| Test Case ID | Description                                                                                  | Expected Outcome                                                                                      |
|--------------|---------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|
| TC01         | Validate happy path: All joins and transformations with complete, valid data                 | Output DataFrame matches expected schema and values; correct row count; derived columns as expected   |
| TC02         | Handle NULLs: Source tables contain NULLs in join and select columns                        | Output DataFrame handles NULLs gracefully; NULLs propagate as per join type; no unexpected errors    |
| TC03         | Empty DataFrames: One or more source DataFrames are empty                                   | Output DataFrame is empty; no exceptions occur                                                        |
| TC04         | Invalid schema: Source DataFrame with missing/extra columns                                 | Raises AnalysisException or appropriate error                                                         |
| TC05         | Type mismatch: Source DataFrame has wrong data types for join columns                       | Raises AnalysisException or appropriate error                                                         |
| TC06         | Duplicate AccountNumbers: Ensure dropDuplicates works correctly                             | Output DataFrame contains only unique AccountNumbers                                                  |
| TC07         | IsActive logic: Test retired flag for both active/inactive accounts                         | IsActive column correctly set (1 for Retired=0, 0 otherwise)                                          |
| TC08         | Derived column logic: AccountTypeName "Premium" triggers "HighValue" in DerivedExample      | DerivedExample column is set as expected                                                              |
| TC09         | Aggregation: Validate AccountTypeName counts                                                | Aggregated DataFrame matches expected counts                                                          |
| TC10         | Performance: DataFrame is cached and partitioned as specified                               | DataFrame is cached and partitioned by AccountTypeName                                                |

# 2. Pytest Script

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit, when, concat, count

# PySpark Test Fixtures

@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("pytest-pyspark").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def sample_data(spark):
    # Mock DataFrames for all source tables
    df_account = spark.createDataFrame([
        Row(ID=1, AccountNumber=1001, AccountName="Acme Corp", AccountType=10, BillingLevel=20, ServiceTier=30,
            SecurityZoneID=40, DeliquencyStatus=50, Segment=60, CloseDate=None, CreateTime="2023-01-01",
            FirstTwicePerMthInvoiceDOM=5, SecondTwicePerMthInvoiceDOM=20, PublicID="PUB1", Retired=0, BeanVersion="A"),
        Row(ID=2, AccountNumber=1002, AccountName="Beta LLC", AccountType=11, BillingLevel=21, ServiceTier=31,
            SecurityZoneID=41, DeliquencyStatus=51, Segment=61, CloseDate=None, CreateTime="2023-01-02",
            FirstTwicePerMthInvoiceDOM=10, SecondTwicePerMthInvoiceDOM=25, PublicID="PUB2", Retired=1, BeanVersion="B"),
        Row(ID=3, AccountNumber=1003, AccountName="Gamma Inc", AccountType=10, BillingLevel=22, ServiceTier=30,
            SecurityZoneID=40, DeliquencyStatus=50, Segment=60, CloseDate=None, CreateTime="2023-01-03",
            FirstTwicePerMthInvoiceDOM=15, SecondTwicePerMthInvoiceDOM=30, PublicID="PUB3", Retired=0, BeanVersion="C"),
    ])
    df_parent_acct = spark.createDataFrame([
        Row(OwnerID=1, ForeignEntityID=1, BeanVersion="X", UpdateTime="2023-01-01"),
        Row(OwnerID=2, ForeignEntityID=2, BeanVersion="Y", UpdateTime="2023-01-02"),
    ])
    df_account_type = spark.createDataFrame([
        Row(ID=10, NAME="Premium"),
        Row(ID=11, NAME="Standard"),
    ])
    df_billing_level = spark.createDataFrame([
        Row(ID=20, NAME="Level1"),
        Row(ID=21, NAME="Level2"),
        Row(ID=22, NAME="Level3"),
    ])
    df_customer_service_tier = spark.createDataFrame([
        Row(ID=30, NAME="Gold"),
        Row(ID=31, NAME="Silver"),
    ])
    df_security_zone = spark.createDataFrame([
        Row(ID=40, Name="ZoneA", BeanVersion="Z1"),
        Row(ID=41, Name="ZoneB", BeanVersion="Z2"),
    ])
    df_account_contact = spark.createDataFrame([
        Row(AccountID=1, FirstName="John", LastName="Doe", AddressLine1="123 Main", AddressLine2=None,
            AddressLine3=None, City="Metropolis", State="NY", PostalCode="10001"),
        Row(AccountID=2, FirstName="Jane", LastName="Smith", AddressLine1="456 Elm", AddressLine2="Apt 2",
            AddressLine3=None, City="Gotham", State="NJ", PostalCode="07001"),
    ])
    df_delinquency_status = spark.createDataFrame([
        Row(ID=50, NAME="Current"),
        Row(ID=51, NAME="Overdue"),
    ])
    df_account_segment = spark.createDataFrame([
        Row(ID=60, NAME="Retail"),
        Row(ID=61, NAME="Wholesale"),
    ])
    df_dim_ba = spark.createDataFrame([
        Row(PublicID="PUB1", BeanVersion="AXZ1"),
        Row(PublicID="PUB2", BeanVersion="BYZ2"),
    ])
    return {
        "df_account": df_account,
        "df_parent_acct": df_parent_acct,
        "df_account_type": df_account_type,
        "df_billing_level": df_billing_level,
        "df_customer_service_tier": df_customer_service_tier,
        "df_security_zone": df_security_zone,
        "df_account_contact": df_account_contact,
        "df_delinquency_status": df_delinquency_status,
        "df_account_segment": df_account_segment,
        "df_dim_ba": df_dim_ba,
    }

# Helper function to replicate main transformation logic
def run_main_transformation(dfs):
    # ParentAcct CTE
    df_parent_acct_joined = dfs["df_parent_acct"].join(
        dfs["df_account"],
        dfs["df_account"]["ID"] == dfs["df_parent_acct"]["ForeignEntityID"],
        "inner"
    ).select(
        dfs["df_parent_acct"]["OwnerID"],
        dfs["df_account"]["AccountNumber"].cast("int").alias("ParentAccountNumber"),
        concat(dfs["df_parent_acct"]["BeanVersion"], dfs["df_account"]["BeanVersion"]).alias("BeanVersion"),
        dfs["df_account"]["UpdateTime"] if "UpdateTime" in dfs["df_account"].columns else lit(None).alias("UpdateTime")
    )

    # InsuredInfo CTE (simplified)
    df_insured_info = dfs["df_account_contact"]

    # Main SELECT logic
    df_main = dfs["df_account"] \
        .join(dfs["df_account_type"], dfs["df_account_type"]["ID"] == dfs["df_account"]["AccountType"], "left") \
        .join(df_parent_acct_joined, df_parent_acct_joined["OwnerID"] == dfs["df_account"]["ID"], "left") \
        .join(dfs["df_billing_level"], dfs["df_billing_level"]["ID"] == dfs["df_account"]["BillingLevel"], "left") \
        .join(dfs["df_customer_service_tier"], dfs["df_customer_service_tier"]["ID"] == dfs["df_account"]["ServiceTier"], "left") \
        .join(dfs["df_security_zone"], dfs["df_security_zone"]["ID"] == dfs["df_account"]["SecurityZoneID"], "left") \
        .join(df_insured_info, df_insured_info["AccountID"] == dfs["df_account"]["ID"], "left") \
        .join(dfs["df_delinquency_status"], dfs["df_delinquency_status"]["ID"] == dfs["df_account"]["DeliquencyStatus"], "left") \
        .join(dfs["df_account_segment"], dfs["df_account_segment"]["ID"] == dfs["df_account"]["Segment"], "left") \
        .select(
            dfs["df_account"]["AccountNumber"],
            dfs["df_account"]["AccountName"],
            dfs["df_account_type"]["NAME"].cast("string").alias("AccountTypeName"),
            df_parent_acct_joined["ParentAccountNumber"],
            dfs["df_billing_level"]["NAME"].cast("string").alias("BillingLevelName"),
            dfs["df_account_segment"]["NAME"].cast("string").alias("Segment"),
            dfs["df_customer_service_tier"]["NAME"].cast("string").alias("ServiceTierName"),
            dfs["df_security_zone"]["Name"].alias("SecurityZone"),
            df_insured_info["FirstName"],
            df_insured_info["LastName"],
            df_insured_info["AddressLine1"],
            df_insured_info["AddressLine2"],
            df_insured_info["AddressLine3"],
            df_insured_info["City"].cast("string"),
            df_insured_info["State"].cast("string"),
            df_insured_info["PostalCode"].cast("string"),
            dfs["df_account"]["CloseDate"].alias("AccountCloseDate"),
            dfs["df_account"]["CreateTime"].alias("AccountCreationDate"),
            dfs["df_delinquency_status"]["NAME"].cast("string").alias("DeliquencyStatusName"),
            dfs["df_account"]["FirstTwicePerMthInvoiceDOM"].alias("FirstTwicePerMonthInvoiceDayOfMonth"),
            dfs["df_account"]["SecondTwicePerMthInvoiceDOM"].alias("SecondTwicePerMonthInvoiceDayOfMonth"),
            dfs["df_account"]["PublicID"],
            dfs["df_account"]["ID"].alias("GWRowNumber"),
            concat(dfs["df_account"]["BeanVersion"], df_parent_acct_joined["BeanVersion"], dfs["df_security_zone"]["BeanVersion"]).cast("string").alias("BeanVersion"),
            when(dfs["df_account"]["Retired"] == 0, lit(1)).otherwise(lit(0)).alias("IsActive"),
            lit("WC").alias("LegacySourceSystem")
        ).dropDuplicates(["AccountNumber"])

    # Derived Columns
    df_main = df_main.withColumn(
        "DerivedExample",
        when(col("AccountTypeName") == "Premium", lit("HighValue")).otherwise(lit("Standard"))
    )
    return df_main

# Test Cases

def test_TC01_happy_path(sample_data):
    """TC01: Validate happy path with all valid data."""
    df_main = run_main_transformation(sample_data)
    assert df_main.count() == 3
    expected_columns = [
        "AccountNumber", "AccountName", "AccountTypeName", "ParentAccountNumber", "BillingLevelName",
        "Segment", "ServiceTierName", "SecurityZone", "FirstName", "LastName", "AddressLine1", "AddressLine2",
        "AddressLine3", "City", "State", "PostalCode", "AccountCloseDate", "AccountCreationDate",
        "DeliquencyStatusName", "FirstTwicePerMonthInvoiceDayOfMonth", "SecondTwicePerMonthInvoiceDayOfMonth",
        "PublicID", "GWRowNumber", "BeanVersion", "IsActive", "LegacySourceSystem", "DerivedExample"
    ]
    assert set(df_main.columns) == set(expected_columns)
    row = df_main.filter(col("AccountNumber") == 1001).collect()[0]
    assert row.AccountName == "Acme Corp"
    assert row.AccountTypeName == "Premium"
    assert row.IsActive == 1
    assert row.DerivedExample == "HighValue"

def test_TC02_null_handling(sample_data, spark):
    """TC02: Handle NULLs in join/select columns."""
    # Make some columns NULL
    df_account = sample_data["df_account"].withColumn("AccountType", lit(None).cast("integer"))
    dfs = sample_data.copy()
    dfs["df_account"] = df_account
    df_main = run_main_transformation(dfs)
    # AccountTypeName should be NULL
    assert df_main.filter(col("AccountNumber") == 1001).select("AccountTypeName").collect()[0][0] is None

def test_TC03_empty_dataframes(sample_data, spark):
    """TC03: Handle empty DataFrames."""
    dfs = sample_data.copy()
    dfs["df_account"] = spark.createDataFrame([], sample_data["df_account"].schema)
    df_main = run_main_transformation(dfs)
    assert df_main.count() == 0

def test_TC04_invalid_schema(sample_data, spark):
    """TC04: Raise error for missing columns."""
    # Remove a required column
    df_account = sample_data["df_account"].drop("AccountType")
    dfs = sample_data.copy()
    dfs["df_account"] = df_account
    with pytest.raises(Exception):
        run_main_transformation(dfs)

def test_TC05_type_mismatch(sample_data, spark):
    """TC05: Raise error for type mismatch in join columns."""
    # Change AccountType to string
    df_account = sample_data["df_account"].withColumn("AccountType", lit("not_an_int"))
    dfs = sample_data.copy()
    dfs["df_account"] = df_account
    with pytest.raises(Exception):
        run_main_transformation(dfs)

def test_TC06_drop_duplicates(sample_data):
    """TC06: Ensure dropDuplicates removes duplicate AccountNumbers."""
    # Add duplicate AccountNumber
    df_account = sample_data["df_account"].union(
        sample_data["df_account"].filter(col("AccountNumber") == 1001)
    )
    dfs = sample_data.copy()
    dfs["df_account"] = df_account
    df_main = run_main_transformation(dfs)
    # Should only be 3 unique AccountNumbers
    assert df_main.count() == 3

def test_TC07_isactive_logic(sample_data):
    """TC07: Test IsActive logic for retired accounts."""
    df_main = run_main_transformation(sample_data)
    # AccountNumber 1002 is retired, so IsActive should be 0
    row = df_main.filter(col("AccountNumber") == 1002).collect()[0]
    assert row.IsActive == 0
    # AccountNumber 1001 is active, so IsActive should be 1
    row = df_main.filter(col("AccountNumber") == 1001).collect()[0]
    assert row.IsActive == 1

def test_TC08_derived_column_logic(sample_data):
    """TC08: Test DerivedExample column logic."""
    df_main = run_main_transformation(sample_data)
    # AccountTypeName "Premium" should be "HighValue"
    row = df_main.filter(col("AccountTypeName") == "Premium").collect()[0]
    assert row.DerivedExample == "HighValue"
    # AccountTypeName "Standard" should be "Standard"
    row = df_main.filter(col("AccountTypeName") == "Standard").collect()[0]
    assert row.DerivedExample == "Standard"

def test_TC09_aggregation(sample_data):
    """TC09: Validate AccountTypeName counts."""
    df_main = run_main_transformation(sample_data)
    df_agg = df_main.groupBy("AccountTypeName").agg(count("*").alias("AccountTypeCount"))
    agg = {row["AccountTypeName"]: row["AccountTypeCount"] for row in df_agg.collect()}
    assert agg["Premium"] == 2
    assert agg["Standard"] == 1

def test_TC10_performance_partition_cache(sample_data):
    """TC10: Test DataFrame is partitioned and cached."""
    df_main = run_main_transformation(sample_data)
    df_main = df_main.repartition("AccountTypeName")
    df_main.cache()
    # Check number of partitions (should be at least 1)
    assert df_main.rdd.getNumPartitions() >= 1
    # Check if cached (storageLevel.useMemory should be True)
    assert df_main.is_cached

# End of Pytest Script

# API Cost Consumed
# API Cost Consumed in dollars: $0.07
```
