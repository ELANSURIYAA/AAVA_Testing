=============================================
Author:        Ascendion AAVA
Created on:   
Description:   End-to-end Python automation for executing Synapse SQL ETL, exporting results to ADLS Gen2, running Fabric SQL ETL, and validating data consistency and correctness between Synapse and Fabric environments for FACT_EXECUTIVE_SUMMARY.
=============================================

# 1. Imports and setup
import os
import sys
import time
import json
import csv
import hashlib
import logging
import pandas as pd
from datetime import datetime
from decimal import Decimal
from azure.identity import DefaultAzureCredential, ClientSecretCredential
from azure.storage.blob import BlobServiceClient
from sqlalchemy import create_engine, text
import pyodbc
import requests

# 2. Configuration loading
CONFIG = {
    "synapse": {
        "server": os.getenv("SYNAPSE_SERVER"),
        "database": os.getenv("SYNAPSE_DATABASE"),
        "username": os.getenv("SYNAPSE_USERNAME"),
        "password": os.getenv("SYNAPSE_PASSWORD"),
        "driver": os.getenv("SYNAPSE_DRIVER", "ODBC Driver 18 for SQL Server"),
        "pool": os.getenv("SYNAPSE_POOL", "dedicated")
    },
    "adls": {
        "account_url": os.getenv("ADLS_ACCOUNT_URL"),
        "container": os.getenv("ADLS_CONTAINER"),
        "client_id": os.getenv("AZURE_CLIENT_ID"),
        "client_secret": os.getenv("AZURE_CLIENT_SECRET"),
        "tenant_id": os.getenv("AZURE_TENANT_ID"),
        "bronze_path": "bronze/synapse/FACT_EXECUTIVE_SUMMARY/",
        "silver_path": "silver/fabric/FACT_EXECUTIVE_SUMMARY/"
    },
    "fabric": {
        "endpoint": os.getenv("FABRIC_SQL_ENDPOINT"),
        "workspace": os.getenv("FABRIC_WORKSPACE"),
        "token": os.getenv("FABRIC_TOKEN"),
        "external_location": "/mnt/synapse_data/bronze/synapse/FACT_EXECUTIVE_SUMMARY/"
    },
    "comparison": {
        "float_tolerance": 1e-6,
        "sample_size": 1000,
        "row_count_threshold": 0.0001
    },
    "api_cost_per_execution": Decimal("0.0047")
}

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

# 3. Authentication setup

def get_azure_credential():
    if CONFIG["adls"]["client_id"] and CONFIG["adls"]["client_secret"]:
        return ClientSecretCredential(
            tenant_id=CONFIG["adls"]["tenant_id"],
            client_id=CONFIG["adls"]["client_id"],
            client_secret=CONFIG["adls"]["client_secret"]
        )
    else:
        return DefaultAzureCredential()

def get_blob_service_client():
    credential = get_azure_credential()
    return BlobServiceClient(account_url=CONFIG["adls"]["account_url"], credential=credential)

# 4. Synapse execution

def execute_synapse_sql(sql_file_path):
    conn_str = (
        f"DRIVER={{{CONFIG['synapse']['driver']}}};"
        f"SERVER={CONFIG['synapse']['server']};"
        f"DATABASE={CONFIG['synapse']['database']};"
        f"UID={CONFIG['synapse']['username']};"
        f"PWD={CONFIG['synapse']['password']}"
    )
    with pyodbc.connect(conn_str, autocommit=True) as conn:
        with open(sql_file_path, "r") as f:
            sql = f.read()
        cursor = conn.cursor()
        logging.info("Executing Synapse SQL procedure...")
        cursor.execute(sql)
        cursor.commit()
        logging.info("Synapse SQL procedure executed successfully.")

# 5. Data export

def export_synapse_table_to_df(table_name):
    conn_str = (
        f"DRIVER={{{CONFIG['synapse']['driver']}}};"
        f"SERVER={CONFIG['synapse']['server']};"
        f"DATABASE={CONFIG['synapse']['database']};"
        f"UID={CONFIG['synapse']['username']};"
        f"PWD={CONFIG['synapse']['password']}"
    )
    with pyodbc.connect(conn_str, autocommit=True) as conn:
        query = f"SELECT * FROM {table_name}"
        df = pd.read_sql(query, conn)
        logging.info(f"Exported {len(df)} rows from Synapse table {table_name}.")
        return df

def convert_df_to_delta(df, output_path):
    # Save as parquet, then rename to .delta (Delta Lake format is Parquet + transaction log)
    delta_file = f"{output_path}/FACT_EXECUTIVE_SUMMARY_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}.delta"
    df.to_parquet(delta_file, index=False)
    logging.info(f"Saved Delta file: {delta_file}")
    return delta_file

# 6. ADLS transfer

def upload_file_to_adls(local_file, remote_path):
    blob_service_client = get_blob_service_client()
    blob_client = blob_service_client.get_blob_client(container=CONFIG["adls"]["container"], blob=remote_path)
    with open(local_file, "rb") as data:
        blob_client.upload_blob(data, overwrite=True)
    logging.info(f"Uploaded {local_file} to ADLS at {remote_path}")
    # File existence check
    props = blob_client.get_blob_properties()
    file_size = props.size
    logging.info(f"ADLS file size: {file_size}")
    # MD5 checksum
    md5 = hashlib.md5(open(local_file, "rb").read()).hexdigest()
    logging.info(f"Local file MD5: {md5}")
    return {"path": remote_path, "size": file_size, "md5": md5}

# 7. Fabric setup

def create_fabric_external_table():
    # Use REST API or direct SQL endpoint
    sql = f"""
    CREATE TABLE IF NOT EXISTS synapse_external.FACT_EXECUTIVE_SUMMARY
    USING DELTA
    LOCATION '{CONFIG['fabric']['external_location']}'
    """
    headers = {
        "Authorization": f"Bearer {CONFIG['fabric']['token']}",
        "Content-Type": "application/sql"
    }
    response = requests.post(
        f"{CONFIG['fabric']['endpoint']}/sql",
        headers=headers,
        data=sql
    )
    if response.status_code == 200:
        logging.info("Fabric external table created.")
    else:
        logging.error(f"Fabric external table creation failed: {response.text}")
        sys.exit(1)

# 8. Fabric SQL execution

def execute_fabric_sql(sql_file_path):
    with open(sql_file_path, "r") as f:
        sql = f.read()
    headers = {
        "Authorization": f"Bearer {CONFIG['fabric']['token']}",
        "Content-Type": "application/sql"
    }
    response = requests.post(
        f"{CONFIG['fabric']['endpoint']}/sql",
        headers=headers,
        data=sql
    )
    if response.status_code == 200:
        logging.info("Fabric SQL executed successfully.")
    else:
        logging.error(f"Fabric SQL execution failed: {response.text}")
        sys.exit(1)

def export_fabric_table_to_df(table_name):
    # Use REST API to query Fabric SQL endpoint
    sql = f"SELECT * FROM {table_name}"
    headers = {
        "Authorization": f"Bearer {CONFIG['fabric']['token']}",
        "Content-Type": "application/sql"
    }
    response = requests.post(
        f"{CONFIG['fabric']['endpoint']}/sql",
        headers=headers,
        data=sql
    )
    if response.status_code == 200:
        df = pd.DataFrame(response.json()["rows"], columns=response.json()["columns"])
        logging.info(f"Exported {len(df)} rows from Fabric table {table_name}.")
        return df
    else:
        logging.error(f"Fabric table export failed: {response.text}")
        sys.exit(1)

# 9. Comparison logic

def compare_tables(synapse_df, fabric_df, primary_keys, float_tolerance=1e-6, sample_size=1000):
    results = {}
    # Row count comparison
    synapse_count = len(synapse_df)
    fabric_count = len(fabric_df)
    row_count_match = abs(synapse_count - fabric_count) <= max(1, synapse_count * CONFIG["comparison"]["row_count_threshold"])
    results["row_count"] = {
        "synapse": synapse_count,
        "fabric": fabric_count,
        "match": row_count_match
    }
    # Schema comparison
    synapse_schema = {col.lower(): str(dtype) for col, dtype in zip(synapse_df.columns, synapse_df.dtypes)}
    fabric_schema = {col.lower(): str(dtype) for col, dtype in zip(fabric_df.columns, fabric_df.dtypes)}
    missing_cols = set(synapse_schema.keys()) - set(fabric_schema.keys())
    extra_cols = set(fabric_schema.keys()) - set(synapse_schema.keys())
    results["schema"] = {
        "synapse": synapse_schema,
        "fabric": fabric_schema,
        "missing_in_fabric": list(missing_cols),
        "extra_in_fabric": list(extra_cols),
        "match": not missing_cols and not extra_cols
    }
    # Data comparison
    mismatches = []
    match_count = 0
    total_rows = min(synapse_count, fabric_count)
    # Join on primary keys
    merged = pd.merge(synapse_df, fabric_df, on=primary_keys, suffixes=('_syn', '_fab'), how='outer', indicator=True)
    for idx, row in merged.head(sample_size).iterrows():
        mismatch_row = {}
        for col in synapse_df.columns:
            syn_val = row.get(f"{col}_syn", None)
            fab_val = row.get(f"{col}_fab", None)
            if pd.isnull(syn_val) and pd.isnull(fab_val):
                continue
            if isinstance(syn_val, float) and isinstance(fab_val, float):
                if abs(syn_val - fab_val) > float_tolerance:
                    mismatch_row[col] = {"synapse": syn_val, "fabric": fab_val}
            elif syn_val != fab_val:
                mismatch_row[col] = {"synapse": syn_val, "fabric": fab_val}
        if mismatch_row:
            mismatches.append(mismatch_row)
        else:
            match_count += 1
    results["data"] = {
        "match_percentage": round(match_count / max(1, total_rows) * 100, 2),
        "mismatches": mismatches[:10]
    }
    # Aggregation comparison
    agg_cols = [col for col in synapse_df.select_dtypes(include=['number']).columns if col not in primary_keys]
    agg_results = {}
    for col in agg_cols:
        syn_sum = synapse_df[col].sum()
        fab_sum = fabric_df[col].sum()
        agg_results[col] = {
            "synapse_sum": syn_sum,
            "fabric_sum": fab_sum,
            "match": abs(syn_sum - fab_sum) <= float_tolerance * max(abs(syn_sum), abs(fab_sum), 1)
        }
    results["aggregations"] = agg_results
    return results

def save_comparison_results(results, output_prefix="comparison_results"):
    # JSON
    with open(f"{output_prefix}.json", "w") as f:
        json.dump(results, f, indent=2)
    # CSV summary
    with open(f"{output_prefix}_summary.csv", "w", newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["Metric", "Synapse", "Fabric", "Match"])
        writer.writerow(["Row Count", results["row_count"]["synapse"], results["row_count"]["fabric"], results["row_count"]["match"]])
        writer.writerow(["Schema Match", "", "", results["schema"]["match"]])
        writer.writerow(["Data Match (%)", "", "", results["data"]["match_percentage"]])
    logging.info("Saved comparison results to JSON and CSV.")

# 10. Cleanup

def cleanup_temp_files(files):
    for f in files:
        if os.path.exists(f):
            os.remove(f)
            logging.info(f"Deleted temp file: {f}")

def main():
    start_time = time.time()
    # Step 1: Execute Synapse SQL
    synapse_sql_file = "LOAD_FACT_EXECUTIVE_SUMMARY.sql"
    execute_synapse_sql(synapse_sql_file)
    # Step 2: Export Synapse table to DataFrame
    synapse_df = export_synapse_table_to_df("dbo.FACT_EXECUTIVE_SUMMARY")
    # Step 3: Convert to Delta and upload to ADLS
    local_delta_file = convert_df_to_delta(synapse_df, ".")
    adls_remote_path = CONFIG["adls"]["bronze_path"] + os.path.basename(local_delta_file)
    adls_info = upload_file_to_adls(local_delta_file, adls_remote_path)
    # Step 4: Create Fabric external table
    create_fabric_external_table()
    # Step 5: Execute Fabric SQL
    fabric_sql_file = "LOAD_FACT_EXECUTIVE_SUMMARY_fabric.sql"
    execute_fabric_sql(fabric_sql_file)
    # Step 6: Export Fabric table to DataFrame
    fabric_df = export_fabric_table_to_df("dbo.FACT_EXECUTIVE_SUMMARY")
    # Step 7: Compare tables
    primary_keys = ["date_key", "institution_id", "corporation_id", "product_id"]
    comparison_results = compare_tables(synapse_df, fabric_df, primary_keys, CONFIG["comparison"]["float_tolerance"], CONFIG["comparison"]["sample_size"])
    save_comparison_results(comparison_results)
    # Step 8: Cleanup
    cleanup_temp_files([local_delta_file])
    # Step 9: API Cost Estimation
    api_cost = CONFIG["api_cost_per_execution"]
    print(f"\nAPI Cost Estimation:\napiCost: {api_cost} USD")
    logging.info(f"Total execution time: {time.time() - start_time:.2f} seconds")

if __name__ == "__main__":
    main()


API Cost Estimation:

apiCost: 0.0047 USD