=============================================
Author: Ascendion AVAA
Date: 
Description: Pytest test suite for validating the PySpark TAMBR Rings workflow, including transformation logic, joins, aggregations, geospatial UDFs, and edge case handling.
=============================================

# 1. Syntactical Changes Made

- Replaced all SAS data steps, SQL, and macros with PySpark DataFrame and SQL operations.
- All SAS DB2 SQL replaced with BigQuery Standard SQL via PySpark's BigQuery connector.
- SAS macro variables replaced with Python variables and helper functions.
- SAS geospatial distance calculation replaced with a PySpark UDF (Haversine formula).
- SAS frequency reporting (PROC FREQ) replaced with DataFrame `.groupBy().count()`.
- Macro drop logic (mdrop_mac) omitted; PySpark/BigQuery overwrites tables programmatically.
- All joins, aggregations, and window logic mapped to PySpark equivalents.
- Error handling and logging added using Python logging.

# 2. Manual Intervention Required

- Macro drop logic (mdrop_mac) is not needed in PySpark/BigQuery; tables are managed programmatically.
- Any SAS macro logic for dynamic table or variable creation should be replaced with config-driven Python functions.
- If any table or field names do not match BigQuery schema, manual mapping is required.
- If `percentile_approx` is not available in your Spark version, use `approxQuantile` instead.
- All manual interventions are commented in the code for clarity.

# 3. Test Case Document

| Test Case ID | Description                                                                                   | Preconditions           | Test Steps                                                                                   | Expected Result                                                                                      | Actual Result | Pass/Fail Status |
|--------------|----------------------------------------------------------------------------------------------|-------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|--------------|------------------|
| TC01         | Happy path: Valid customer and branch data, all joins and aggregations work as expected       | Valid input data        | Run workflow with valid sample data                                                         | Output DataFrames match expected schema and data, distances are correct, percentiles are accurate     | As observed  | Pass/Fail        |
| TC02         | Edge case: NULL/zero latitude or longitude in customer/branch data                           | Input with nulls/zeros  | Run workflow with null/zero lat/long                                                        | NULL/defaults handled, records routed to bad_latlong tables as expected                              | As observed  | Pass/Fail        |
| TC03         | Edge case: Empty input DataFrames                                                            | Empty DataFrames        | Run workflow with empty DataFrames                                                          | Output DataFrames are empty, no errors thrown                                                        | As observed  | Pass/Fail        |
| TC04         | Edge case: Duplicate LP_IDs or BR_IDs                                                        | Duplicates in input     | Run workflow with duplicate LP_IDs/BR_IDs                                                   | Window/row_number logic deduplicates as expected, only one record per partition                      | As observed  | Pass/Fail        |
| TC05         | Error handling: Type mismatch in latitude/longitude columns                                  | Bad types in input      | Run workflow with string in lat/long fields                                                 | Exception is raised or rows with bad types are excluded                                              | As observed  | Pass/Fail        |
| TC06         | Filtering: Only open branches (BR_OPN_FLG='Y') and valid branch types are included           | Mixed open/closed data  | Run workflow with open and closed branches                                                  | Output contains only valid branches                                                                  | As observed  | Pass/Fail        |
| TC07         | Aggregation: 80th percentile calculation for distance rings                                  | Known input values      | Run percentile_approx on known data                                                         | Percentile values are correct for known input                                                        | As observed  | Pass/Fail        |
| TC08         | UDF: geodist returns correct distance for known coordinates                                  | Known coordinates       | Run geodist UDF on NYC-LA coordinates                                                       | Output matches Haversine formula result                                                              | As observed  | Pass/Fail        |
| TC09         | Schema: Output DataFrames have expected columns and types                                    | Valid input             | Check schema of output DataFrames                                                           | Assert schema matches specification                                                                  | As observed  | Pass/Fail        |
| TC10         | Performance: Large input DataFrames complete within reasonable time (mocked)                 | Large sample data       | Run workflow with large DataFrames                                                          | Test completes within timeout, no memory errors                                                      | As observed  | Pass/Fail        |

# 4. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit, when
import math

# --- Fixtures and Helpers ---

@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("tambr_rings_test").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def sample_branch_df(spark):
    data = [
        Row(HGN_BR_ID="B001", BR_TYP="R", BR_OPN_FLG="Y", branchlat=40.0, branchlong=-74.0, METRO_COMMUNITY_CDE="M1", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Main", CITY="NY", ST="NY", ZIP_CDE="10001"),
        Row(HGN_BR_ID="B002", BR_TYP="U", BR_OPN_FLG="Y", branchlat=41.0, branchlong=-75.0, METRO_COMMUNITY_CDE="M2", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Second", CITY="LA", ST="CA", ZIP_CDE="90001"),
    ]
    schema = StructType([
        StructField("HGN_BR_ID", StringType()),
        StructField("BR_TYP", StringType()),
        StructField("BR_OPN_FLG", StringType()),
        StructField("branchlat", DoubleType()),
        StructField("branchlong", DoubleType()),
        StructField("METRO_COMMUNITY_CDE", StringType()),
        StructField("GEN_CDE", StringType()),
        StructField("TBA_CLS_DVSTD_DT", StringType()),
        StructField("BRICK_AND_MORTOR_NM", StringType()),
        StructField("CITY", StringType()),
        StructField("ST", StringType()),
        StructField("ZIP_CDE", StringType()),
    ])
    return spark.createDataFrame(data, schema=schema)

@pytest.fixture
def sample_customer_df(spark):
    data = [
        Row(LP_ID="C001", PRTY_BR="B001", CUST_PRTY_ACCT_ID="A001", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, MOST_USED_BR="B002", HGN_CUST_TYP_CDE="T1", OPN_ACCT_CNT=1, geomatchcode="1", custlat=40.1, custlong=-74.1),
        Row(LP_ID="C002", PRTY_BR="B002", CUST_PRTY_ACCT_ID="A002", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=24, MOST_USED_BR="B001", HGN_CUST_TYP_CDE="T2", OPN_ACCT_CNT=2, geomatchcode="1", custlat=41.1, custlong=-75.1),
    ]
    schema = StructType([
        StructField("LP_ID", StringType()),
        StructField("PRTY_BR", StringType()),
        StructField("CUST_PRTY_ACCT_ID", StringType()),
        StructField("OPN_ACCT_FLG", StringType()),
        StructField("NBR_OF_MOS_OPN", IntegerType()),
        StructField("MOST_USED_BR", StringType()),
        StructField("HGN_CUST_TYP_CDE", StringType()),
        StructField("OPN_ACCT_CNT", IntegerType()),
        StructField("geomatchcode", StringType()),
        StructField("custlat", DoubleType()),
        StructField("custlong", DoubleType()),
    ])
    return spark.createDataFrame(data, schema=schema)

@pytest.fixture
def geodist_udf():
    def geodist(lat1, lon1, lat2, lon2):
        try:
            lat1, lon1, lat2, lon2 = map(float, [lat1, lon1, lat2, lon2])
            phi1, phi2 = math.radians(lat1), math.radians(lat2)
            dphi = math.radians(lat2 - lat1)
            dlambda = math.radians(lon2 - lon1)
            a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2
            c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
            miles = 3958.8 * c
            return float(miles)
        except Exception:
            return None
    from pyspark.sql.functions import udf
    from pyspark.sql.types import DoubleType
    return udf(geodist, DoubleType())

# --- Test Cases ---

def test_TC01_happy_path(spark, sample_branch_df, sample_customer_df, geodist_udf):
    """
    Test Case TC01: Happy path
    Purpose: Validate that with valid data, all joins and aggregations work as expected.
    Related to: Core SAS logic for joining and aggregating customer/branch data.
    """
    df = sample_customer_df.join(
        sample_branch_df,
        sample_customer_df.PRTY_BR == sample_branch_df.HGN_BR_ID,
        how="inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf(col("branchlat"), col("branchlong"), col("custlat"), col("custlong"))
    )
    assert "dist_to_prty_br" in df.columns
    rows = df.collect()
    assert len(rows) == 2
    assert all(isinstance(r.dist_to_prty_br, float) for r in rows)

def test_TC02_null_latlong(spark, sample_branch_df, sample_customer_df, geodist_udf):
    """
    Test Case TC02: Edge case with null/zero lat/long
    Purpose: Validate handling of null/zero lat/long, routing to bad_latlong tables.
    Related to: Filtering and error handling logic in PySpark code.
    """
    null_branch = Row(HGN_BR_ID="B003", BR_TYP="R", BR_OPN_FLG="Y", branchlat=None, branchlong=None, METRO_COMMUNITY_CDE="M3", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Null", CITY="SF", ST="CA", ZIP_CDE="94101")
    branch_df = sample_branch_df.union(spark.createDataFrame([null_branch], sample_branch_df.schema))
    null_cust = Row(LP_ID="C003", PRTY_BR="B003", CUST_PRTY_ACCT_ID="A003", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=10, MOST_USED_BR="B003", HGN_CUST_TYP_CDE="T3", OPN_ACCT_CNT=1, geomatchcode="1", custlat=None, custlong=None)
    customer_df = sample_customer_df.union(spark.createDataFrame([null_cust], sample_customer_df.schema))
    df = customer_df.join(
        branch_df,
        customer_df.PRTY_BR == branch_df.HGN_BR_ID,
        how="inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf(col("branchlat"), col("branchlong"), col("custlat"), col("custlong"))
    )
    row = df.filter(col("LP_ID") == "C003").collect()[0]
    assert row.dist_to_prty_br is None

def test_TC03_empty_input(spark, geodist_udf):
    """
    Test Case TC03: Edge case with empty input DataFrames
    Purpose: Validate that empty input produces empty output without errors.
    Related to: Defensive programming and error handling.
    """
    schema = StructType([
        StructField("LP_ID", StringType()),
        StructField("PRTY_BR", StringType()),
        StructField("CUST_PRTY_ACCT_ID", StringType()),
        StructField("OPN_ACCT_FLG", StringType()),
        StructField("NBR_OF_MOS_OPN", IntegerType()),
        StructField("MOST_USED_BR", StringType()),
        StructField("HGN_CUST_TYP_CDE", StringType()),
        StructField("OPN_ACCT_CNT", IntegerType()),
        StructField("geomatchcode", StringType()),
        StructField("custlat", DoubleType()),
        StructField("custlong", DoubleType()),
    ])
    empty_cust = spark.createDataFrame([], schema)
    empty_branch = spark.createDataFrame([], StructType([
        StructField("HGN_BR_ID", StringType()),
        StructField("BR_TYP", StringType()),
        StructField("BR_OPN_FLG", StringType()),
        StructField("branchlat", DoubleType()),
        StructField("branchlong", DoubleType()),
        StructField("METRO_COMMUNITY_CDE", StringType()),
        StructField("GEN_CDE", StringType()),
        StructField("TBA_CLS_DVSTD_DT", StringType()),
        StructField("BRICK_AND_MORTOR_NM", StringType()),
        StructField("CITY", StringType()),
        StructField("ST", StringType()),
        StructField("ZIP_CDE", StringType()),
    ]))
    df = empty_cust.join(
        empty_branch,
        empty_cust.PRTY_BR == empty_branch.HGN_BR_ID,
        how="inner"
    )
    assert df.count() == 0

def test_TC04_duplicate_lp_ids(spark, sample_customer_df):
    """
    Test Case TC04: Edge case with duplicate LP_IDs
    Purpose: Validate window/row_number deduplication logic.
    Related to: Window/row_number logic in PySpark code.
    """
    dup_data = sample_customer_df.union(sample_customer_df)
    from pyspark.sql import Window
    from pyspark.sql.functions import row_number
    w = Window.partitionBy("LP_ID").orderBy("NBR_OF_MOS_OPN")
    df = dup_data.withColumn("rn", row_number().over(w)).filter(col("rn") == 1)
    assert df.groupBy("LP_ID").count().filter(col("count") > 1).count() == 0

def test_TC05_type_mismatch_latlong(spark, sample_branch_df, sample_customer_df, geodist_udf):
    """
    Test Case TC05: Type mismatch in lat/long columns
    Purpose: Validate error handling for type mismatches.
    Related to: Defensive programming and geodist UDF.
    """
    bad_branch = Row(HGN_BR_ID="B004", BR_TYP="R", BR_OPN_FLG="Y", branchlat="bad", branchlong="bad", METRO_COMMUNITY_CDE="M4", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Bad", CITY="SF", ST="CA", ZIP_CDE="94102")
    branch_df = sample_branch_df.union(spark.createDataFrame([bad_branch], sample_branch_df.schema))
    bad_cust = Row(LP_ID="C004", PRTY_BR="B004", CUST_PRTY_ACCT_ID="A004", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=5, MOST_USED_BR="B004", HGN_CUST_TYP_CDE="T4", OPN_ACCT_CNT=1, geomatchcode="1", custlat="bad", custlong="bad")
    customer_df = sample_customer_df.union(spark.createDataFrame([bad_cust], sample_customer_df.schema))
    df = customer_df.join(
        branch_df,
        customer_df.PRTY_BR == branch_df.HGN_BR_ID,
        how="inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf(col("branchlat"), col("branchlong"), col("custlat"), col("custlong"))
    )
    row = df.filter(col("LP_ID") == "C004").collect()[0]
    assert row.dist_to_prty_br is None

def test_TC06_filter_open_branches(spark, sample_branch_df):
    """
    Test Case TC06: Filtering for open branches
    Purpose: Validate that only open branches of valid types are included.
    Related to: Filtering logic in PySpark code.
    """
    closed_branch = Row(HGN_BR_ID="B005", BR_TYP="R", BR_OPN_FLG="N", branchlat=42.0, branchlong=-76.0, METRO_COMMUNITY_CDE="M5", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Closed", CITY="TX", ST="TX", ZIP_CDE="75001")
    branch_df = sample_branch_df.union(spark.createDataFrame([closed_branch], sample_branch_df.schema))
    open_branches = branch_df.filter((col("BR_OPN_FLG") == "Y") & (col("BR_TYP").isin(["R", "U", "I", "T", "C"])))
    assert open_branches.filter(col("HGN_BR_ID") == "B005").count() == 0

def test_TC07_percentile_aggregation(spark):
    """
    Test Case TC07: 80th percentile aggregation
    Purpose: Validate percentile_approx calculation for known input.
    Related to: percentile_approx logic in PySpark code.
    """
    data = [Row(PRTY_BR="B001", dist_to_prty_br=x) for x in range(1, 101)]
    df = spark.createDataFrame(data)
    df.createOrReplaceTempView("rings_priority_cust")
    percentile = spark.sql("""
        SELECT PRTY_BR, percentile_approx(dist_to_prty_br, 0.8) as prtybr80
        FROM rings_priority_cust
        GROUP BY PRTY_BR
    """).collect()[0].prtybr80
    assert percentile == 80

def test_TC08_geodist_udf(geodist_udf, spark):
    """
    Test Case TC08: geodist UDF
    Purpose: Validate geodist UDF returns correct distance for known coordinates.
    Related to: geodist UDF in PySpark code.
    """
    df = spark.createDataFrame([
        Row(branchlat=40.7128, branchlong=-74.0060, custlat=34.0522, custlong=-118.2437)
    ])
    df = df.withColumn("dist", geodist_udf("branchlat", "branchlong", "custlat", "custlong"))
    dist = df.collect()[0].dist
    assert abs(dist - 2445) < 10

def test_TC09_schema(sample_branch_df):
    """
    Test Case TC09: Output schema
    Purpose: Validate schema of output DataFrames.
    Related to: Output schema in PySpark code.
    """
    expected_fields = {"HGN_BR_ID", "BR_TYP", "BR_OPN_FLG", "branchlat", "branchlong", "METRO_COMMUNITY_CDE", "BRICK_AND_MORTOR_NM", "CITY", "ST", "ZIP_CDE"}
    actual_fields = set(sample_branch_df.columns)
    assert expected_fields.issubset(actual_fields)

def test_TC10_performance(spark, sample_branch_df, sample_customer_df, geodist_udf):
    """
    Test Case TC10: Performance
    Purpose: Validate workflow performance with large DataFrames (mocked).
    Related to: End-to-end workflow performance.
    """
    big_branches = sample_branch_df
    for _ in range(10):  # 2^10 = 1024 rows
        big_branches = big_branches.union(big_branches)
    big_customers = sample_customer_df
    for _ in range(10):
        big_customers = big_customers.union(big_customers)
    import time
    start = time.time()
    df = big_customers.join(
        big_branches,
        big_customers.PRTY_BR == big_branches.HGN_BR_ID,
        how="inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf(col("branchlat"), col("branchlong"), col("custlat"), col("custlong"))
    )
    df.count()
    end = time.time()
    assert (end - start) < 10
```

# 5. API Cost

apiCost: 0.0023 USD