=============================================
Author: Ascendion AVAA
Date: 
Description: Pytest test suite for validating PySpark transformation logic, joins, aggregations, and geospatial calculations for TAMBr branch ring analytics, ensuring correctness, schema/data integrity, and robust edge case handling.
=============================================

# 1. Test Case List

| Test Case ID | Description                                                                 | Expected Outcome                                                                                  |
|--------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| TC01         | Happy path: Valid branch/customer data, all joins succeed, no NULLs         | Output DataFrames have correct schema, values, and expected row counts                            |
| TC02         | Edge: Customer with NULL latitude/longitude                                 | Customer excluded from rings_cust_data; handled gracefully, no crash                              |
| TC03         | Edge: Branch with invalid lat/long (outside valid range)                    | Branch excluded from rings_branch_data; handled gracefully, no crash                              |
| TC04         | Edge: No customers for a branch                                             | priority_ring and most_used_ring set to 0 for that branch                                         |
| TC05         | Edge: Empty input DataFrames                                                | All output DataFrames are empty, no crash                                                         |
| TC06         | Edge: Duplicate lp_id in branch_active                                      | Only top-ranked (by days, count, amount) is selected as most_used                                 |
| TC07         | Error: Type mismatch in geodist UDF (string instead of float)               | geodist returns None, no crash                                                                    |
| TC08         | Error: Missing required fields in input DataFrames                          | Raises AnalysisException or handled error, test passes if error is raised                         |
| TC09         | Aggregation: 80th percentile calculation matches manual calculation         | percentile_approx output matches manual calculation for small test set                             |
| TC10         | Filtering: Only open branches, correct types, and valid hgn_br_id selected  | Output contains only expected branch rows                                                         |
| TC11         | Schema: Output DataFrames match expected schema (columns & types)           | Schemas match exactly                                                                             |
| TC12         | Performance: Caching and partitioning improves runtime (mocked/tested)      | Runtime is less with caching/partitioning enabled (mocked, not measured in unit test)             |

# 2. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
import math

# Helper: geodist UDF (copied from production code)
def geodist(lat1, lon1, lat2, lon2):
    try:
        if None in (lat1, lon1, lat2, lon2):
            return None
        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2
        c = 2 * math.asin(math.sqrt(a))
        miles = 3956 * c
        return miles
    except Exception:
        return None

@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("pytest-TAMBr").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def branch_schema():
    return StructType([
        StructField("hgn_br_id", StringType()),
        StructField("br_typ", StringType()),
        StructField("br_opn_flg", StringType()),
        StructField("branchlat", DoubleType()),
        StructField("branchlong", DoubleType()),
        StructField("metro_community_cde", StringType()),
        StructField("brick_and_mortor_nm", StringType()),
        StructField("city", StringType()),
        StructField("st", StringType()),
    ])

@pytest.fixture
def customer_schema():
    return StructType([
        StructField("lp_id", StringType()),
        StructField("prty_br", StringType()),
        StructField("opn_acct_flg", StringType()),
        StructField("nbr_of_mos_opn", IntegerType()),
        StructField("custlat", DoubleType()),
        StructField("custlong", DoubleType()),
        StructField("most_used_br", StringType()),
        StructField("opn_acct_cnt", IntegerType()),
        StructField("geomatchcode", StringType()),
    ])

@pytest.fixture
def minimal_data(spark, branch_schema, customer_schema):
    # One branch, one customer, both valid
    branches = [
        ("00002", "R", "Y", 40.0, -75.0, "A", "Branch A", "CityA", "ST"),
    ]
    customers = [
        ("CUST1", "00002", "Y", 10, 40.1, -75.1, "00002", 1, "1"),
    ]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    customer_df = spark.createDataFrame(customers, schema=customer_schema)
    return branch_df, customer_df

def test_happy_path(spark, minimal_data):
    branch_df, customer_df = minimal_data
    # Simulate transformation: join, filter, geodist
    from pyspark.sql.functions import udf
    from pyspark.sql.types import DoubleType

    geodist_udf = udf(geodist, DoubleType())
    joined = customer_df.join(branch_df, customer_df.prty_br == branch_df.hgn_br_id, "inner")
    joined = joined.withColumn("dist_to_prty_br", geodist_udf("branchlat", "branchlong", "custlat", "custlong"))
    assert joined.count() == 1
    assert "dist_to_prty_br" in joined.columns
    val = joined.select("dist_to_prty_br").collect()[0][0]
    assert val is not None and val > 0

def test_null_latlong_customer(spark, branch_schema, customer_schema):
    branches = [("00002", "R", "Y", 40.0, -75.0, "A", "Branch A", "CityA", "ST")]
    customers = [("CUST2", "00002", "Y", 5, None, -75.1, "00002", 1, "1")]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    customer_df = spark.createDataFrame(customers, schema=customer_schema)
    # Should be filtered out before geodist
    filtered = customer_df.filter(customer_df.custlat.isNotNull() & customer_df.custlong.isNotNull())
    assert filtered.count() == 0

def test_invalid_latlong_branch(spark, branch_schema, customer_schema):
    branches = [("00003", "R", "Y", 0.0, 0.0, "A", "Branch B", "CityB", "ST")]
    customers = [("CUST3", "00003", "Y", 5, 40.1, -75.1, "00003", 1, "1")]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    # Should be filtered out: branchlat <= 1 or branchlong >= -1
    valid_branches = branch_df.filter((F.col("branchlat") > 1) & (F.col("branchlong") < -1))
    assert valid_branches.count() == 0

def test_no_customers_for_branch(spark, branch_schema, customer_schema):
    branches = [("00004", "R", "Y", 40.0, -75.0, "A", "Branch C", "CityC", "ST")]
    customers = []
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    customer_df = spark.createDataFrame(customers, schema=customer_schema)
    # Simulate left join for ring calculation
    joined = customer_df.join(branch_df, customer_df.prty_br == branch_df.hgn_br_id, "right")
    # If no customers, should fill with 0 for ring
    joined = joined.withColumn("priority_ring", F.lit(0))
    assert joined.select("priority_ring").collect()[0][0] == 0

def test_empty_inputs(spark, branch_schema, customer_schema):
    branch_df = spark.createDataFrame([], schema=branch_schema)
    customer_df = spark.createDataFrame([], schema=customer_schema)
    joined = customer_df.join(branch_df, customer_df.prty_br == branch_df.hgn_br_id, "inner")
    assert joined.count() == 0

def test_duplicate_lp_id_branch_active(spark):
    schema = StructType([
        StructField("lp_id", StringType()),
        StructField("br_id", StringType()),
        StructField("branch_used_days_3mo", IntegerType()),
        StructField("branch_used_days_prev", IntegerType()),
        StructField("branch_trans_count_3mo", IntegerType()),
        StructField("branch_trans_count_prev", IntegerType()),
        StructField("branch_trans_amount_3mo", DoubleType()),
        StructField("branch_trans_amount_prev", DoubleType()),
    ])
    data = [
        ("CUST4", "B1", 10, 5, 20, 10, 100.0, 50.0),
        ("CUST4", "B2", 8, 4, 15, 8, 90.0, 45.0),
    ]
    df = spark.createDataFrame(data, schema=schema)
    from pyspark.sql.window import Window
    window_spec = Window.partitionBy("lp_id").orderBy(
        F.desc("branch_used_days_3mo"),
        F.desc("branch_used_days_prev"),
        F.desc("branch_trans_count_3mo"),
        F.desc("branch_trans_count_prev"),
        F.desc("branch_trans_amount_3mo"),
        F.desc("branch_trans_amount_prev")
    )
    sorted_df = df.withColumn("rn", F.row_number().over(window_spec))
    most_used = sorted_df.filter(F.col("rn") == 1)
    assert most_used.count() == 1
    assert most_used.collect()[0]["br_id"] == "B1"

def test_geodist_type_mismatch():
    # Should return None, not raise
    assert geodist("bad", -75.0, 40.0, -75.1) is None

def test_missing_fields_error(spark, branch_schema):
    # Missing 'branchlong'
    schema = StructType([
        StructField("hgn_br_id", StringType()),
        StructField("br_typ", StringType()),
        StructField("br_opn_flg", StringType()),
        StructField("branchlat", DoubleType()),
    ])
    data = [("00005", "R", "Y", 40.0)]
    branch_df = spark.createDataFrame(data, schema=schema)
    with pytest.raises(Exception):
        branch_df.select("branchlong").collect()

def test_percentile_approx(spark):
    schema = StructType([StructField("prty_br", StringType()), StructField("dist_to_prty_br", DoubleType())])
    data = [("B1", 1.0), ("B1", 2.0), ("B1", 3.0), ("B1", 4.0), ("B1", 5.0)]
    df = spark.createDataFrame(data, schema=schema)
    approx = df.groupBy("prty_br").agg(F.expr("percentile_approx(dist_to_prty_br, 0.8)").alias("priority_ring"))
    val = approx.collect()[0]["priority_ring"]
    assert val == 4.0

def test_branch_filtering(spark, branch_schema):
    branches = [
        ("00006", "R", "Y", 40.0, -75.0, "A", "Branch D", "CityD", "ST"),
        ("00007", "X", "N", 41.0, -76.0, "B", "Branch E", "CityE", "ST"),
    ]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    filtered = branch_df.filter(
        (F.col("br_typ").isin("R", "U", "I", "T", "C")) &
        (F.col("br_opn_flg") == "Y") &
        (F.col("hgn_br_id") != "00001") &
        (F.col("branchlat").isNotNull())
    )
    assert filtered.count() == 1
    assert filtered.collect()[0]["hgn_br_id"] == "00006"

def test_schema_output(spark, branch_schema):
    # Output schema check
    expected = set([f.name for f in branch_schema.fields])
    branches = [("00008", "R", "Y", 40.0, -75.0, "A", "Branch F", "CityF", "ST")]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    actual = set(branch_df.columns)
    assert expected == actual

def test_performance_caching_partitioning(spark, minimal_data):
    # This is a mock test to ensure caching/partitioning can be applied
    branch_df, customer_df = minimal_data
    branch_df.cache()
    branch_df = branch_df.repartition(1)
    assert branch_df.count() == 1  # Just ensures no error

```

# 3. API Cost

apiCost: 0.0023 USD