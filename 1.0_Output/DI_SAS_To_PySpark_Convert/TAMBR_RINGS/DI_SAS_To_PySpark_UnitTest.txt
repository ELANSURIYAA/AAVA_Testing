```
=============================================
Author: Ascendion AVAA
Date: 
Description: Pytest suite for validating PySpark TAMBr ring calculation logic, including joins, aggregations, UDFs, and edge cases.
=============================================

# =========================
# 1. Test Case List
# =========================

| Test Case ID | Description                                                                                     | Expected Outcome                                                                                   |
|--------------|------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| TC01         | Happy path: Valid customer, branch, and geocode data, all joins succeed, distances calculated   | Output DataFrame has correct schema, correct distances, correct rings, and expected row count      |
| TC02         | Edge: Customer with NULL latitude/longitude                                                     | Distance is NULL, customer excluded from ring calculation, schema/data correct                     |
| TC03         | Edge: Branch with invalid lat/long (<=1 or >=-1)                                                | Branch excluded from ring calculation, not present in output                                       |
| TC04         | Edge: Customer with no matching branch (priority or most used)                                  | Customer excluded from final ring calculation, output as expected                                  |
| TC05         | Edge: Duplicate customers (same LP_ID, different NBR_OF_MOS_OPN)                                | Only first record per LP_ID is kept (lowest NBR_OF_MOS_OPN)                                        |
| TC06         | Edge: No data (empty DataFrames)                                                                | Output DataFrame is empty with correct schema                                                      |
| TC07         | Error: Type mismatch in latitude/longitude (e.g., string instead of float)                      | Exception is raised, error handling/logging is triggered                                           |
| TC08         | Error: Missing required columns in input DataFrames                                             | Exception is raised, error handling/logging is triggered                                           |
| TC09         | Happy path: Percentile calculation for ring is correct                                          | 80th percentile distance is correctly calculated for each branch                                   |
| TC10         | Edge: All distances are NULL for a branch                                                       | Ring value for that branch is set to 0 (as per coalesce logic)                                     |

# =========================
# 2. Pytest Script
# =========================

import pytest
from pyspark.sql import SparkSession, Row, functions as F, types as T, Window
from math import radians, cos, sin, asin, sqrt

# Helper: geodist function/UDF
def geodist(lat1, lon1, lat2, lon2):
    if None in [lat1, lon1, lat2, lon2]:
        return None
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    miles = 3956 * c
    return miles

geodist_udf = F.udf(geodist, T.DoubleType())

# Pytest fixture for SparkSession
@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("tambr_test").getOrCreate()

# Helper: DataFrame equality check (schema & data)
def assert_df_equality(df1, df2, ignore_nullable=True, ignore_row_order=True):
    # Compare schema
    assert df1.schema == df2.schema, f"Schema mismatch: {df1.schema} != {df2.schema}"
    # Compare data
    data1 = df1.collect()
    data2 = df2.collect()
    if ignore_row_order:
        assert sorted(data1) == sorted(data2), f"Data mismatch: {data1} != {data2}"
    else:
        assert data1 == data2, f"Data mismatch: {data1} != {data2}"

# =========================
# TC01: Happy path
# =========================
def test_happy_path(spark):
    # Mock input DataFrames
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat=40.0, custlong=-75.0, MOST_USED_BR="B2"),
        Row(LP_ID="C2", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=10, custlat=41.0, custlong=-76.0, MOST_USED_BR="B2"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
        Row(HGN_BR_ID="B2", BR_TYP="I", branchlat=41.1, branchlong=-76.1, BRICK_AND_MORTOR_NM="Branch 2", CITY="CityB", ST="PA", METRO_COMMUNITY_CDE="Y"),
    ])
    # Priority join
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    ).filter(
        (rings_cust_data.OPN_ACCT_FLG == 'Y') &
        (rings_cust_data.NBR_OF_MOS_OPN <= 24) &
        (rings_cust_data.NBR_OF_MOS_OPN >= 0)
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf("branchlat", "branchlong", "custlat", "custlong")
    )
    # Percentile
    priority_ring = priority_cust.groupBy("PRTY_BR").agg(
        F.expr("percentile_approx(dist_to_prty_br, 0.8)").alias("priority_ring")
    ).withColumnRenamed("PRTY_BR", "TAMBR")
    # Check schema and data
    expected_schema = T.StructType([
        T.StructField("TAMBR", T.StringType(), True),
        T.StructField("priority_ring", T.DoubleType(), True),
    ])
    assert priority_ring.schema == expected_schema
    assert priority_ring.count() == 1
    assert abs(priority_ring.collect()[0]["priority_ring"] - geodist(40.1, -75.1, 41.0, -76.0)) < 1.0

# =========================
# TC02: NULL latitude/longitude
# =========================
def test_null_latlong(spark):
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat=None, custlong=None, MOST_USED_BR="B2"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
    ])
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf("branchlat", "branchlong", "custlat", "custlong")
    )
    assert priority_cust.collect()[0]["dist_to_prty_br"] is None

# =========================
# TC03: Invalid branch lat/long
# =========================
def test_invalid_branch_latlong(spark):
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=0.0, branchlong=0.0, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
        Row(HGN_BR_ID="B2", BR_TYP="I", branchlat=41.1, branchlong=-76.1, BRICK_AND_MORTOR_NM="Branch 2", CITY="CityB", ST="PA", METRO_COMMUNITY_CDE="Y"),
    ])
    good_branches = rings_branch_data.filter((F.col("branchlat") > 1) & (F.col("branchlong") < -1))
    assert good_branches.count() == 1
    assert good_branches.collect()[0]["HGN_BR_ID"] == "B2"

# =========================
# TC04: No matching branch
# =========================
def test_no_matching_branch(spark):
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", PRTY_BR="B99", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat=40.0, custlong=-75.0, MOST_USED_BR="B99"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
    ])
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    )
    assert priority_cust.count() == 0

# =========================
# TC05: Duplicate customers
# =========================
def test_duplicate_customers(spark):
    customers = spark.createDataFrame([
        Row(LP_ID="C1", NBR_OF_MOS_OPN=5),
        Row(LP_ID="C1", NBR_OF_MOS_OPN=10),
        Row(LP_ID="C2", NBR_OF_MOS_OPN=3),
    ])
    window_first = Window.partitionBy("LP_ID").orderBy("NBR_OF_MOS_OPN")
    customers1 = customers.withColumn("rn", F.row_number().over(window_first)).filter(F.col("rn") == 1).drop("rn")
    result = customers1.select("LP_ID", "NBR_OF_MOS_OPN").collect()
    assert Row(LP_ID="C1", NBR_OF_MOS_OPN=5) in result
    assert Row(LP_ID="C2", NBR_OF_MOS_OPN=3) in result
    assert len(result) == 2

# =========================
# TC06: Empty DataFrames
# =========================
def test_empty_dataframes(spark):
    schema = T.StructType([
        T.StructField("LP_ID", T.StringType(), True),
        T.StructField("PRTY_BR", T.StringType(), True),
        T.StructField("OPN_ACCT_FLG", T.StringType(), True),
        T.StructField("NBR_OF_MOS_OPN", T.IntegerType(), True),
        T.StructField("custlat", T.DoubleType(), True),
        T.StructField("custlong", T.DoubleType(), True),
        T.StructField("MOST_USED_BR", T.StringType(), True),
    ])
    rings_cust_data = spark.createDataFrame([], schema)
    rings_branch_data = spark.createDataFrame([], T.StructType([
        T.StructField("HGN_BR_ID", T.StringType(), True),
        T.StructField("BR_TYP", T.StringType(), True),
        T.StructField("branchlat", T.DoubleType(), True),
        T.StructField("branchlong", T.DoubleType(), True),
        T.StructField("BRICK_AND_MORTOR_NM", T.StringType(), True),
        T.StructField("CITY", T.StringType(), True),
        T.StructField("ST", T.StringType(), True),
        T.StructField("METRO_COMMUNITY_CDE", T.StringType(), True),
    ]))
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    )
    assert priority_cust.count() == 0

# =========================
# TC07: Type mismatch in lat/long
# =========================
def test_type_mismatch_latlong(spark):
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat="not_a_float", custlong=-75.0, MOST_USED_BR="B2"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
    ])
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    )
    with pytest.raises(Exception):
        priority_cust.withColumn(
            "dist_to_prty_br",
            geodist_udf("branchlat", "branchlong", "custlat", "custlong")
        ).collect()

# =========================
# TC08: Missing required columns
# =========================
def test_missing_columns(spark):
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat=40.0, custlong=-75.0, MOST_USED_BR="B2"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
    ])
    # PRTY_BR is missing from rings_cust_data
    with pytest.raises(Exception):
        rings_cust_data.join(
            rings_branch_data,
            rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
            "inner"
        ).collect()

# =========================
# TC09: Percentile calculation
# =========================
def test_percentile_calculation(spark):
    data = [
        Row(PRTY_BR="B1", dist_to_prty_br=1.0),
        Row(PRTY_BR="B1", dist_to_prty_br=2.0),
        Row(PRTY_BR="B1", dist_to_prty_br=3.0),
        Row(PRTY_BR="B1", dist_to_prty_br=4.0),
        Row(PRTY_BR="B1", dist_to_prty_br=5.0),
    ]
    df = spark.createDataFrame(data)
    priority_ring = df.groupBy("PRTY_BR").agg(
        F.expr("percentile_approx(dist_to_prty_br, 0.8)").alias("priority_ring")
    ).withColumnRenamed("PRTY_BR", "TAMBR")
    val = priority_ring.collect()[0]["priority_ring"]
    assert abs(val - 4.2) < 0.1  # 80th percentile between 4 and 5

# =========================
# TC10: All distances NULL
# =========================
def test_all_distances_null(spark):
    data = [
        Row(PRTY_BR="B1", dist_to_prty_br=None),
        Row(PRTY_BR="B1", dist_to_prty_br=None),
    ]
    df = spark.createDataFrame(data)
    priority_ring = df.groupBy("PRTY_BR").agg(
        F.expr("percentile_approx(dist_to_prty_br, 0.8)").alias("priority_ring")
    ).withColumnRenamed("PRTY_BR", "TAMBR")
    # Coalesce logic in main code sets to 0 if null
    priority_ring = priority_ring.withColumn("priority_ring", F.coalesce("priority_ring", F.lit(0)))
    assert priority_ring.collect()[0]["priority_ring"] == 0

# =========================
# 3. API Cost
# =========================
apiCost: 0.0023 USD
```
