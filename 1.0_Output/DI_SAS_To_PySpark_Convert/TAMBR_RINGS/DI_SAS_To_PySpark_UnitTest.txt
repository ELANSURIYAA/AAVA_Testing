=============================================
Author: Ascendion AVAA
Date: 
Description: Pytest test suite for validating the PySpark TAMBR Rings workflow, including transformation logic, joins, aggregations, geospatial UDFs, and edge case handling.
=============================================

# 1. Test Case List

| Test Case ID | Description                                                                                   | Expected Outcome                                                                                      |
|--------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: Valid customer and branch data, all joins and aggregations work as expected       | Output DataFrames match expected schema and data, distances are correct, percentiles are accurate     |
| TC02         | Edge case: NULL/zero latitude or longitude in customer/branch data                           | NULL or default values handled, records routed to bad_latlong tables as expected                     |
| TC03         | Edge case: Empty input DataFrames                                                            | Output DataFrames are empty, no errors thrown                                                        |
| TC04         | Edge case: Duplicate LP_IDs or BR_IDs                                                        | Window/row_number logic deduplicates as expected, only one record per partition                      |
| TC05         | Error handling: Type mismatch in latitude/longitude columns                                  | Exception is raised or rows with bad types are excluded                                              |
| TC06         | Filtering: Only open branches (BR_OPN_FLG='Y') and valid branch types are included           | Output contains only valid branches                                                                  |
| TC07         | Aggregation: 80th percentile calculation for distance rings                                  | Percentile values are correct for known input                                                        |
| TC08         | UDF: geodist returns correct distance for known coordinates                                  | Output matches Haversine formula result                                                              |
| TC09         | Schema: Output DataFrames have expected columns and types                                    | Assert schema matches specification                                                                  |
| TC10         | Performance: Large input DataFrames complete within reasonable time (mocked)                 | Test completes within timeout, no memory errors                                                      |

# 2. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit, when
import math

# --- Fixtures and Helpers ---

@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("tambr_rings_test").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def sample_branch_df(spark):
    data = [
        Row(HGN_BR_ID="B001", BR_TYP="R", BR_OPN_FLG="Y", branchlat=40.0, branchlong=-74.0, METRO_COMMUNITY_CDE="M1", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Main", CITY="NY", ST="NY", ZIP_CDE="10001"),
        Row(HGN_BR_ID="B002", BR_TYP="U", BR_OPN_FLG="Y", branchlat=41.0, branchlong=-75.0, METRO_COMMUNITY_CDE="M2", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Second", CITY="LA", ST="CA", ZIP_CDE="90001"),
    ]
    schema = StructType([
        StructField("HGN_BR_ID", StringType()),
        StructField("BR_TYP", StringType()),
        StructField("BR_OPN_FLG", StringType()),
        StructField("branchlat", DoubleType()),
        StructField("branchlong", DoubleType()),
        StructField("METRO_COMMUNITY_CDE", StringType()),
        StructField("GEN_CDE", StringType()),
        StructField("TBA_CLS_DVSTD_DT", StringType()),
        StructField("BRICK_AND_MORTOR_NM", StringType()),
        StructField("CITY", StringType()),
        StructField("ST", StringType()),
        StructField("ZIP_CDE", StringType()),
    ])
    return spark.createDataFrame(data, schema=schema)

@pytest.fixture
def sample_customer_df(spark):
    data = [
        Row(LP_ID="C001", PRTY_BR="B001", CUST_PRTY_ACCT_ID="A001", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, MOST_USED_BR="B002", HGN_CUST_TYP_CDE="T1", OPN_ACCT_CNT=1, geomatchcode="1", custlat=40.1, custlong=-74.1),
        Row(LP_ID="C002", PRTY_BR="B002", CUST_PRTY_ACCT_ID="A002", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=24, MOST_USED_BR="B001", HGN_CUST_TYP_CDE="T2", OPN_ACCT_CNT=2, geomatchcode="1", custlat=41.1, custlong=-75.1),
    ]
    schema = StructType([
        StructField("LP_ID", StringType()),
        StructField("PRTY_BR", StringType()),
        StructField("CUST_PRTY_ACCT_ID", StringType()),
        StructField("OPN_ACCT_FLG", StringType()),
        StructField("NBR_OF_MOS_OPN", IntegerType()),
        StructField("MOST_USED_BR", StringType()),
        StructField("HGN_CUST_TYP_CDE", StringType()),
        StructField("OPN_ACCT_CNT", IntegerType()),
        StructField("geomatchcode", StringType()),
        StructField("custlat", DoubleType()),
        StructField("custlong", DoubleType()),
    ])
    return spark.createDataFrame(data, schema=schema)

@pytest.fixture
def geodist_udf():
    def geodist(lat1, lon1, lat2, lon2):
        try:
            lat1, lon1, lat2, lon2 = map(float, [lat1, lon1, lat2, lon2])
            phi1, phi2 = math.radians(lat1), math.radians(lat2)
            dphi = math.radians(lat2 - lat1)
            dlambda = math.radians(lon2 - lon1)
            a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2
            c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
            miles = 3958.8 * c
            return float(miles)
        except Exception:
            return None
    from pyspark.sql.functions import udf
    from pyspark.sql.types import DoubleType
    return udf(geodist, DoubleType())

# --- Test Cases ---

def test_TC01_happy_path(spark, sample_branch_df, sample_customer_df, geodist_udf):
    # Join customer and branch, calculate distance
    df = sample_customer_df.join(
        sample_branch_df,
        sample_customer_df.PRTY_BR == sample_branch_df.HGN_BR_ID,
        how="inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf(col("branchlat"), col("branchlong"), col("custlat"), col("custlong"))
    )
    # Check schema
    assert "dist_to_prty_br" in df.columns
    # Check data
    rows = df.collect()
    assert len(rows) == 2
    assert all(isinstance(r.dist_to_prty_br, float) for r in rows)

def test_TC02_null_latlong(spark, sample_branch_df, sample_customer_df, geodist_udf):
    # Add a branch with null lat/long
    null_branch = Row(HGN_BR_ID="B003", BR_TYP="R", BR_OPN_FLG="Y", branchlat=None, branchlong=None, METRO_COMMUNITY_CDE="M3", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Null", CITY="SF", ST="CA", ZIP_CDE="94101")
    branch_df = sample_branch_df.union(spark.createDataFrame([null_branch], sample_branch_df.schema))
    # Add a customer referencing null branch
    null_cust = Row(LP_ID="C003", PRTY_BR="B003", CUST_PRTY_ACCT_ID="A003", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=10, MOST_USED_BR="B003", HGN_CUST_TYP_CDE="T3", OPN_ACCT_CNT=1, geomatchcode="1", custlat=None, custlong=None)
    customer_df = sample_customer_df.union(spark.createDataFrame([null_cust], sample_customer_df.schema))
    df = customer_df.join(
        branch_df,
        customer_df.PRTY_BR == branch_df.HGN_BR_ID,
        how="inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf(col("branchlat"), col("branchlong"), col("custlat"), col("custlong"))
    )
    # The distance should be None for nulls
    row = df.filter(col("LP_ID") == "C003").collect()[0]
    assert row.dist_to_prty_br is None

def test_TC03_empty_input(spark, geodist_udf):
    schema = StructType([
        StructField("LP_ID", StringType()),
        StructField("PRTY_BR", StringType()),
        StructField("CUST_PRTY_ACCT_ID", StringType()),
        StructField("OPN_ACCT_FLG", StringType()),
        StructField("NBR_OF_MOS_OPN", IntegerType()),
        StructField("MOST_USED_BR", StringType()),
        StructField("HGN_CUST_TYP_CDE", StringType()),
        StructField("OPN_ACCT_CNT", IntegerType()),
        StructField("geomatchcode", StringType()),
        StructField("custlat", DoubleType()),
        StructField("custlong", DoubleType()),
    ])
    empty_cust = spark.createDataFrame([], schema)
    empty_branch = spark.createDataFrame([], StructType([
        StructField("HGN_BR_ID", StringType()),
        StructField("BR_TYP", StringType()),
        StructField("BR_OPN_FLG", StringType()),
        StructField("branchlat", DoubleType()),
        StructField("branchlong", DoubleType()),
        StructField("METRO_COMMUNITY_CDE", StringType()),
        StructField("GEN_CDE", StringType()),
        StructField("TBA_CLS_DVSTD_DT", StringType()),
        StructField("BRICK_AND_MORTOR_NM", StringType()),
        StructField("CITY", StringType()),
        StructField("ST", StringType()),
        StructField("ZIP_CDE", StringType()),
    ]))
    df = empty_cust.join(
        empty_branch,
        empty_cust.PRTY_BR == empty_branch.HGN_BR_ID,
        how="inner"
    )
    assert df.count() == 0

def test_TC04_duplicate_lp_ids(spark, sample_customer_df):
    # Duplicate LP_ID in customer
    dup_data = sample_customer_df.union(sample_customer_df)
    from pyspark.sql import Window
    from pyspark.sql.functions import row_number
    w = Window.partitionBy("LP_ID").orderBy("NBR_OF_MOS_OPN")
    df = dup_data.withColumn("rn", row_number().over(w)).filter(col("rn") == 1)
    # Only one record per LP_ID
    assert df.groupBy("LP_ID").count().filter(col("count") > 1).count() == 0

def test_TC05_type_mismatch_latlong(spark, sample_branch_df, sample_customer_df, geodist_udf):
    # Add a branch with string lat/long
    bad_branch = Row(HGN_BR_ID="B004", BR_TYP="R", BR_OPN_FLG="Y", branchlat="bad", branchlong="bad", METRO_COMMUNITY_CDE="M4", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Bad", CITY="SF", ST="CA", ZIP_CDE="94102")
    branch_df = sample_branch_df.union(spark.createDataFrame([bad_branch], sample_branch_df.schema))
    bad_cust = Row(LP_ID="C004", PRTY_BR="B004", CUST_PRTY_ACCT_ID="A004", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=5, MOST_USED_BR="B004", HGN_CUST_TYP_CDE="T4", OPN_ACCT_CNT=1, geomatchcode="1", custlat="bad", custlong="bad")
    customer_df = sample_customer_df.union(spark.createDataFrame([bad_cust], sample_customer_df.schema))
    df = customer_df.join(
        branch_df,
        customer_df.PRTY_BR == branch_df.HGN_BR_ID,
        how="inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf(col("branchlat"), col("branchlong"), col("custlat"), col("custlong"))
    )
    # The distance should be None for bad types
    row = df.filter(col("LP_ID") == "C004").collect()[0]
    assert row.dist_to_prty_br is None

def test_TC06_filter_open_branches(spark, sample_branch_df):
    # Add a closed branch
    closed_branch = Row(HGN_BR_ID="B005", BR_TYP="R", BR_OPN_FLG="N", branchlat=42.0, branchlong=-76.0, METRO_COMMUNITY_CDE="M5", GEN_CDE=None, TBA_CLS_DVSTD_DT=None, BRICK_AND_MORTOR_NM="Closed", CITY="TX", ST="TX", ZIP_CDE="75001")
    branch_df = sample_branch_df.union(spark.createDataFrame([closed_branch], sample_branch_df.schema))
    open_branches = branch_df.filter((col("BR_OPN_FLG") == "Y") & (col("BR_TYP").isin(["R", "U", "I", "T", "C"])))
    assert open_branches.filter(col("HGN_BR_ID") == "B005").count() == 0

def test_TC07_percentile_aggregation(spark):
    # Test percentile_approx
    data = [Row(PRTY_BR="B001", dist_to_prty_br=x) for x in range(1, 101)]
    df = spark.createDataFrame(data)
    df.createOrReplaceTempView("rings_priority_cust")
    percentile = spark.sql("""
        SELECT PRTY_BR, percentile_approx(dist_to_prty_br, 0.8) as prtybr80
        FROM rings_priority_cust
        GROUP BY PRTY_BR
    """).collect()[0].prtybr80
    # 80th percentile of 1..100 is 80
    assert percentile == 80

def test_TC08_geodist_udf(geodist_udf, spark):
    # NYC (40.7128, -74.0060) to LA (34.0522, -118.2437)
    df = spark.createDataFrame([
        Row(branchlat=40.7128, branchlong=-74.0060, custlat=34.0522, custlong=-118.2437)
    ])
    df = df.withColumn("dist", geodist_udf("branchlat", "branchlong", "custlat", "custlong"))
    dist = df.collect()[0].dist
    # Should be around 2445 miles
    assert abs(dist - 2445) < 10

def test_TC09_schema(sample_branch_df):
    expected_fields = {"HGN_BR_ID", "BR_TYP", "BR_OPN_FLG", "branchlat", "branchlong", "METRO_COMMUNITY_CDE", "BRICK_AND_MORTOR_NM", "CITY", "ST", "ZIP_CDE"}
    actual_fields = set(sample_branch_df.columns)
    assert expected_fields.issubset(actual_fields)

def test_TC10_performance(spark, sample_branch_df, sample_customer_df, geodist_udf):
    # Large DataFrame (simulate, but keep small for test)
    big_branches = sample_branch_df
    for _ in range(10):  # 2^10 = 1024 rows
        big_branches = big_branches.union(big_branches)
    big_customers = sample_customer_df
    for _ in range(10):
        big_customers = big_customers.union(big_customers)
    import time
    start = time.time()
    df = big_customers.join(
        big_branches,
        big_customers.PRTY_BR == big_branches.HGN_BR_ID,
        how="inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf(col("branchlat"), col("branchlong"), col("custlat"), col("custlong"))
    )
    df.count()
    end = time.time()
    # Should finish within 10 seconds
    assert (end - start) < 10

```

# 3. API Cost

apiCost: 0.0023 USD