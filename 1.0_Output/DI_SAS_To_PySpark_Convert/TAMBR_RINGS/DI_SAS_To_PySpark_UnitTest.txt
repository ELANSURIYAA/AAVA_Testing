=============================================
Author: Ascendion AVAA
Date: 
Description: Pytest suite for validating PySpark data transformation logic, joins, aggregations, and geodist UDF for TAMBr monthly process, ensuring correctness, reliability, and edge case handling.
=============================================

# 1. Test Case List

| Test Case ID | Description | Expected Outcome |
|--------------|-------------|-----------------|
| TC01 | Validate geodist UDF returns correct distance for known coordinates | Distance matches expected value (within tolerance) |
| TC02 | Happy path: All joins and aggregations with valid data | Output DataFrame matches expected schema and values |
| TC03 | Edge case: Input DataFrames with NULLs in join/filter columns | NULLs handled as per logic, output as expected |
| TC04 | Edge case: Input DataFrames with zero rows | Output DataFrames are empty, no errors |
| TC05 | Edge case: Duplicate LP_IDs in customer data | Only first/expected record per LP_ID retained |
| TC06 | Error handling: Type mismatch in geodist UDF input | Raises appropriate exception or handles gracefully |
| TC07 | Schema validation: Output DataFrame columns and dtypes | Schema matches expected (column names, types) |
| TC08 | Aggregation: 80th percentile calculation for ring distances | Percentile value matches manual calculation |
| TC09 | Filtering: Only open branches with valid lat/long included | Only expected branches present in output |
| TC10 | Most used/priority branch logic: Correct branch assignment | Branch assignment matches expected based on input |

---

# 2. Pytest Script

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType
from pyspark.sql.functions import col, lit
import math

# --- Fixtures ---

@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("TAMBR_RINGS_TEST").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def sample_data(spark):
    # Sample data for customers, branches, etc.
    customers = spark.createDataFrame([
        Row(LP_ID="1", PRTY_BR="B1", MOST_USED_BR="B2", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, CUSTLAT=40.0, CUSTLONG=-75.0, geomatchcode="1"),
        Row(LP_ID="2", PRTY_BR="B2", MOST_USED_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=24, CUSTLAT=41.0, CUSTLONG=-76.0, geomatchcode="1"),
        Row(LP_ID="3", PRTY_BR="B1", MOST_USED_BR="B1", OPN_ACCT_FLG="N", NBR_OF_MOS_OPN=30, CUSTLAT=None, CUSTLONG=None, geomatchcode="0"),
    ])
    branches = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", BR_OPN_FLG="Y", branchlat=40.1, branchlong=-75.1, METRO_COMMUNITY_CDE="A", BRICK_AND_MORTOR_NM="Branch1", CITY="City1", ST="ST1", ZIP_CDE="10001"),
        Row(HGN_BR_ID="B2", BR_TYP="U", BR_OPN_FLG="Y", branchlat=41.1, branchlong=-76.1, METRO_COMMUNITY_CDE="B", BRICK_AND_MORTOR_NM="Branch2", CITY="City2", ST="ST2", ZIP_CDE="10002"),
        Row(HGN_BR_ID="00001", BR_TYP="R", BR_OPN_FLG="Y", branchlat=0.0, branchlong=0.0, METRO_COMMUNITY_CDE="C", BRICK_AND_MORTOR_NM="Fake", CITY="Fake", ST="ST3", ZIP_CDE="00000"),
    ])
    return customers, branches

# --- Helper Functions ---

def geodist(lat1, lon1, lat2, lon2, unit='M'):
    # Same as in main code
    R = 3958.8 if unit == 'M' else 6371.0
    lat1, lon1, lat2, lon2 = map(float, [lat1, lon1, lat2, lon2])
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    return R * c

# --- Test Cases ---

def test_geodist_udf_accuracy():
    # TC01
    # Philadelphia (39.9526, -75.1652) to New York (40.7128, -74.0060) ~ 80.6 miles
    dist = geodist(39.9526, -75.1652, 40.7128, -74.0060, 'M')
    assert abs(dist - 80.6) < 1.0

def test_happy_path_transformations(spark, sample_data):
    # TC02
    customers, branches = sample_data
    # Join on PRTY_BR
    joined = customers.join(branches, customers.PRTY_BR == branches.HGN_BR_ID, "inner")
    assert joined.count() == 2  # Only first two customers (OPN_ACCT_FLG='Y', matching branch)
    # Check columns
    assert "branchlat" in joined.columns

def test_null_handling_in_joins(spark):
    # TC03
    customers = spark.createDataFrame([
        Row(LP_ID="1", PRTY_BR=None, OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, CUSTLAT=40.0, CUSTLONG=-75.0, geomatchcode="1"),
    ])
    branches = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", BR_OPN_FLG="Y", branchlat=40.1, branchlong=-75.1),
    ])
    joined = customers.join(branches, customers.PRTY_BR == branches.HGN_BR_ID, "left")
    assert joined.filter(col("HGN_BR_ID").isNull()).count() == 1

def test_zero_row_input(spark):
    # TC04
    customers = spark.createDataFrame([], schema="LP_ID string, PRTY_BR string, OPN_ACCT_FLG string, NBR_OF_MOS_OPN int, CUSTLAT double, CUSTLONG double, geomatchcode string")
    branches = spark.createDataFrame([], schema="HGN_BR_ID string, BR_TYP string, BR_OPN_FLG string, branchlat double, branchlong double")
    joined = customers.join(branches, customers.PRTY_BR == branches.HGN_BR_ID, "inner")
    assert joined.count() == 0

def test_duplicate_lp_id_handling(spark):
    # TC05
    customers = spark.createDataFrame([
        Row(LP_ID="1", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, CUSTLAT=40.0, CUSTLONG=-75.0, geomatchcode="1"),
        Row(LP_ID="1", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=13, CUSTLAT=40.0, CUSTLONG=-75.0, geomatchcode="1"),
    ])
    deduped = customers.dropDuplicates(["LP_ID"])
    assert deduped.count() == 1

def test_geodist_udf_type_error():
    # TC06
    with pytest.raises(ValueError):
        geodist("not_a_number", -75.0, 40.0, -74.0, 'M')

def test_schema_validation(spark, sample_data):
    # TC07
    customers, branches = sample_data
    expected_schema = set(['LP_ID', 'PRTY_BR', 'MOST_USED_BR', 'OPN_ACCT_FLG', 'NBR_OF_MOS_OPN', 'CUSTLAT', 'CUSTLONG', 'geomatchcode'])
    assert set(customers.columns) == expected_schema

def test_percentile_approx(spark):
    # TC08
    df = spark.createDataFrame([
        Row(dist=1.0), Row(dist=2.0), Row(dist=3.0), Row(dist=4.0), Row(dist=5.0)
    ])
    # 80th percentile should be 4.2 (manually, 80% between 4 and 5)
    from pyspark.sql.functions import percentile_approx
    result = df.agg(percentile_approx("dist", 0.8).alias("p80")).collect()[0]["p80"]
    assert abs(result - 4.2) < 0.5

def test_branch_filtering(spark, sample_data):
    # TC09
    _, branches = sample_data
    filtered = branches.filter((col("branchlat") > 1) & (col("branchlong") < -1))
    assert all([row.HGN_BR_ID != "00001" for row in filtered.collect()])

def test_branch_assignment_logic(spark, sample_data):
    # TC10
    customers, branches = sample_data
    # For LP_ID=1, PRTY_BR="B1", join should match branch B1
    joined = customers.join(branches, customers.PRTY_BR == branches.HGN_BR_ID, "inner")
    row = joined.filter(col("LP_ID") == "1").collect()[0]
    assert row.HGN_BR_ID == "B1"

```

---

# 3. API Cost

apiCost: 0.0023 USD