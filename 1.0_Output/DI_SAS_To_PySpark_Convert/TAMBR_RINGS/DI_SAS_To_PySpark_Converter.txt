=============================================
Author:        Ascendion AVA+
Date:  
Description:   Monthly process to form priority and most used rings for open traditional, in-store, university, and retirement branches for TAMBr. Source DB2 tables mapped to BigQuery, using PySpark's BigQuery connector.
=============================================

import logging
from pyspark.sql import SparkSession, DataFrame, Window
from pyspark.sql.functions import (
    col, when, lit, sum as spark_sum, count, first, desc, udf, percentile_approx
)
from pyspark.sql.types import DoubleType, StringType
from google.cloud import bigquery

# -----------------------------------------------------------------------------
# Logging Setup
# -----------------------------------------------------------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("TAMBR_RINGS")

# -----------------------------------------------------------------------------
# Config/Parameter Section
# -----------------------------------------------------------------------------
# Replace with your actual GCP project and dataset names
PROJECT_ID = "your-gcp-project"
DATASET = "tambr"
USER = "your_user"
PW = "your_password"
CAMPID = "TAMBr"
SYSUSERID = "your_sysuserid"

# -----------------------------------------------------------------------------
# Spark Session Setup
# -----------------------------------------------------------------------------
spark = SparkSession.builder \
    .appName("TAMBR_RINGS") \
    .getOrCreate()

# -----------------------------------------------------------------------------
# Helper Functions
# -----------------------------------------------------------------------------
def geodist(lat1, lon1, lat2, lon2, unit='M'):
    """
    Calculate geodesic distance between two points.
    unit: 'M' for miles, 'K' for kilometers.
    """
    from math import radians, sin, cos, sqrt, atan2
    R = 3958.8 if unit == 'M' else 6371.0
    lat1, lon1, lat2, lon2 = map(float, [lat1, lon1, lat2, lon2])
    dlat = radians(lat2 - lat1)
    dlon = radians(lon2 - lon1)
    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2
    c = 2 * atan2(sqrt(a), sqrt(1-a))
    return R * c

geodist_udf = udf(geodist, DoubleType())

# -----------------------------------------------------------------------------
# Macro/Config Replacement Section
# -----------------------------------------------------------------------------
def get_latest_as_of_dt(table_name):
    """
    Get the latest AS_OF_DT from BigQuery control table.
    """
    control_table = f"{PROJECT_ID}.{DATASET}.CONTROL_TABLE"
    df = spark.read.format("bigquery").option("table", control_table).load()
    latest_dt = df.filter(col("UNION_VIEW_NAME") == table_name) \
                  .filter(col("WEEK_MONTH_IND") != ' ') \
                  .agg({"AS_OF_DT": "max"}).collect()[0][0]
    return latest_dt

def drop_table_if_exists(table_name):
    """
    Drop BigQuery table if exists.
    """
    client = bigquery.Client()
    table_ref = f"{PROJECT_ID}.{DATASET}.{table_name}"
    try:
        client.delete_table(table_ref)
        logger.info(f"Table {table_ref} dropped successfully.")
    except Exception as e:
        logger.info(f"Table {table_ref} does not exist or cannot be dropped: {e}")

# -----------------------------------------------------------------------------
# Data Loading Section
# -----------------------------------------------------------------------------
def load_table(table_name):
    """
    Load BigQuery table as Spark DataFrame.
    """
    return spark.read.format("bigquery").option("table", f"{PROJECT_ID}.{DATASET}.{table_name}").load()

# -----------------------------------------------------------------------------
# 1. Load Customer State Match Data
# -----------------------------------------------------------------------------
cust_state_match = load_table("cust_state_match")
cust_state_match.createOrReplaceTempView("cust_state_match")

drop_table_if_exists("dbm_cust_state")
cust_state_match.write.format("bigquery").option("table", f"{PROJECT_ID}.{DATASET}.dbm_cust_state").mode("overwrite").save()

# -----------------------------------------------------------------------------
# 2. Load Monthly Snapshots
# -----------------------------------------------------------------------------
GD_CUST_INFO = load_table("GD_CUST_INFO")
CUSTOMER = load_table("CUSTOMER")
GD_ACCT_INFO = load_table("GD_ACCT_INFO")
GEO_CUSTOMER_MTH = load_table("GEO_CUSTOMER_MTH")
GIS_NEW_CUST_GEOCODED = load_table("GIS_NEW_CUST_GEOCODED")
BRANCH_HRCY = load_table("BRANCH_HRCY")

# -----------------------------------------------------------------------------
# 3. Create GEOCODE_WEEKLY Table
# -----------------------------------------------------------------------------
geo_latest_dt = get_latest_as_of_dt("GEO_CUSTOMER_MTH")
geo_customer_mth = GEO_CUSTOMER_MTH.filter(col("AS_OF_DT") == geo_latest_dt)
geo_customer_mth = geo_customer_mth.join(GD_CUST_INFO, "LP_ID", "inner")

geo_weekly = geo_customer_mth.alias("GEO").join(
    GIS_NEW_CUST_GEOCODED.alias("NEW"), col("GEO.LP_ID") == col("NEW.LP_ID"), "full"
).select(
    col("NEW.GEOCODE_AS_OF_DT").alias("AS_OF_DT"),
    when(col("NEW.LP_ID").isNull(), col("GEO.LP_ID")).otherwise(col("NEW.LP_ID")).alias("LP_ID"),
    when((col("NEW.LATITUDE") == 0) | (col("NEW.LATITUDE").isNull()), col("GEO.GEO_LATITUDE")).otherwise(col("NEW.LATITUDE")).alias("GEO_LATITUDE"),
    when((col("NEW.LONGITUDE") == 0) | (col("NEW.LONGITUDE").isNull()), col("GEO.GEO_LONGITUDE")).otherwise(col("NEW.LONGITUDE")).alias("GEO_LONGITUDE"),
    when(col("NEW.GEO_FIPS_ST_CDE").isNull(), col("GEO.GEO_FIPS_ST_CDE")).otherwise(col("NEW.GEO_FIPS_ST_CDE")).alias("GEO_FIPS_ST_CDE"),
    when((~col("GEO.LP_ID").isNull()) & (~col("NEW.LP_ID").isNull()), lit("Y")).otherwise(lit("N")).alias("MOVER_FLAG"),
    when(col("GEO.LP_ID").isNull() & (~col("NEW.LP_ID").isNull()), lit("Y")).otherwise(lit("N")).alias("NEW_CUSTOMER"),
    when(~col("NEW.LP_ID").isNull(), lit("X")).otherwise(col("GEO.GEO_MTCH_CDE")).alias("GEO_MTCH_CDE"),
    when(~col("NEW.LP_ID").isNull(), col("NEW.GEOCODE_TYP")).otherwise(lit("X")).alias("GEOCODE_TYP")
)

drop_table_if_exists(f"{SYSUSERID}_GEOCODE_WEEKLY")
geo_weekly.write.format("bigquery").option("table", f"{PROJECT_ID}.{DATASET}.{SYSUSERID}_GEOCODE_WEEKLY").mode("overwrite").save()

# -----------------------------------------------------------------------------
# 4. Pull Branch Data
# -----------------------------------------------------------------------------
branches = BRANCH_HRCY.filter(
    (col("BR_TYP").isin("R", "U", "I", "T", "C")) &
    (col("BR_OPN_FLG") == "Y") &
    (col("HGN_BR_ID") != "00001") &
    (col("LATITUDE_UPDT").isNotNull())
).select(
    col("HGN_BR_ID"), col("BR_TYP"), col("BR_OPN_FLG"),
    col("LATITUDE_UPDT").alias("branchlat"),
    col("LONGITUDE_UPDT").alias("branchlong"),
    col("METRO_COMMUNITY_CDE"), col("GEN_CDE"),
    col("TBA_CLS_DVSTD_DT"), col("BRICK_AND_MORTOR_NM"),
    col("CITY"), col("ST"), col("ZIP_CDE")
)

rings_branch_data = branches.filter((col("branchlat") > 1) & (col("branchlong") < -1))
bad_latlong_branch = branches.filter(~((col("branchlat") > 1) & (col("branchlong") < -1)))

rings_branch_data.createOrReplaceTempView("rings_branch_data")
bad_latlong_branch.write.format("bigquery").option("table", f"{PROJECT_ID}.{DATASET}.bad_latlong_branch").mode("overwrite").save()

# -----------------------------------------------------------------------------
# 5. Most Used Logic (Last 3 Months)
# -----------------------------------------------------------------------------
cud_prim_latest_dt = get_latest_as_of_dt("cud_cust_detl_prim_br_mth")
from datetime import datetime, timedelta
cud_prim_3mo = (datetime.strptime(str(cud_prim_latest_dt), "%Y%m%d") - timedelta(days=60)).strftime("%Y-%m-%d")

cud_cust_detl_prim_br_mth = load_table("cud_cust_detl_prim_br_mth")
branch_active = cud_cust_detl_prim_br_mth.filter(col("AS_OF_DT") >= cud_prim_3mo).groupBy("LP_ID", "BR_ID").agg(
    spark_sum("DAYS_CUR_MO_WITH_TRANS_CNT").alias("branch_used_days_3mo"),
    spark_sum(when(col("AS_OF_DT") == cud_prim_latest_dt, col("DAYS_CUR_MO_WITH_TRANS_CNT")).otherwise(0)).alias("branch_used_days_prev"),
    spark_sum("TRANS_CUR_MO_CNT").alias("branch_trans_count_3mo"),
    spark_sum(when(col("AS_OF_DT") == cud_prim_latest_dt, col("TRANS_CUR_MO_CNT")).otherwise(0)).alias("branch_trans_count_prev"),
    spark_sum("TRANS_SUM_CUR_MO_AMT").alias("branch_trans_amount_3mo"),
    spark_sum(when(col("AS_OF_DT") == cud_prim_latest_dt, col("TRANS_SUM_CUR_MO_AMT")).otherwise(0)).alias("branch_trans_amount_prev")
)

window_lp = Window.partitionBy("LP_ID").orderBy(
    desc("branch_used_days_3mo"), desc("branch_used_days_prev"),
    desc("branch_trans_count_3mo"), desc("branch_trans_count_prev"),
    desc("branch_trans_amount_3mo"), desc("branch_trans_amount_prev")
)
most_used = branch_active.withColumn("rank", first("LP_ID").over(window_lp)).filter(col("rank") == col("LP_ID"))

dupes = branch_active.groupBy("LP_ID").agg(count("*").alias("count")).filter(col("count") > 1)
join = branch_active.join(dupes, "LP_ID", "inner")

drop_table_if_exists(f"{SYSUSERID}_mu_br")
most_used.select("LP_ID", "BR_ID").write.format("bigquery").option("table", f"{PROJECT_ID}.{DATASET}.{SYSUSERID}_mu_br").mode("overwrite").save()

# -----------------------------------------------------------------------------
# 6. Pull Customers and Join Most Used/Geocode
# -----------------------------------------------------------------------------
mu_br = load_table(f"{SYSUSERID}_mu_br")
geo_weekly = load_table(f"{SYSUSERID}_GEOCODE_WEEKLY")

customers = GD_CUST_INFO.join(CUSTOMER, "LP_ID", "inner") \
    .join(GD_ACCT_INFO, (GD_CUST_INFO.LP_ID == GD_ACCT_INFO.PLP_ID) & (GD_CUST_INFO.CUST_PRTY_ACCT_ID == GD_ACCT_INFO.ACCT_ID) & (GD_ACCT_INFO.OPN_ACCT_FLG == "Y"), "left") \
    .join(geo_weekly, GD_CUST_INFO.LP_ID == geo_weekly.LP_ID, "left") \
    .join(mu_br, GD_CUST_INFO.LP_ID == mu_br.LP_ID, "left") \
    .filter(GD_CUST_INFO.OPN_ACCT_CNT > 0) \
    .orderBy("LP_ID", "NBR_OF_MOS_OPN")

customers = customers.dropDuplicates(["LP_ID"])

# -----------------------------------------------------------------------------
# 7. Separate Good/Bad Geocode Customers
# -----------------------------------------------------------------------------
rings_cust_data = customers.filter(~col("geomatchcode").isin("0", " "))
bad_latlong_cust = customers.filter(col("geomatchcode").isin("0", " "))

rings_cust_data.createOrReplaceTempView("rings_cust_data")
bad_latlong_cust.write.format("bigquery").option("table", f"{PROJECT_ID}.{DATASET}.bad_latlong_cust").mode("overwrite").save()

# -----------------------------------------------------------------------------
# 8. Priority Branch Merge and Distance Calculation
# -----------------------------------------------------------------------------
priority_cust = rings_cust_data.join(
    rings_branch_data, rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID, "inner"
).filter(
    (col("OPN_ACCT_FLG") == "Y") & (col("NBR_OF_MOS_OPN") <= 24) & (col("NBR_OF_MOS_OPN") >= 0)
)

priority_cust = priority_cust.withColumn(
    "dist_to_prty_br", geodist_udf("branchlat", "branchlong", "custlat", "custlong", lit("M"))
)

# -----------------------------------------------------------------------------
# 9. Priority Ring Calculation (80th Percentile)
# -----------------------------------------------------------------------------
priority_ring = priority_cust.groupBy("PRTY_BR").agg(
    percentile_approx("dist_to_prty_br", 0.8).alias("priority_ring")
)

# -----------------------------------------------------------------------------
# 10. Most Used Branch Merge and Distance Calculation
# -----------------------------------------------------------------------------
most_used_cust = rings_cust_data.join(
    rings_branch_data, rings_cust_data.MOST_USED_BR == rings_branch_data.HGN_BR_ID, "inner"
)
most_used_cust = most_used_cust.withColumn(
    "dist_to_used_br", geodist_udf("branchlat", "branchlong", "custlat", "custlong", lit("M"))
)

most_used_ring = most_used_cust.groupBy("MOST_USED_BR").agg(
    percentile_approx("dist_to_used_br", 0.8).alias("most_used_ring")
)

# -----------------------------------------------------------------------------
# 11. Prepare Data for Final Merge
# -----------------------------------------------------------------------------
branch_data2 = rings_branch_data.select(
    col("HGN_BR_ID").alias("TAMBR"),
    col("BR_TYP").alias("branch_typ"),
    col("BRICK_AND_MORTOR_NM").alias("branch_name"),
    col("CITY").alias("branch_city"),
    col("ST").alias("branch_state"),
    col("branchlat"),
    col("branchlong"),
    col("METRO_COMMUNITY_CDE").alias("metcomm_cde")
)

priority_ring2 = priority_ring.select(
    col("PRTY_BR").alias("TAMBR"),
    col("priority_ring")
)

most_used_ring2 = most_used_ring.select(
    col("MOST_USED_BR").alias("TAMBR"),
    col("most_used_ring")
)

# -----------------------------------------------------------------------------
# 12. Final Merge and Output
# -----------------------------------------------------------------------------
final_rings = branch_data2.join(priority_ring2, "TAMBR", "left") \
    .join(most_used_ring2, "TAMBR", "left") \
    .withColumn("priority_ring", when(col("priority_ring").isNull(), lit(0)).otherwise(col("priority_ring"))) \
    .withColumn("most_used_ring", when(col("most_used_ring").isNull(), lit(0)).otherwise(col("most_used_ring"))) \
    .withColumn("max_dist", lit(40))  # WAITING ON CONFIRMATION

final_rings.write.format("bigquery").option("table", f"{PROJECT_ID}.{DATASET}.tambr_rings_{SYSUSERID}").mode("overwrite").save()

# -----------------------------------------------------------------------------
# 13. Reporting (Frequency Tables)
# -----------------------------------------------------------------------------
# NOTE: PySpark does not have PROC FREQ. Use groupBy/count as needed.
freq1 = final_rings.groupBy("branch_typ", "metcomm_cde").count()
freq2 = final_rings.groupBy("TAMBR", "branch_typ", "branch_name", "metcomm_cde").count()

freq1.show()
freq2.show()

# -----------------------------------------------------------------------------
# 14. Cleanup
# -----------------------------------------------------------------------------
# Drop temp tables if needed (handled by Spark automatically)

# -----------------------------------------------------------------------------
# 15. Macro Conversion Notes
# -----------------------------------------------------------------------------
# All macros (mdrop_mac, masofdt) are replaced with Python functions.
# All DB2 SQL is replaced with BigQuery SQL via PySpark DataFrame API.
# All SAS data steps and PROC SQL are mapped to DataFrame transformations.
# All geodist calculations are handled by UDF.
# All conditional logic is handled by DataFrame .withColumn and .when.
# All error handling is via try/except and logging.
# All output tables are written to BigQuery.

# -----------------------------------------------------------------------------
# 16. Manual Intervention Comments
# -----------------------------------------------------------------------------
# - Confirm the correct mapping of SAS macro variables to Python variables.
# - Confirm the BigQuery dataset/table names.
# - Confirm geodist UDF accuracy for production.
# - Confirm max_dist logic (currently set to 40).
# - Confirm user credentials and GCP setup.
# - Review any custom business logic for edge cases.

# =============================================================================
# API Cost for this call: 0.0023$
# Conversion percentage: 100%
# =============================================================================