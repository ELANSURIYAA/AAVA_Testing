# =============================================
# Author:        Ascendion AVA+
# Date:  
# Description:   PySpark implementation of the TAMBR Rings workflow for generating priority and most used rings for branches and customers. This code fully replicates the SAS logic, including data steps, SQL, macros, joins, aggregations, geospatial calculations, and conditional logic. All DB2 SQL is replaced with BigQuery Standard SQL, using PySpark's BigQuery connector. Robust error handling and logging are included. Any logic that could not be converted is commented for manual intervention.
# =============================================

import logging
from datetime import datetime, timedelta
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import (
    col, when, lit, sum as _sum, count, max as _max, min as _min, first, row_number,
    monotonically_increasing_id, udf, isnan, coalesce, expr, desc, broadcast
)
from pyspark.sql.types import DoubleType, StringType, IntegerType
import math

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("tambr_rings")

# Initialize Spark Session with BigQuery connector
spark = SparkSession.builder \
    .appName("TAMBR_RINGS") \
    .config("spark.jars", "/path/to/spark-bigquery-with-dependencies_2.12-0.32.2.jar") \
    .getOrCreate()

# Configuration (replace with actual values or config management)
project_id = "your-gcp-project"
dataset = "tambr"
bq_temp_bucket = "your-bq-temp-bucket"
sysuserid = "avauser"  # Replace with actual user id logic
cust_occr = "202407"   # Replace with actual logic for as_of_dt
campid = "TAMBr"

# Helper: BigQuery table path
def bq_table(table):
    return f"{project_id}.{dataset}.{table}"

# Helper: Read BigQuery table
def read_bq(table, **options):
    logger.info(f"Reading table {table} from BigQuery")
    return spark.read.format("bigquery") \
        .option("table", table) \
        .option("parentProject", project_id) \
        .option("temporaryGcsBucket", bq_temp_bucket) \
        .load()

# Helper: Write DataFrame to BigQuery
def write_bq(df, table, mode="overwrite"):
    logger.info(f"Writing DataFrame to {table} in BigQuery")
    df.write.format("bigquery") \
        .option("table", table) \
        .option("parentProject", project_id) \
        .option("temporaryGcsBucket", bq_temp_bucket) \
        .mode(mode) \
        .save()

# Helper: Geodist UDF (Haversine formula, miles)
def geodist(lat1, lon1, lat2, lon2):
    try:
        # Convert degrees to radians
        lat1, lon1, lat2, lon2 = map(float, [lat1, lon1, lat2, lon2])
        phi1, phi2 = math.radians(lat1), math.radians(lat2)
        dphi = math.radians(lat2 - lat1)
        dlambda = math.radians(lon2 - lon1)
        a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        miles = 3958.8 * c
        return float(miles)
    except Exception:
        return None
geodist_udf = udf(geodist, DoubleType())

# -----------------------------------------------------------------------------
# 1. Load Customer State Match Table
# -----------------------------------------------------------------------------
try:
    cust_state_match = read_bq(bq_table("cust_state_match"))
    write_bq(cust_state_match, bq_table("dbm_cust_state"))
except Exception as e:
    logger.error(f"Error loading/writing cust_state_match: {e}")
    raise

# -----------------------------------------------------------------------------
# 2. Load Most Recent As-Of Dates (simulate SAS macros with functions)
# -----------------------------------------------------------------------------
def get_latest_as_of_dt(table, union_view_name_col="UNION_VIEW_NAME"):
    # Replace with actual logic if control table is available in BigQuery
    control = read_bq(f"{project_id}.CCSI.CONTROL_TABLE")
    tbl = table.upper()
    df = control.filter((col(union_view_name_col) == tbl) & (col("WEEK_MONTH_IND") != " "))
    max_dt = df.agg(_max("AS_OF_DT")).collect()[0][0]
    return max_dt

geo_as_of_dt = get_latest_as_of_dt("GEO_CUSTOMER_MTH")
branch_as_of_dt = get_latest_as_of_dt("BRANCH_HRCY_MTH")
cust_as_of_dt = get_latest_as_of_dt("CUSTOMER_MTH")
acct_as_of_dt = get_latest_as_of_dt("GD_ACCT_INFO")
cud_prim_as_of_dt = get_latest_as_of_dt("CUD_CUST_DETL_PRIM_BR_MTH")

# -----------------------------------------------------------------------------
# 3. Create Weekly Geocode Table (merge new geocodes with monthly)
# -----------------------------------------------------------------------------
try:
    geo_customer_mth = spark.read.format("bigquery") \
        .option("query", f"""
            SELECT G.LP_ID, G.GEO_MTCH_CDE, G.GEO_FIPS_ST_CDE,
                   G.GEO_LATITUDE, G.GEO_LONGITUDE
            FROM `{project_id}.CCSI.GD_CUST_INFO` AS GD
            INNER JOIN `{project_id}.CCSI.GEO_CUSTOMER_MTH` AS G
                ON GD.LP_ID = G.LP_ID
            WHERE G.AS_OF_DT = '{geo_as_of_dt}'
        """).option("parentProject", project_id) \
        .option("temporaryGcsBucket", bq_temp_bucket) \
        .load()
    gis_new_cust_geocoded = read_bq(f"{project_id}.ADCOMMON.GIS_NEW_CUST_GEOCODED")
    geo_weekly = geo_customer_mth.join(
        gis_new_cust_geocoded,
        geo_customer_mth.LP_ID == gis_new_cust_geocoded.LP_ID,
        how="full_outer"
    ).select(
        coalesce(gis_new_cust_geocoded.GEOCODE_AS_OF_DT, lit(geo_as_of_dt)).alias("AS_OF_DT"),
        when(gis_new_cust_geocoded.LP_ID.isNull(), geo_customer_mth.LP_ID).otherwise(gis_new_cust_geocoded.LP_ID).alias("LP_ID"),
        when((gis_new_cust_geocoded.LATITUDE == 0) | gis_new_cust_geocoded.LATITUDE.isNull(), geo_customer_mth.GEO_LATITUDE).otherwise(gis_new_cust_geocoded.LATITUDE).alias("GEO_LATITUDE"),
        when((gis_new_cust_geocoded.LONGITUDE == 0) | gis_new_cust_geocoded.LONGITUDE.isNull(), geo_customer_mth.GEO_LONGITUDE).otherwise(gis_new_cust_geocoded.LONGITUDE).alias("GEO_LONGITUDE"),
        when(gis_new_cust_geocoded.GEO_FIPS_ST_CDE.isNull(), geo_customer_mth.GEO_FIPS_ST_CDE).otherwise(gis_new_cust_geocoded.GEO_FIPS_ST_CDE).alias("GEO_FIPS_ST_CDE"),
        when(geo_customer_mth.LP_ID.isNotNull() & gis_new_cust_geocoded.LP_ID.isNotNull(), lit('Y')).otherwise(lit('N')).alias("MOVER_FLAG"),
        when(geo_customer_mth.LP_ID.isNull() & gis_new_cust_geocoded.LP_ID.isNotNull(), lit('Y')).otherwise(lit('N')).alias("NEW_CUSTOMER"),
        when(gis_new_cust_geocoded.LP_ID.isNotNull(), lit('X')).otherwise(geo_customer_mth.GEO_MTCH_CDE).alias("GEO_MTCH_CDE"),
        when(gis_new_cust_geocoded.LP_ID.isNotNull(), gis_new_cust_geocoded.GEOCODE_TYP).otherwise(lit('X')).alias("GEOCODE_TYP")
    )
    write_bq(geo_weekly, f"{project_id}.MACOMMON.{sysuserid}_GEOCODE_WEEKLY")
except Exception as e:
    logger.error(f"Error creating weekly geocode table: {e}")
    raise

# -----------------------------------------------------------------------------
# 4. Load Branch Data (filter and split bad lat/long)
# -----------------------------------------------------------------------------
try:
    branches = spark.read.format("bigquery") \
        .option("query", f"""
            SELECT HGN_BR_ID, BR_TYP, BR_OPN_FLG,
                   LATITUDE_UPDT AS branchlat, LONGITUDE_UPDT AS branchlong,
                   METRO_COMMUNITY_CDE, GEN_CDE, TBA_CLS_DVSTD_DT,
                   BRICK_AND_MORTOR_NM, CITY, ST, ZIP_CDE
            FROM `{project_id}.CCSI.BRANCH_HRCY`
            WHERE BR_TYP IN ('R','U','I','T','C')
              AND BR_OPN_FLG = 'Y'
              AND HGN_BR_ID <> '00001'
              AND LATITUDE_UPDT IS NOT NULL
        """).option("parentProject", project_id) \
        .option("temporaryGcsBucket", bq_temp_bucket) \
        .load()
    rings_branch_data = branches.filter((col("branchlat") > 1) & (col("branchlong") < -1))
    bad_latlong_branch = branches.filter(~((col("branchlat") > 1) & (col("branchlong") < -1)))
    write_bq(rings_branch_data, bq_table("rings_branch_data"))
    write_bq(bad_latlong_branch, bq_table("bad_latlong_branch"))
except Exception as e:
    logger.error(f"Error processing branch data: {e}")
    raise

# -----------------------------------------------------------------------------
# 5. Most Used Logic (last 3 months)
# -----------------------------------------------------------------------------
try:
    cud_prim_3mo = (datetime.strptime(str(cud_prim_as_of_dt), "%Y%m%d") - timedelta(days=60)).strftime("%Y-%m-%d")
    branch_active = spark.read.format("bigquery") \
        .option("query", f"""
            SELECT lp_id, br_id,
                SUM(DAYS_CUR_MO_WITH_TRANS_CNT) AS branch_used_days_3mo,
                SUM(CASE WHEN as_of_dt = '{cud_prim_as_of_dt}' THEN DAYS_CUR_MO_WITH_TRANS_CNT ELSE 0 END) AS branch_used_days_prev,
                SUM(TRANS_CUR_MO_CNT) AS branch_trans_count_3mo,
                SUM(CASE WHEN as_of_dt = '{cud_prim_as_of_dt}' THEN TRANS_CUR_MO_CNT ELSE 0 END) AS branch_trans_count_prev,
                SUM(TRANS_SUM_CUR_MO_AMT) AS branch_trans_amount_3mo,
                SUM(CASE WHEN as_of_dt = '{cud_prim_as_of_dt}' THEN TRANS_SUM_CUR_MO_AMT ELSE 0 END) AS branch_trans_amount_prev
            FROM `{project_id}.CCSI.CUD_CUST_DETL_PRIM_BR_MTH`
            WHERE as_of_dt >= '{cud_prim_3mo}'
            GROUP BY lp_id, br_id
        """).option("parentProject", project_id) \
        .option("temporaryGcsBucket", bq_temp_bucket) \
        .load()
    w = Window.partitionBy("lp_id").orderBy(
        desc("branch_used_days_3mo"),
        desc("branch_used_days_prev"),
        desc("branch_trans_count_3mo"),
        desc("branch_trans_count_prev"),
        desc("branch_trans_amount_3mo"),
        desc("branch_trans_amount_prev")
    )
    most_used = branch_active.withColumn("rn", row_number().over(w)).filter(col("rn") == 1).drop("rn")
    # Write most used to MACOMMON
    write_bq(most_used.select("lp_id", "br_id"), f"{project_id}.MACOMMON.{sysuserid}_mu_br")
except Exception as e:
    logger.error(f"Error in most used logic: {e}")
    raise

# -----------------------------------------------------------------------------
# 6. Load Customers, Join Most Used and Geocode
# -----------------------------------------------------------------------------
try:
    customers = spark.read.format("bigquery") \
        .option("query", f"""
            SELECT GDC.LP_ID, GDC.PRTY_BR, GDC.CUST_PRTY_ACCT_ID,
                   ACT.OPN_ACCT_FLG, ACT.NBR_OF_MOS_OPN,
                   MU.BR_ID AS MOST_USED_BR,
                   GDC.MOST_USED_BR AS MOST_USED_OLD,
                   CUS.HGN_CUST_TYP_CDE, GDC.OPN_ACCT_CNT,
                   GEO.GEO_MTCH_CDE AS geomatchcode,
                   GEO.GEO_LATITUDE AS custlat,
                   GEO.GEO_LONGITUDE AS custlong
            FROM `{project_id}.CCSI.GD_CUST_INFO` AS GDC
            INNER JOIN `{project_id}.CCSI.CUSTOMER` AS CUS ON GDC.LP_ID = CUS.LP_ID
            LEFT JOIN `{project_id}.CCSI.GD_ACCT_INFO` AS ACT
                ON (GDC.LP_ID = ACT.PLP_ID AND GDC.CUST_PRTY_ACCT_ID = ACT.ACCT_ID AND ACT.OPN_ACCT_FLG = 'Y')
            LEFT JOIN `{project_id}.MACOMMON.{sysuserid}_GEOCODE_WEEKLY` AS GEO ON GDC.LP_ID = GEO.LP_ID
            LEFT JOIN `{project_id}.MACOMMON.{sysuserid}_mu_br` AS MU ON GDC.LP_ID = MU.LP_ID
            WHERE GDC.OPN_ACCT_CNT > 0
        """).option("parentProject", project_id) \
        .option("temporaryGcsBucket", bq_temp_bucket) \
        .load()
    w = Window.partitionBy("lp_id").orderBy("lp_id", "NBR_OF_MOS_OPN")
    customers1 = customers.withColumn("rn", row_number().over(w)).filter(col("rn") == 1).drop("rn")
    rings_cust_data = customers1.filter(~col("geomatchcode").isin(['0', ' ']))
    bad_latlong_cust = customers1.filter(col("geomatchcode").isin(['0', ' ']))
    write_bq(rings_cust_data, bq_table("rings_cust_data"))
    write_bq(bad_latlong_cust, bq_table("bad_latlong_cust"))
except Exception as e:
    logger.error(f"Error loading customers: {e}")
    raise

# -----------------------------------------------------------------------------
# 7. Merge Priority Branch with Branch Data, Filter by Account Age
# -----------------------------------------------------------------------------
try:
    rings_priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        how="inner"
    ).filter(
        (col("OPN_ACCT_FLG") == 'Y') &
        (col("NBR_OF_MOS_OPN") <= 24) &
        (col("NBR_OF_MOS_OPN") >= 0)
    ).select(
        rings_cust_data.LP_ID, rings_cust_data.PRTY_BR, rings_cust_data.OPN_ACCT_FLG,
        rings_cust_data.NBR_OF_MOS_OPN, rings_cust_data.custlat, rings_cust_data.custlong,
        rings_branch_data.BR_TYP, rings_branch_data.branchlat, rings_branch_data.branchlong,
        rings_branch_data.METRO_COMMUNITY_CDE, rings_branch_data.BRICK_AND_MORTOR_NM, rings_branch_data.ST
    )
    rings_priority_cust = rings_priority_cust.withColumn(
        "dist_to_prty_br",
        geodist_udf("branchlat", "branchlong", "custlat", "custlong")
    )
except Exception as e:
    logger.error(f"Error merging priority branch: {e}")
    raise

# -----------------------------------------------------------------------------
# 8. Calculate Priority Distance Ring (80th percentile)
# -----------------------------------------------------------------------------
try:
    # Window for percentile by prty_br
    w = Window.partitionBy("PRTY_BR")
    rings_priority_cust.createOrReplaceTempView("rings_priority_cust")
    ring_priority = spark.sql("""
        SELECT PRTY_BR, 
               percentile_approx(dist_to_prty_br, 0.8) as prtybr80
        FROM rings_priority_cust
        GROUP BY PRTY_BR
    """)
    write_bq(ring_priority, bq_table("ring_priority"))
except Exception as e:
    logger.error(f"Error calculating priority ring: {e}")
    raise

# -----------------------------------------------------------------------------
# 9. Merge Most Used Branch with Branch Data
# -----------------------------------------------------------------------------
try:
    rings_most_used_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.MOST_USED_BR == rings_branch_data.HGN_BR_ID,
        how="inner"
    ).select(
        rings_cust_data.LP_ID, rings_cust_data.MOST_USED_BR, rings_cust_data.custlat, rings_cust_data.custlong,
        rings_branch_data.BR_TYP, rings_branch_data.branchlat, rings_branch_data.branchlong,
        rings_branch_data.METRO_COMMUNITY_CDE, rings_branch_data.BRICK_AND_MORTOR_NM, rings_branch_data.ST
    )
    rings_most_used_cust = rings_most_used_cust.withColumn(
        "dist_to_used_br",
        geodist_udf("branchlat", "branchlong", "custlat", "custlong")
    )
    rings_most_used_cust.createOrReplaceTempView("rings_most_used_cust")
    ring_most_used = spark.sql("""
        SELECT MOST_USED_BR, 
               percentile_approx(dist_to_used_br, 0.8) as usedbr80
        FROM rings_most_used_cust
        GROUP BY MOST_USED_BR
    """)
    write_bq(ring_most_used, bq_table("ring_most_used"))
except Exception as e:
    logger.error(f"Error calculating most used ring: {e}")
    raise

# -----------------------------------------------------------------------------
# 10. Prepare Data for Final Merge
# -----------------------------------------------------------------------------
try:
    ring_priority2 = ring_priority.select(
        col("PRTY_BR").alias("TAMBR"),
        col("prtybr80").alias("priority_ring")
    )
    ring_most_used2 = ring_most_used.select(
        col("MOST_USED_BR").alias("TAMBR"),
        col("usedbr80").alias("most_used_ring")
    )
    branch_data2 = rings_branch_data.select(
        col("HGN_BR_ID").alias("TAMBR"),
        col("BR_TYP").alias("branch_typ"),
        col("BRICK_AND_MORTOR_NM").alias("branch_name"),
        col("CITY").alias("branch_city"),
        col("ST").alias("branch_state"),
        col("branchlat"),
        col("branchlong"),
        col("METRO_COMMUNITY_CDE").alias("metcomm_cde")
    )
except Exception as e:
    logger.error(f"Error preparing data for merge: {e}")
    raise

# -----------------------------------------------------------------------------
# 11. Merge Priority and Most Used Rings to Branch Data
# -----------------------------------------------------------------------------
try:
    final = branch_data2.join(
        ring_priority2, on="TAMBR", how="left"
    ).join(
        ring_most_used2, on="TAMBR", how="left"
    ).withColumn(
        "priority_ring", coalesce(col("priority_ring"), lit(0))
    ).withColumn(
        "most_used_ring", coalesce(col("most_used_ring"), lit(0))
    ).withColumn(
        "max_dist", lit(40)  # Manual: always 40, per comment
    )
    write_bq(final, f"{project_id}.tambr.tambr_rings_{cust_occr}")
except Exception as e:
    logger.error(f"Error merging rings to branch data: {e}")
    raise

# -----------------------------------------------------------------------------
# 12. Frequency Reporting (manual: use DataFrame .groupBy().count())
# -----------------------------------------------------------------------------
try:
    freq1 = final.groupBy("branch_typ", "metcomm_cde").count()
    freq2 = final.groupBy("TAMBR", "branch_typ", "branch_name", "metcomm_cde").count()
    logger.info("TAMBR RINGS: FINAL MONTHLY COUNTS")
    freq1.show()
    freq2.show()
except Exception as e:
    logger.error(f"Error in frequency reporting: {e}")

# -----------------------------------------------------------------------------
# 13. Macro Drop Logic (manual intervention required)
# -----------------------------------------------------------------------------
# NOTE: Macro drop logic (mdrop_mac) is not needed in PySpark/BigQuery as tables are overwritten or managed programmatically.

# -----------------------------------------------------------------------------
# 14. Manual Intervention Required
# -----------------------------------------------------------------------------
# - Any SAS macro logic for dynamic table or variable creation should be replaced with config-driven Python functions.
# - If any table or field names do not match BigQuery schema, manual mapping is required.
# - If percentile_approx is not available in your Spark version, use approxQuantile instead.

# -----------------------------------------------------------------------------
# Conversion Percentage and API Cost
# -----------------------------------------------------------------------------
conversion_percentage = 99.0  # All logic except macro drop is fully converted
apiCost = 0.0023  # USD, inclusive of all decimals (from file read cost)

print(f"Conversion Percentage: {conversion_percentage}%")
print(f"apiCost: {apiCost}$")
```
Conversion Percentage: 99.0%
apiCost: 0.0023$