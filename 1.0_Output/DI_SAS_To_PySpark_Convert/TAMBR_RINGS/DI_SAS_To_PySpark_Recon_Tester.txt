=============================================
Author: Ascendion AVAA
Date: 
Description: Pytest suite for validating PySpark TAMBr ring calculation logic, including joins, aggregations, UDFs, and edge cases.
=============================================

# =========================
# 1. Syntactical Changes Made
# =========================

- SAS DATA/PROC steps mapped to PySpark DataFrame operations.
- SAS macros replaced with Python functions (e.g., get_latest_as_of_dt, mdrop_mac).
- DB2 SQL replaced with BigQuery SQL via PySpark's BigQuery connector.
- Macro variables replaced with Python variables/configs.
- Conditional logic, joins, groupings, and aggregations implemented with PySpark.
- SAS data types/functions mapped to PySpark/BigQuery equivalents.
- Error handling and logging implemented with Python logging.
- Temporary table handling adapted for PySpark/BigQuery.
- SAS percentile logic (PROC UNIVARIATE) replaced with percentile_approx in PySpark.
- SAS UDFs (e.g., geodist) implemented as Python functions and registered as PySpark UDFs.
- PROC FREQ reporting not converted (requires manual intervention).

# =========================
# 2. Manual Intervention Required
# =========================

- PROC FREQ reporting (statistical tabulations) is not directly converted. Use Spark SQL or BigQuery for reporting as needed.
- Some SAS macro logging and dynamic macro variable creation are replaced by Python functions/configs but may require review for edge cases.
- Manual review required for any business logic that depends on SAS macro variable scoping or dynamic macro execution.
- Confirm max_dist logic (set to 40) with business stakeholders.

# =========================
# 3. Test Case Document
# =========================

| Test Case ID | Description                                                                                     | Preconditions                   | Test Steps                                                                                 | Expected Result                                                                                   | Actual Result | Pass/Fail Status |
|--------------|------------------------------------------------------------------------------------------------|----------------------------------|-------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|--------------|------------------|
| TC01         | Happy path: Valid customer, branch, and geocode data, all joins succeed, distances calculated   | Valid mock data                  | Run join, distance, percentile, check schema/data                                          | Output DataFrame has correct schema, correct distances, correct rings, and expected row count      | As observed  | To be filled     |
| TC02         | Edge: Customer with NULL latitude/longitude                                                     | custlat/custlong is NULL         | Run join, distance calculation                                                            | Distance is NULL, customer excluded from ring calculation, schema/data correct                     | As observed  | To be filled     |
| TC03         | Edge: Branch with invalid lat/long (<=1 or >=-1)                                                | branchlat/branchlong invalid     | Filter branches                                                                           | Branch excluded from ring calculation, not present in output                                       | As observed  | To be filled     |
| TC04         | Edge: Customer with no matching branch (priority or most used)                                  | No matching branch in join       | Run join                                                                                  | Customer excluded from final ring calculation, output as expected                                  | As observed  | To be filled     |
| TC05         | Edge: Duplicate customers (same LP_ID, different NBR_OF_MOS_OPN)                                | Duplicate LP_IDs                 | Window, row_number, filter                                                                | Only first record per LP_ID is kept (lowest NBR_OF_MOS_OPN)                                        | As observed  | To be filled     |
| TC06         | Edge: No data (empty DataFrames)                                                                | Empty DataFrames                 | Run full logic                                                                            | Output DataFrame is empty with correct schema                                                      | As observed  | To be filled     |
| TC07         | Error: Type mismatch in latitude/longitude (e.g., string instead of float)                      | Wrong type in lat/long           | Run geodist_udf                                                                           | Exception is raised, error handling/logging is triggered                                           | As observed  | To be filled     |
| TC08         | Error: Missing required columns in input DataFrames                                             | Missing columns                  | Attempt join                                                                              | Exception is raised, error handling/logging is triggered                                           | As observed  | To be filled     |
| TC09         | Happy path: Percentile calculation for ring is correct                                          | Valid distances                  | Run percentile_approx                                                                     | 80th percentile distance is correctly calculated for each branch                                   | As observed  | To be filled     |
| TC10         | Edge: All distances are NULL for a branch                                                       | All distances NULL               | Run percentile_approx, coalesce                                                           | Ring value for that branch is set to 0 (as per coalesce logic)                                     | As observed  | To be filled     |

# =========================
# 4. Pytest Script for Each Test Case
# =========================

import pytest
from pyspark.sql import SparkSession, Row, functions as F, types as T, Window
from math import radians, cos, sin, asin, sqrt

# Helper: geodist function/UDF
def geodist(lat1, lon1, lat2, lon2):
    """
    Calculate the great circle distance in miles between two points.
    Returns None if any input is None.
    """
    if None in [lat1, lon1, lat2, lon2]:
        return None
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    miles = 3956 * c
    return miles

geodist_udf = F.udf(geodist, T.DoubleType())

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("tambr_test").getOrCreate()

def assert_df_equality(df1, df2, ignore_nullable=True, ignore_row_order=True):
    assert df1.schema == df2.schema, f"Schema mismatch: {df1.schema} != {df2.schema}"
    data1 = df1.collect()
    data2 = df2.collect()
    if ignore_row_order:
        assert sorted(data1) == sorted(data2), f"Data mismatch: {data1} != {data2}"
    else:
        assert data1 == data2, f"Data mismatch: {data1} != {data2}"

# TC01: Happy path
def test_happy_path(spark):
    """
    Test that valid customer and branch data produces correct distances and ring calculation.
    Relates to main join, geodist, and percentile logic from SAS DATA/PROC steps.
    """
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat=40.0, custlong=-75.0, MOST_USED_BR="B2"),
        Row(LP_ID="C2", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=10, custlat=41.0, custlong=-76.0, MOST_USED_BR="B2"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
        Row(HGN_BR_ID="B2", BR_TYP="I", branchlat=41.1, branchlong=-76.1, BRICK_AND_MORTOR_NM="Branch 2", CITY="CityB", ST="PA", METRO_COMMUNITY_CDE="Y"),
    ])
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    ).filter(
        (rings_cust_data.OPN_ACCT_FLG == 'Y') &
        (rings_cust_data.NBR_OF_MOS_OPN <= 24) &
        (rings_cust_data.NBR_OF_MOS_OPN >= 0)
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf("branchlat", "branchlong", "custlat", "custlong")
    )
    priority_ring = priority_cust.groupBy("PRTY_BR").agg(
        F.expr("percentile_approx(dist_to_prty_br, 0.8)").alias("priority_ring")
    ).withColumnRenamed("PRTY_BR", "TAMBR")
    expected_schema = T.StructType([
        T.StructField("TAMBR", T.StringType(), True),
        T.StructField("priority_ring", T.DoubleType(), True),
    ])
    assert priority_ring.schema == expected_schema
    assert priority_ring.count() == 1
    assert abs(priority_ring.collect()[0]["priority_ring"] - geodist(40.1, -75.1, 41.0, -76.0)) < 1.0

# TC02: NULL latitude/longitude
def test_null_latlong(spark):
    """
    Test that NULL lat/long in customer results in NULL distance.
    Relates to geodist logic and edge case handling.
    """
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat=None, custlong=None, MOST_USED_BR="B2"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
    ])
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    ).withColumn(
        "dist_to_prty_br",
        geodist_udf("branchlat", "branchlong", "custlat", "custlong")
    )
    assert priority_cust.collect()[0]["dist_to_prty_br"] is None

# TC03: Invalid branch lat/long
def test_invalid_branch_latlong(spark):
    """
    Test that branches with invalid lat/long are excluded.
    Relates to branch filtering logic.
    """
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=0.0, branchlong=0.0, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
        Row(HGN_BR_ID="B2", BR_TYP="I", branchlat=41.1, branchlong=-76.1, BRICK_AND_MORTOR_NM="Branch 2", CITY="CityB", ST="PA", METRO_COMMUNITY_CDE="Y"),
    ])
    good_branches = rings_branch_data.filter((F.col("branchlat") > 1) & (F.col("branchlong") < -1))
    assert good_branches.count() == 1
    assert good_branches.collect()[0]["HGN_BR_ID"] == "B2"

# TC04: No matching branch
def test_no_matching_branch(spark):
    """
    Test that customers with no matching branch are excluded from join.
    Relates to join logic.
    """
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", PRTY_BR="B99", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat=40.0, custlong=-75.0, MOST_USED_BR="B99"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
    ])
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    )
    assert priority_cust.count() == 0

# TC05: Duplicate customers
def test_duplicate_customers(spark):
    """
    Test that only the first record per LP_ID (lowest NBR_OF_MOS_OPN) is kept.
    Relates to window/row_number logic.
    """
    customers = spark.createDataFrame([
        Row(LP_ID="C1", NBR_OF_MOS_OPN=5),
        Row(LP_ID="C1", NBR_OF_MOS_OPN=10),
        Row(LP_ID="C2", NBR_OF_MOS_OPN=3),
    ])
    window_first = Window.partitionBy("LP_ID").orderBy("NBR_OF_MOS_OPN")
    customers1 = customers.withColumn("rn", F.row_number().over(window_first)).filter(F.col("rn") == 1).drop("rn")
    result = customers1.select("LP_ID", "NBR_OF_MOS_OPN").collect()
    assert Row(LP_ID="C1", NBR_OF_MOS_OPN=5) in result
    assert Row(LP_ID="C2", NBR_OF_MOS_OPN=3) in result
    assert len(result) == 2

# TC06: Empty DataFrames
def test_empty_dataframes(spark):
    """
    Test that empty DataFrames produce empty output with correct schema.
    Relates to edge case handling.
    """
    schema = T.StructType([
        T.StructField("LP_ID", T.StringType(), True),
        T.StructField("PRTY_BR", T.StringType(), True),
        T.StructField("OPN_ACCT_FLG", T.StringType(), True),
        T.StructField("NBR_OF_MOS_OPN", T.IntegerType(), True),
        T.StructField("custlat", T.DoubleType(), True),
        T.StructField("custlong", T.DoubleType(), True),
        T.StructField("MOST_USED_BR", T.StringType(), True),
    ])
    rings_cust_data = spark.createDataFrame([], schema)
    rings_branch_data = spark.createDataFrame([], T.StructType([
        T.StructField("HGN_BR_ID", T.StringType(), True),
        T.StructField("BR_TYP", T.StringType(), True),
        T.StructField("branchlat", T.DoubleType(), True),
        T.StructField("branchlong", T.DoubleType(), True),
        T.StructField("BRICK_AND_MORTOR_NM", T.StringType(), True),
        T.StructField("CITY", T.StringType(), True),
        T.StructField("ST", T.StringType(), True),
        T.StructField("METRO_COMMUNITY_CDE", T.StringType(), True),
    ]))
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    )
    assert priority_cust.count() == 0

# TC07: Type mismatch in lat/long
def test_type_mismatch_latlong(spark):
    """
    Test that type mismatch in lat/long raises an exception.
    Relates to error handling and logging.
    """
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", PRTY_BR="B1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat="not_a_float", custlong=-75.0, MOST_USED_BR="B2"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
    ])
    priority_cust = rings_cust_data.join(
        rings_branch_data,
        rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
        "inner"
    )
    with pytest.raises(Exception):
        priority_cust.withColumn(
            "dist_to_prty_br",
            geodist_udf("branchlat", "branchlong", "custlat", "custlong")
        ).collect()

# TC08: Missing required columns
def test_missing_columns(spark):
    """
    Test that missing required columns raises an exception.
    Relates to error handling and logging.
    """
    rings_cust_data = spark.createDataFrame([
        Row(LP_ID="C1", OPN_ACCT_FLG="Y", NBR_OF_MOS_OPN=12, custlat=40.0, custlong=-75.0, MOST_USED_BR="B2"),
    ])
    rings_branch_data = spark.createDataFrame([
        Row(HGN_BR_ID="B1", BR_TYP="R", branchlat=40.1, branchlong=-75.1, BRICK_AND_MORTOR_NM="Branch 1", CITY="CityA", ST="PA", METRO_COMMUNITY_CDE="X"),
    ])
    # PRTY_BR is missing from rings_cust_data
    with pytest.raises(Exception):
        rings_cust_data.join(
            rings_branch_data,
            rings_cust_data.PRTY_BR == rings_branch_data.HGN_BR_ID,
            "inner"
        ).collect()

# TC09: Percentile calculation
def test_percentile_calculation(spark):
    """
    Test that percentile_approx calculates the 80th percentile correctly.
    Relates to percentile logic from SAS PROC UNIVARIATE.
    """
    data = [
        Row(PRTY_BR="B1", dist_to_prty_br=1.0),
        Row(PRTY_BR="B1", dist_to_prty_br=2.0),
        Row(PRTY_BR="B1", dist_to_prty_br=3.0),
        Row(PRTY_BR="B1", dist_to_prty_br=4.0),
        Row(PRTY_BR="B1", dist_to_prty_br=5.0),
    ]
    df = spark.createDataFrame(data)
    priority_ring = df.groupBy("PRTY_BR").agg(
        F.expr("percentile_approx(dist_to_prty_br, 0.8)").alias("priority_ring")
    ).withColumnRenamed("PRTY_BR", "TAMBR")
    val = priority_ring.collect()[0]["priority_ring"]
    assert abs(val - 4.2) < 0.1  # 80th percentile between 4 and 5

# TC10: All distances NULL
def test_all_distances_null(spark):
    """
    Test that if all distances are NULL, ring value is set to 0 (as per coalesce logic).
    Relates to null handling in percentile logic.
    """
    data = [
        Row(PRTY_BR="B1", dist_to_prty_br=None),
        Row(PRTY_BR="B1", dist_to_prty_br=None),
    ]
    df = spark.createDataFrame(data)
    priority_ring = df.groupBy("PRTY_BR").agg(
        F.expr("percentile_approx(dist_to_prty_br, 0.8)").alias("priority_ring")
    ).withColumnRenamed("PRTY_BR", "TAMBR")
    # Coalesce logic in main code sets to 0 if null
    priority_ring = priority_ring.withColumn("priority_ring", F.coalesce("priority_ring", F.lit(0)))
    assert priority_ring.collect()[0]["priority_ring"] == 0

# =========================
# 5. API Cost
# =========================

apiCost: 0.0023 USD