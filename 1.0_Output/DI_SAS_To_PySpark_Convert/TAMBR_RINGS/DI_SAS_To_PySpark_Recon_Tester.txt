=============================================
Author: Ascendion AVAA
Date: 
Description: Pytest test suite for validating PySpark transformation logic, joins, aggregations, and geospatial calculations for TAMBr branch ring analytics, ensuring correctness, schema/data integrity, and robust edge case handling.
=============================================

# 1. Syntactical Changes Made

- SAS PROC SQL, PROC SORT, and PROC UNIVARIATE replaced with PySpark DataFrame API (joins, filters, groupBy, approxQuantile).
- SAS macro variables replaced with Python variables.
- SAS geodist('M') function replaced with Python Haversine formula in a PySpark UDF.
- Data step merges replaced with DataFrame joins.
- SAS IF/ELSE and WHERE replaced with DataFrame filter and when.
- Output to BigQuery via PySpark DataFrame.write.
- Windowing (row_number, partitionBy) for most-used branch logic.
- Null handling and defaulting (e.g., .withColumn("priority_ring", when(...))) via PySpark.
- SAS percentile calculation (pctlpts=80) replaced with percentile_approx in PySpark.

# 2. Manual Intervention Required

- Review percentile_approx for accuracy vs SAS proc univariate pctlpts=80.
- Confirm geodist UDF matches SAS geodist('M') output.
- Confirm BigQuery table names and field mappings.
- Confirm default values (e.g., max_dist = 40) are correct as per business rules.
- Validate error handling for missing/invalid data is robust and matches SAS logic.

# 3. Test Case Document

| Test Case ID | Description                                                                 | Preconditions                     | Test Steps                                                                                                         | Expected Result                                                                                  | Actual Result | Pass/Fail Status |
|--------------|-----------------------------------------------------------------------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|--------------|------------------|
| TC01         | Happy path: Valid branch/customer data, all joins succeed, no NULLs         | Valid branch & customer dataframes | Join, calculate geodist, percentile, output final DataFrame                                                       | Output DataFrames have correct schema, values, and expected row counts                            | As observed  | Pass/Fail        |
| TC02         | Edge: Customer with NULL latitude/longitude                                 | Customer with NULL lat/long        | Filter, attempt geodist, check exclusion                                                                          | Customer excluded from rings_cust_data; handled gracefully, no crash                              | As observed  | Pass/Fail        |
| TC03         | Edge: Branch with invalid lat/long (outside valid range)                    | Branch with invalid lat/long       | Filter branches, check output                                                                                     | Branch excluded from rings_branch_data; handled gracefully, no crash                              | As observed  | Pass/Fail        |
| TC04         | Edge: No customers for a branch                                             | Branch with no matching customers  | Left join, calculate ring                                                                                         | priority_ring and most_used_ring set to 0 for that branch                                         | As observed  | Pass/Fail        |
| TC05         | Edge: Empty input DataFrames                                                | Empty DataFrames                   | Run pipeline                                                                                                      | All output DataFrames are empty, no crash                                                         | As observed  | Pass/Fail        |
| TC06         | Edge: Duplicate lp_id in branch_active                                      | Duplicate lp_id in input           | Window/row_number, select top                                                                                     | Only top-ranked (by days, count, amount) is selected as most_used                                 | As observed  | Pass/Fail        |
| TC07         | Error: Type mismatch in geodist UDF (string instead of float)               | Bad data types                     | Call geodist with string/float mix                                                                                | geodist returns None, no crash                                                                    | As observed  | Pass/Fail        |
| TC08         | Error: Missing required fields in input DataFrames                          | Schema missing fields              | Access missing field                                                                                              | Raises AnalysisException or handled error, test passes if error is raised                         | As observed  | Pass/Fail        |
| TC09         | Aggregation: 80th percentile calculation matches manual calculation         | Known small dataset                | Calculate percentile_approx, compare to manual                                                                    | percentile_approx output matches manual calculation for small test set                             | As observed  | Pass/Fail        |
| TC10         | Filtering: Only open branches, correct types, and valid hgn_br_id selected  | Mixed branch data                  | Apply filter                                                                                                      | Output contains only expected branch rows                                                         | As observed  | Pass/Fail        |
| TC11         | Schema: Output DataFrames match expected schema (columns & types)           | Valid DataFrames                   | Check schema                                                                                                      | Schemas match exactly                                                                             | As observed  | Pass/Fail        |
| TC12         | Performance: Caching and partitioning improves runtime (mocked/tested)      | Any DataFrame                      | Cache, repartition, measure/compare runtime                                                                       | Runtime is less with caching/partitioning enabled (mocked, not measured in unit test)             | As observed  | Pass/Fail        |

# 4. Pytest Script for Each Test Case

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
import math

# Helper: geodist UDF (copied from production code)
def geodist(lat1, lon1, lat2, lon2):
    try:
        if None in (lat1, lon1, lat2, lon2):
            return None
        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2
        c = 2 * math.asin(math.sqrt(a))
        miles = 3956 * c
        return miles
    except Exception:
        return None

@pytest.fixture(scope="session")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("pytest-TAMBr").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def branch_schema():
    return StructType([
        StructField("hgn_br_id", StringType()),
        StructField("br_typ", StringType()),
        StructField("br_opn_flg", StringType()),
        StructField("branchlat", DoubleType()),
        StructField("branchlong", DoubleType()),
        StructField("metro_community_cde", StringType()),
        StructField("brick_and_mortor_nm", StringType()),
        StructField("city", StringType()),
        StructField("st", StringType()),
    ])

@pytest.fixture
def customer_schema():
    return StructType([
        StructField("lp_id", StringType()),
        StructField("prty_br", StringType()),
        StructField("opn_acct_flg", StringType()),
        StructField("nbr_of_mos_opn", IntegerType()),
        StructField("custlat", DoubleType()),
        StructField("custlong", DoubleType()),
        StructField("most_used_br", StringType()),
        StructField("opn_acct_cnt", IntegerType()),
        StructField("geomatchcode", StringType()),
    ])

@pytest.fixture
def minimal_data(spark, branch_schema, customer_schema):
    # One branch, one customer, both valid
    branches = [
        ("00002", "R", "Y", 40.0, -75.0, "A", "Branch A", "CityA", "ST"),
    ]
    customers = [
        ("CUST1", "00002", "Y", 10, 40.1, -75.1, "00002", 1, "1"),
    ]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    customer_df = spark.createDataFrame(customers, schema=customer_schema)
    return branch_df, customer_df

def test_happy_path(spark, minimal_data):
    """
    TC01: Happy path - Valid branch/customer data, all joins succeed, no NULLs.
    Ensures output DataFrames have correct schema, values, and expected row counts.
    """
    branch_df, customer_df = minimal_data
    from pyspark.sql.functions import udf
    from pyspark.sql.types import DoubleType

    geodist_udf = udf(geodist, DoubleType())
    joined = customer_df.join(branch_df, customer_df.prty_br == branch_df.hgn_br_id, "inner")
    joined = joined.withColumn("dist_to_prty_br", geodist_udf("branchlat", "branchlong", "custlat", "custlong"))
    assert joined.count() == 1
    assert "dist_to_prty_br" in joined.columns
    val = joined.select("dist_to_prty_br").collect()[0][0]
    assert val is not None and val > 0

def test_null_latlong_customer(spark, branch_schema, customer_schema):
    """
    TC02: Edge - Customer with NULL latitude/longitude.
    Ensures customer is excluded from rings_cust_data and handled gracefully.
    """
    branches = [("00002", "R", "Y", 40.0, -75.0, "A", "Branch A", "CityA", "ST")]
    customers = [("CUST2", "00002", "Y", 5, None, -75.1, "00002", 1, "1")]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    customer_df = spark.createDataFrame(customers, schema=customer_schema)
    filtered = customer_df.filter(customer_df.custlat.isNotNull() & customer_df.custlong.isNotNull())
    assert filtered.count() == 0

def test_invalid_latlong_branch(spark, branch_schema, customer_schema):
    """
    TC03: Edge - Branch with invalid lat/long (outside valid range).
    Ensures branch is excluded from rings_branch_data and handled gracefully.
    """
    branches = [("00003", "R", "Y", 0.0, 0.0, "A", "Branch B", "CityB", "ST")]
    customers = [("CUST3", "00003", "Y", 5, 40.1, -75.1, "00003", 1, "1")]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    valid_branches = branch_df.filter((F.col("branchlat") > 1) & (F.col("branchlong") < -1))
    assert valid_branches.count() == 0

def test_no_customers_for_branch(spark, branch_schema, customer_schema):
    """
    TC04: Edge - No customers for a branch.
    Ensures priority_ring and most_used_ring set to 0 for that branch.
    """
    branches = [("00004", "R", "Y", 40.0, -75.0, "A", "Branch C", "CityC", "ST")]
    customers = []
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    customer_df = spark.createDataFrame(customers, schema=customer_schema)
    joined = customer_df.join(branch_df, customer_df.prty_br == branch_df.hgn_br_id, "right")
    joined = joined.withColumn("priority_ring", F.lit(0))
    assert joined.select("priority_ring").collect()[0][0] == 0

def test_empty_inputs(spark, branch_schema, customer_schema):
    """
    TC05: Edge - Empty input DataFrames.
    Ensures all output DataFrames are empty, no crash.
    """
    branch_df = spark.createDataFrame([], schema=branch_schema)
    customer_df = spark.createDataFrame([], schema=customer_schema)
    joined = customer_df.join(branch_df, customer_df.prty_br == branch_df.hgn_br_id, "inner")
    assert joined.count() == 0

def test_duplicate_lp_id_branch_active(spark):
    """
    TC06: Edge - Duplicate lp_id in branch_active.
    Ensures only top-ranked (by days, count, amount) is selected as most_used.
    """
    schema = StructType([
        StructField("lp_id", StringType()),
        StructField("br_id", StringType()),
        StructField("branch_used_days_3mo", IntegerType()),
        StructField("branch_used_days_prev", IntegerType()),
        StructField("branch_trans_count_3mo", IntegerType()),
        StructField("branch_trans_count_prev", IntegerType()),
        StructField("branch_trans_amount_3mo", DoubleType()),
        StructField("branch_trans_amount_prev", DoubleType()),
    ])
    data = [
        ("CUST4", "B1", 10, 5, 20, 10, 100.0, 50.0),
        ("CUST4", "B2", 8, 4, 15, 8, 90.0, 45.0),
    ]
    df = spark.createDataFrame(data, schema=schema)
    from pyspark.sql.window import Window
    window_spec = Window.partitionBy("lp_id").orderBy(
        F.desc("branch_used_days_3mo"),
        F.desc("branch_used_days_prev"),
        F.desc("branch_trans_count_3mo"),
        F.desc("branch_trans_count_prev"),
        F.desc("branch_trans_amount_3mo"),
        F.desc("branch_trans_amount_prev")
    )
    sorted_df = df.withColumn("rn", F.row_number().over(window_spec))
    most_used = sorted_df.filter(F.col("rn") == 1)
    assert most_used.count() == 1
    assert most_used.collect()[0]["br_id"] == "B1"

def test_geodist_type_mismatch():
    """
    TC07: Error - Type mismatch in geodist UDF (string instead of float).
    Ensures geodist returns None, no crash.
    """
    assert geodist("bad", -75.0, 40.0, -75.1) is None

def test_missing_fields_error(spark, branch_schema):
    """
    TC08: Error - Missing required fields in input DataFrames.
    Ensures error is raised when accessing missing field.
    """
    schema = StructType([
        StructField("hgn_br_id", StringType()),
        StructField("br_typ", StringType()),
        StructField("br_opn_flg", StringType()),
        StructField("branchlat", DoubleType()),
    ])
    data = [("00005", "R", "Y", 40.0)]
    branch_df = spark.createDataFrame(data, schema=schema)
    with pytest.raises(Exception):
        branch_df.select("branchlong").collect()

def test_percentile_approx(spark):
    """
    TC09: Aggregation - 80th percentile calculation matches manual calculation.
    Ensures percentile_approx output matches manual calculation for small test set.
    """
    schema = StructType([StructField("prty_br", StringType()), StructField("dist_to_prty_br", DoubleType())])
    data = [("B1", 1.0), ("B1", 2.0), ("B1", 3.0), ("B1", 4.0), ("B1", 5.0)]
    df = spark.createDataFrame(data, schema=schema)
    approx = df.groupBy("prty_br").agg(F.expr("percentile_approx(dist_to_prty_br, 0.8)").alias("priority_ring"))
    val = approx.collect()[0]["priority_ring"]
    assert val == 4.0

def test_branch_filtering(spark, branch_schema):
    """
    TC10: Filtering - Only open branches, correct types, and valid hgn_br_id selected.
    Ensures output contains only expected branch rows.
    """
    branches = [
        ("00006", "R", "Y", 40.0, -75.0, "A", "Branch D", "CityD", "ST"),
        ("00007", "X", "N", 41.0, -76.0, "B", "Branch E", "CityE", "ST"),
    ]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    filtered = branch_df.filter(
        (F.col("br_typ").isin("R", "U", "I", "T", "C")) &
        (F.col("br_opn_flg") == "Y") &
        (F.col("hgn_br_id") != "00001") &
        (F.col("branchlat").isNotNull())
    )
    assert filtered.count() == 1
    assert filtered.collect()[0]["hgn_br_id"] == "00006"

def test_schema_output(spark, branch_schema):
    """
    TC11: Schema - Output DataFrames match expected schema (columns & types).
    Ensures schemas match exactly.
    """
    expected = set([f.name for f in branch_schema.fields])
    branches = [("00008", "R", "Y", 40.0, -75.0, "A", "Branch F", "CityF", "ST")]
    branch_df = spark.createDataFrame(branches, schema=branch_schema)
    actual = set(branch_df.columns)
    assert expected == actual

def test_performance_caching_partitioning(spark, minimal_data):
    """
    TC12: Performance - Caching and partitioning improves runtime (mocked/tested).
    Ensures runtime is less with caching/partitioning enabled (mocked, not measured in unit test).
    """
    branch_df, customer_df = minimal_data
    branch_df.cache()
    branch_df = branch_df.repartition(1)
    assert branch_df.count() == 1  # Just ensures no error
```

# 5. API Cost

apiCost: 0.0023 USD