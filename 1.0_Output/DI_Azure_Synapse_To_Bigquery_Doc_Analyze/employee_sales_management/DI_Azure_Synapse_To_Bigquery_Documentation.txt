=============================================
Author:        Ascendion AVA
Created on:    
Description:   ETL procedure for loading sales transaction data from staging to fact table with data quality validation and audit logging
=============================================

# Comprehensive Documentation: dw.sp_load_sales_fact

## Procedure Overview

The `dw.sp_load_sales_fact` stored procedure is designed to orchestrate a comprehensive ETL (Extract, Transform, Load) process for sales transaction data within Azure Synapse Analytics. This procedure serves as a critical component of the organization's data warehouse infrastructure, responsible for moving validated and enriched sales data from staging tables into the production fact table.

**Business Purpose:**
This procedure solves the fundamental business challenge of maintaining high-quality, reliable sales data in the data warehouse. It ensures that only validated transactions with complete customer information and valid quantities are loaded into the production environment, while maintaining a complete audit trail for compliance and operational monitoring. The procedure supports business intelligence and analytics by providing clean, enriched sales data that includes regional information and customer segmentation details.

**Organizational Value:**
- Maintains data integrity through comprehensive validation rules
- Provides complete traceability and audit capabilities for regulatory compliance
- Enables reliable business reporting and analytics through clean data
- Supports operational monitoring with detailed processing metrics
- Implements robust error handling to prevent data corruption

## Logic Details

**Main Purpose:** The stored procedure implements a robust ETL pipeline that validates, cleanses, transforms, and loads sales transaction data while maintaining comprehensive audit trails and error handling capabilities.

**Structure Overview:**
The procedure is organized into 10 distinct logical sections: initialization and audit setup, temporary table creation for validation tracking, data quality checks implementation, invalid record removal, data transformation with dimensional enrichment, fact table loading, staging area cleanup, validation failure logging, audit completion, and comprehensive error handling with cleanup operations.

**Key SQL Operations:**
- Data validation using conditional SELECT statements to identify invalid records
- DELETE operations with JOIN conditions to remove invalid data from staging
- Common Table Expression (CTE) for data transformation and enrichment
- INSERT operations for loading validated data into production tables
- UPDATE operations for maintaining audit logs and processing status

**Input Parameters:** The procedure accepts no external parameters, operating as a self-contained batch processing unit that processes all available data in the staging table.

**Output:** The procedure generates enriched fact table records, comprehensive audit logs, data quality failure reports, and processing metrics including row counts and execution timestamps.

## Data Flow Details

**Step 1: Initialization and Audit Setup**
The procedure begins by generating a unique batch identifier using NEWID() and capturing the start timestamp. An initial audit record is inserted into dw.Audit_Log with status 'STARTED' to begin tracking the processing session.

**Step 2: Validation Infrastructure Setup**
A temporary table #InvalidRows is created to capture transactions that fail validation rules. This table stores the Transaction_ID and specific failure reasons for each rejected record.

**Step 3: Data Quality Validation**
Two critical validation rules are applied:
- Customer ID Validation: Identifies transactions where Customer_ID is NULL and flags them as 'Missing CustomerID'
- Quantity Validation: Identifies transactions where Quantity is less than or equal to zero and flags them as 'Invalid Quantity'

**Step 4: Data Cleansing**
Invalid records identified in the validation step are removed from the staging table using a DELETE operation with INNER JOIN to the #InvalidRows temporary table. The @@ROWCOUNT system variable captures the number of rejected records.

**Step 5: Data Transformation and Enrichment**
A Common Table Expression (CTE) named 'transformed' performs the following operations:
- Retrieves all valid fields from the staging table
- Calculates Total_Sales_Amount as Quantity × Unit_Price
- Enriches data with Region_ID from dw.Dim_Date based on Sales_Date
- Adds Customer_Segment from dw.Dim_Customer based on Customer_ID
- Appends Load_Timestamp and Batch_ID for tracking purposes

**Step 6: Fact Table Loading**
The transformed and enriched data is inserted into dw.Fact_Sales using an INSERT-SELECT operation from the CTE. The @@ROWCOUNT captures the number of successfully inserted records.

**Step 7: Staging Area Cleanup**
The staging table stg.Sales_Transactions is truncated to prepare for the next batch processing cycle.

**Step 8: Data Quality Failure Logging**
All validation failures captured in #InvalidRows are logged to dw.DQ_Failures with timestamps and batch identifiers for future analysis and reporting.

**Step 9: Audit Completion**
The audit log is updated with end timestamp, processing statistics (rows inserted and rejected), completion status, and a summary message.

**Step 10: Error Handling and Cleanup**
If any errors occur during processing, the CATCH block captures the error message, updates the audit log with failure status, and re-throws the error for pipeline monitoring. The temporary table is cleaned up regardless of success or failure.

## Data Transformations

**Calculated Field Transformations:**
- **Total Sales Amount Calculation:** The procedure computes Total_Sales_Amount by multiplying Quantity × Unit_Price, providing the monetary value of each transaction
- **Timestamp Generation:** Load_Timestamp is populated with SYSDATETIME() to track when each record was processed
- **Batch Tracking:** Each record receives the current @batch_id for complete traceability

**Data Enrichment Operations:**
- **Regional Information:** Region_ID is retrieved from the Date dimension (dw.Dim_Date) by joining on the Sales_Date, enabling regional sales analysis
- **Customer Segmentation:** Customer_Segment is obtained from the Customer dimension (dw.Dim_Customer) through Customer_ID lookup, supporting customer analytics
- **Data Type Standardization:** Sales_Date is cast to DATE format to ensure proper joining with the Date dimension

**Data Quality and Validation Rules:**
- **Customer Completeness:** Records with NULL Customer_ID values are identified and removed to ensure referential integrity
- **Business Logic Validation:** Transactions with zero or negative quantities are rejected as they violate business rules
- **Data Consistency:** All transformations maintain data type consistency and format standardization

**Business Logic Implementation:**
The transformations align with specific business requirements including revenue calculation accuracy, customer analytics enablement, regional performance tracking, and data quality assurance. These transformations ensure that the resulting fact table supports comprehensive business intelligence and reporting requirements while maintaining data integrity and consistency across the data warehouse.

## Data Mapping

| Target Table Name | Target Column Name | Source Table Name | Source Column Name | Remarks (1 to 1 mapping, Transformation, Validation) |
|-------------------|-------------------|-------------------|-------------------|---------------------------------------------------|
| dw.Fact_Sales | Transaction_ID | stg.Sales_Transactions | Transaction_ID | 1 to 1 mapping |
| dw.Fact_Sales | Customer_ID | stg.Sales_Transactions | Customer_ID | Validation - NULL values rejected |
| dw.Fact_Sales | Product_ID | stg.Sales_Transactions | Product_ID | 1 to 1 mapping |
| dw.Fact_Sales | Sales_Date | stg.Sales_Transactions | Sales_Date | 1 to 1 mapping |
| dw.Fact_Sales | Quantity | stg.Sales_Transactions | Quantity | Validation - Values <= 0 rejected |
| dw.Fact_Sales | Unit_Price | stg.Sales_Transactions | Unit_Price | 1 to 1 mapping |
| dw.Fact_Sales | Total_Sales_Amount | stg.Sales_Transactions | Quantity × Unit_Price | Transformation - Calculated field |
| dw.Fact_Sales | Region_ID | dw.Dim_Date | Region_ID | Transformation - Lookup via Sales_Date |
| dw.Fact_Sales | Customer_Segment | dw.Dim_Customer | Customer_Segment | Transformation - Lookup via Customer_ID |
| dw.Fact_Sales | Load_Timestamp | System Function | SYSDATETIME() | Transformation - System generated timestamp |
| dw.Fact_Sales | Batch_ID | Variable | @batch_id | Transformation - Process tracking identifier |
| dw.Audit_Log | Batch_ID | Variable | @batch_id | Transformation - Process tracking identifier |
| dw.Audit_Log | Procedure_Name | System Function | OBJECT_NAME(@@PROCID) | Transformation - System generated procedure name |
| dw.Audit_Log | Start_Time | Variable | @start_time | Transformation - Process start timestamp |
| dw.Audit_Log | End_Time | Variable | @end_time | Transformation - Process end timestamp |
| dw.Audit_Log | Rows_Inserted | Variable | @rows_inserted | Transformation - Processing statistics |
| dw.Audit_Log | Rows_Rejected | Variable | @rows_rejected | Transformation - Processing statistics |
| dw.DQ_Failures | Transaction_ID | #InvalidRows | Transaction_ID | Transformation - Data quality tracking |
| dw.DQ_Failures | Failure_Reason | #InvalidRows | Reason | Transformation - Validation failure description |
| dw.DQ_Failures | Logged_Timestamp | System Function | SYSDATETIME() | Transformation - System generated timestamp |
| dw.DQ_Failures | Batch_ID | Variable | @batch_id | Transformation - Process tracking identifier |

## Technical Complexity

| Parameter | Value |
|-----------|-------|
| Procedure Name | dw.sp_load_sales_fact |
| Source Tables | 3 |
| Target Tables | 3 |
| Data Flows | 8 |
| Transformations | 12 |
| Joins and Filters | 6 |
| Variables | 7 |
| Parameters | 0 |
| Dependencies | 5 |
| Complexity Score | 75 |

## Key Outputs

**Primary Data Outputs:**
- **Clean Sales Fact Records:** Validated and enriched sales transaction data loaded into dw.Fact_Sales, ready for business intelligence and reporting applications
- **Comprehensive Audit Trail:** Complete processing history stored in dw.Audit_Log with batch tracking, timing information, and processing statistics
- **Data Quality Reports:** Detailed validation failure logs in dw.DQ_Failures identifying specific records and reasons for rejection

**Processing Metrics and Statistics:**
- **Row Processing Counts:** Precise counts of successfully inserted records (@rows_inserted) and rejected records (@rows_rejected)
- **Execution Timing:** Start and end timestamps enabling performance monitoring and optimization
- **Batch Identification:** Unique batch identifiers enabling end-to-end traceability and process correlation

**Business Intelligence Enablement:**
The outputs directly support business decision-making by providing clean, reliable sales data enriched with regional and customer segment information. The fact table enables comprehensive sales analytics including revenue analysis, customer performance tracking, regional comparisons, and trend analysis.

**Operational Monitoring Capabilities:**
The audit outputs provide operational teams with complete visibility into ETL processing including success/failure status, processing duration, data quality metrics, and error details. This information supports proactive monitoring, troubleshooting, and process optimization.

**Data Lineage and Compliance:**
The comprehensive logging and batch tracking provide complete data lineage from source to target, supporting regulatory compliance requirements and enabling detailed impact analysis for data governance initiatives.

**Format and Accessibility:**
All outputs are stored in structured database tables with appropriate indexing and partitioning for efficient querying and reporting. The standardized format ensures compatibility with existing business intelligence tools and reporting frameworks.

apiCost: 0.0245