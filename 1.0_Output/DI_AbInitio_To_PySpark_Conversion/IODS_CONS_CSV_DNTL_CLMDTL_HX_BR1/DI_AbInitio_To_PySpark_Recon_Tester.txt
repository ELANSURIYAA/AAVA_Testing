```python
"""
==================================================================
Author:        Ascendion AVA+
Created on:    2024-06-14
Description:   End-to-end orchestration script for Ab Initio to PySpark migration validation on GCP.
               - Executes Ab Initio graph and PySpark job on GCP.
               - Performs deep reconciliation of outputs.
               - Generates structured comparison report.
==================================================================
"""

import os
import sys
import subprocess
import logging
import json
import time
import tempfile
from datetime import datetime

from google.cloud import storage
from google.cloud import dataproc_v1 as dataproc

# =========================
# CONFIGURATION & LOGGING
# =========================

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)

# Load configuration from environment variables (never hardcode secrets!)
GCP_PROJECT = os.environ.get("GCP_PROJECT")
GCS_BUCKET = os.environ.get("GCS_BUCKET")
DATAPROC_CLUSTER = os.environ.get("DATAPROC_CLUSTER")
GCP_REGION = os.environ.get("GCP_REGION", "us-central1")
ABINITIO_HOST = os.environ.get("ABINITIO_HOST")  # Hostname or IP for Ab Initio execution
ABINITIO_USER = os.environ.get("ABINITIO_USER")
ABINITIO_KEY_PATH = os.environ.get("ABINITIO_KEY_PATH")  # Path to SSH key for Ab Initio host

# Output locations (timestamped for uniqueness)
RUN_TIMESTAMP = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
ABINITIO_OUTPUT_PATH = f"gs://{GCS_BUCKET}/abinitio_output_{RUN_TIMESTAMP}/"
PYSPARK_OUTPUT_PATH = f"gs://{GCS_BUCKET}/pyspark_output_{RUN_TIMESTAMP}/"
RECONCILE_REPORT_PATH = f"gs://{GCS_BUCKET}/reconcile_report_{RUN_TIMESTAMP}/"

# =========================
# UTILITY FUNCTIONS
# =========================

def run_shell_command(cmd, env=None, capture_output=True):
    """Run a shell command and return output, raise on error."""
    logging.info(f"Running shell command: {cmd}")
    try:
        result = subprocess.run(
            cmd, shell=True, check=True,
            stdout=subprocess.PIPE if capture_output else None,
            stderr=subprocess.PIPE if capture_output else None,
            env=env
        )
        if capture_output:
            stdout = result.stdout.decode()
            stderr = result.stderr.decode()
            logging.info(f"Command output: {stdout}")
            if stderr:
                logging.warning(f"Command stderr: {stderr}")
            return stdout
        return ""
    except subprocess.CalledProcessError as e:
        logging.error(f"Command failed: {e.stderr.decode() if e.stderr else str(e)}")
        raise

def check_gcs_path_exists(gcs_path):
    """Check if a GCS path exists."""
    client = storage.Client()
    bucket_name, prefix = gcs_path.replace("gs://", "").split("/", 1)
    bucket = client.bucket(bucket_name)
    blobs = list(client.list_blobs(bucket, prefix=prefix))
    return len(blobs) > 0

def wait_for_gcs_output(gcs_path, timeout=900, poll_interval=30):
    """Wait for GCS output to appear, with timeout."""
    logging.info(f"Waiting for GCS output at {gcs_path} (timeout={timeout}s)...")
    start = time.time()
    while time.time() - start < timeout:
        if check_gcs_path_exists(gcs_path):
            logging.info(f"Output found at {gcs_path}")
            return True
        time.sleep(poll_interval)
    raise TimeoutError(f"Timed out waiting for GCS output at {gcs_path}")

def upload_to_gcs(local_path, gcs_path):
    """Upload a local file to GCS."""
    client = storage.Client()
    bucket_name, blob_name = gcs_path.replace("gs://", "").split("/", 1)
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.upload_from_filename(local_path)
    logging.info(f"Uploaded {local_path} to {gcs_path}")

# =========================
# 1. EXECUTE AB INITIO GRAPH
# =========================

def run_abinitio_graph(graph_path, abinitio_output_path):
    """
    SSH into Ab Initio host and run the graph, writing output to GCS.
    Assumes Ab Initio environment is configured to allow GCS writes.
    """
    graph_name = os.path.basename(graph_path)
    remote_cmd = (
        f"air sandbox run {graph_name} "
        f"--output_path {abinitio_output_path} "
        f"--logfile abinitio_run_{RUN_TIMESTAMP}.log"
    )
    ssh_cmd = (
        f"ssh -i {ABINITIO_KEY_PATH} {ABINITIO_USER}@{ABINITIO_HOST} "
        f"'{remote_cmd}'"
    )
    logging.info(f"Submitting Ab Initio graph via SSH: {ssh_cmd}")
    run_shell_command(ssh_cmd)
    logging.info("Ab Initio graph execution submitted.")

# =========================
# 2. EXECUTE PYSPARK JOB ON DATAPROC
# =========================

def submit_pyspark_job(pyspark_code_path, pyspark_output_path, job_args=None):
    """
    Submit the PySpark job to Dataproc.
    pyspark_code_path: local path to the PySpark script.
    pyspark_output_path: GCS path for output.
    job_args: list of additional arguments to pass to the script.
    """
    # Upload PySpark script to GCS (Dataproc requires GCS location)
    client = storage.Client()
    bucket = client.bucket(GCS_BUCKET)
    blob_name = f"pyspark_scripts/{os.path.basename(pyspark_code_path)}_{RUN_TIMESTAMP}.py"
    blob = bucket.blob(blob_name)
    blob.upload_from_filename(pyspark_code_path)
    pyspark_gcs_path = f"gs://{GCS_BUCKET}/{blob_name}"

    # Build Dataproc job submission command
    args = [
        "gcloud", "dataproc", "jobs", "submit", "pyspark", pyspark_gcs_path,
        "--cluster", DATAPROC_CLUSTER,
        "--region", GCP_REGION,
        "--project", GCP_PROJECT,
        "--",
        f"--output_path={pyspark_output_path}"
    ]
    if job_args:
        args.extend(job_args)
    cmd = " ".join(args)
    run_shell_command(cmd)
    logging.info("PySpark job submitted to Dataproc.")

# =========================
# 3. RECONCILIATION PYSPARK SCRIPT GENERATION
# =========================

def generate_reconcile_script(local_path, abinitio_output_path, pyspark_output_path, report_output_path):
    """
    Generate a PySpark script that compares two datasets in GCS and writes a JSON report.
    """
    script = f"""
from pyspark.sql import SparkSession
import json
import sys
import os

abinitio_path = "{abinitio_output_path}"
pyspark_path = "{pyspark_output_path}"
report_path = "{report_output_path}"

spark = SparkSession.builder.appName("AbInitio_vs_PySpark_Reconcile").getOrCreate()

def load_df(path):
    return spark.read.option("header", True).parquet(path) if path.endswith(".parquet") else spark.read.option("header", True).csv(path)

try:
    df_ab = load_df(abinitio_path)
    df_py = load_df(pyspark_path)

    # Align schemas (by column name, ignore order)
    ab_cols = set(df_ab.columns)
    py_cols = set(df_py.columns)
    common_cols = list(ab_cols & py_cols)
    ab_only = list(ab_cols - py_cols)
    py_only = list(py_cols - ab_cols)

    df_ab = df_ab.select(common_cols)
    df_py = df_py.select(common_cols)

    # Row counts
    ab_count = df_ab.count()
    py_count = df_py.count()

    # exceptAll both ways for mismatches
    ab_minus_py = df_ab.exceptAll(df_py)
    py_minus_ab = df_py.exceptAll(df_ab)
    ab_minus_py_count = ab_minus_py.count()
    py_minus_ab_count = py_minus_ab.count()
    mismatch_count = ab_minus_py_count + py_minus_ab_count

    # Match percentage
    match_pct = 100.0 if ab_count == py_count and mismatch_count == 0 else \
        100.0 * (max(ab_count, py_count) - mismatch_count) / max(ab_count, py_count, 1)

    # Sample mismatches
    ab_minus_py_sample = ab_minus_py.limit(5).toPandas().to_dict(orient="records")
    py_minus_ab_sample = py_minus_ab.limit(5).toPandas().to_dict(orient="records")

    # Status
    if ab_count == py_count and mismatch_count == 0:
        status = "MATCH"
    elif mismatch_count == 0:
        status = "PARTIAL MATCH (row counts differ, but data matches)"
    else:
        status = "NO MATCH"

    report = {{
        "match_status": status,
        "row_counts": {{
            "abinitio": ab_count,
            "pyspark": py_count,
            "difference": ab_count - py_count
        }},
        "schema_comparison": {{
            "common_columns": common_cols,
            "abinitio_only_columns": ab_only,
            "pyspark_only_columns": py_only
        }},
        "data_discrepancies": {{
            "mismatched_rows": mismatch_count,
            "abinitio_minus_pyspark_sample": ab_minus_py_sample,
            "pyspark_minus_abinitio_sample": py_minus_ab_sample
        }},
        "match_percentage": match_pct
    }}

    # Write report as JSON to GCS
    with open("/tmp/reconcile_report.json", "w") as f:
        json.dump(report, f, indent=2)
    # Upload to GCS
    from google.cloud import storage
    client = storage.Client()
    bucket_name, blob_name = report_path.replace("gs://", "").split("/", 1)
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.upload_from_filename("/tmp/reconcile_report.json")

    print(json.dumps(report, indent=2))
    spark.stop()
except Exception as e:
    print("ERROR:", str(e))
    sys.exit(1)
"""
    with open(local_path, "w") as f:
        f.write(script)
    logging.info(f"Generated reconciliation PySpark script at {local_path}")

# =========================
# 4. MAIN ORCHESTRATION LOGIC
# =========================

def main():
    try:
        # --- 1. Run Ab Initio graph ---
        abinitio_graph_path = sys.argv[1]  # e.g., "IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1.mp"
        logging.info("Step 1: Running Ab Initio graph...")
        run_abinitio_graph(abinitio_graph_path, ABINITIO_OUTPUT_PATH)

        # --- 2. Wait for Ab Initio output on GCS ---
        wait_for_gcs_output(ABINITIO_OUTPUT_PATH)

        # --- 3. Run PySpark job on Dataproc ---
        pyspark_code_path = sys.argv[2]  # Path to converted PySpark code file
        logging.info("Step 2: Running PySpark job on Dataproc...")
        submit_pyspark_job(pyspark_code_path, PYSPARK_OUTPUT_PATH)

        # --- 4. Wait for PySpark output on GCS ---
        wait_for_gcs_output(PYSPARK_OUTPUT_PATH)

        # --- 5. Generate and run reconciliation PySpark script ---
        with tempfile.NamedTemporaryFile("w", suffix=".py", delete=False) as tf:
            reconcile_script_path = tf.name
        report_blob_path = f"reconcile_report_{RUN_TIMESTAMP}/report.json"
        report_gcs_path = f"gs://{GCS_BUCKET}/{report_blob_path}"
        generate_reconcile_script(
            reconcile_script_path,
            ABINITIO_OUTPUT_PATH,
            PYSPARK_OUTPUT_PATH,
            report_gcs_path
        )
        logging.info("Step 3: Submitting reconciliation PySpark job...")
        submit_pyspark_job(reconcile_script_path, "", job_args=[])

        # --- 6. Wait for report to appear in GCS ---
        wait_for_gcs_output(report_gcs_path)

        # --- 7. Download and print the report ---
        client = storage.Client()
        bucket_name, blob_name = report_gcs_path.replace("gs://", "").split("/", 1)
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        local_report = f"/tmp/reconcile_report_{RUN_TIMESTAMP}.json"
        blob.download_to_filename(local_report)
        with open(local_report) as f:
            report = json.load(f)
        logging.info("===== FINAL RECONCILIATION REPORT =====")
        print(json.dumps(report, indent=2))

    except Exception as e:
        logging.error(f"Orchestration failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python orchestrate_abinitio_pyspark_validation.py <abinitio_graph_path> <converted_pyspark_code_path>")
        sys.exit(1)
    main()

"""
========================
API Cost for this call: 0.00043752 USD
========================
"""

# =========================
# END OF SCRIPT
# =========================
```

**Instructions:**
- Save this script as `orchestrate_abinitio_pyspark_validation.py`.
- Run with:
  ```
  python orchestrate_abinitio_pyspark_validation.py IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1.mp converted_pyspark_code.py
  ```
- Ensure all required environment variables are set for GCP and Ab Initio connectivity.
- The script will:
    - SSH to the Ab Initio host and execute the graph, writing output to GCS.
    - Submit the PySpark job to Dataproc, writing output to GCS.
    - Launch a reconciliation PySpark job to compare outputs and write a JSON report to GCS.
    - Download and print the reconciliation report.
- All credentials/configuration are handled via environment variables.
- All steps are logged and robust error handling is implemented.
- The script is CI/CD ready and can be extended for more advanced orchestration.

**API Cost for this call:** 0.00043752 USD