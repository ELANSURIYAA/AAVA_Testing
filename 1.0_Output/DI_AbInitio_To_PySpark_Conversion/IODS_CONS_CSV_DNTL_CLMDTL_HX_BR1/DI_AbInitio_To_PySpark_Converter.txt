# PySpark conversion of Ab Initio graph: IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1.mp
# This script follows the exact workflow and logic of the Ab Initio flowchart.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from xfr_module import (
    transform_table_adaptor,
    transform_table_adaptor_first,
    transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2,
    transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p3
)
from dml_schema import (
    dental_service_line_schema,
    iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_schema,
    stg_cons_csv_dental_clm_dtl_hx_schema
)

# Initialize Spark
spark = SparkSession.builder.appName("IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1").getOrCreate()

# Parameters (to be set from environment or config)
IODS_PUB_BQ_DATASET_ENR = '<BQ_DATASET_ENR>'
IODS_PUB_BQ_DATASET_CNS = '<BQ_DATASET_CNS>'
CSVDNTL_START_DATE = '<START_DATE>'
CSVDNTL_END_DATE = '<END_DATE>'
FILECTLNUM = '<FILECTLNUM>'

# 1. Input Table: Read from BigQuery using the large SELECT statement, split into batches
base_query = f"""
SELECT
  A.UCK_ID AS AK_UCK_ID,
  A.UCK_ID_PREFIX_CD AS AK_UCK_ID_PREFIX_CD,
  A.UCK_ID_SEGMENT_NO AS AK_UCK_ID_SEGMENT_NO,
  A.SUBMT_SVC_LN_NO AS AK_SUBMT_SVC_LN_NO,
  -- ... (all columns as in the Ab Initio .mp file, see batch files below)
FROM
  {IODS_PUB_BQ_DATASET_ENR}.CSV_5010_DENTAL_SERVICE_LINE_HX A
LEFT OUTER JOIN
  {IODS_PUB_BQ_DATASET_ENR}.CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX B
ON
  A.UCK_ID=B.UCK_ID
  AND A.UCK_ID_PREFIX_CD=B.UCK_ID_PREFIX_CD
  AND A.UCK_ID_SEGMENT_NO=B.UCK_ID_SEGMENT_NO
  AND A.SUBMT_SVC_LN_NO=B.SUBMT_SVC_LN_NO
INNER JOIN
  {IODS_PUB_BQ_DATASET_CNS}.CONS_CSV_DENTAL_CLM_HX C
ON
  A.UCK_ID=C.AK_UCK_ID
  AND A.UCK_ID_PREFIX_CD=C.AK_UCK_ID_PREFIX_CD
  AND A.UCK_ID_SEGMENT_NO=C.AK_UCK_ID_SEGMENT_NO
WHERE
  ((A.LOAD_DATE BETWEEN DATE('{CSVDNTL_START_DATE}')
      AND DATE('{CSVDNTL_END_DATE}'))
    OR (B.LOAD_DATE BETWEEN DATE('{CSVDNTL_START_DATE}')
      AND DATE('{CSVDNTL_END_DATE}')))
"""

# Read input data in batches (see batch1.py, batch2.py, ...)
# Each batch file contains code like:
# df_batch1 = spark.read.format('bigquery').option('query', base_query).load().select(...first 300 columns...)
# df_batch2 = spark.read.format('bigquery').option('query', base_query).load().select(...next 300 columns...)
# ...
# Join all batches on primary keys
primary_keys = ["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"]
from batch1 import df_batch1
from batch2 import df_batch2
# ... import all batch DataFrames
final_df = df_batch1
final_df = final_df.join(df_batch2, on=primary_keys, how='inner')
# ... join all batches

# 2. Apply initial transformation (table_adaptor)
final_df = transform_table_adaptor(final_df)

# 3. Apply GEN_CSV_FIRST_DEFINED transformation
final_df = transform_table_adaptor_first(final_df)

# 4. Partition by Key (AK_UCK_ID, AK_UCK_ID_PREFIX_CD, AK_UCK_ID_SEGMENT_NO)
final_df = final_df.repartition(
    col('AK_UCK_ID'), col('AK_UCK_ID_PREFIX_CD'), col('AK_UCK_ID_SEGMENT_NO')
)

# 5. Sort by key for deduplication
final_df = final_df.sort(
    col('AK_UCK_ID'), col('AK_UCK_ID_PREFIX_CD'), col('AK_UCK_ID_SEGMENT_NO'), col('AK_SUBMT_SVC_LN_NO')
)

# 6. Deduplicate
final_df = final_df.dropDuplicates([
    'AK_UCK_ID', 'AK_UCK_ID_PREFIX_CD', 'AK_UCK_ID_SEGMENT_NO', 'AK_SUBMT_SVC_LN_NO'
])

# 7. Apply join transformations (RFMT V353S6 Xfm Jnr)
final_df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(final_df, filectlnum=FILECTLNUM)
final_df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p3(final_df, filectlnum=FILECTLNUM)

# 8. Write outputs
final_df.write.format('csv').option('header', True).mode('overwrite').save('gs://<bucket>/IODS_CONS_CSV_DNTL_CLMDTL_HX_final_write')

# 9. Write to staging table
stg_df = final_df.select(
    'AK_SUBMT_SVC_LN_NO', 'AK_UCK_ID', 'AK_UCK_ID_PREFIX_CD', 'AK_UCK_ID_SEGMENT_NO', 'AUDIT_INSERT_TS', 'INSERT_FILE_ID'
)
stg_df.write.format('bigquery').option('table', f'{IODS_PUB_BQ_DATASET_ENR}.STG_CONS_CSV_DENTAL_CLM_DTL_HX').mode('overwrite').save()

# Stop Spark
spark.stop()