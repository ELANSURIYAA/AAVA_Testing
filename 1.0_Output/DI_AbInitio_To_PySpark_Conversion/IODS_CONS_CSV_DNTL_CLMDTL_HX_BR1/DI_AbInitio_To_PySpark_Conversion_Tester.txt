===================================================================
Author:        Ascendion AVA+
Created on:    
Description:   Validation suite for Ab Initio to PySpark Conversion
===================================================================

#### 1. Test Case Document:

| Test Case ID | Description | Expected Result |
|--------------|-------------|-----------------|
| TC001 | Validate join with matching keys | Output contains combined data from both sources |
| TC002 | Handle nulls in input fields during transformation | Nulls are processed without error or as per `.xfr` logic |
| TC003 | Check reject logic on missing fields | Row is excluded or logged in reject path equivalent |
| TC004 | Verify lookup failure case returns default value | Default logic executes as expected |
| TC005 | Ensure empty input produces empty output without errors | No exception is thrown |
| TC006 | Deduplication logic with duplicate keys | Only first record per key retained |
| TC007 | Data type mismatch in input | Raise error or handle gracefully |
| TC008 | Reject handling on transformation error | Row is routed to reject output or flagged |
| TC009 | Boundary values for numeric/date fields | Correct handling of min/max values |
| TC010 | Unexpected field order in input | Raise error or handle as per schema |

---

#### 2. Pytest Script Example:

```python
import pytest
from pyspark.sql import SparkSession
from chispa.dataframe_comparer import assert_df_equality
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
import pandas as pd

# Assume these are implemented and importable
from xfr_module import (
    table_adaptor,
    gen_csv_first_defined,
    iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_transform
)
from dml_schema import (
    iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema,
    iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_schema,
    stg_cons_csv_dental_clm_dtl_hx_schema
)

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[2]").appName("unit-test").getOrCreate()

def make_df(spark, data, schema):
    return spark.createDataFrame(pd.DataFrame(data), schema=schema)

def test_TC001_join_matching_keys(spark):
    # Happy Path: minimal valid input
    input_data = [
        {
            "AK_UCK_ID": "ID1",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        }
    ]
    input_df = make_df(spark, input_data, iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema)
    df1 = table_adaptor(input_df)
    df2 = df1.repartition("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO")
    df3 = df2.sort("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO")
    df4 = df3.dropDuplicates(["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"])
    df5 = gen_csv_first_defined(df4)
    result_df = iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_transform(df5)
    expected_data = [
        {
            "AK_UCK_ID": "ID1",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        }
    ]
    expected_df = make_df(spark, expected_data, iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_schema)
    assert_df_equality(result_df, expected_df, ignore_nullable=True, ignore_column_order=True)

def test_TC002_null_handling(spark):
    input_data = [
        {
            "AK_UCK_ID": None,
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": None,
            "STK_UCK_ID": 123,
            "SVC_DT": None,
        }
    ]
    input_df = make_df(spark, input_data, iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema)
    df1 = table_adaptor(input_df)
    df2 = df1.repartition("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO")
    df3 = df2.sort("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO")
    df4 = df3.dropDuplicates(["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"])
    df5 = gen_csv_first_defined(df4)
    result_df = iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_transform(df5)
    expected_data = [
        {
            "AK_UCK_ID": None,
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": None,
            "STK_UCK_ID": 123,
            "SVC_DT": None,
        }
    ]
    expected_df = make_df(spark, expected_data, iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_schema)
    assert_df_equality(result_df, expected_df, ignore_nullable=True, ignore_column_order=True)

def test_TC003_missing_column(spark):
    input_data = [
        {
            # "AK_UCK_ID" is missing
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        }
    ]
    partial_schema = StructType([f for f in iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema.fields if f.name != "AK_UCK_ID"])
    input_df = make_df(spark, input_data, partial_schema)
    with pytest.raises(Exception):
        table_adaptor(input_df)

def test_TC004_lookup_failure(spark):
    input_data = [
        {
            "AK_UCK_ID": "ID_NOT_FOUND",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        }
    ]
    input_df = make_df(spark, input_data, iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema)
    df1 = table_adaptor(input_df)
    df2 = df1.repartition("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO")
    df3 = df2.sort("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO")
    df4 = df3.dropDuplicates(["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"])
    df5 = gen_csv_first_defined(df4)
    result_df = iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_transform(df5)
    expected_data = [
        {
            "AK_UCK_ID": "ID_NOT_FOUND",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        }
    ]
    expected_df = make_df(spark, expected_data, iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_schema)
    assert_df_equality(result_df, expected_df, ignore_nullable=True, ignore_column_order=True)

def test_TC005_empty_input(spark):
    input_df = spark.createDataFrame([], iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema)
    df1 = table_adaptor(input_df)
    df2 = df1.repartition("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO")
    df3 = df2.sort("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO")
    df4 = df3.dropDuplicates(["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"])
    df5 = gen_csv_first_defined(df4)
    result_df = iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_transform(df5)
    assert result_df.count() == 0

def test_TC006_deduplication(spark):
    input_data = [
        {
            "AK_UCK_ID": "ID1",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        },
        {
            "AK_UCK_ID": "ID1",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "B",
            "STK_UCK_ID": 124,
            "SVC_DT": "2024-01-02",
        }
    ]
    input_df = make_df(spark, input_data, iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema)
    df1 = table_adaptor(input_df)
    df2 = df1.repartition("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO")
    df3 = df2.sort("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO")
    df4 = df3.dropDuplicates(["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"])
    df5 = gen_csv_first_defined(df4)
    result_df = iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_transform(df5)
    expected_data = [
        {
            "AK_UCK_ID": "ID1",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        }
    ]
    expected_df = make_df(spark, expected_data, iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_schema)
    assert_df_equality(result_df, expected_df, ignore_nullable=True, ignore_column_order=True)

def test_TC007_data_type_mismatch(spark):
    input_data = [
        {
            "AK_UCK_ID": "ID1",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": "not_an_int",
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        }
    ]
    with pytest.raises(Exception):
        make_df(spark, input_data, iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema)

def test_TC008_reject_handling(spark):
    input_data = [
        {
            "AK_UCK_ID": "ID1",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_SUBMT_SVC_LN_NO": 10,
            "CLM_TYP_CD": "INVALID",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        }
    ]
    input_df = make_df(spark, input_data, iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema)
    df1 = table_adaptor(input_df)
    df2 = df1.repartition("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO")
    df3 = df2.sort("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO")
    df4 = df3.dropDuplicates(["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"])
    df5 = gen_csv_first_defined(df4)
    result_df = iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_transform(df5)
    assert result_df.count() == 0

def test_TC009_boundary_values(spark):
    input_data = [
        {
            "AK_UCK_ID": "ID1",
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID_SEGMENT_NO": 0,
            "AK_SUBMT_SVC_LN_NO": 0,
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 0,
            "SVC_DT": "1900-01-01",
        },
        {
            "AK_UCK_ID": "ID2",
            "AK_UCK_ID_PREFIX_CD": "P2",
            "AK_UCK_ID_SEGMENT_NO": 2**63-1,
            "AK_SUBMT_SVC_LN_NO": 2**63-1,
            "CLM_TYP_CD": "B",
            "STK_UCK_ID": 2**63-1,
            "SVC_DT": "2100-12-31",
        }
    ]
    input_df = make_df(spark, input_data, iods_cons_csv_dntl_clmdtl_hx_br1_v351s3p1_schema)
    df1 = table_adaptor(input_df)
    df2 = df1.repartition("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO")
    df3 = df2.sort("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO")
    df4 = df3.dropDuplicates(["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"])
    df5 = gen_csv_first_defined(df4)
    result_df = iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_transform(df5)
    expected_df = make_df(spark, input_data, iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2_schema)
    assert_df_equality(result_df, expected_df, ignore_nullable=True, ignore_column_order=True)

def test_TC010_unexpected_field_order(spark):
    input_data = [
        {
            "AK_SUBMT_SVC_LN_NO": 10,
            "AK_UCK_ID_SEGMENT_NO": 1,
            "AK_UCK_ID_PREFIX_CD": "P1",
            "AK_UCK_ID": "ID1",
            "CLM_TYP_CD": "A",
            "STK_UCK_ID": 123,
            "SVC_DT": "2024-01-01",
        }
    ]
    shuffled_schema = StructType([
        StructField("AK_SUBMT_SVC_LN_NO", IntegerType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", IntegerType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID", StringType(), True),
        StructField("CLM_TYP_CD", StringType(), True),
        StructField("STK_UCK_ID", IntegerType(), True),
        StructField("SVC_DT", StringType(), True),
    ])
    input_df = make_df(spark, input_data, shuffled_schema)
    try:
        df1 = table_adaptor(input_df)
        assert True
    except Exception:
        assert True
```

---

#### 3. API Cost Consumption:
apiCost: 0.00043752 USD