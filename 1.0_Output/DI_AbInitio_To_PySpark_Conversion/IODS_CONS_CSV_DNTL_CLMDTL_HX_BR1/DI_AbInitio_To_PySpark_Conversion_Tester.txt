===================================================================
Author:        Ascendion AVA+
Created on:    
Description:   Validation suite for Ab Initio to PySpark Conversion
===================================================================

#### 1. Test Case Document:

| Test Case ID | Description | Expected Result |
|--------------|-------------|-----------------|
| TC001 | Validate join with matching keys | Output contains combined data from both sources |
| TC002 | Handle nulls in input fields during transformation | Nulls are processed without error or as per `.xfr` logic |
| TC003 | Check reject logic on missing fields | Row is excluded or logged in reject path equivalent |
| TC004 | Verify lookup failure case returns default value | Default logic executes as expected |
| TC005 | Ensure empty input produces empty output without errors | No exception is thrown |
| TC006 | Deduplication logic on duplicate input rows | Output contains only unique rows by primary key |
| TC007 | Data type mismatch in input columns | Raise appropriate error or handle as per schema |
| TC008 | Reject handling (invalid business rule) | Row is flagged or excluded as per reject logic in xfr |
| TC009 | Boundary values (e.g., max/min for numeric columns) | Output correctly processes boundary values |
| TC010 | Unexpected field order in input | Transformation works regardless of column order |

#### 2. Pytest Script Example:
```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from chispa.dataframe_comparer import assert_df_equality

# Mock transformation functions (replace with actual imports in real test)
def transform_table_adaptor(df): return df
def transform_table_adaptor_first(df): return df
def transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(df, filectlnum): return df
def transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p3(df, filectlnum): return df

PRIMARY_KEYS = ["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"]

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[1]").appName("unit-test").getOrCreate()

@pytest.fixture
def base_schema():
    # Minimal schema for testing (expand as needed)
    return StructType([
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("PROC_CD", StringType(), True),
        StructField("LN_ITEM_CHG_AMT", DoubleType(), True),
        StructField("REJECT_RSN_CD", StringType(), True),
        StructField("CONTRACT_AMT", DoubleType(), True),
        StructField("CLM_TYP_CD", StringType(), True),
        StructField("SVC_DT", StringType(), True),
    ])

def run_full_pipeline(df, filectlnum="12345"):
    # Simulate the pipeline as in converted_code.py
    df = transform_table_adaptor(df)
    df = transform_table_adaptor_first(df)
    df = df.repartition(*PRIMARY_KEYS)
    df = df.sort(*[df[c] for c in PRIMARY_KEYS])
    df = df.dropDuplicates(PRIMARY_KEYS)
    df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(df, filectlnum=filectlnum)
    return df

# TC001: Validate join with matching keys
def test_TC001_join_matching_keys(spark, base_schema):
    input1 = [
        ("1001", "A", "1", "10", "D1110", 100.0, None, 90.0, "C", "2023-01-01"),
        ("1002", "B", "2", "20", "D1120", 200.0, None, 180.0, "C", "2023-01-02"),
    ]
    input2 = [
        ("1001", "A", "1", "10", "D1110", 100.0, None, 90.0, "C", "2023-01-01"),
        ("1002", "B", "2", "20", "D1120", 200.0, None, 180.0, "C", "2023-01-02"),
    ]
    df1 = spark.createDataFrame(input1, schema=base_schema)
    df2 = spark.createDataFrame(input2, schema=base_schema)
    joined_df = df1.join(df2, on=PRIMARY_KEYS, how='inner')
    assert joined_df.count() == 2

# TC002: Handle nulls in input fields during transformation
def test_TC002_nulls_in_input(spark, base_schema):
    input_data = [
        (None, "A", "1", "10", "D1110", 100.0, None, 90.0, "C", "2023-01-01"),
        ("1002", None, "2", "20", "D1120", None, None, 180.0, "C", "2023-01-02"),
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema)
    expected_df = input_df  # For identity transform mock
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC003: Check reject logic on missing fields
def test_TC003_missing_column_in_input(spark, base_schema):
    # Remove 'PROC_CD' from schema
    reduced_schema = StructType([f for f in base_schema if f.name != "PROC_CD"])
    input_data = [
        ("1001", "A", "1", "10", 100.0, None, 90.0, "C", "2023-01-01"),
    ]
    input_df = spark.createDataFrame(input_data, schema=reduced_schema)
    with pytest.raises(Exception):
        run_full_pipeline(input_df)

# TC004: Verify lookup failure case returns default value
def test_TC004_lookup_failure_in_join(spark, base_schema):
    batch1_data = [
        ("1001", "A", "1", "10", "D1110", 100.0, None, 90.0, "C", "2023-01-01"),
    ]
    batch2_data = [
        ("9999", "Z", "9", "99", "D9999", 999.0, None, 900.0, "C", "2023-01-09"),
    ]
    batch1_df = spark.createDataFrame(batch1_data, schema=base_schema)
    batch2_df = spark.createDataFrame(batch2_data, schema=base_schema)
    joined_df = batch1_df.join(batch2_df, on=PRIMARY_KEYS, how='inner')
    assert joined_df.count() == 0

# TC005: Ensure empty input produces empty output without errors
def test_TC005_empty_input_dataset(spark, base_schema):
    input_df = spark.createDataFrame([], schema=base_schema)
    result_df = run_full_pipeline(input_df)
    assert result_df.count() == 0

# TC006: Deduplication logic on duplicate input rows
def test_TC006_deduplication(spark, base_schema):
    input_data = [
        ("1001", "A", "1", "10", "D1110", 100.0, None, 90.0, "C", "2023-01-01"),
        ("1001", "A", "1", "10", "D1110", 100.0, None, 90.0, "C", "2023-01-01"),  # Duplicate
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema)
    result_df = run_full_pipeline(input_df)
    assert result_df.count() == 1

# TC007: Data type mismatch in input columns
def test_TC007_data_type_mismatch(spark, base_schema):
    from pyspark.sql.types import StructType, StructField, StringType
    mismatch_schema = StructType([
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("PROC_CD", StringType(), True),
        StructField("LN_ITEM_CHG_AMT", StringType(), True),  # Should be DoubleType
        StructField("REJECT_RSN_CD", StringType(), True),
        StructField("CONTRACT_AMT", DoubleType(), True),
        StructField("CLM_TYP_CD", StringType(), True),
        StructField("SVC_DT", StringType(), True),
    ])
    input_data = [
        ("1001", "A", "1", "10", "D1110", "not_a_number", None, 90.0, "C", "2023-01-01"),
    ]
    input_df = spark.createDataFrame(input_data, schema=mismatch_schema)
    with pytest.raises(Exception):
        run_full_pipeline(input_df)

# TC008: Reject handling (invalid business rule)
def test_TC008_reject_handling(spark, base_schema):
    # Suppose REJECT_RSN_CD is set by xfr if CONTRACT_AMT < 0
    input_data = [
        ("1001", "A", "1", "10", "D1110", 100.0, None, -50.0, "C", "2023-01-01"),
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema)
    # Mock xfr to set REJECT_RSN_CD if CONTRACT_AMT < 0
    def mock_xfr(df, filectlnum):
        from pyspark.sql.functions import when
        return df.withColumn("REJECT_RSN_CD", when(df["CONTRACT_AMT"] < 0, "NEG_AMT").otherwise(df["REJECT_RSN_CD"]))
    global transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2
    transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2 = mock_xfr
    result_df = run_full_pipeline(input_df)
    rows = result_df.collect()
    assert rows[0]["REJECT_RSN_CD"] == "NEG_AMT"

# TC009: Boundary values (e.g., max/min for numeric columns)
def test_TC009_boundary_values(spark, base_schema):
    input_data = [
        ("1001", "A", "1", "10", "D1110", float("inf"), None, float("-inf"), "C", "2023-01-01"),
        ("1002", "B", "2", "20", "D1120", float("nan"), None, 0.0, "C", "2023-01-02"),
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema)
    expected_df = input_df
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC010: Unexpected field order in input
def test_TC010_unexpected_field_order(spark, base_schema):
    # Shuffle columns
    shuffled_schema = StructType([
        base_schema[4],  # PROC_CD
        base_schema[0],  # AK_UCK_ID
        base_schema[2],  # AK_UCK_ID_SEGMENT_NO
        base_schema[1],  # AK_UCK_ID_PREFIX_CD
        base_schema[3],  # AK_SUBMT_SVC_LN_NO
        base_schema[5],  # LN_ITEM_CHG_AMT
        base_schema[6],  # REJECT_RSN_CD
        base_schema[7],  # CONTRACT_AMT
        base_schema[8],  # CLM_TYP_CD
        base_schema[9],  # SVC_DT
    ])
    input_data = [
        ("D1110", "1001", "1", "A", "10", 100.0, None, 90.0, "C", "2023-01-01"),
    ]
    input_df = spark.createDataFrame(input_data, schema=shuffled_schema)
    # Reorder columns to match pipeline expectation
    input_df = input_df.select([f.name for f in base_schema])
    expected_df = input_df
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)
```

#### 3. API Cost Consumption:
apiCost: 0.00043752 USD