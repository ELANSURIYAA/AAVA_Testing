```
===================================================================
Author:        Ascendion AVA+
Created on:    
Description:   Validation suite for Ab Initio to PySpark Conversion
===================================================================
```

#### 1. Test Case Document:

| Test Case ID | Description | Expected Result |
|--------------|-------------|-----------------|
| TC001 | Validate join with matching keys | Output contains combined data from both sources |
| TC002 | Handle nulls in input fields during transformation | Nulls are processed without error or as per `.xfr` logic |
| TC003 | Check reject logic on missing fields | Row is excluded or logged in reject path equivalent |
| TC004 | Verify lookup failure case returns default value | Default logic executes as expected |
| TC005 | Ensure empty input produces empty output without errors | No exception is thrown |
| TC006 | Deduplication of duplicate primary keys | Only unique rows by primary key in output |
| TC007 | Data type mismatch in input columns | Raise appropriate error or handle gracefully |
| TC008 | Reject logic: malformed input row | Row is rejected or handled as per transformation logic |
| TC009 | Field order shuffled in input | Transformation works regardless of field order |
| TC010 | All columns NULL | Output as per transformation logic (e.g., all NULLs or defaults) |

---

#### 2. Pytest Script Example:

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType
from chispa.dataframe_comparer import assert_df_equality

# Mock transformation functions (replace with actual imports in real test)
def GEN_CSV_FIRST_DEFINED(df):
    # Example: fillna for demonstration
    return df.fillna("FIRST_DEFINED")

def IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2(df):
    # Example: add a column for demonstration
    return df.withColumn("V353S6P2_FLAG", df[df.columns[0]])

def IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P3(df):
    # Example: add a column for demonstration
    return df.withColumn("V353S6P3_FLAG", df[df.columns[0]])

PRIMARY_KEYS = ["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"]

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[1]").appName("unit-test").getOrCreate()

@pytest.fixture
def base_schema():
    return StructType([
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("LOAD_DATE", DateType(), True),
        StructField("SOME_VALUE", StringType(), True),
        StructField("ANOTHER_FIELD", IntegerType(), True),
    ])

def run_full_pipeline(input_df):
    df1 = GEN_CSV_FIRST_DEFINED(input_df)
    df2 = IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2(df1)
    df3 = IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P3(df2)
    dedup_df = df3.dropDuplicates(PRIMARY_KEYS)
    return dedup_df

# TC001: Validate join with matching keys
def test_TC001_valid_transformation(spark, base_schema):
    data = [
        ("1", "A", "001", "10", None, "foo", 123),
        ("2", "B", "002", "20", None, "bar", 456),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("1", "A", "001", "10", None, "foo", 123, "1", "1"),
        ("2", "B", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC002: Handle nulls in input fields during transformation
def test_TC002_nulls_in_critical_columns(spark, base_schema):
    data = [
        (None, "A", "001", "10", None, "foo", 123),
        ("2", None, "002", "20", None, "bar", 456),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("FIRST_DEFINED", "A", "001", "10", None, "foo", 123, "FIRST_DEFINED", "FIRST_DEFINED"),
        ("2", "FIRST_DEFINED", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC003: Check reject logic on missing fields
def test_TC003_missing_column(spark, base_schema):
    schema_missing = StructType([f for f in base_schema if f.name != "AK_UCK_ID"])
    data = [
        ("A", "001", "10", None, "foo", 123),
    ]
    input_df = spark.createDataFrame(data, schema=schema_missing)
    with pytest.raises(Exception):
        run_full_pipeline(input_df)

# TC004: Verify lookup failure case returns default value
def test_TC004_lookup_failure(spark, base_schema):
    data = [
        (None, "A", "001", "10", None, "foo", 123),
        ("2", "B", "002", "20", None, "bar", 456),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("FIRST_DEFINED", "A", "001", "10", None, "foo", 123, "FIRST_DEFINED", "FIRST_DEFINED"),
        ("2", "B", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC005: Ensure empty input produces empty output without errors
def test_TC005_empty_input(spark, base_schema):
    input_df = spark.createDataFrame([], schema=base_schema)
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame([], schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC006: Deduplication of duplicate primary keys
def test_TC006_deduplication(spark, base_schema):
    data = [
        ("1", "A", "001", "10", None, "foo", 123),
        ("1", "A", "001", "10", None, "foo", 123),
        ("2", "B", "002", "20", None, "bar", 456),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("1", "A", "001", "10", None, "foo", 123, "1", "1"),
        ("2", "B", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC007: Data type mismatch in input columns
def test_TC007_data_type_mismatch(spark, base_schema):
    data = [
        ("1", "A", "001", "10", None, "foo", "not_an_int"),
    ]
    wrong_schema = StructType([
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("LOAD_DATE", DateType(), True),
        StructField("SOME_VALUE", StringType(), True),
        StructField("ANOTHER_FIELD", StringType(), True),  # Should be IntegerType
    ])
    input_df = spark.createDataFrame(data, schema=wrong_schema)
    with pytest.raises(Exception):
        run_full_pipeline(input_df)

# TC008: Reject logic: malformed input row
def test_TC008_malformed_row(spark, base_schema):
    data = [
        ("1", "A", "001", "10", None, "foo"),  # Missing ANOTHER_FIELD
    ]
    with pytest.raises(Exception):
        spark.createDataFrame(data, schema=base_schema)

# TC009: Field order shuffled in input
def test_TC009_field_order_shuffled(spark, base_schema):
    shuffled_schema = StructType([
        StructField("SOME_VALUE", StringType(), True),
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("LOAD_DATE", DateType(), True),
        StructField("ANOTHER_FIELD", IntegerType(), True),
    ])
    data = [
        ("foo", "1", "A", "001", "10", None, 123),
        ("bar", "2", "B", "002", "20", None, 456),
    ]
    input_df = spark.createDataFrame(data, schema=shuffled_schema)
    input_df = input_df.select([f.name for f in base_schema])
    expected_data = [
        ("1", "A", "001", "10", None, "foo", 123, "1", "1"),
        ("2", "B", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC010: All columns NULL
def test_TC010_all_nulls(spark, base_schema):
    data = [
        (None, None, None, None, None, None, None),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("FIRST_DEFINED", "FIRST_DEFINED", "FIRST_DEFINED", "FIRST_DEFINED", None, "FIRST_DEFINED", None, "FIRST_DEFINED", "FIRST_DEFINED"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)
```

---

#### 3. API Cost Consumption:
apiCost: 0.00043752 USD

---