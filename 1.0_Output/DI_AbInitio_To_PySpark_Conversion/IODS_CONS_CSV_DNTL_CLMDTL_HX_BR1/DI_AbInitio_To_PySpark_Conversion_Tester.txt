```
===================================================================
Author:        Ascendion AVA+
Created on:    
Description:   Validation suite for Ab Initio to PySpark Conversion
===================================================================
```

#### 1. Test Case Document:

| Test Case ID | Description | Expected Result |
|--------------|-------------|-----------------|
| TC001 | Validate join with matching keys | Output contains combined data from both sources |
| TC002 | Handle nulls in input fields during transformation | Nulls are processed without error or as per `.xfr` logic |
| TC003 | Check reject logic on missing fields | Row is excluded or logged in reject path equivalent |
| TC004 | Verify lookup failure case returns default value | Default logic executes as expected |
| TC005 | Ensure empty input produces empty output without errors | No exception is thrown |
| TC006 | Data type mismatch in input | Error is raised or handled gracefully |
| TC007 | Reject logic: input row fails business rule | Row is excluded or flagged as rejected |
| TC008 | Deduplication logic | Only unique rows are present in output |
| TC009 | Malformed input data (e.g., string in numeric field) | Error is raised or handled as per logic |
| TC010 | Field order shuffled in input | Transformation works regardless of column order |

---

#### 2. Pytest Script Example:

```python
import pytest
from pyspark.sql import SparkSession
from chispa.dataframe_comparer import assert_df_equality
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql.utils import AnalysisException

# Mock transformation function (replace with actual import)
def transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df):
    # Example transformation logic for demonstration purposes.
    # Join on 'lookup_key', add lookup_value, filter out negative amounts, drop duplicates.
    df = input_df.join(lookup_df, on="lookup_key", how="left")
    df = df.filter((df.amount.isNull()) | (df.amount > 0))
    df = df.dropDuplicates()
    # Reorder columns for output
    out_cols = ["id", "code", "amount", "lookup_key", "lookup_value"]
    for col in out_cols:
        if col not in df.columns:
            df = df.withColumn(col, None)
    return df.select(out_cols)

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[1]").appName("unit-test").getOrCreate()

def test_TC001_transformation_valid_input(spark):
    input_data = [
        (1, "A", 100.0, "X"),
        (2, "B", 200.0, "Y")
    ]
    input_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True)
    ])
    input_df = spark.createDataFrame(input_data, schema=input_schema)

    lookup_data = [
        ("X", "L1"),
        ("Y", "L2")
    ]
    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame(lookup_data, schema=lookup_schema)

    expected_data = [
        (1, "A", 100.0, "X", "L1"),
        (2, "B", 200.0, "Y", "L2")
    ]
    expected_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)

    result_df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

def test_TC002_nulls_in_critical_columns(spark):
    input_data = [
        (None, "A", 100.0, "X"),
        (2, None, None, "Y")
    ]
    input_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True)
    ])
    input_df = spark.createDataFrame(input_data, schema=input_schema)

    lookup_data = [
        ("X", "L1"),
        ("Y", "L2")
    ]
    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame(lookup_data, schema=lookup_schema)

    expected_data = [
        (None, "A", 100.0, "X", "L1"),
        (2, None, None, "Y", "L2")
    ]
    expected_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)

    result_df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

def test_TC003_missing_column_in_input(spark):
    input_data = [
        (1, "A", 100.0)
    ]
    input_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True)
        # missing 'lookup_key'
    ])
    input_df = spark.createDataFrame(input_data, schema=input_schema)

    lookup_data = [("X", "L1")]
    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame(lookup_data, schema=lookup_schema)

    with pytest.raises(AnalysisException):
        transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)

def test_TC004_lookup_failure(spark):
    input_data = [
        (1, "A", 100.0, "Z")  # 'Z' not in lookup
    ]
    input_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True)
    ])
    input_df = spark.createDataFrame(input_data, schema=input_schema)

    lookup_data = [
        ("X", "L1"),
        ("Y", "L2")
    ]
    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame(lookup_data, schema=lookup_schema)

    expected_data = [
        (1, "A", 100.0, "Z", None)
    ]
    expected_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)

    result_df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

def test_TC005_empty_input(spark):
    input_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True)
    ])
    input_df = spark.createDataFrame([], schema=input_schema)

    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame([], schema=lookup_schema)

    expected_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    expected_df = spark.createDataFrame([], schema=expected_schema)

    result_df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

def test_TC006_data_type_mismatch(spark):
    input_data = [
        (1, "A", "not_a_number", "X")
    ]
    input_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", StringType(), True),  # Should be DoubleType
        StructField("lookup_key", StringType(), True)
    ])
    input_df = spark.createDataFrame(input_data, schema=input_schema)

    lookup_data = [("X", "L1")]
    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame(lookup_data, schema=lookup_schema)

    with pytest.raises(Exception):
        transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)

def test_TC007_reject_logic(spark):
    input_data = [
        (1, "A", -10.0, "X"),  # Should be rejected
        (2, "B", 100.0, "Y")   # Should pass
    ]
    input_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True)
    ])
    input_df = spark.createDataFrame(input_data, schema=input_schema)

    lookup_data = [
        ("X", "L1"),
        ("Y", "L2")
    ]
    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame(lookup_data, schema=lookup_schema)

    expected_data = [
        (2, "B", 100.0, "Y", "L2")
    ]
    expected_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)

    result_df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

def test_TC008_deduplication(spark):
    input_data = [
        (1, "A", 100.0, "X"),
        (1, "A", 100.0, "X"),
        (2, "B", 200.0, "Y")
    ]
    input_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True)
    ])
    input_df = spark.createDataFrame(input_data, schema=input_schema)

    lookup_data = [
        ("X", "L1"),
        ("Y", "L2")
    ]
    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame(lookup_data, schema=lookup_schema)

    expected_data = [
        (1, "A", 100.0, "X", "L1"),
        (2, "B", 200.0, "Y", "L2")
    ]
    expected_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)

    result_df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

def test_TC009_malformed_input(spark):
    input_data = [
        (1, "A", None, "X"),  # amount is None, may be required
        (2, "B", "bad", "Y")  # amount is string, should be numeric
    ]
    input_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", StringType(), True),
        StructField("lookup_key", StringType(), True)
    ])
    input_df = spark.createDataFrame(input_data, schema=input_schema)

    lookup_data = [
        ("X", "L1"),
        ("Y", "L2")
    ]
    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame(lookup_data, schema=lookup_schema)

    with pytest.raises(Exception):
        transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)

def test_TC010_field_order_shuffled(spark):
    input_data = [
        ("A", 1, "X", 100.0)
    ]
    input_schema = StructType([
        StructField("code", StringType(), True),
        StructField("id", IntegerType(), True),
        StructField("lookup_key", StringType(), True),
        StructField("amount", DoubleType(), True)
    ])
    input_df = spark.createDataFrame(input_data, schema=input_schema)

    lookup_data = [
        ("X", "L1")
    ]
    lookup_schema = StructType([
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    lookup_df = spark.createDataFrame(lookup_data, schema=lookup_schema)

    expected_data = [
        (1, "A", 100.0, "X", "L1")
    ]
    expected_schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("code", StringType(), True),
        StructField("amount", DoubleType(), True),
        StructField("lookup_key", StringType(), True),
        StructField("lookup_value", StringType(), True)
    ])
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)

    result_df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(input_df, lookup_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)
```

---

#### 3. API Cost Consumption:
apiCost: 0.00043752 USD

---

**Note:**  
- The transformation function is mocked for demonstration; replace with the actual function from your PySpark conversion.
- The test suite covers join logic, null handling, reject logic, lookup miss, deduplication, malformed data, and column order robustness.
- All tests use `chispa`'s `assert_df_equality` for DataFrame comparison.
- Expand schemas and test data as per your actual business logic and Ab Initio conversion details.
