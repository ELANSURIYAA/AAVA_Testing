```
==================================================================
Author:        Ascendion AVA+
Created on:    
Description:   Unit Test Suite for Ab Initio to PySpark Conversion
==================================================================
```

#### 1. Test Case Inventory:

| Test Case ID | Description | Scenario Type | Expected Outcome |
|--------------|-------------|----------------|------------------|
| TC001 | Validate successful transformation with valid input | Happy Path | Transformed DataFrame matches expected output |
| TC002 | Test behavior with NULL values in critical columns | Edge Case | NULLs handled correctly without failure |
| TC003 | Missing column in input | Negative Test | Raise appropriate error |
| TC004 | Lookup failure scenario (join miss) | Edge Case | Rows with no match handled per spec (e.g., dropped or defaulted) |
| TC005 | Empty input dataset | Edge Case | Output DataFrame is empty but no crash |
| TC006 | Deduplication of duplicate primary keys | Happy Path | Only unique rows by primary key in output |
| TC007 | Data type mismatch in input columns | Negative Test | Raise appropriate error or handle gracefully |
| TC008 | Reject logic: malformed input row | Negative Test | Row is rejected or handled as per transformation logic |
| TC009 | Field order shuffled in input | Edge Case | Transformation works regardless of field order |
| TC010 | All columns NULL | Edge Case | Output as per transformation logic (e.g., all NULLs or defaults) |

---

#### 2. Pytest Script Template and Implementation

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
from chispa.dataframe_comparer import assert_df_equality

# Mock transformation functions (replace with actual imports in real test)
def GEN_CSV_FIRST_DEFINED(df):
    # Example: fillna for demonstration
    return df.fillna("FIRST_DEFINED")

def IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2(df):
    # Example: add a column for demonstration
    return df.withColumn("V353S6P2_FLAG", df[df.columns[0]])

def IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P3(df):
    # Example: add a column for demonstration
    return df.withColumn("V353S6P3_FLAG", df[df.columns[0]])

# Primary key columns for deduplication and joins
PRIMARY_KEYS = ["AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO"]

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[1]").appName("unit-test").getOrCreate()

@pytest.fixture
def base_schema():
    return StructType([
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("LOAD_DATE", DateType(), True),
        StructField("SOME_VALUE", StringType(), True),
        StructField("ANOTHER_FIELD", IntegerType(), True),
    ])

def run_full_pipeline(input_df):
    # Simulate the transformation pipeline
    df1 = GEN_CSV_FIRST_DEFINED(input_df)
    df2 = IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2(df1)
    df3 = IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P3(df2)
    dedup_df = df3.dropDuplicates(PRIMARY_KEYS)
    return dedup_df

# TC001: Happy Path
def test_TC001_valid_transformation(spark, base_schema):
    data = [
        ("1", "A", "001", "10", None, "foo", 123),
        ("2", "B", "002", "20", None, "bar", 456),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("1", "A", "001", "10", None, "foo", 123, "1", "1"),
        ("2", "B", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC002: NULLs in critical columns
def test_TC002_nulls_in_critical_columns(spark, base_schema):
    data = [
        (None, "A", "001", "10", None, "foo", 123),
        ("2", None, "002", "20", None, "bar", 456),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("FIRST_DEFINED", "A", "001", "10", None, "foo", 123, "FIRST_DEFINED", "FIRST_DEFINED"),
        ("2", "FIRST_DEFINED", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC003: Missing column in input
def test_TC003_missing_column(spark, base_schema):
    # Remove a primary key column
    schema_missing = StructType([f for f in base_schema if f.name != "AK_UCK_ID"])
    data = [
        # ("1", "A", "001", "10", None, "foo", 123),  # AK_UCK_ID missing
        ("A", "001", "10", None, "foo", 123),
    ]
    input_df = spark.createDataFrame(data, schema=schema_missing)
    with pytest.raises(Exception):
        run_full_pipeline(input_df)

# TC004: Lookup failure scenario (simulate join miss)
def test_TC004_lookup_failure(spark, base_schema):
    # Simulate by having a row with a NULL join key (would not join in real pipeline)
    data = [
        (None, "A", "001", "10", None, "foo", 123),
        ("2", "B", "002", "20", None, "bar", 456),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("FIRST_DEFINED", "A", "001", "10", None, "foo", 123, "FIRST_DEFINED", "FIRST_DEFINED"),
        ("2", "B", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC005: Empty input dataset
def test_TC005_empty_input(spark, base_schema):
    input_df = spark.createDataFrame([], schema=base_schema)
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame([], schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC006: Deduplication of duplicate primary keys
def test_TC006_deduplication(spark, base_schema):
    data = [
        ("1", "A", "001", "10", None, "foo", 123),
        ("1", "A", "001", "10", None, "foo", 123),  # Duplicate
        ("2", "B", "002", "20", None, "bar", 456),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("1", "A", "001", "10", None, "foo", 123, "1", "1"),
        ("2", "B", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC007: Data type mismatch in input columns
def test_TC007_data_type_mismatch(spark, base_schema):
    # Provide a string where integer is expected
    data = [
        ("1", "A", "001", "10", None, "foo", "not_an_int"),
    ]
    # Override schema to force wrong type
    wrong_schema = StructType([
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("LOAD_DATE", DateType(), True),
        StructField("SOME_VALUE", StringType(), True),
        StructField("ANOTHER_FIELD", StringType(), True),  # Should be IntegerType
    ])
    input_df = spark.createDataFrame(data, schema=wrong_schema)
    with pytest.raises(Exception):
        run_full_pipeline(input_df)

# TC008: Reject logic: malformed input row
def test_TC008_malformed_row(spark, base_schema):
    # Too few columns
    data = [
        ("1", "A", "001", "10", None, "foo"),  # Missing ANOTHER_FIELD
    ]
    with pytest.raises(Exception):
        spark.createDataFrame(data, schema=base_schema)

# TC009: Field order shuffled in input
def test_TC009_field_order_shuffled(spark, base_schema):
    # Reorder columns
    shuffled_schema = StructType([
        StructField("SOME_VALUE", StringType(), True),
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("LOAD_DATE", DateType(), True),
        StructField("ANOTHER_FIELD", IntegerType(), True),
    ])
    data = [
        ("foo", "1", "A", "001", "10", None, 123),
        ("bar", "2", "B", "002", "20", None, 456),
    ]
    input_df = spark.createDataFrame(data, schema=shuffled_schema)
    # Reorder to match expected schema for comparison
    input_df = input_df.select([f.name for f in base_schema])
    expected_data = [
        ("1", "A", "001", "10", None, "foo", 123, "1", "1"),
        ("2", "B", "002", "20", None, "bar", 456, "2", "2"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)

# TC010: All columns NULL
def test_TC010_all_nulls(spark, base_schema):
    data = [
        (None, None, None, None, None, None, None),
    ]
    input_df = spark.createDataFrame(data, schema=base_schema)
    expected_data = [
        ("FIRST_DEFINED", "FIRST_DEFINED", "FIRST_DEFINED", "FIRST_DEFINED", None, "FIRST_DEFINED", None, "FIRST_DEFINED", "FIRST_DEFINED"),
    ]
    expected_schema = base_schema.add("V353S6P2_FLAG", StringType(), True).add("V353S6P3_FLAG", StringType(), True)
    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)
    result_df = run_full_pipeline(input_df)
    assert_df_equality(result_df, expected_df, ignore_nullable=True)
```

---

#### 3. API Cost:
apiCost: 0.00043752 USD

---

**Instructions for use:**
- Replace the mock transformation functions (`GEN_CSV_FIRST_DEFINED`, etc.) with the actual implementations from your `xfr_module`.
- Adjust the schemas and expected data to match the real business logic and output of your pipeline.
- This suite covers happy path, edge cases, negative tests, deduplication, and reject logic as per the Ab Initio to PySpark conversion requirements.
