```
==================================================================
Author:        Ascendion AVA+
Created on:    
Description:   Unit Test Suite for Ab Initio to PySpark Conversion
==================================================================
```

#### 1. Test Case Inventory:
| Test Case ID | Description | Scenario Type | Expected Outcome |
|--------------|-------------|----------------|------------------|
| TC001 | Validate successful transformation with valid input | Happy Path | Transformed DataFrame matches expected output |
| TC002 | Test behavior with NULL values in critical columns | Edge Case | NULLs handled correctly without failure |
| TC003 | Missing column in input | Negative Test | Raise appropriate error |
| TC004 | Lookup failure scenario (join miss) | Edge Case | Rows with no match handled per spec (e.g., dropped or defaulted) |
| TC005 | Empty input dataset | Edge Case | Output DataFrame is empty but no crash |
| TC006 | Deduplication logic | Happy Path | Only unique records by PK are present in output |
| TC007 | Malformed input data (wrong types) | Negative Test | Raise appropriate error or handle gracefully |
| TC008 | Reject handling (if implemented in xfr) | Edge Case | Rejected records are handled per spec |
| TC009 | Boundary values (e.g., max/min dates, numbers) | Edge Case | Boundary values processed correctly |
| TC010 | Unexpected schema/field order | Negative Test | Raise error or handle as per spec |

---

#### 2. Pytest Script Template and Full Test Suite

```python
import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
from chispa.dataframe_comparer import assert_df_equality
from pyspark.sql import functions as F

# Mock transformation functions (replace with actual imports in real test)
from xfr_module import (
    transform_table_adaptor,
    transform_table_adaptor_first,
    transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2,
    transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p3
)

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[1]").appName("unit-test").getOrCreate()

# Helper: Minimal schema for PK and a few business columns
def base_schema():
    return StructType([
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("LOAD_DATE", StringType(), True),
        StructField("SOME_BUSINESS_COL", StringType(), True),
        StructField("AUDIT_INSERT_TS", TimestampType(), True),
        StructField("INSERT_FILE_ID", StringType(), True)
    ])

# Helper: Run the pipeline up to the final_df output
def run_pipeline(input_df, filectlnum="12345"):
    df = transform_table_adaptor(input_df)
    df = transform_table_adaptor_first(df)
    df = df.dropDuplicates([
        'AK_UCK_ID', 'AK_UCK_ID_PREFIX_CD', 'AK_UCK_ID_SEGMENT_NO', 'AK_SUBMT_SVC_LN_NO'
    ])
    df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p2(df, filectlnum=filectlnum)
    df = transform_iods_cons_csv_dntl_clmdtl_hx_br1_v353s6p3(df, filectlnum=filectlnum)
    return df

# TC001: Happy Path
def test_TC001_happy_path(spark):
    input_data = [
        ("1001", "A", "1", "10", "2023-01-01", "VAL1", None, None),
        ("1002", "A", "1", "20", "2023-01-02", "VAL2", None, None)
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema())
    # Expected output: apply all transformations manually or mock expected
    expected_data = [
        # Assuming xfrs append/transform SOME_BUSINESS_COL to "VAL1_XFR", etc.
        ("1001", "A", "1", "10", "2023-01-01", "VAL1_XFR", None, "12345"),
        ("1002", "A", "1", "20", "2023-01-02", "VAL2_XFR", None, "12345")
    ]
    expected_df = spark.createDataFrame(expected_data, schema=base_schema())
    result_df = run_pipeline(input_df)
    assert_df_equality(
        result_df.select("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO", "SOME_BUSINESS_COL", "INSERT_FILE_ID"),
        expected_df.select("AK_UCK_ID", "AK_UCK_ID_PREFIX_CD", "AK_UCK_ID_SEGMENT_NO", "AK_SUBMT_SVC_LN_NO", "SOME_BUSINESS_COL", "INSERT_FILE_ID"),
        ignore_nullable=True
    )

# TC002: Nulls in critical columns
def test_TC002_null_critical_columns(spark):
    input_data = [
        (None, "A", "1", "10", "2023-01-01", "VAL1", None, None),
        ("1002", None, "1", "20", "2023-01-02", "VAL2", None, None)
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema())
    # Expected: rows with null PKs may be dropped or processed as per logic
    # Here, let's assume they are dropped by dedup or join logic
    expected_data = [
        # Only the row with all PKs non-null survives
        # In this case, none, as both have null in PKs
    ]
    expected_df = spark.createDataFrame(expected_data, schema=base_schema())
    result_df = run_pipeline(input_df)
    assert result_df.count() == 0

# TC003: Missing column in input
def test_TC003_missing_column(spark):
    # Remove a PK column
    schema = StructType([
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        # "AK_SUBMT_SVC_LN_NO" missing
        StructField("LOAD_DATE", StringType(), True),
        StructField("SOME_BUSINESS_COL", StringType(), True),
        StructField("AUDIT_INSERT_TS", TimestampType(), True),
        StructField("INSERT_FILE_ID", StringType(), True)
    ])
    input_data = [
        ("1001", "A", "1", "2023-01-01", "VAL1", None, None)
    ]
    input_df = spark.createDataFrame(input_data, schema=schema)
    with pytest.raises(Exception):
        run_pipeline(input_df)

# TC004: Lookup failure scenario (join miss)
def test_TC004_lookup_failure(spark):
    # Simulate a row that would not find a match in a join (e.g., by PK)
    # For this test, we assume the xfr/join logic drops unmatched rows
    input_data = [
        ("9999", "Z", "9", "99", "2023-01-01", "VALX", None, None)
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema())
    expected_data = []  # No match, so output is empty
    expected_df = spark.createDataFrame(expected_data, schema=base_schema())
    result_df = run_pipeline(input_df)
    assert result_df.count() == 0

# TC005: Empty input dataset
def test_TC005_empty_input(spark):
    input_df = spark.createDataFrame([], schema=base_schema())
    result_df = run_pipeline(input_df)
    assert result_df.count() == 0

# TC006: Deduplication logic
def test_TC006_deduplication(spark):
    input_data = [
        ("1001", "A", "1", "10", "2023-01-01", "VAL1", None, None),
        ("1001", "A", "1", "10", "2023-01-01", "VAL1_DUP", None, None)
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema())
    # Only one row should survive deduplication
    result_df = run_pipeline(input_df)
    assert result_df.count() == 1

# TC007: Malformed input data (wrong types)
def test_TC007_malformed_input(spark):
    # LOAD_DATE as int instead of string
    input_data = [
        ("1001", "A", "1", "10", 20230101, "VAL1", None, None)
    ]
    schema = StructType([
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("LOAD_DATE", IntegerType(), True),  # Should be StringType
        StructField("SOME_BUSINESS_COL", StringType(), True),
        StructField("AUDIT_INSERT_TS", TimestampType(), True),
        StructField("INSERT_FILE_ID", StringType(), True)
    ])
    input_df = spark.createDataFrame(input_data, schema=schema)
    with pytest.raises(Exception):
        run_pipeline(input_df)

# TC008: Reject handling (if implemented in xfr)
def test_TC008_reject_handling(spark):
    # If xfr logic rejects rows with specific values, test that
    input_data = [
        ("1001", "A", "1", "10", "2023-01-01", "REJECT_ME", None, None),
        ("1002", "A", "1", "20", "2023-01-02", "VAL2", None, None)
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema())
    # Assume "REJECT_ME" is rejected by xfr, only second row survives
    expected_data = [
        ("1002", "A", "1", "20", "2023-01-02", "VAL2_XFR", None, "12345")
    ]
    expected_df = spark.createDataFrame(expected_data, schema=base_schema())
    result_df = run_pipeline(input_df)
    assert_df_equality(
        result_df.select("AK_UCK_ID", "AK_SUBMT_SVC_LN_NO", "SOME_BUSINESS_COL"),
        expected_df.select("AK_UCK_ID", "AK_SUBMT_SVC_LN_NO", "SOME_BUSINESS_COL"),
        ignore_nullable=True
    )

# TC009: Boundary values
def test_TC009_boundary_values(spark):
    input_data = [
        ("1001", "A", "1", "10", "1900-01-01", "MINVAL", None, None),
        ("1002", "A", "1", "20", "9999-12-31", "MAXVAL", None, None)
    ]
    input_df = spark.createDataFrame(input_data, schema=base_schema())
    expected_data = [
        ("1001", "A", "1", "10", "1900-01-01", "MINVAL_XFR", None, "12345"),
        ("1002", "A", "1", "20", "9999-12-31", "MAXVAL_XFR", None, "12345")
    ]
    expected_df = spark.createDataFrame(expected_data, schema=base_schema())
    result_df = run_pipeline(input_df)
    assert_df_equality(
        result_df.select("AK_UCK_ID", "AK_SUBMT_SVC_LN_NO", "SOME_BUSINESS_COL"),
        expected_df.select("AK_UCK_ID", "AK_SUBMT_SVC_LN_NO", "SOME_BUSINESS_COL"),
        ignore_nullable=True
    )

# TC010: Unexpected schema/field order
def test_TC010_unexpected_schema_order(spark):
    # Columns in different order
    schema = StructType([
        StructField("SOME_BUSINESS_COL", StringType(), True),
        StructField("AK_UCK_ID", StringType(), True),
        StructField("AK_UCK_ID_PREFIX_CD", StringType(), True),
        StructField("AK_UCK_ID_SEGMENT_NO", StringType(), True),
        StructField("AK_SUBMT_SVC_LN_NO", StringType(), True),
        StructField("LOAD_DATE", StringType(), True),
        StructField("AUDIT_INSERT_TS", TimestampType(), True),
        StructField("INSERT_FILE_ID", StringType(), True)
    ])
    input_data = [
        ("VAL1", "1001", "A", "1", "10", "2023-01-01", None, None)
    ]
    input_df = spark.createDataFrame(input_data, schema=schema)
    # Should still work if logic is robust to column order, else error
    try:
        result_df = run_pipeline(input_df)
        assert result_df.count() == 1
    except Exception:
        # If error is expected, test passes
        assert True

```

---

#### 3. API Cost:
apiCost: 0.00043752 USD

---

**Note:**  
- The above test suite assumes that the transformation functions (`transform_table_adaptor`, etc.) are available and that their business logic is as described or mocked for test purposes.
- Adjust expected outputs as per the actual transformation logic in your xfr modules.
- The test suite covers all major Ab Initio-to-PySpark migration risks: joins, deduplication, nulls, schema, edge cases, and error handling.
- For real-world use, expand the schemas and expected outputs to match your actual business rules and data contracts.
