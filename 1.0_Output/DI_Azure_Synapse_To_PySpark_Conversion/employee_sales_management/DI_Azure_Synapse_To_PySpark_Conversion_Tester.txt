=============================================
Author:        AAVA
Created on:   
Description:   Unit tests and Pytest script for validating the Databricks PySpark implementation of the sales fact loading process, including data quality checks, transformation logic, error handling, and audit logging.
=============================================

### Test Case List

| Test Case ID | Test Case Description                                                                 | Expected Outcome                                                                                  |
|--------------|--------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All staging data valid, all dimension joins succeed                      | All rows loaded to Fact_Sales, Audit_Log shows correct counts, DQ_Failures empty                 |
| TC02         | Edge: Staging contains NULL Customer_ID                                              | Rows with NULL Customer_ID rejected, logged to DQ_Failures, not loaded to Fact_Sales              |
| TC03         | Edge: Staging contains Quantity <= 0                                                 | Rows with Quantity <= 0 rejected, logged to DQ_Failures, not loaded to Fact_Sales                 |
| TC04         | Edge: Staging table is empty                                                         | No rows loaded or rejected, Audit_Log reflects zero counts, DQ_Failures empty                     |
| TC05         | Edge: Staging contains rows with missing dimension join (Customer_ID not in Dim)     | Rows without matching Customer_ID in Dim_Customer not loaded, not rejected as DQ failure          |
| TC06         | Edge: Staging contains rows with Sales_Date not in Dim_Date                          | Rows without matching Sales_Date in Dim_Date not loaded, not rejected as DQ failure               |
| TC07         | Error: Staging table missing required columns                                        | Job fails, Audit_Log status is FAILED, error message logged                                       |
| TC08         | Error: Invalid data types in staging columns (e.g., Quantity as string)              | Job fails, Audit_Log status is FAILED, error message logged                                       |
| TC09         | Edge: All rows invalid (all fail DQ checks)                                          | No rows loaded to Fact_Sales, all rows logged to DQ_Failures, Audit_Log reflects zero inserted    |
| TC10         | Edge: Duplicate Transaction_ID in staging                                            | All valid rows loaded, duplicates handled as per PySpark insert (no explicit deduplication logic) |

---

### Pytest Script


import pytest
import pandas as pd
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import *
from pyspark.sql.utils import AnalysisException

# Assume the main() function and helpers are imported from the PySpark script

# Helper to create SparkSession for testing
@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("unit-test").getOrCreate()
    yield spark
    spark.stop()

# Helper to clear and create test tables
def setup_tables(spark, sales_data=None, dim_customer_data=None, dim_date_data=None):
    # Drop and recreate tables as Delta tables for test isolation
    spark.sql("DROP TABLE IF EXISTS stg.Sales_Transactions")
    spark.sql("DROP TABLE IF EXISTS dw.Dim_Customer")
    spark.sql("DROP TABLE IF EXISTS dw.Dim_Date")
    spark.sql("DROP TABLE IF EXISTS dw.Fact_Sales")
    spark.sql("DROP TABLE IF EXISTS dw.DQ_Failures")
    spark.sql("DROP TABLE IF EXISTS dw.Audit_Log")

    # Create schemas
    sales_schema = StructType([
        StructField("Transaction_ID", LongType(), False),
        StructField("Customer_ID", LongType(), True),
        StructField("Product_ID", LongType(), False),
        StructField("Sales_Date", StringType(), False),
        StructField("Quantity", IntegerType(), True),
        StructField("Unit_Price", DoubleType(), True)
    ])
    customer_schema = StructType([
        StructField("Customer_ID", LongType(), False),
        StructField("Customer_Segment", StringType(), False)
    ])
    date_schema = StructType([
        StructField("Date_Value", DateType(), False),
        StructField("Region_ID", IntegerType(), False)
    ])

    # Create DataFrames
    sales_df = spark.createDataFrame(sales_data or [], schema=sales_schema)
    customer_df = spark.createDataFrame(dim_customer_data or [], schema=customer_schema)
    date_df = spark.createDataFrame(dim_date_data or [], schema=date_schema)

    # Save as Delta tables
    sales_df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    customer_df.write.format("delta").mode("overwrite").saveAsTable("dw.Dim_Customer")
    date_df.write.format("delta").mode("overwrite").saveAsTable("dw.Dim_Date")

    # Empty target tables
    spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType(), False),
        StructField("Customer_ID", LongType(), True),
        StructField("Product_ID", LongType(), False),
        StructField("Sales_Date", StringType(), False),
        StructField("Quantity", IntegerType(), True),
        StructField("Unit_Price", DoubleType(), True),
        StructField("Total_Sales_Amount", DoubleType(), True),
        StructField("Region_ID", IntegerType(), True),
        StructField("Customer_Segment", StringType(), True),
        StructField("Load_Timestamp", TimestampType(), True),
        StructField("Batch_ID", StringType(), True)
    ])).write.format("delta").mode("overwrite").saveAsTable("dw.Fact_Sales")

    spark.createDataFrame([], StructType([
        StructField("Transaction_ID", LongType(), False),
        StructField("Reason", StringType(), True),
        StructField("Logged_Timestamp", TimestampType(), True),
        StructField("Batch_ID", StringType(), True)
    ])).write.format("delta").mode("overwrite").saveAsTable("dw.DQ_Failures")

    spark.createDataFrame([], StructType([
        StructField("Batch_ID", StringType(), False),
        StructField("Procedure_Name", StringType(), True),
        StructField("Start_Time", TimestampType(), True),
        StructField("End_Time", TimestampType(), True),
        StructField("Rows_Inserted", IntegerType(), True),
        StructField("Rows_Rejected", IntegerType(), True),
        StructField("Status", StringType(), True),
        StructField("Message", StringType(), True)
    ])).write.format("delta").mode("overwrite").saveAsTable("dw.Audit_Log")

# Helper to fetch table as pandas DataFrame
def fetch_table(spark, table):
    return spark.table(table).toPandas()

# TC01: Happy path
def test_happy_path(spark):
    sales_data = [
        (1, 10, 100, "2023-01-01", 2, 50.0),
        (2, 20, 101, "2023-01-02", 1, 30.0),
    ]
    dim_customer_data = [
        (10, "Retail"),
        (20, "Wholesale"),
    ]
    dim_date_data = [
        (pd.to_datetime("2023-01-01"), 1),
        (pd.to_datetime("2023-01-02"), 2),
    ]
    setup_tables(spark, sales_data, dim_customer_data, dim_date_data)
    from main import main  # Import the main function from the PySpark script
    main()
    fact = fetch_table(spark, "dw.Fact_Sales")
    assert len(fact) == 2
    assert set(fact["Customer_Segment"]) == {"Retail", "Wholesale"}
    dq = fetch_table(spark, "dw.DQ_Failures")
    assert dq.empty
    audit = fetch_table(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Rows_Inserted"] == 2
    assert audit.iloc[-1]["Rows_Rejected"] == 0
    assert audit.iloc[-1]["Status"] == "COMPLETED"

# TC02: NULL Customer_ID
def test_null_customer_id(spark):
    sales_data = [
        (1, None, 100, "2023-01-01", 2, 50.0),
        (2, 20, 101, "2023-01-02", 1, 30.0),
    ]
    dim_customer_data = [
        (20, "Wholesale"),
    ]
    dim_date_data = [
        (pd.to_datetime("2023-01-01"), 1),
        (pd.to_datetime("2023-01-02"), 2),
    ]
    setup_tables(spark, sales_data, dim_customer_data, dim_date_data)
    from main import main
    main()
    fact = fetch_table(spark, "dw.Fact_Sales")
    assert len(fact) == 1
    assert fact.iloc[0]["Customer_ID"] == 20
    dq = fetch_table(spark, "dw.DQ_Failures")
    assert len(dq) == 1
    assert dq.iloc[0]["Reason"] == "Missing CustomerID"
    audit = fetch_table(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Rows_Inserted"] == 1
    assert audit.iloc[-1]["Rows_Rejected"] == 1

# TC03: Quantity <= 0
def test_invalid_quantity(spark):
    sales_data = [
        (1, 10, 100, "2023-01-01", 0, 50.0),
        (2, 20, 101, "2023-01-02", -1, 30.0),
        (3, 30, 102, "2023-01-03", 2, 40.0),
    ]
    dim_customer_data = [
        (10, "Retail"),
        (20, "Wholesale"),
        (30, "Online"),
    ]
    dim_date_data = [
        (pd.to_datetime("2023-01-01"), 1),
        (pd.to_datetime("2023-01-02"), 2),
        (pd.to_datetime("2023-01-03"), 3),
    ]
    setup_tables(spark, sales_data, dim_customer_data, dim_date_data)
    from main import main
    main()
    fact = fetch_table(spark, "dw.Fact_Sales")
    assert len(fact) == 1
    assert fact.iloc[0]["Customer_ID"] == 30
    dq = fetch_table(spark, "dw.DQ_Failures")
    assert len(dq) == 2
    assert set(dq["Reason"]) == {"Invalid Quantity"}
    audit = fetch_table(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Rows_Inserted"] == 1
    assert audit.iloc[-1]["Rows_Rejected"] == 2

# TC04: Empty staging table
def test_empty_staging(spark):
    setup_tables(spark, [], [(10, "Retail")], [(pd.to_datetime("2023-01-01"), 1)])
    from main import main
    main()
    fact = fetch_table(spark, "dw.Fact_Sales")
    assert fact.empty
    dq = fetch_table(spark, "dw.DQ_Failures")
    assert dq.empty
    audit = fetch_table(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Rows_Inserted"] == 0
    assert audit.iloc[-1]["Rows_Rejected"] == 0

# TC05: Missing dimension join (Customer_ID not in Dim_Customer)
def test_missing_customer_dim(spark):
    sales_data = [
        (1, 99, 100, "2023-01-01", 2, 50.0),
        (2, 20, 101, "2023-01-02", 1, 30.0),
    ]
    dim_customer_data = [
        (20, "Wholesale"),
    ]
    dim_date_data = [
        (pd.to_datetime("2023-01-01"), 1),
        (pd.to_datetime("2023-01-02"), 2),
    ]
    setup_tables(spark, sales_data, dim_customer_data, dim_date_data)
    from main import main
    main()
    fact = fetch_table(spark, "dw.Fact_Sales")
    assert len(fact) == 1
    assert fact.iloc[0]["Customer_ID"] == 20
    dq = fetch_table(spark, "dw.DQ_Failures")
    assert dq.empty  # Not a DQ failure, just not loaded

# TC06: Missing dimension join (Sales_Date not in Dim_Date)
def test_missing_date_dim(spark):
    sales_data = [
        (1, 10, 100, "2023-01-05", 2, 50.0),
        (2, 20, 101, "2023-01-02", 1, 30.0),
    ]
    dim_customer_data = [
        (10, "Retail"),
        (20, "Wholesale"),
    ]
    dim_date_data = [
        (pd.to_datetime("2023-01-02"), 2),
    ]
    setup_tables(spark, sales_data, dim_customer_data, dim_date_data)
    from main import main
    main()
    fact = fetch_table(spark, "dw.Fact_Sales")
    assert len(fact) == 1
    assert fact.iloc[0]["Customer_ID"] == 20
    dq = fetch_table(spark, "dw.DQ_Failures")
    assert dq.empty

# TC07: Staging missing required columns
def test_missing_columns(spark):
    # Omit 'Quantity' column
    sales_schema = StructType([
        StructField("Transaction_ID", LongType(), False),
        StructField("Customer_ID", LongType(), True),
        StructField("Product_ID", LongType(), False),
        StructField("Sales_Date", StringType(), False),
        StructField("Unit_Price", DoubleType(), True)
    ])
    sales_data = [
        (1, 10, 100, "2023-01-01", 50.0),
    ]
    customer_data = [(10, "Retail")]
    date_data = [(pd.to_datetime("2023-01-01"), 1)]
    spark.sql("DROP TABLE IF EXISTS stg.Sales_Transactions")
    spark.createDataFrame(sales_data, schema=sales_schema).write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    setup_tables(spark, [], customer_data, date_data)  # Only to create other tables
    from main import main
    with pytest.raises(Exception):
        main()
    audit = fetch_table(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Status"] == "FAILED"
    assert "Quantity" in audit.iloc[-1]["Message"]

# TC08: Invalid data types in staging columns
def test_invalid_datatype(spark):
    sales_data = [
        (1, 10, 100, "2023-01-01", "two", 50.0),
    ]
    sales_schema = StructType([
        StructField("Transaction_ID", LongType(), False),
        StructField("Customer_ID", LongType(), True),
        StructField("Product_ID", LongType(), False),
        StructField("Sales_Date", StringType(), False),
        StructField("Quantity", StringType(), True),  # Should be IntegerType
        StructField("Unit_Price", DoubleType(), True)
    ])
    customer_data = [(10, "Retail")]
    date_data = [(pd.to_datetime("2023-01-01"), 1)]
    spark.sql("DROP TABLE IF EXISTS stg.Sales_Transactions")
    spark.createDataFrame(sales_data, schema=sales_schema).write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    setup_tables(spark, [], customer_data, date_data)
    from main import main
    with pytest.raises(Exception):
        main()
    audit = fetch_table(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Status"] == "FAILED"
    assert "invalid" in audit.iloc[-1]["Message"].lower() or "error" in audit.iloc[-1]["Message"].lower()

# TC09: All rows invalid (fail DQ checks)
def test_all_invalid_rows(spark):
    sales_data = [
        (1, None, 100, "2023-01-01", 0, 50.0),
        (2, None, 101, "2023-01-02", -1, 30.0),
    ]
    dim_customer_data = [
        (10, "Retail"),
    ]
    dim_date_data = [
        (pd.to_datetime("2023-01-01"), 1),
        (pd.to_datetime("2023-01-02"), 2),
    ]
    setup_tables(spark, sales_data, dim_customer_data, dim_date_data)
    from main import main
    main()
    fact = fetch_table(spark, "dw.Fact_Sales")
    assert fact.empty
    dq = fetch_table(spark, "dw.DQ_Failures")
    assert len(dq) == 2
    audit = fetch_table(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Rows_Inserted"] == 0
    assert audit.iloc[-1]["Rows_Rejected"] == 2

# TC10: Duplicate Transaction_ID
def test_duplicate_transaction_id(spark):
    sales_data = [
        (1, 10, 100, "2023-01-01", 2, 50.0),
        (1, 10, 100, "2023-01-01", 2, 50.0),
        (2, 20, 101, "2023-01-02", 1, 30.0),
    ]
    dim_customer_data = [
        (10, "Retail"),
        (20, "Wholesale"),
    ]
    dim_date_data = [
        (pd.to_datetime("2023-01-01"), 1),
        (pd.to_datetime("2023-01-02"), 2),
    ]
    setup_tables(spark, sales_data, dim_customer_data, dim_date_data)
    from main import main
    main()
    fact = fetch_table(spark, "dw.Fact_Sales")
    assert len(fact) == 3  # All rows loaded, duplicates allowed
    assert (fact["Transaction_ID"] == 1).sum() == 2
    dq = fetch_table(spark, "dw.DQ_Failures")
    assert dq.empty


---

apiCost: 0.0047 USD