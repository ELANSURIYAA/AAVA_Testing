=============================================
Author:    AAVA
Created on:    
Description:   Pytest-based unit test suite for validating the Databricks PySpark implementation of the dw.sp_load_sales_fact ETL process, including data quality checks, fact table loading, auditing, and error handling.
=============================================

## Test Case List

| Test Case ID | Test Case Description                                                                 | Expected Outcome                                                                                           |
|--------------|--------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All staging data is valid and matches dimension tables                    | All rows loaded to dw.Fact_Sales; Audit_Log and DQ_Failures updated correctly; staging truncated           |
| TC02         | Edge: Staging contains rows with NULL Customer_ID                                    | Those rows rejected, logged in DQ_Failures, not loaded to Fact_Sales; Audit_Log reflects rejections        |
| TC03         | Edge: Staging contains rows with Quantity <= 0                                       | Those rows rejected, logged in DQ_Failures, not loaded to Fact_Sales; Audit_Log reflects rejections        |
| TC04         | Edge: Staging contains rows with both NULL Customer_ID and invalid Quantity          | Both rejection reasons logged; rows not loaded; DQ_Failures contains both reasons                          |
| TC05         | Edge: Empty staging table                                                            | No rows loaded or rejected; Audit_Log and DQ_Failures reflect zero activity                                |
| TC06         | Edge: Staging rows with missing dimension references (Customer_ID or Date not found) | Those rows not loaded; only valid dimension matches loaded; Audit_Log reflects correct counts              |
| TC07         | Error: Staging table missing required columns                                        | Process fails; Audit_Log status is FAILED; error message logged                                            |
| TC08         | Error: Unexpected data types in staging columns                                      | Process fails; Audit_Log status is FAILED; error message logged                                            |
| TC09         | Edge: Duplicate Transaction_ID in staging                                            | Both rows processed (no deduplication in logic); both loaded if valid; Audit_Log reflects correct count    |
| TC10         | Edge: Large batch (performance/volume test)                                          | All valid rows loaded; process completes successfully; Audit_Log reflects correct counts                   |

---

## Pytest Script


import pytest
import pandas as pd
import uuid
from datetime import datetime
from pyspark.sql import SparkSession, Row, functions as F
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType, LongType

# ------------------ Helper Functions ------------------

def create_spark():
    return SparkSession.builder.master("local[1]").appName("TestSalesFactLoad").getOrCreate()

def reset_tables(spark):
    # Drop and recreate all relevant tables for isolation
    spark.sql("DROP TABLE IF EXISTS stg.Sales_Transactions")
    spark.sql("DROP TABLE IF EXISTS dw.Dim_Customer")
    spark.sql("DROP TABLE IF EXISTS dw.Dim_Date")
    spark.sql("DROP TABLE IF EXISTS dw.Fact_Sales")
    spark.sql("DROP TABLE IF EXISTS dw.Audit_Log")
    spark.sql("DROP TABLE IF EXISTS dw.DQ_Failures")
    # Create empty tables with expected schema
    spark.createDataFrame([], schema=StructType([
        StructField("Transaction_ID", LongType()),
        StructField("Customer_ID", IntegerType()),
        StructField("Product_ID", IntegerType()),
        StructField("Sales_Date", StringType()),
        StructField("Quantity", IntegerType()),
        StructField("Unit_Price", DoubleType())
    ])).write.format("delta").saveAsTable("stg.Sales_Transactions")
    spark.createDataFrame([
        Row(Customer_ID=1, Customer_Segment="Retail"),
        Row(Customer_ID=2, Customer_Segment="Wholesale"),
    ]).write.format("delta").mode("overwrite").saveAsTable("dw.Dim_Customer")
    spark.createDataFrame([
        Row(Date_Value=datetime(2023, 1, 1).date(), Region_ID=10),
        Row(Date_Value=datetime(2023, 1, 2).date(), Region_ID=20),
    ]).write.format("delta").mode("overwrite").saveAsTable("dw.Dim_Date")
    spark.createDataFrame([], schema=StructType([
        StructField("Transaction_ID", LongType()),
        StructField("Customer_ID", IntegerType()),
        StructField("Product_ID", IntegerType()),
        StructField("Sales_Date", StringType()),
        StructField("Quantity", IntegerType()),
        StructField("Unit_Price", DoubleType()),
        StructField("Total_Sales_Amount", DoubleType()),
        StructField("Region_ID", IntegerType()),
        StructField("Customer_Segment", StringType()),
        StructField("Load_Timestamp", TimestampType()),
        StructField("Batch_ID", StringType())
    ])).write.format("delta").saveAsTable("dw.Fact_Sales")
    spark.createDataFrame([], schema=StructType([
        StructField("Batch_ID", StringType()),
        StructField("Procedure_Name", StringType()),
        StructField("Start_Time", TimestampType()),
        StructField("Status", StringType()),
        StructField("Message", StringType()),
        StructField("Rows_Inserted", IntegerType()),
        StructField("Rows_Rejected", IntegerType()),
        StructField("End_Time", TimestampType())
    ])).write.format("delta").saveAsTable("dw.Audit_Log")
    spark.createDataFrame([], schema=StructType([
        StructField("Transaction_ID", LongType()),
        StructField("Reason", StringType()),
        StructField("Logged_Timestamp", TimestampType()),
        StructField("Batch_ID", StringType())
    ])).write.format("delta").saveAsTable("dw.DQ_Failures")

def run_etl_script(spark):
    # The ETL process as per the provided PySpark code
    from datetime import datetime
    import uuid
    import traceback

    batch_id = str(uuid.uuid4())
    start_time = datetime.utcnow()
    end_time = None
    rows_inserted = 0
    rows_rejected = 0
    error_message = None
    proc_name = "sp_load_sales_fact"

    audit_log_df = spark.createDataFrame(
        [(batch_id, proc_name, start_time, "STARTED", "Sales Fact Load Initiated", None, None, None)],
        ["Batch_ID", "Procedure_Name", "Start_Time", "Status", "Message", "Rows_Inserted", "Rows_Rejected", "End_Time"]
    )
    audit_log_df.write.format("delta").mode("append").saveAsTable("dw.Audit_Log")

    try:
        stg_sales_df = spark.table("stg.Sales_Transactions")

        invalid_customer_df = stg_sales_df.filter(F.col("Customer_ID").isNull()) \
            .select(F.col("Transaction_ID"), F.lit("Missing CustomerID").alias("Reason"))
        invalid_quantity_df = stg_sales_df.filter(F.col("Quantity") <= 0) \
            .select(F.col("Transaction_ID"), F.lit("Invalid Quantity").alias("Reason"))
        invalid_rows_df = invalid_customer_df.unionByName(invalid_quantity_df).dropDuplicates()

        valid_sales_df = stg_sales_df.join(
            invalid_rows_df.select("Transaction_ID"),
            on="Transaction_ID",
            how="left_anti"
        )
        rows_rejected = invalid_rows_df.count()

        dim_customer_df = spark.table("dw.Dim_Customer")
        dim_date_df = spark.table("dw.Dim_Date")

        dim_customer_df = F.broadcast(dim_customer_df)
        dim_date_df = F.broadcast(dim_date_df)

        transformed_df = valid_sales_df \
            .join(dim_customer_df, "Customer_ID", "inner") \
            .join(dim_date_df, F.to_date(valid_sales_df["Sales_Date"]) == dim_date_df["Date_Value"], "inner") \
            .select(
                valid_sales_df["Transaction_ID"],
                valid_sales_df["Customer_ID"],
                valid_sales_df["Product_ID"],
                valid_sales_df["Sales_Date"],
                valid_sales_df["Quantity"],
                valid_sales_df["Unit_Price"],
                (valid_sales_df["Quantity"] * valid_sales_df["Unit_Price"]).alias("Total_Sales_Amount"),
                dim_date_df["Region_ID"],
                dim_customer_df["Customer_Segment"],
                F.current_timestamp().alias("Load_Timestamp"),
                F.lit(batch_id).alias("Batch_ID")
            )

        transformed_df.write.format("delta").mode("append").saveAsTable("dw.Fact_Sales")
        rows_inserted = transformed_df.count()

        spark.sql("TRUNCATE TABLE stg.Sales_Transactions")

        dq_failures_df = invalid_rows_df \
            .withColumn("Logged_Timestamp", F.current_timestamp()) \
            .withColumn("Batch_ID", F.lit(batch_id))
        dq_failures_df.write.format("delta").mode("append").saveAsTable("dw.DQ_Failures")

        end_time = datetime.utcnow()
        spark.sql(f"""
            UPDATE dw.Audit_Log
            SET End_Time = TIMESTAMP('{end_time}'),
                Rows_Inserted = {rows_inserted},
                Rows_Rejected = {rows_rejected},
                Status = 'COMPLETED',
                Message = 'Inserted {rows_inserted} rows; Rejected {rows_rejected} rows.'
            WHERE Batch_ID = '{batch_id}'
        """)

    except Exception as e:
        end_time = datetime.utcnow()
        error_message = traceback.format_exc()
        spark.sql(f"""
            UPDATE dw.Audit_Log
            SET End_Time = TIMESTAMP('{end_time}'),
                Status = 'FAILED',
                Message = '{error_message}'
            WHERE Batch_ID = '{batch_id}'
        """)
        raise

# ------------------ Fixtures ------------------

@pytest.fixture(scope="module")
def spark():
    spark = create_spark()
    yield spark
    spark.stop()

@pytest.fixture(autouse=True)
def setup_teardown(spark):
    reset_tables(spark)
    yield
    reset_tables(spark)

# ------------------ Test Cases ------------------

def test_TC01_happy_path(spark):
    # All valid data
    data = [
        (1001, 1, 101, "2023-01-01", 5, 10.0),
        (1002, 2, 102, "2023-01-02", 3, 20.0)
    ]
    df = spark.createDataFrame(data, schema=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"])
    df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 2
    assert set(fact["Transaction_ID"]) == {1001, 1002}
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 2
    assert audit["Rows_Rejected"] == 0
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert len(dq) == 0

def test_TC02_null_customer_id(spark):
    # One row with NULL Customer_ID
    data = [
        (1001, None, 101, "2023-01-01", 5, 10.0),
        (1002, 2, 102, "2023-01-02", 3, 20.0)
    ]
    df = spark.createDataFrame(data, schema=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"])
    df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert fact["Transaction_ID"].iloc[0] == 1002
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert len(dq) == 1
    assert dq["Transaction_ID"].iloc[0] == 1001
    assert dq["Reason"].iloc[0] == "Missing CustomerID"
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 1
    assert audit["Rows_Rejected"] == 1

def test_TC03_invalid_quantity(spark):
    # One row with invalid quantity
    data = [
        (1001, 1, 101, "2023-01-01", -2, 10.0),
        (1002, 2, 102, "2023-01-02", 3, 20.0)
    ]
    df = spark.createDataFrame(data, schema=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"])
    df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert fact["Transaction_ID"].iloc[0] == 1002
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert len(dq) == 1
    assert dq["Transaction_ID"].iloc[0] == 1001
    assert dq["Reason"].iloc[0] == "Invalid Quantity"
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 1
    assert audit["Rows_Rejected"] == 1

def test_TC04_both_null_and_invalid_quantity(spark):
    # One row with NULL Customer_ID, one with invalid quantity, one with both
    data = [
        (1001, None, 101, "2023-01-01", 0, 10.0),   # Both reasons
        (1002, None, 102, "2023-01-02", 3, 20.0),   # Missing CustomerID
        (1003, 1, 103, "2023-01-01", -1, 15.0),     # Invalid Quantity
        (1004, 2, 104, "2023-01-02", 2, 30.0)       # Valid
    ]
    df = spark.createDataFrame(data, schema=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"])
    df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert fact["Transaction_ID"].iloc[0] == 1004
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert set(dq["Transaction_ID"]) == {1001, 1002, 1003}
    assert set(dq["Reason"]) >= {"Missing CustomerID", "Invalid Quantity"}
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 1
    assert audit["Rows_Rejected"] == 3

def test_TC05_empty_staging(spark):
    # No data in staging
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 0
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert len(dq) == 0
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 0
    assert audit["Rows_Rejected"] == 0

def test_TC06_missing_dimension_references(spark):
    # Row with non-matching Customer_ID and Date
    data = [
        (1001, 99, 101, "2023-01-01", 5, 10.0),    # Customer_ID not in dim
        (1002, 1, 102, "2023-01-31", 3, 20.0),     # Date not in dim
        (1003, 1, 103, "2023-01-01", 2, 15.0)      # Valid
    ]
    df = spark.createDataFrame(data, schema=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"])
    df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert fact["Transaction_ID"].iloc[0] == 1003
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 1

def test_TC07_missing_required_columns(spark):
    # Staging table missing 'Quantity' column
    data = [
        (1001, 1, 101, "2023-01-01", 10.0)
    ]
    df = spark.createDataFrame(data, schema=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Unit_Price"])
    df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    with pytest.raises(Exception):
        run_etl_script(spark)
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Status"] == "FAILED"
    assert "Quantity" in audit["Message"]

def test_TC08_unexpected_data_types(spark):
    # Quantity as string
    data = [
        (1001, 1, 101, "2023-01-01", "five", 10.0)
    ]
    df = spark.createDataFrame(data, schema=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"])
    df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    with pytest.raises(Exception):
        run_etl_script(spark)
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Status"] == "FAILED"
    assert "Quantity" in audit["Message"] or "five" in audit["Message"]

def test_TC09_duplicate_transaction_id(spark):
    # Duplicate Transaction_IDs
    data = [
        (1001, 1, 101, "2023-01-01", 5, 10.0),
        (1001, 1, 101, "2023-01-01", 3, 10.0)
    ]
    df = spark.createDataFrame(data, schema=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"])
    df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 2
    assert all(fact["Transaction_ID"] == 1001)
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 2

def test_TC10_large_batch(spark):
    # Large volume test
    data = [(i, 1, 101, "2023-01-01", 1, 10.0) for i in range(10000)]
    df = spark.createDataFrame(data, schema=["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price"])
    df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 10000
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).limit(1).toPandas().iloc[0]
    assert audit["Rows_Inserted"] == 10000
    assert audit["Rows_Rejected"] == 0


---

**API Cost Consumed in dollars:**  
apiCost: 0.0047 USD