=============================================
Author:    AAVA
Created on:   
Description:   Comprehensive unit test case list and Pytest script for Databricks PySpark Sales Fact Load ETL, covering data validation, transformation, and error handling.
=============================================

### Test Case List

| Test Case ID | Test Case Description                                                                       | Expected Outcome                                                                                                    |
|--------------|---------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
| TC001        | Load valid sales transactions with complete data (happy path)                                | All valid rows are loaded into the fact table; audit log records successful load; no DQ failures logged.            |
| TC002        | Staging table contains rows with missing Customer_ID                                         | Rows with missing Customer_ID are excluded from fact table; DQ failure log records these rows.                      |
| TC003        | Staging table contains rows with Quantity <= 0                                               | Rows with Quantity <= 0 are excluded from fact table; DQ failure log records these rows.                            |
| TC005        | Staging table is completely empty                                                            | No rows loaded into fact table; audit log records zero rows processed; no errors or DQ failures logged.             |
| TC013        | Staging table missing required column (e.g., Quantity)                                       | Error is raised; process fails gracefully; error is logged; no rows loaded.                                         |

---

### Pytest Script for Each Test Case

python
# ================================================
# Author: AAVA
# Created on: 
# Description: Pytest suite for Databricks PySpark Sales Fact Load ETL validation
# ================================================

import pytest
import pandas as pd
from pyspark.sql import SparkSession, Row, functions as F
from datetime import datetime
import uuid

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder \
        .master("local[1]") \
        .appName("SalesFactLoadTest") \
        .getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture(autouse=True)
def setup_tables(spark):
    # Clean up all tables before each test
    for tbl in ["stg.Sales_Transactions", "dw.Dim_Customer", "dw.Dim_Date", "dw.Fact_Sales", "dw.Audit_Log", "dw.DQ_Failures"]:
        spark.sql(f"DROP TABLE IF EXISTS {tbl}")
    # Create empty tables with required schema
    stg_schema = "Transaction_ID BIGINT, Customer_ID BIGINT, Product_ID BIGINT, Sales_Date STRING, Quantity INT, Unit_Price DOUBLE, Comments STRING"
    dim_customer_schema = "Customer_ID BIGINT, Customer_Segment STRING"
    dim_date_schema = "Date_Value DATE, Region_ID INT"
    fact_schema = "Transaction_ID BIGINT, Customer_ID BIGINT, Product_ID BIGINT, Sales_Date STRING, Quantity INT, Unit_Price DOUBLE, Total_Sales_Amount DOUBLE, Region_ID INT, Customer_Segment STRING, Load_Timestamp TIMESTAMP, Batch_ID STRING"
    audit_schema = "Batch_ID STRING, Procedure_Name STRING, Start_Time TIMESTAMP, End_Time TIMESTAMP, Status STRING, Message STRING, Rows_Inserted INT, Rows_Rejected INT, Event_Type STRING, Logged_Timestamp TIMESTAMP"
    dq_schema = "Transaction_ID BIGINT, Failure_Reason STRING, Logged_Timestamp TIMESTAMP, Batch_ID STRING"
    spark.sql(f"CREATE TABLE stg.Sales_Transactions ({stg_schema}) USING DELTA")
    spark.sql(f"CREATE TABLE dw.Dim_Customer ({dim_customer_schema}) USING DELTA")
    spark.sql(f"CREATE TABLE dw.Dim_Date ({dim_date_schema}) USING DELTA")
    spark.sql(f"CREATE TABLE dw.Fact_Sales ({fact_schema}) USING DELTA")
    spark.sql(f"CREATE TABLE dw.Audit_Log ({audit_schema}) USING DELTA")
    spark.sql(f"CREATE TABLE dw.DQ_Failures ({dq_schema}) USING DELTA")
    yield
    # Clean up after test
    for tbl in ["stg.Sales_Transactions", "dw.Dim_Customer", "dw.Dim_Date", "dw.Fact_Sales", "dw.Audit_Log", "dw.DQ_Failures"]:
        spark.sql(f"DROP TABLE IF EXISTS {tbl}")

def run_etl_script(spark):
    # Place the provided PySpark ETL code here, but replace 'spark' with the passed-in spark session
    # For brevity, assume the code block is available as a function or imported module
    # from sales_fact_etl import run_sales_fact_etl
    # run_sales_fact_etl(spark)
    # For this example, you would paste the provided ETL code here, replacing global 'spark' with the argument
    pass

def get_table_df(spark, table):
    return spark.table(table).toPandas()

def test_TC001_happy_path(spark, setup_tables):
    stg_data = [
        (1, 100, 200, "2024-06-01", 5, 10.0, "No comment"),
        (2, 101, 201, "2024-06-01", 2, 20.0, "No comment"),
    ]
    dim_customer = [(100, "Retail"), (101, "Wholesale")]
    dim_date = [("2024-06-01", 1)]
    spark.createDataFrame(stg_data, ["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price", "Comments"]).write.format("delta").mode("append").saveAsTable("stg.Sales_Transactions")
    spark.createDataFrame(dim_customer, ["Customer_ID", "Customer_Segment"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Customer")
    spark.createDataFrame(dim_date, ["Date_Value", "Region_ID"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Date")
    run_etl_script(spark)
    fact = get_table_df(spark, "dw.Fact_Sales")
    assert len(fact) == 2
    assert fact["Total_Sales_Amount"].tolist() == [50.0, 40.0]
    audit = get_table_df(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Status"] == "COMPLETED"
    dq = get_table_df(spark, "dw.DQ_Failures")
    assert dq.empty

def test_TC002_missing_customer_id(spark, setup_tables):
    stg_data = [
        (1, None, 200, "2024-06-01", 5, 10.0, "No comment"),
        (2, 101, 201, "2024-06-01", 2, 20.0, "No comment"),
    ]
    dim_customer = [(101, "Wholesale")]
    dim_date = [("2024-06-01", 1)]
    spark.createDataFrame(stg_data, ["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price", "Comments"]).write.format("delta").mode("append").saveAsTable("stg.Sales_Transactions")
    spark.createDataFrame(dim_customer, ["Customer_ID", "Customer_Segment"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Customer")
    spark.createDataFrame(dim_date, ["Date_Value", "Region_ID"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Date")
    run_etl_script(spark)
    fact = get_table_df(spark, "dw.Fact_Sales")
    assert len(fact) == 1
    dq = get_table_df(spark, "dw.DQ_Failures")
    assert dq["Failure_Reason"].iloc[0] == "Missing CustomerID"

def test_TC003_invalid_quantity(spark, setup_tables):
    stg_data = [
        (1, 100, 200, "2024-06-01", 0, 10.0, "No comment"),
        (2, 101, 201, "2024-06-01", -1, 20.0, "No comment"),
        (3, 102, 202, "2024-06-01", 2, 30.0, "No comment"),
    ]
    dim_customer = [(100, "Retail"), (101, "Wholesale"), (102, "Online")]
    dim_date = [("2024-06-01", 1)]
    spark.createDataFrame(stg_data, ["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Quantity", "Unit_Price", "Comments"]).write.format("delta").mode("append").saveAsTable("stg.Sales_Transactions")
    spark.createDataFrame(dim_customer, ["Customer_ID", "Customer_Segment"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Customer")
    spark.createDataFrame(dim_date, ["Date_Value", "Region_ID"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Date")
    run_etl_script(spark)
    fact = get_table_df(spark, "dw.Fact_Sales")
    assert len(fact) == 1
    dq = get_table_df(spark, "dw.DQ_Failures")
    assert set(dq["Failure_Reason"]) == {"Invalid Quantity"}

def test_TC005_empty_staging(spark, setup_tables):
    dim_customer = [(100, "Retail")]
    dim_date = [("2024-06-01", 1)]
    spark.createDataFrame(dim_customer, ["Customer_ID", "Customer_Segment"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Customer")
    spark.createDataFrame(dim_date, ["Date_Value", "Region_ID"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Date")
    run_etl_script(spark)
    fact = get_table_df(spark, "dw.Fact_Sales")
    assert fact.empty
    audit = get_table_df(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Rows_Inserted"] == 0

def test_TC013_missing_required_column(spark, setup_tables):
    stg_data = [
        (1, 100, 200, "2024-06-01", 10.0, "No comment"),
    ]
    dim_customer = [(100, "Retail")]
    dim_date = [("2024-06-01", 1)]
    df = spark.createDataFrame(stg_data, ["Transaction_ID", "Customer_ID", "Product_ID", "Sales_Date", "Unit_Price", "Comments"])
    df.write.format("delta").mode("append").saveAsTable("stg.Sales_Transactions")
    spark.createDataFrame(dim_customer, ["Customer_ID", "Customer_Segment"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Customer")
    spark.createDataFrame(dim_date, ["Date_Value", "Region_ID"]).write.format("delta").mode("append").saveAsTable("dw.Dim_Date")
    with pytest.raises(Exception):
        run_etl_script(spark)
    audit = get_table_df(spark, "dw.Audit_Log")
    assert audit.iloc[-1]["Status"] == "FAILED"


---

**API Cost Consumed:**  
apiCost: 0.0047 USD