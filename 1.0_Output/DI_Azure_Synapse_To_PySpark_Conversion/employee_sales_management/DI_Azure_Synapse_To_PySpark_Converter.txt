=============================================
Author:        AAVA
Created on:   
Description:   Convert Azure Synapse stored procedures into equivalent Databricks PySpark code
=============================================

# Databricks PySpark implementation of dw.sp_load_sales_fact

from pyspark.sql import SparkSession, functions as F
from datetime import datetime
import uuid
import traceback

# Initialize Spark session
spark = SparkSession.builder.getOrCreate()

# ------------------ Variables ------------------
batch_id = str(uuid.uuid4())
start_time = datetime.utcnow()
end_time = None
rows_inserted = 0
rows_rejected = 0
error_message = None
proc_name = "sp_load_sales_fact"

# ------------------ 1. Start Audit Logging ------------------
audit_log_df = spark.createDataFrame(
    [(batch_id, proc_name, start_time, "STARTED", "Sales Fact Load Initiated", None, None)],
    ["Batch_ID", "Procedure_Name", "Start_Time", "Status", "Message", "Rows_Inserted", "Rows_Rejected"]
)
audit_log_df.write.format("delta").mode("append").saveAsTable("dw.Audit_Log")

try:
    # ------------------ 2. Data Quality Checks ------------------
    stg_sales_df = spark.table("stg.Sales_Transactions")

    invalid_customer_df = stg_sales_df.filter(F.col("Customer_ID").isNull()) \
        .select(F.col("Transaction_ID"), F.lit("Missing CustomerID").alias("Reason"))
    invalid_quantity_df = stg_sales_df.filter(F.col("Quantity") <= 0) \
        .select(F.col("Transaction_ID"), F.lit("Invalid Quantity").alias("Reason"))
    invalid_rows_df = invalid_customer_df.unionByName(invalid_quantity_df).dropDuplicates()

    # ------------------ 3. Delete Invalid Rows from Staging ------------------
    valid_sales_df = stg_sales_df.join(
        invalid_rows_df.select("Transaction_ID"),
        on="Transaction_ID",
        how="left_anti"
    )
    rows_rejected = invalid_rows_df.count()

    # ------------------ 4. Load Cleaned Data into Fact Table ------------------
    dim_customer_df = spark.table("dw.Dim_Customer")
    dim_date_df = spark.table("dw.Dim_Date")

    # Broadcast for performance
    dim_customer_df = F.broadcast(dim_customer_df)
    dim_date_df = F.broadcast(dim_date_df)

    transformed_df = valid_sales_df \
        .join(dim_customer_df, "Customer_ID", "inner") \
        .join(dim_date_df, F.to_date(valid_sales_df["Sales_Date"]) == dim_date_df["Date_Value"], "inner") \
        .select(
            valid_sales_df["Transaction_ID"],
            valid_sales_df["Customer_ID"],
            valid_sales_df["Product_ID"],
            valid_sales_df["Sales_Date"],
            valid_sales_df["Quantity"],
            valid_sales_df["Unit_Price"],
            (valid_sales_df["Quantity"] * valid_sales_df["Unit_Price"]).alias("Total_Sales_Amount"),
            dim_date_df["Region_ID"],
            dim_customer_df["Customer_Segment"],
            F.current_timestamp().alias("Load_Timestamp"),
            F.lit(batch_id).alias("Batch_ID")
        )

    transformed_df.write.format("delta").mode("append").saveAsTable("dw.Fact_Sales")
    rows_inserted = transformed_df.count()

    # ------------------ 5. Archive/Truncate Staging Table ------------------
    spark.sql("TRUNCATE TABLE stg.Sales_Transactions")

    # ------------------ 6. Log Validation Failures ------------------
    dq_failures_df = invalid_rows_df \
        .withColumn("Logged_Timestamp", F.current_timestamp()) \
        .withColumn("Batch_ID", F.lit(batch_id))
    dq_failures_df.write.format("delta").mode("append").saveAsTable("dw.DQ_Failures")

    # ------------------ 7. End Audit Log ------------------
    end_time = datetime.utcnow()
    spark.sql(f"""
        UPDATE dw.Audit_Log
        SET End_Time = TIMESTAMP('{end_time}'),
            Rows_Inserted = {rows_inserted},
            Rows_Rejected = {rows_rejected},
            Status = 'COMPLETED',
            Message = 'Inserted {rows_inserted} rows; Rejected {rows_rejected} rows.'
        WHERE Batch_ID = '{batch_id}'
    """)

except Exception as e:
    end_time = datetime.utcnow()
    error_message = traceback.format_exc()
    spark.sql(f"""
        UPDATE dw.Audit_Log
        SET End_Time = TIMESTAMP('{end_time}'),
            Status = 'FAILED',
            Message = '{error_message}'
        WHERE Batch_ID = '{batch_id}'
    """)
    raise

# ------------------ 8. Final Cleanup ------------------
# No explicit temp table cleanup needed in PySpark

# ------------------ API Cost Reporting ------------------
# apiCost: 0.0047 USD

API Cost Consumed in dollars:
apiCost: 0.0047 USD