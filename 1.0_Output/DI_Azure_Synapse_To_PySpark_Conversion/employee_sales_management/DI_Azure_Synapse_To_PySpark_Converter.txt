=============================================
Author:        AAVA
Created on:   
Description:   Convert Azure Synapse stored procedures into equivalent Databricks PySpark code for loading sales fact data, including audit logging, data quality checks, joins, inserts, error handling, and cleanup.
=============================================

from pyspark.sql import functions as F
from pyspark.sql import SparkSession
from delta.tables import DeltaTable
import uuid
from datetime import datetime

# Table names
AUDIT_LOG = "dw.Audit_Log"
FACT_SALES = "dw.Fact_Sales"
DQ_FAILURES = "dw.DQ_Failures"
SALES_STAGING = "stg.Sales_Transactions"
DIM_CUSTOMER = "dw.Dim_Customer"
DIM_DATE = "dw.Dim_Date"

def get_new_batch_id():
    return str(uuid.uuid4())

def get_current_time():
    return datetime.now()

def log_audit(spark, batch_id, proc_name, start_time, end_time, rows_inserted, rows_rejected, status, message):
    audit_df = spark.createDataFrame([
        (batch_id, proc_name, start_time, end_time, rows_inserted, rows_rejected, status, message)
    ], ["Batch_ID", "Procedure_Name", "Start_Time", "End_Time", "Rows_Inserted", "Rows_Rejected", "Status", "Message"])
    audit_df.write.format("delta").mode("append").saveAsTable(AUDIT_LOG)

def main():
    spark = SparkSession.builder.getOrCreate()
    batch_id = get_new_batch_id()
    start_time = get_current_time()
    proc_name = "sp_load_sales_fact"
    rows_inserted = 0
    rows_rejected = 0
    error_message = None

    # 1. Start Audit Logging
    log_audit(spark, batch_id, proc_name, start_time, None, 0, 0, "STARTED", "Sales Fact Load Initiated")

    try:
        # 2. Data Quality Checks
        stg_df = spark.table(SALES_STAGING)
        invalid_customer_df = stg_df.filter(F.col("Customer_ID").isNull()) \
            .select(F.col("Transaction_ID"), F.lit("Missing CustomerID").alias("Reason"))
        invalid_quantity_df = stg_df.filter(F.col("Quantity") <= 0) \
            .select(F.col("Transaction_ID"), F.lit("Invalid Quantity").alias("Reason"))
        invalid_rows_df = invalid_customer_df.unionByName(invalid_quantity_df)

        # 3. Remove Invalid Rows from Staging
        valid_stg_df = stg_df.join(invalid_rows_df, "Transaction_ID", "left_anti")
        rows_rejected = invalid_rows_df.count()

        # 4. Load Cleaned Data into Fact Table
        dim_customer_df = spark.table(DIM_CUSTOMER)
        dim_date_df = spark.table(DIM_DATE)
        # Broadcast dimension tables for performance if small
        dim_customer_df = F.broadcast(dim_customer_df)
        dim_date_df = F.broadcast(dim_date_df)

        transformed_df = valid_stg_df \
            .join(dim_customer_df, "Customer_ID", "inner") \
            .join(dim_date_df, F.to_date(valid_stg_df["Sales_Date"]) == dim_date_df["Date_Value"], "inner") \
            .select(
                valid_stg_df["Transaction_ID"],
                valid_stg_df["Customer_ID"],
                valid_stg_df["Product_ID"],
                valid_stg_df["Sales_Date"],
                valid_stg_df["Quantity"],
                valid_stg_df["Unit_Price"],
                (valid_stg_df["Quantity"] * valid_stg_df["Unit_Price"]).alias("Total_Sales_Amount"),
                dim_date_df["Region_ID"],
                dim_customer_df["Customer_Segment"],
                F.current_timestamp().alias("Load_Timestamp"),
                F.lit(batch_id).alias("Batch_ID")
            )

        # Insert into Fact_Sales (Delta Lake for upsert efficiency)
        transformed_df.write.format("delta").mode("append").saveAsTable(FACT_SALES)
        rows_inserted = transformed_df.count()

        # 5. Truncate Staging Table
        spark.sql(f"TRUNCATE TABLE {SALES_STAGING}")

        # 6. Log Validation Failures
        dq_failures_df = invalid_rows_df.withColumn("Logged_Timestamp", F.current_timestamp()) \
            .withColumn("Batch_ID", F.lit(batch_id)) \
            .select("Transaction_ID", "Reason", "Logged_Timestamp", "Batch_ID")
        dq_failures_df.write.format("delta").mode("append").saveAsTable(DQ_FAILURES)

        # 7. End Audit Log
        end_time = get_current_time()
        message = f"Inserted {rows_inserted} rows; Rejected {rows_rejected} rows."
        log_audit(spark, batch_id, proc_name, start_time, end_time, rows_inserted, rows_rejected, "COMPLETED", message)

    except Exception as e:
        end_time = get_current_time()
        error_message = str(e)
        log_audit(spark, batch_id, proc_name, start_time, end_time, 0, rows_rejected, "FAILED", error_message)
        raise

if __name__ == "__main__":
    main()

# apiCost: 0.0047 USD