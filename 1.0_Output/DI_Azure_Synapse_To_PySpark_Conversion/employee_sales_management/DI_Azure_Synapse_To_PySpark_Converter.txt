=============================================
Author:        AAVA
Created on:   
Description:   Convert Azure Synapse stored procedures into equivalent Databricks PySpark code
=============================================


# Databricks PySpark script for loading sales fact data with audit logging, data quality checks, error handling, and cleanup.

from pyspark.sql import SparkSession, functions as F
from datetime import datetime
import uuid

# Initialize Spark session
spark = SparkSession.builder.getOrCreate()

# ------------------- Variables (equivalent to DECLARE/SET in T-SQL) -------------------
batch_id = str(uuid.uuid4())
start_time = datetime.now()
end_time = None
rows_inserted = 0
rows_rejected = 0
error_message = None
proc_name = "sp_load_sales_fact"

# ------------------- 1. Start Audit Logging -------------------
audit_log_start = spark.createDataFrame(
    [(batch_id, proc_name, start_time, "STARTED", "Sales Fact Load Initiated", None, None, None, None)],
    ["Batch_ID", "Procedure_Name", "Start_Time", "Status", "Message", "End_Time", "Rows_Inserted", "Rows_Rejected", "Error_Message"]
)
audit_log_start.write.format("delta").mode("append").saveAsTable("dw.Audit_Log")

try:
    # ------------------- 2. Temporary Table for Validation Failures -------------------
    # In PySpark, use a DataFrame instead of a temp table
    invalid_rows = spark.createDataFrame([], "Transaction_ID BIGINT, Reason STRING")

    # ------------------- 3. Basic Data Quality Checks -------------------
    sales_df = spark.table("stg.Sales_Transactions")
    # Check for missing Customer_ID
    missing_customer = sales_df.filter(F.col("Customer_ID").isNull()) \
        .select(F.col("Transaction_ID"), F.lit("Missing CustomerID").alias("Reason"))
    # Check for invalid Quantity
    invalid_quantity = sales_df.filter(F.col("Quantity") <= 0) \
        .select(F.col("Transaction_ID"), F.lit("Invalid Quantity").alias("Reason"))
    # Union all invalids
    invalid_rows = missing_customer.unionByName(invalid_quantity)

    # ------------------- 4. Delete Invalid Rows from Staging for This Batch -------------------
    # Remove invalid rows from staging
    valid_sales_df = sales_df.join(invalid_rows, on="Transaction_ID", how="left_anti")
    rows_rejected = sales_df.count() - valid_sales_df.count()

    # ------------------- 5. Load Cleaned Data into Fact Table -------------------
    # Join with dimensions
    dim_customer = spark.table("dw.Dim_Customer")
    dim_date = spark.table("dw.Dim_Date")
    transformed_df = (
        valid_sales_df
        .join(dim_customer, "Customer_ID", "inner")
        .join(dim_date, F.to_date(valid_sales_df["Sales_Date"]) == dim_date["Date_Value"], "inner")
        .select(
            valid_sales_df["Transaction_ID"],
            valid_sales_df["Customer_ID"],
            valid_sales_df["Product_ID"],
            valid_sales_df["Sales_Date"],
            valid_sales_df["Quantity"],
            valid_sales_df["Unit_Price"],
            (valid_sales_df["Quantity"] * valid_sales_df["Unit_Price"]).alias("Total_Sales_Amount"),
            dim_date["Region_ID"],
            dim_customer["Customer_Segment"],
            F.lit(datetime.now()).alias("Load_Timestamp"),
            F.lit(batch_id).alias("Batch_ID")
        )
    )
    transformed_df.write.format("delta").mode("append").saveAsTable("dw.Fact_Sales")
    rows_inserted = transformed_df.count()

    # ------------------- 6. Archive or Truncate Staging Table (optional) -------------------
    # Archive (optional): Uncomment to archive
    # valid_sales_df.withColumn("Batch_ID", F.lit(batch_id)).write.format("delta").mode("append").saveAsTable("archive.Sales_Transactions")
    # Truncate
    spark.sql("TRUNCATE TABLE stg.Sales_Transactions")

    # ------------------- 7. Log Validation Failures -------------------
    if invalid_rows.count() > 0:
        dq_failures = invalid_rows.withColumn("Logged_Timestamp", F.lit(datetime.now())) \
            .withColumn("Batch_ID", F.lit(batch_id))
        dq_failures.write.format("delta").mode("append").saveAsTable("dw.DQ_Failures")

    # ------------------- 8. End Audit Log -------------------
    end_time = datetime.now()
    audit_log_update = (
        spark.table("dw.Audit_Log")
        .filter(F.col("Batch_ID") == batch_id)
        .withColumn("End_Time", F.lit(end_time))
        .withColumn("Rows_Inserted", F.lit(rows_inserted))
        .withColumn("Rows_Rejected", F.lit(rows_rejected))
        .withColumn("Status", F.lit("COMPLETED"))
        .withColumn("Message", F.concat(F.lit("Inserted "), F.lit(rows_inserted), F.lit(" rows; Rejected "), F.lit(rows_rejected), F.lit(" rows.")))
    )
    audit_log_update.write.format("delta").mode("overwrite").option("replaceWhere", f"Batch_ID = '{batch_id}'").saveAsTable("dw.Audit_Log")

except Exception as e:
    # ------------------- 9. Error Handling -------------------
    end_time = datetime.now()
    error_message = str(e)
    audit_log_error = (
        spark.table("dw.Audit_Log")
        .filter(F.col("Batch_ID") == batch_id)
        .withColumn("End_Time", F.lit(end_time))
        .withColumn("Status", F.lit("FAILED"))
        .withColumn("Message", F.lit(error_message))
    )
    audit_log_error.write.format("delta").mode("overwrite").option("replaceWhere", f"Batch_ID = '{batch_id}'").saveAsTable("dw.Audit_Log")
    raise

# ------------------- 10. Final Cleanup -------------------
# No temp tables to drop in PySpark; DataFrames are cleaned up by Spark automatically.

# ------------------- API Cost Consumption -------------------
apiCost = 0.0047  # Example cost in USD
print(f"API Cost Consumed in dollars: {apiCost} USD")


API Cost Consumed in dollars: 0.0047 USD