================================
Author: AAVA
Created on: 
Description: Unit tests and Pytest script for Databricks PySpark ETL loading sales fact table, covering data quality, transformation, audit logging, and error handling logic.
================================

### Test Case List

| Test Case ID | Test Case Description                                                         | Expected Outcome                                                                                           |
|--------------|-------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All staging data valid, correct joins with dimensions             | All records loaded to Fact_Sales, Audit_Log status COMPLETED, correct row counts, DQ_Failures empty        |
| TC02         | Edge: Staging contains NULL Customer_ID                                       | Rows with NULL Customer_ID rejected, logged in DQ_Failures, not loaded to Fact_Sales                       |
| TC03         | Edge: Staging contains Quantity <= 0 or NULL                                  | Rows with invalid Quantity rejected, logged in DQ_Failures, not loaded to Fact_Sales                       |
| TC04         | Edge: Staging contains both invalid Customer_ID and Quantity                  | Rows with either issue rejected, all logged in DQ_Failures, not loaded to Fact_Sales                       |
| TC05         | Edge: Empty staging table                                                     | No rows inserted or rejected, Audit_Log status COMPLETED, DQ_Failures empty                                |
| TC06         | Edge: Staging contains rows with missing Product_ID (should load if not null) | Only rows with valid Customer_ID and Quantity loaded, Product_ID nulls handled as per schema               |
| TC07         | Edge: No matching Customer_ID in dimension                                   | Rows with unmatched Customer_ID not loaded to Fact_Sales, not counted as rejected, not in DQ_Failures      |
| TC08         | Edge: No matching date in dimension                                          | Rows with unmatched Sales_Date not loaded to Fact_Sales, not counted as rejected, not in DQ_Failures       |
| TC09         | Error: Missing required column in staging                                    | Job fails, Audit_Log status FAILED, error message logged                                                   |
| TC10         | Error: Unexpected data type in Quantity                                      | Job fails, Audit_Log status FAILED, error message logged                                                   |
| TC11         | Audit: Audit_Log records correct counts and statuses                         | Audit_Log contains correct batch info, row counts, statuses, and messages                                  |
| TC12         | Audit: DQ_Failures logs correct Transaction_ID and reasons                   | DQ_Failures contains correct Transaction_ID and reason for each rejected row                               |
| TC13         | Truncate: Staging table is empty after load                                  | stg.Sales_Transactions is empty after successful run                                                       |

---

### Pytest Script


import pytest
import uuid
from datetime import datetime
import pandas as pd
from pyspark.sql import SparkSession, Row
from pyspark.sql import functions as F
from pyspark.sql import types as T

# Fixtures for Spark session and test data setup/teardown
@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder.master("local[2]").appName("UnitTest").getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture(autouse=True)
def cleanup_tables(spark):
    # Cleanup before and after each test
    for tbl in [
        "stg.Sales_Transactions", "dw.Dim_Customer", "dw.Dim_Date",
        "dw.Fact_Sales", "dw.Audit_Log", "dw.DQ_Failures"
    ]:
        spark.sql(f"DROP TABLE IF EXISTS {tbl}")
    yield
    for tbl in [
        "stg.Sales_Transactions", "dw.Dim_Customer", "dw.Dim_Date",
        "dw.Fact_Sales", "dw.Audit_Log", "dw.DQ_Failures"
    ]:
        spark.sql(f"DROP TABLE IF EXISTS {tbl}")

# Helper to run the ETL script
def run_etl_script(spark):
    # Place the provided PySpark ETL code here, replacing 'spark' with the fixture argument
    # For brevity, assume the ETL code is in a function called load_sales_fact(spark)
    load_sales_fact(spark)

# Helper to create test tables
def create_tables(spark, stg_data, dim_customer_data, dim_date_data):
    stg_df = spark.createDataFrame(stg_data)
    stg_df.write.format("delta").mode("overwrite").saveAsTable("stg.Sales_Transactions")
    dim_customer_df = spark.createDataFrame(dim_customer_data)
    dim_customer_df.write.format("delta").mode("overwrite").saveAsTable("dw.Dim_Customer")
    dim_date_df = spark.createDataFrame(dim_date_data)
    dim_date_df.write.format("delta").mode("overwrite").saveAsTable("dw.Dim_Date")

# Test cases

def test_TC01_happy_path(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=101, Product_ID=201, Sales_Date="2023-01-01", Quantity=2, Unit_Price=10.0),
        Row(Transaction_ID=2, Customer_ID=102, Product_ID=202, Sales_Date="2023-01-02", Quantity=1, Unit_Price=20.0)
    ]
    dim_customer_data = [
        Row(Customer_ID=101, Customer_Segment="Retail"),
        Row(Customer_ID=102, Customer_Segment="Wholesale")
    ]
    dim_date_data = [
        Row(Date_Value=datetime(2023, 1, 1).date(), Region_ID=1),
        Row(Date_Value=datetime(2023, 1, 2).date(), Region_ID=2)
    ]
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 2
    assert set(fact['Transaction_ID']) == {1, 2}
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).first()
    assert audit.Status == "COMPLETED"
    assert audit.Rows_Inserted == 2
    assert audit.Rows_Rejected == 0
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert dq.empty

def test_TC02_null_customer_id(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=None, Product_ID=201, Sales_Date="2023-01-01", Quantity=2, Unit_Price=10.0),
        Row(Transaction_ID=2, Customer_ID=102, Product_ID=202, Sales_Date="2023-01-02", Quantity=1, Unit_Price=20.0)
    ]
    dim_customer_data = [Row(Customer_ID=102, Customer_Segment="Wholesale")]
    dim_date_data = [Row(Date_Value=datetime(2023, 1, 2).date(), Region_ID=2)]
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert fact['Transaction_ID'].iloc[0] == 2
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert len(dq) == 1
    assert dq['Transaction_ID'].iloc[0] == 1
    assert dq['Failure_Reason'].iloc[0] == "Missing CustomerID"

def test_TC03_invalid_quantity(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=101, Product_ID=201, Sales_Date="2023-01-01", Quantity=0, Unit_Price=10.0),
        Row(Transaction_ID=2, Customer_ID=102, Product_ID=202, Sales_Date="2023-01-02", Quantity=-1, Unit_Price=20.0),
        Row(Transaction_ID=3, Customer_ID=103, Product_ID=203, Sales_Date="2023-01-03", Quantity=None, Unit_Price=30.0),
        Row(Transaction_ID=4, Customer_ID=104, Product_ID=204, Sales_Date="2023-01-04", Quantity=2, Unit_Price=40.0)
    ]
    dim_customer_data = [
        Row(Customer_ID=104, Customer_Segment="Retail")
    ]
    dim_date_data = [
        Row(Date_Value=datetime(2023, 1, 4).date(), Region_ID=4)
    ]
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert fact['Transaction_ID'].iloc[0] == 4
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert set(dq['Transaction_ID']) == {1, 2, 3}
    assert all(reason == "Invalid Quantity" for reason in dq['Failure_Reason'])

def test_TC04_both_invalid(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=None, Product_ID=201, Sales_Date="2023-01-01", Quantity=0, Unit_Price=10.0),
        Row(Transaction_ID=2, Customer_ID=None, Product_ID=202, Sales_Date="2023-01-02", Quantity=-1, Unit_Price=20.0),
        Row(Transaction_ID=3, Customer_ID=103, Product_ID=203, Sales_Date="2023-01-03", Quantity=None, Unit_Price=30.0)
    ]
    dim_customer_data = []
    dim_date_data = []
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert fact.empty
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert set(dq['Transaction_ID']) == {1, 2, 3}

def test_TC05_empty_staging(spark):
    stg_data = []
    dim_customer_data = []
    dim_date_data = []
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert fact.empty
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert dq.empty
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).first()
    assert audit.Rows_Inserted == 0
    assert audit.Rows_Rejected == 0

def test_TC06_missing_product_id(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=101, Product_ID=None, Sales_Date="2023-01-01", Quantity=2, Unit_Price=10.0)
    ]
    dim_customer_data = [Row(Customer_ID=101, Customer_Segment="Retail")]
    dim_date_data = [Row(Date_Value=datetime(2023, 1, 1).date(), Region_ID=1)]
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert len(fact) == 1
    assert pd.isnull(fact['Product_ID'].iloc[0])

def test_TC07_no_matching_customer(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=999, Product_ID=201, Sales_Date="2023-01-01", Quantity=2, Unit_Price=10.0)
    ]
    dim_customer_data = []
    dim_date_data = [Row(Date_Value=datetime(2023, 1, 1).date(), Region_ID=1)]
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert fact.empty
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert dq.empty

def test_TC08_no_matching_date(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=101, Product_ID=201, Sales_Date="2023-01-05", Quantity=2, Unit_Price=10.0)
    ]
    dim_customer_data = [Row(Customer_ID=101, Customer_Segment="Retail")]
    dim_date_data = []
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    fact = spark.table("dw.Fact_Sales").toPandas()
    assert fact.empty
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert dq.empty

def test_TC09_missing_column(spark):
    stg_data = [
        Row(Transaction_ID=1, Product_ID=201, Sales_Date="2023-01-01", Quantity=2, Unit_Price=10.0)
    ]  # Missing Customer_ID
    dim_customer_data = []
    dim_date_data = []
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    with pytest.raises(Exception):
        run_etl_script(spark)
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).first()
    assert audit.Status == "FAILED"

def test_TC10_invalid_data_type(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=101, Product_ID=201, Sales_Date="2023-01-01", Quantity="invalid", Unit_Price=10.0)
    ]
    dim_customer_data = [Row(Customer_ID=101, Customer_Segment="Retail")]
    dim_date_data = [Row(Date_Value=datetime(2023, 1, 1).date(), Region_ID=1)]
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    with pytest.raises(Exception):
        run_etl_script(spark)
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).first()
    assert audit.Status == "FAILED"

def test_TC11_audit_log_correctness(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=101, Product_ID=201, Sales_Date="2023-01-01", Quantity=2, Unit_Price=10.0),
        Row(Transaction_ID=2, Customer_ID=None, Product_ID=202, Sales_Date="2023-01-02", Quantity=1, Unit_Price=20.0)
    ]
    dim_customer_data = [Row(Customer_ID=101, Customer_Segment="Retail")]
    dim_date_data = [Row(Date_Value=datetime(2023, 1, 1).date(), Region_ID=1)]
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    audit = spark.table("dw.Audit_Log").orderBy(F.desc("Start_Time")).first()
    assert audit.Rows_Inserted == 1
    assert audit.Rows_Rejected == 1
    assert audit.Status == "COMPLETED"
    assert "Inserted 1 rows; Rejected 1 rows." in audit.Message

def test_TC12_dq_failures_content(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=None, Product_ID=201, Sales_Date="2023-01-01", Quantity=2, Unit_Price=10.0),
        Row(Transaction_ID=2, Customer_ID=102, Product_ID=202, Sales_Date="2023-01-02", Quantity=0, Unit_Price=20.0)
    ]
    dim_customer_data = [Row(Customer_ID=102, Customer_Segment="Wholesale")]
    dim_date_data = [Row(Date_Value=datetime(2023, 1, 2).date(), Region_ID=2)]
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    dq = spark.table("dw.DQ_Failures").toPandas()
    assert set(dq['Transaction_ID']) == {1, 2}
    assert set(dq['Failure_Reason']) == {"Missing CustomerID", "Invalid Quantity"}

def test_TC13_staging_truncated(spark):
    stg_data = [
        Row(Transaction_ID=1, Customer_ID=101, Product_ID=201, Sales_Date="2023-01-01", Quantity=2, Unit_Price=10.0)
    ]
    dim_customer_data = [Row(Customer_ID=101, Customer_Segment="Retail")]
    dim_date_data = [Row(Date_Value=datetime(2023, 1, 1).date(), Region_ID=1)]
    create_tables(spark, stg_data, dim_customer_data, dim_date_data)
    run_etl_script(spark)
    stg = spark.table("stg.Sales_Transactions").toPandas()
    assert stg.empty

# The function load_sales_fact(spark) should contain the ETL logic from the provided PySpark code.


---

**apiCost: 0.2500 USD**