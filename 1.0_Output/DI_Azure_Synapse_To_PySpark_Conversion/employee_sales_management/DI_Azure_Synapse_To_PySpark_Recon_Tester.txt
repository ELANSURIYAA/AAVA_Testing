=============================================
Author:        Ascendion AAVA
Created on:   
Description:   End-to-end automation script to execute Synapse SQL code, export results to ADLS Gen2, run equivalent Databricks PySpark code, and reconcile outputs for correctness, consistency, and completeness at scale.
=============================================

# 1. Imports and setup
import os
import sys
import time
import json
import csv
import uuid
import hashlib
import tempfile
import shutil
import logging
from datetime import datetime
import pandas as pd

import pyodbc
from azure.identity import DefaultAzureCredential, ClientSecretCredential
from azure.storage.filedatalake import DataLakeServiceClient
from azure.storage.blob import BlobServiceClient
from sqlalchemy import create_engine, text

from pyspark.sql import SparkSession, functions as F
from delta.tables import DeltaTable

# 2. Configuration loading
CONFIG = {
    # Synapse
    "synapse_server": os.getenv("SYNAPSE_SERVER"),
    "synapse_db": os.getenv("SYNAPSE_DB"),
    "synapse_user": os.getenv("SYNAPSE_USER"),
    "synapse_password": os.getenv("SYNAPSE_PASSWORD"),
    "synapse_driver": os.getenv("SYNAPSE_ODBC_DRIVER", "ODBC Driver 17 for SQL Server"),
    # ADLS
    "adls_account_name": os.getenv("ADLS_ACCOUNT_NAME"),
    "adls_container": os.getenv("ADLS_CONTAINER"),
    "adls_client_id": os.getenv("ADLS_CLIENT_ID"),
    "adls_client_secret": os.getenv("ADLS_CLIENT_SECRET"),
    "adls_tenant_id": os.getenv("ADLS_TENANT_ID"),
    # Databricks
    "databricks_host": os.getenv("DATABRICKS_HOST"),
    "databricks_token": os.getenv("DATABRICKS_TOKEN"),
    "databricks_cluster_id": os.getenv("DATABRICKS_CLUSTER_ID"),
    # Data locations
    "bronze_prefix": "bronze/synapse",
    "silver_prefix": "silver/databricks",
    # General
    "comparison_sample_size": int(os.getenv("COMPARISON_SAMPLE_SIZE", "100000")),
    "float_tolerance": float(os.getenv("FLOAT_TOLERANCE", "1e-6")),
    "row_count_threshold": float(os.getenv("ROW_COUNT_THRESHOLD", "0.0001")),
    "output_dir": os.getenv("OUTPUT_DIR", "./reconciliation_output"),
}

os.makedirs(CONFIG["output_dir"], exist_ok=True)
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

# 3. Authentication setup
def get_adls_service_client():
    if CONFIG["adls_client_id"] and CONFIG["adls_client_secret"] and CONFIG["adls_tenant_id"]:
        credential = ClientSecretCredential(
            tenant_id=CONFIG["adls_tenant_id"],
            client_id=CONFIG["adls_client_id"],
            client_secret=CONFIG["adls_client_secret"]
        )
    else:
        credential = DefaultAzureCredential()
    return DataLakeServiceClient(
        account_url=f"https://{CONFIG['adls_account_name']}.dfs.core.windows.net",
        credential=credential
    )

def get_blob_service_client():
    if CONFIG["adls_client_id"] and CONFIG["adls_client_secret"] and CONFIG["adls_tenant_id"]:
        credential = ClientSecretCredential(
            tenant_id=CONFIG["adls_tenant_id"],
            client_id=CONFIG["adls_client_id"],
            client_secret=CONFIG["adls_client_secret"]
        )
    else:
        credential = DefaultAzureCredential()
    return BlobServiceClient(
        account_url=f"https://{CONFIG['adls_account_name']}.blob.core.windows.net",
        credential=credential
    )

def get_synapse_connection():
    conn_str = (
        f"DRIVER={{{CONFIG['synapse_driver']}}};"
        f"SERVER={CONFIG['synapse_server']};"
        f"DATABASE={CONFIG['synapse_db']};"
        f"UID={CONFIG['synapse_user']};"
        f"PWD={CONFIG['synapse_password']};"
        "Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;"
    )
    return pyodbc.connect(conn_str)

# 4. Synapse execution
def execute_synapse_sql(sql_code):
    logging.info("Executing Synapse SQL code...")
    with get_synapse_connection() as conn:
        cursor = conn.cursor()
        start = time.time()
        cursor.execute(sql_code)
        conn.commit()
        elapsed = time.time() - start
        logging.info(f"Synapse SQL executed in {elapsed:.2f}s.")
        # For stored procs, output tables are not directly returned
        # We'll fetch row counts for target tables after execution
    return True

def get_synapse_table_to_df(table_name):
    with get_synapse_connection() as conn:
        query = f"SELECT * FROM {table_name}"
        df = pd.read_sql(query, conn)
    return df

# 5. Data export
def export_table_to_delta_and_adls(table_name, adls_path):
    logging.info(f"Exporting {table_name} to ADLS Delta: {adls_path}")
    df = get_synapse_table_to_df(table_name)
    # Use Spark to write as Delta
    spark = SparkSession.builder.getOrCreate()
    sdf = spark.createDataFrame(df)
    sdf.write.format("delta").mode("overwrite").save(adls_path)
    # Optionally, compute MD5 for validation
    return True

# 6. ADLS transfer
def verify_adls_delta(adls_path):
    # Check if directory exists and has _delta_log
    service_client = get_adls_service_client()
    file_system_client = service_client.get_file_system_client(CONFIG["adls_container"])
    try:
        paths = list(file_system_client.get_paths(adls_path))
        found = any("_delta_log" in p.name for p in paths)
        if not found:
            raise Exception(f"Delta log not found in {adls_path}")
        logging.info(f"Delta files verified at {adls_path}")
        return True
    except Exception as ex:
        logging.error(f"ADLS verification failed: {ex}")
        return False

# 7. Databricks setup
def mount_adls_to_databricks(spark, mount_point="/mnt/synapse_data"):
    configs = {
        f"fs.azure.account.auth.type.{CONFIG['adls_account_name']}.dfs.core.windows.net": "OAuth",
        f"fs.azure.account.oauth.provider.type.{CONFIG['adls_account_name']}.dfs.core.windows.net": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
        f"fs.azure.account.oauth2.client.id.{CONFIG['adls_account_name']}.dfs.core.windows.net": CONFIG["adls_client_id"],
        f"fs.azure.account.oauth2.client.secret.{CONFIG['adls_account_name']}.dfs.core.windows.net": CONFIG["adls_client_secret"],
        f"fs.azure.account.oauth2.client.endpoint.{CONFIG['adls_account_name']}.dfs.core.windows.net": f"https://login.microsoftonline.com/{CONFIG['adls_tenant_id']}/oauth2/token",
    }
    try:
        spark._jvm.dbutils.fs.mount(
            source=f"abfss://{CONFIG['adls_container']}@{CONFIG['adls_account_name']}.dfs.core.windows.net/",
            mountPoint=mount_point,
            extraConfigs=configs
        )
        logging.info(f"Mounted ADLS at {mount_point}")
    except Exception as ex:
        logging.warning(f"Mount may already exist: {ex}")

def create_external_delta_table(spark, table_name, adls_path, schema):
    sql = f"""
    CREATE TABLE IF NOT EXISTS synapse_external.{table_name}
    USING DELTA
    LOCATION '{adls_path}'
    """
    spark.sql(sql)
    logging.info(f"External table synapse_external.{table_name} created.")

# 8. PySpark execution
def run_pyspark_job(pyspark_code_path):
    # Option 1: Databricks REST API job submission (not implemented here for brevity)
    # Option 2: Local execution
    logging.info(f"Running PySpark job: {pyspark_code_path}")
    exec(open(pyspark_code_path).read(), globals())

# 9. Comparison logic
def compare_tables(spark, synapse_table, databricks_table, primary_keys=None, float_tolerance=1e-6, sample_size=100000):
    logging.info(f"Comparing {synapse_table} vs {databricks_table}")
    df1 = spark.table(synapse_table)
    df2 = spark.table(databricks_table)
    # Row count
    count1 = df1.count()
    count2 = df2.count()
    row_count_diff = abs(count1 - count2)
    row_count_pct = row_count_diff / max(1, count1)
    # Schema comparison
    schema1 = {f.name.lower(): f.dataType for f in df1.schema.fields}
    schema2 = {f.name.lower(): f.dataType for f in df2.schema.fields}
    missing_cols = set(schema1) - set(schema2)
    extra_cols = set(schema2) - set(schema1)
    # Data comparison (sampled if large)
    join_cols = primary_keys or list(schema1.keys())
    if len(join_cols) == 0:
        join_cols = [df1.columns[0]]
    df1_sample = df1.limit(sample_size)
    df2_sample = df2.limit(sample_size)
    joined = df1_sample.alias("a").join(
        df2_sample.alias("b"),
        on=[F.col(f"a.{col}") == F.col(f"b.{col}") for col in join_cols],
        how="outer"
    )
    mismatches = []
    for col in schema1:
        if col not in schema2:
            continue
        col_a = F.col(f"a.{col}")
        col_b = F.col(f"b.{col}")
        if str(schema1[col]) in ("DoubleType", "FloatType", "DecimalType"):
            cond = (F.abs(col_a - col_b) > float_tolerance) | (col_a.isNull() != col_b.isNull())
        elif str(schema1[col]) == "TimestampType":
            cond = (F.unix_timestamp(col_a) != F.unix_timestamp(col_b)) | (col_a.isNull() != col_b.isNull())
        else:
            cond = (col_a != col_b) | (col_a.isNull() != col_b.isNull())
        mismatch_rows = joined.filter(cond).select([f"a.{col}", f"b.{col}"]).limit(10).collect()
        if mismatch_rows:
            mismatches.append({"column": col, "examples": [row.asDict() for row in mismatch_rows]})
    # Aggregation comparison
    numeric_cols = [col for col, dtype in schema1.items() if str(dtype) in ("DoubleType", "FloatType", "DecimalType", "LongType", "IntegerType", "ShortType")]
    aggregations = {}
    for col in numeric_cols:
        agg1 = df1.agg(F.sum(col).alias("sum"), F.avg(col).alias("avg"), F.min(col).alias("min"), F.max(col).alias("max")).collect()[0].asDict()
        agg2 = df2.agg(F.sum(col).alias("sum"), F.avg(col).alias("avg"), F.min(col).alias("min"), F.max(col).alias("max")).collect()[0].asDict()
        aggregations[col] = {"synapse": agg1, "databricks": agg2}
    # Match percentage
    match_pct = 100.0 * (1.0 - row_count_pct)
    # Output
    result = {
        "synapse_table": synapse_table,
        "databricks_table": databricks_table,
        "row_count_synapse": count1,
        "row_count_databricks": count2,
        "row_count_difference": row_count_diff,
        "row_count_pct_difference": row_count_pct,
        "schema_missing_columns": list(missing_cols),
        "schema_extra_columns": list(extra_cols),
        "mismatches": mismatches,
        "aggregations": aggregations,
        "match_percentage": match_pct,
        "timestamp": datetime.utcnow().isoformat()
    }
    return result

# 10. Cleanup
def cleanup_temp_dirs(temp_dirs):
    for d in temp_dirs:
        shutil.rmtree(d, ignore_errors=True)

# ---- MAIN EXECUTION ----
def main(synapse_sql_code, pyspark_code_path):
    api_cost = 0.0
    start_time = time.time()
    # Step 1: Execute Synapse SQL code
    execute_synapse_sql(synapse_sql_code)
    api_cost += 0.001  # Estimate

    # Step 2: Identify target tables
    target_tables = ["dw.Fact_Sales"]  # From parsing, or static for this example

    # Step 3: Export each target table to ADLS Delta
    spark = SparkSession.builder.appName("Reconciliation").getOrCreate()
    adls_base = f"abfss://{CONFIG['adls_container']}@{CONFIG['adls_account_name']}.dfs.core.windows.net"
    exported_paths = []
    for table in target_tables:
        delta_path = f"{adls_base}/{CONFIG['bronze_prefix']}/{table.replace('.', '_')}_{int(time.time())}.delta"
        export_table_to_delta_and_adls(table, delta_path)
        verify_adls_delta(delta_path)
        exported_paths.append((table, delta_path))
        api_cost += 0.0005

    # Step 4: Mount ADLS in Databricks and create external tables
    mount_adls_to_databricks(spark)
    for table, delta_path in exported_paths:
        create_external_delta_table(spark, table.replace('.', '_'), delta_path, None)
        api_cost += 0.0002

    # Step 5: Run Databricks PySpark code
    run_pyspark_job(pyspark_code_path)
    api_cost += 0.001

    # Step 6: Compare tables
    results = []
    for table, _ in exported_paths:
        synapse_ext_table = f"synapse_external.{table.replace('.', '_')}"
        databricks_table = f"dw.Fact_Sales"
        result = compare_tables(
            spark,
            synapse_ext_table,
            databricks_table,
            primary_keys=["Transaction_ID"],
            float_tolerance=CONFIG["float_tolerance"],
            sample_size=CONFIG["comparison_sample_size"]
        )
        results.append(result)
        api_cost += 0.001

    # Step 7: Output results
    json_path = os.path.join(CONFIG["output_dir"], "reconciliation_results.json")
    csv_path = os.path.join(CONFIG["output_dir"], "reconciliation_summary.csv")
    with open(json_path, "w") as f:
        json.dump(results, f, indent=2)
    with open(csv_path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=results[0].keys())
        writer.writeheader()
        for row in results:
            writer.writerow(row)
    logging.info(f"Reconciliation results written to {json_path} and {csv_path}")

    # Step 8: Print API cost estimation
    elapsed = time.time() - start_time
    print(f"apiCost: {api_cost:.4f} USD, elapsed: {elapsed:.2f}s")

if __name__ == "__main__":
    # For demo, read SQL and PySpark code from files or arguments
    if len(sys.argv) < 3:
        print("Usage: python reconcile.py <synapse_sql_file> <pyspark_code_file>")
        sys.exit(1)
    with open(sys.argv[1], "r") as f:
        synapse_sql_code = f.read()
    pyspark_code_path = sys.argv[2]
    main(synapse_sql_code, pyspark_code_path)