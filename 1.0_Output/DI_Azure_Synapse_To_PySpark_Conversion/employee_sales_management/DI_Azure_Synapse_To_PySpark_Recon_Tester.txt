=============================================
Author:        Ascendion AAVA
Created on:   
Description:   End-to-end Python script for automating reconciliation and validation between Synapse stored procedure and Databricks PySpark ETL, including data export, ADLS transfer, Databricks execution, and comprehensive result comparison.
=============================================

# 1. Imports and setup
import os
import sys
import time
import uuid
import json
import csv
import hashlib
import tempfile
import shutil
from datetime import datetime
from typing import List, Dict, Any
import pandas as pd

import pyodbc
from azure.identity import DefaultAzureCredential, ClientSecretCredential
from azure.storage.filedatalake import DataLakeServiceClient
from azure.storage.blob import BlobServiceClient
from sqlalchemy import create_engine, text

from pyspark.sql import SparkSession, functions as F, types as T

# 2. Configuration loading
# Load credentials and config from environment variables or config file
SYNAPSE_SERVER = os.getenv("SYNAPSE_SERVER")
SYNAPSE_DB = os.getenv("SYNAPSE_DB")
SYNAPSE_USER = os.getenv("SYNAPSE_USER")
SYNAPSE_PASS = os.getenv("SYNAPSE_PASS")
SYNAPSE_DRIVER = os.getenv("SYNAPSE_DRIVER", "ODBC Driver 18 for SQL Server")
ADLS_ACCOUNT_NAME = os.getenv("ADLS_ACCOUNT_NAME")
ADLS_CONTAINER = os.getenv("ADLS_CONTAINER")
ADLS_CLIENT_ID = os.getenv("ADLS_CLIENT_ID")
ADLS_CLIENT_SECRET = os.getenv("ADLS_CLIENT_SECRET")
ADLS_TENANT_ID = os.getenv("ADLS_TENANT_ID")
DATABRICKS_HOST = os.getenv("DATABRICKS_HOST")
DATABRICKS_TOKEN = os.getenv("DATABRICKS_TOKEN")
DATABRICKS_CLUSTER_ID = os.getenv("DATABRICKS_CLUSTER_ID")
DATABRICKS_MOUNT_POINT = os.getenv("DATABRICKS_MOUNT_POINT", "/mnt/synapse_data")
EXPORT_BASE_PATH = f"bronze/synapse"
PYSPARK_SCRIPT_PATH = os.getenv("PYSPARK_SCRIPT_PATH", "converted_etl.py")
SYNAPSE_SQL_PATH = os.getenv("SYNAPSE_SQL_PATH", "sp_load_sales_fact.sql")
API_COST_PER_CALL = 0.0047  # USD

# 3. Authentication setup
def get_adls_service_client():
    credential = ClientSecretCredential(
        tenant_id=ADLS_TENANT_ID,
        client_id=ADLS_CLIENT_ID,
        client_secret=ADLS_CLIENT_SECRET
    )
    service_client = DataLakeServiceClient(
        account_url=f"https://{ADLS_ACCOUNT_NAME}.dfs.core.windows.net",
        credential=credential
    )
    return service_client

def get_blob_service_client():
    credential = ClientSecretCredential(
        tenant_id=ADLS_TENANT_ID,
        client_id=ADLS_CLIENT_ID,
        client_secret=ADLS_CLIENT_SECRET
    )
    blob_service_client = BlobServiceClient(
        account_url=f"https://{ADLS_ACCOUNT_NAME}.blob.core.windows.net",
        credential=credential
    )
    return blob_service_client

def get_synapse_connection():
    conn_str = (
        f"DRIVER={{{SYNAPSE_DRIVER}}};"
        f"SERVER={SYNAPSE_SERVER};"
        f"DATABASE={SYNAPSE_DB};"
        f"UID={SYNAPSE_USER};"
        f"PWD={SYNAPSE_PASS};"
        "Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;"
    )
    return pyodbc.connect(conn_str)

# 4. Synapse execution
def execute_synapse_sql(sql_code: str):
    conn = get_synapse_connection()
    cursor = conn.cursor()
    start = time.time()
    cursor.execute(sql_code)
    conn.commit()
    end = time.time()
    stats = {
        "execution_time_sec": end - start,
        "rowcount": cursor.rowcount
    }
    cursor.close()
    conn.close()
    return stats

# 5. Data export
def export_table_to_adls(table_name: str, adls_path: str, service_client):
    # Export Synapse table to local temp CSV, then upload to ADLS
    conn = get_synapse_connection()
    df = pd.read_sql(f"SELECT * FROM {table_name}", conn)
    temp_dir = tempfile.mkdtemp()
    local_file = os.path.join(temp_dir, f"{table_name}.csv")
    df.to_csv(local_file, index=False, encoding="utf-8")
    # Upload to ADLS
    file_system_client = service_client.get_file_system_client(ADLS_CONTAINER)
    file_client = file_system_client.get_file_client(adls_path)
    with open(local_file, "rb") as data:
        file_client.upload_data(data, overwrite=True)
    # Clean up
    shutil.rmtree(temp_dir)
    return df.shape[0]

def convert_csv_to_delta(spark, adls_path: str, delta_path: str):
    # Read CSV from ADLS, write as Delta
    df = spark.read.option("header", True).csv(adls_path)
    df.write.format("delta").mode("overwrite").save(delta_path)

# 6. ADLS transfer
def verify_adls_file(adls_path: str, service_client):
    file_system_client = service_client.get_file_system_client(ADLS_CONTAINER)
    file_client = file_system_client.get_file_client(adls_path)
    props = file_client.get_file_properties()
    return props.size

# 7. Databricks setup
def mount_adls_to_databricks(spark):
    configs = {
        f"fs.azure.account.auth.type.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net": "OAuth",
        f"fs.azure.account.oauth.provider.type.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
        f"fs.azure.account.oauth2.client.id.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net": ADLS_CLIENT_ID,
        f"fs.azure.account.oauth2.client.secret.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net": ADLS_CLIENT_SECRET,
        f"fs.azure.account.oauth2.client.endpoint.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net": f"https://login.microsoftonline.com/{ADLS_TENANT_ID}/oauth2/token"
    }
    try:
        spark._jvm.dbutils.fs.mount(
            source=f"abfss://{ADLS_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/",
            mountPoint=DATABRICKS_MOUNT_POINT,
            extraConfigs=configs
        )
    except Exception as e:
        if "Directory already mounted" not in str(e):
            raise

def create_external_delta_table(spark, table_name: str, delta_path: str, schema: T.StructType):
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS synapse_external.{table_name}
        USING DELTA
        LOCATION '{delta_path}'
    """)
    # Optionally, enforce schema

# 8. PySpark execution
def run_pyspark_etl(spark, script_path: str):
    # Dynamically execute the PySpark ETL script
    with open(script_path, "r") as f:
        code = compile(f.read(), script_path, 'exec')
        exec(code, {'spark': spark})

# 9. Comparison logic
def compare_tables(spark, synapse_table: str, databricks_table: str, key_columns: List[str], float_tolerance=1e-6, sample_size=10):
    result = {}
    # Row count comparison
    synapse_df = spark.table(synapse_table)
    databricks_df = spark.table(databricks_table)
    row_count_synapse = synapse_df.count()
    row_count_databricks = databricks_df.count()
    result['row_count'] = {
        "synapse": row_count_synapse,
        "databricks": row_count_databricks,
        "match": abs(row_count_synapse - row_count_databricks) <= max(1, 0.0001 * row_count_synapse)
    }
    # Schema comparison
    synapse_schema = {f.name.lower(): f.dataType for f in synapse_df.schema.fields}
    databricks_schema = {f.name.lower(): f.dataType for f in databricks_df.schema.fields}
    missing_cols = set(synapse_schema.keys()) - set(databricks_schema.keys())
    extra_cols = set(databricks_schema.keys()) - set(synapse_schema.keys())
    result['schema'] = {
        "missing_columns": list(missing_cols),
        "extra_columns": list(extra_cols),
        "match": len(missing_cols) == 0 and len(extra_cols) == 0
    }
    # Data comparison (join on key columns)
    join_expr = [synapse_df[k] == databricks_df[k] for k in key_columns]
    joined = synapse_df.alias("s").join(databricks_df.alias("d"), on=key_columns, how="outer")
    mismatches = []
    for col in synapse_schema.keys():
        if col in databricks_schema:
            s_col = F.col(f"s.{col}")
            d_col = F.col(f"d.{col}")
            if isinstance(synapse_schema[col], T.DoubleType) or isinstance(synapse_schema[col], T.FloatType):
                cond = (F.abs(s_col - d_col) > float_tolerance) | (s_col.isNull() != d_col.isNull())
            elif isinstance(synapse_schema[col], T.TimestampType):
                cond = (F.unix_timestamp(s_col) != F.unix_timestamp(d_col)) | (s_col.isNull() != d_col.isNull())
            else:
                cond = (s_col != d_col) | (s_col.isNull() != d_col.isNull())
            mismatches.append(F.when(cond, F.lit(1)).otherwise(F.lit(0)).alias(col + "_mismatch"))
    mismatch_expr = F.sum(sum([F.col(c) for c in [m.alias for m in mismatches]]))
    mismatch_rows = joined.withColumn("row_mismatch", mismatch_expr)
    mismatch_count = mismatch_rows.filter(F.col("row_mismatch") > 0).count()
    total_rows = joined.count()
    match_percent = 100.0 * (total_rows - mismatch_count) / total_rows if total_rows > 0 else 100.0
    result['data'] = {
        "mismatch_count": mismatch_count,
        "total_rows": total_rows,
        "match_percent": match_percent,
        "sample_mismatches": mismatch_rows.filter(F.col("row_mismatch") > 0).limit(sample_size).toPandas().to_dict(orient="records")
    }
    # Aggregation comparison
    agg_results = {}
    for col, dtype in synapse_schema.items():
        if isinstance(dtype, (T.IntegerType, T.DoubleType, T.FloatType, T.LongType, T.DecimalType)):
            agg_syn = synapse_df.agg(
                F.sum(col).alias("sum"), F.avg(col).alias("avg"),
                F.min(col).alias("min"), F.max(col).alias("max")
            ).collect()[0]
            agg_dat = databricks_df.agg(
                F.sum(col).alias("sum"), F.avg(col).alias("avg"),
                F.min(col).alias("min"), F.max(col).alias("max")
            ).collect()[0]
            agg_results[col] = {
                "synapse": dict(agg_syn.asDict()),
                "databricks": dict(agg_dat.asDict())
            }
    result['aggregations'] = agg_results
    return result

# 10. Cleanup
def cleanup_temp_files(paths: List[str]):
    for p in paths:
        if os.path.exists(p):
            if os.path.isdir(p):
                shutil.rmtree(p)
            else:
                os.remove(p)

def main():
    # Step 1: Parse Synapse SQL and PySpark code to identify target tables
    target_table = "dw.Fact_Sales"
    key_columns = ["Transaction_ID"]  # Example: adjust as per schema

    # Step 2: Connect and execute Synapse stored procedure
    with open(SYNAPSE_SQL_PATH, "r") as f:
        synapse_sql_code = f.read()
    synapse_stats = execute_synapse_sql(synapse_sql_code)
    print(f"Executed Synapse SQL. Stats: {synapse_stats}")

    # Step 3: Export Synapse target table to ADLS as CSV
    adls_export_path = f"{EXPORT_BASE_PATH}/{target_table}/export_{int(time.time())}.csv"
    adls_delta_path = f"{EXPORT_BASE_PATH}/{target_table}/delta_{int(time.time())}"
    adls_service_client = get_adls_service_client()
    row_count_exported = export_table_to_adls(target_table, adls_export_path, adls_service_client)
    print(f"Exported {row_count_exported} rows from {target_table} to ADLS at {adls_export_path}")

    # Step 4: Convert CSV to Delta format using Spark
    spark = SparkSession.builder.appName("SynapseToDatabricksReconciliation").getOrCreate()
    convert_csv_to_delta(spark, f"abfss://{ADLS_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{adls_export_path}", adls_delta_path)
    print(f"Converted CSV to Delta format at {adls_delta_path}")

    # Step 5: Mount ADLS in Databricks and create external table
    mount_adls_to_databricks(spark)
    create_external_delta_table(spark, target_table, f"{DATABRICKS_MOUNT_POINT}/{adls_delta_path}", spark.read.format("delta").load(adls_delta_path).schema)
    print(f"Created external Delta table for {target_table}")

    # Step 6: Run Databricks PySpark ETL
    run_pyspark_etl(spark, PYSPARK_SCRIPT_PATH)
    print("Executed Databricks PySpark ETL script.")

    # Step 7: Compare results
    comparison_result = compare_tables(
        spark,
        f"synapse_external.{target_table}",
        target_table,
        key_columns
    )
    print("Comparison Result:", json.dumps(comparison_result, indent=2))

    # Step 8: Output results
    with open("comparison_result.json", "w") as f:
        json.dump(comparison_result, f, indent=2)
    pd.DataFrame([comparison_result['row_count']]).to_csv("comparison_rowcount.csv", index=False)
    # Optionally, Excel output

    # Step 9: Cleanup
    cleanup_temp_files([])  # Add any temp files/dirs if needed

    # Step 10: API Cost Estimation
    print(f"API Cost Consumed in dollars:\napiCost: {API_COST_PER_CALL:.4f} USD")

if __name__ == "__main__":
    main()