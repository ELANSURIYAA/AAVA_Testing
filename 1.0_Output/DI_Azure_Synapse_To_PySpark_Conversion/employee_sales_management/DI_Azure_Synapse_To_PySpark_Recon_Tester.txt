=============================================
Author:        Ascendion AAVA
Created on:   
Description:   End-to-end automation for executing Synapse SQL, exporting results to ADLS, running Databricks PySpark, and validating data consistency at scale.
=============================================

# 1. Imports and setup
import os
import sys
import uuid
import json
import time
import hashlib
import logging
from datetime import datetime
from typing import List, Dict, Any

import pandas as pd

# Synapse SQL
import pyodbc

# Azure authentication and storage
from azure.identity import DefaultAzureCredential, ClientSecretCredential
from azure.storage.filedatalake import DataLakeServiceClient

# Databricks
import requests
from pyspark.sql import SparkSession, functions as F

# 2. Configuration loading
CONFIG = {
    "synapse": {
        "server": os.getenv("SYNAPSE_SERVER"),
        "database": os.getenv("SYNAPSE_DATABASE"),
        "username": os.getenv("SYNAPSE_USERNAME"),
        "password": os.getenv("SYNAPSE_PASSWORD"),
        "driver": os.getenv("SYNAPSE_ODBC_DRIVER", "ODBC Driver 17 for SQL Server"),
    },
    "adls": {
        "account_name": os.getenv("ADLS_ACCOUNT_NAME"),
        "container": os.getenv("ADLS_CONTAINER"),
        "client_id": os.getenv("AZURE_CLIENT_ID"),
        "client_secret": os.getenv("AZURE_CLIENT_SECRET"),
        "tenant_id": os.getenv("AZURE_TENANT_ID"),
    },
    "databricks": {
        "workspace_url": os.getenv("DATABRICKS_WORKSPACE_URL"),
        "token": os.getenv("DATABRICKS_TOKEN"),
        "cluster_id": os.getenv("DATABRICKS_CLUSTER_ID"),
    },
    "comparison": {
        "float_tolerance": 1e-6,
        "row_count_threshold": 0.0001,
        "sample_size": 10
    },
    "export": {
        "adls_base_path": "bronze/synapse",
        "delta_format": True
    }
}

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

# 3. Authentication setup
def get_adls_service_client():
    credential = ClientSecretCredential(
        tenant_id=CONFIG["adls"]["tenant_id"],
        client_id=CONFIG["adls"]["client_id"],
        client_secret=CONFIG["adls"]["client_secret"]
    )
    service_client = DataLakeServiceClient(
        account_url=f"https://{CONFIG['adls']['account_name']}.dfs.core.windows.net",
        credential=credential
    )
    return service_client

def get_synapse_connection():
    conn_str = (
        f"DRIVER={{{CONFIG['synapse']['driver']}}};"
        f"SERVER={CONFIG['synapse']['server']};"
        f"DATABASE={CONFIG['synapse']['database']};"
        f"UID={CONFIG['synapse']['username']};"
        f"PWD={CONFIG['synapse']['password']}"
    )
    return pyodbc.connect(conn_str)

# 4. Synapse execution
def execute_synapse_sql(sql_code: str):
    logging.info("Executing Synapse SQL code...")
    conn = get_synapse_connection()
    cursor = conn.cursor()
    start = time.time()
    try:
        cursor.execute(sql_code)
        conn.commit()
        logging.info("Synapse SQL executed successfully.")
    except Exception as e:
        logging.error(f"Error executing Synapse SQL: {e}")
        raise
    finally:
        cursor.close()
        conn.close()
    return time.time() - start

# 5. Data export
def export_table_to_adls(table_name: str, adls_path: str):
    logging.info(f"Exporting table {table_name} to ADLS path {adls_path}...")
    conn = get_synapse_connection()
    query = f"SELECT * FROM {table_name}"
    df = pd.read_sql(query, conn)
    conn.close()
    # Save as parquet locally
    local_file = f"/tmp/{table_name}_{uuid.uuid4().hex}.parquet"
    df.to_parquet(local_file, index=False)
    # Upload to ADLS
    service_client = get_adls_service_client()
    file_system_client = service_client.get_file_system_client(CONFIG["adls"]["container"])
    dir_client = file_system_client.get_directory_client(adls_path)
    file_client = dir_client.create_file(f"{table_name}_{int(time.time())}.parquet")
    with open(local_file, "rb") as f:
        file_client.append_data(f.read(), 0)
        file_client.flush_data(f.tell())
    os.remove(local_file)
    logging.info(f"Exported {table_name} to ADLS.")
    return f"{adls_path}/{table_name}_{int(time.time())}.parquet"

# 6. ADLS transfer
def verify_adls_file(adls_path: str, file_name: str, expected_size: int = None, expected_md5: str = None):
    service_client = get_adls_service_client()
    file_system_client = service_client.get_file_system_client(CONFIG["adls"]["container"])
    file_client = file_system_client.get_file_client(f"{adls_path}/{file_name}")
    props = file_client.get_file_properties()
    if expected_size and props.size != expected_size:
        raise Exception(f"File size mismatch: {props.size} != {expected_size}")
    if expected_md5:
        md5 = hashlib.md5(file_client.download_file().readall()).hexdigest()
        if md5 != expected_md5:
            raise Exception(f"MD5 mismatch: {md5} != {expected_md5}")
    logging.info(f"Verified ADLS file {file_name}.")

# 7. Databricks setup
def mount_adls_to_databricks(spark: SparkSession, mount_point: str):
    configs = {
        f"fs.azure.account.auth.type.{CONFIG['adls']['account_name']}.dfs.core.windows.net": "OAuth",
        f"fs.azure.account.oauth.provider.type.{CONFIG['adls']['account_name']}.dfs.core.windows.net": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
        f"fs.azure.account.oauth2.client.id.{CONFIG['adls']['account_name']}.dfs.core.windows.net": CONFIG['adls']['client_id'],
        f"fs.azure.account.oauth2.client.secret.{CONFIG['adls']['account_name']}.dfs.core.windows.net": CONFIG['adls']['client_secret'],
        f"fs.azure.account.oauth2.client.endpoint.{CONFIG['adls']['account_name']}.dfs.core.windows.net": f"https://login.microsoftonline.com/{CONFIG['adls']['tenant_id']}/oauth2/token",
    }
    try:
        spark._jvm.dbutils.fs.mount(
            source=f"abfss://{CONFIG['adls']['container']}@{CONFIG['adls']['account_name']}.dfs.core.windows.net/",
            mountPoint=mount_point,
            extraConfigs=configs
        )
        logging.info(f"Mounted ADLS Gen2 at {mount_point}")
    except Exception as e:
        if "Directory already mounted" in str(e):
            logging.info(f"Already mounted: {mount_point}")
        else:
            raise

def create_external_delta_table(spark: SparkSession, table_name: str, adls_path: str, schema: str):
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS synapse_external.{table_name}
        ({schema})
        USING DELTA
        LOCATION '{adls_path}'
    """)
    logging.info(f"Created external Delta table: synapse_external.{table_name}")

# 8. PySpark execution
def run_databricks_job(pyspark_code: str, job_name: str = "PySparkValidationJob"):
    url = f"{CONFIG['databricks']['workspace_url']}/api/2.0/jobs/runs/submit"
    headers = {"Authorization": f"Bearer {CONFIG['databricks']['token']}"}
    payload = {
        "run_name": job_name,
        "existing_cluster_id": CONFIG['databricks']['cluster_id'],
        "spark_python_task": {
            "python_file": "dbfs:/FileStore/scripts/converted_pyspark_script.py",
            "parameters": []
        }
    }
    # Upload code to DBFS (not shown here for brevity)
    # ...
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code != 200:
        raise Exception(f"Failed to submit Databricks job: {response.text}")
    run_id = response.json()["run_id"]
    logging.info(f"Databricks job submitted: run_id={run_id}")
    # Poll for completion
    while True:
        run_resp = requests.get(
            f"{CONFIG['databricks']['workspace_url']}/api/2.0/jobs/runs/get?run_id={run_id}",
            headers=headers
        )
        run_state = run_resp.json()["state"]["life_cycle_state"]
        if run_state in ("TERMINATED", "SKIPPED", "INTERNAL_ERROR"):
            break
        time.sleep(10)
    logging.info("Databricks PySpark job completed.")
    return run_id

# 9. Comparison logic
def compare_tables(spark: SparkSession, synapse_table: str, databricks_table: str, primary_keys: List[str], float_tolerance: float = 1e-6, sample_size: int = 10):
    logging.info(f"Comparing tables: {synapse_table} vs {databricks_table}")
    syn_df = spark.table(synapse_table)
    dbx_df = spark.table(databricks_table)
    # Row count
    syn_count = syn_df.count()
    dbx_count = dbx_df.count()
    row_count_match = abs(syn_count - dbx_count) <= max(1, int(syn_count * CONFIG['comparison']['row_count_threshold']))
    # Schema
    syn_schema = {f.name.lower(): f.dataType for f in syn_df.schema.fields}
    dbx_schema = {f.name.lower(): f.dataType for f in dbx_df.schema.fields}
    missing_cols = set(syn_schema) - set(dbx_schema)
    extra_cols = set(dbx_schema) - set(syn_schema)
    # Join on PK or all columns
    join_cols = primary_keys if primary_keys else [c for c in syn_schema if c in dbx_schema]
    joined = syn_df.alias("a").join(dbx_df.alias("b"), [F.col(f"a.{col}") == F.col(f"b.{col}") for col in join_cols], "outer")
    # Per-column comparison
    mismatches = []
    for col in join_cols:
        a_col = F.col(f"a.{col}")
        b_col = F.col(f"b.{col}")
        if str(syn_schema[col]).lower() in ("float", "double", "decimal"):
            cond = (F.abs(a_col - b_col) > float_tolerance) | (a_col.isNull() != b_col.isNull())
        else:
            cond = (a_col != b_col) | (a_col.isNull() != b_col.isNull())
        mismatches.append(cond)
    mismatch_df = joined.filter(F.reduce(lambda x, y: x | y, mismatches))
    mismatch_count = mismatch_df.count()
    match_percentage = 100 * (1 - mismatch_count / max(syn_count, 1))
    # Aggregation
    agg_results = {}
    for col, dtype in syn_schema.items():
        if str(dtype).lower() in ("int", "bigint", "float", "double", "decimal"):
            agg_results[col] = {
                "sum_synapse": syn_df.agg(F.sum(col)).first()[0],
                "sum_databricks": dbx_df.agg(F.sum(col)).first()[0]
            }
    # Sample mismatches
    sample_rows = mismatch_df.limit(sample_size).toPandas()
    # Output
    result = {
        "row_count_synapse": syn_count,
        "row_count_databricks": dbx_count,
        "row_count_match": row_count_match,
        "schema_missing_columns": list(missing_cols),
        "schema_extra_columns": list(extra_cols),
        "match_percentage": match_percentage,
        "agg_results": agg_results,
        "sample_mismatches": sample_rows.to_dict(orient="records")
    }
    return result

# 10. Cleanup
def cleanup_temp_files(adls_paths: List[str]):
    service_client = get_adls_service_client()
    file_system_client = service_client.get_file_system_client(CONFIG["adls"]["container"])
    for path in adls_paths:
        try:
            file_client = file_system_client.get_file_client(path)
            file_client.delete_file()
            logging.info(f"Deleted temp file: {path}")
        except Exception as e:
            logging.warning(f"Failed to delete {path}: {e}")

# === MAIN EXECUTION ===
def main(synapse_sql_code: str, pyspark_code: str, target_tables: List[str], primary_keys: Dict[str, List[str]]):
    api_cost = 0.0
    # Step 1: Execute Synapse SQL
    t0 = time.time()
    synapse_exec_time = execute_synapse_sql(synapse_sql_code)
    api_cost += 0.001  # Example cost
    # Step 2: Export tables to ADLS
    adls_paths = []
    for tbl in target_tables:
        adls_path = f"{CONFIG['export']['adls_base_path']}/{tbl}"
        file_path = export_table_to_adls(tbl, adls_path)
        adls_paths.append(file_path)
        verify_adls_file(adls_path, os.path.basename(file_path))
        api_cost += 0.0005
    # Step 3: Mount ADLS in Databricks and create external tables
    spark = SparkSession.builder.getOrCreate()
    mount_adls_to_databricks(spark, "/mnt/synapse_data")
    for tbl in target_tables:
        # For demo, assume schema is inferred from export
        df = spark.read.parquet(f"/mnt/synapse_data/{CONFIG['export']['adls_base_path']}/{tbl}/")
        schema_str = ", ".join([f"{f.name} {f.dataType.simpleString().upper()}" for f in df.schema.fields])
        create_external_delta_table(spark, tbl, f"/mnt/synapse_data/{CONFIG['export']['adls_base_path']}/{tbl}/", schema_str)
    # Step 4: Run Databricks PySpark code
    run_databricks_job(pyspark_code)
    api_cost += 0.002
    # Step 5: Compare tables
    results = {}
    for tbl in target_tables:
        res = compare_tables(
            spark,
            f"synapse_external.{tbl}",
            f"silver.databricks.{tbl}",
            primary_keys.get(tbl, []),
            float_tolerance=CONFIG['comparison']['float_tolerance'],
            sample_size=CONFIG['comparison']['sample_size']
        )
        results[tbl] = res
    # Step 6: Output results
    with open("reconciliation_results.json", "w") as f:
        json.dump(results, f, indent=2)
    pd.DataFrame([
        {
            "table": tbl,
            "row_count_synapse": res["row_count_synapse"],
            "row_count_databricks": res["row_count_databricks"],
            "match_percentage": res["match_percentage"]
        }
        for tbl, res in results.items()
    ]).to_csv("reconciliation_summary.csv", index=False)
    # Step 7: Cleanup
    cleanup_temp_files(adls_paths)
    # Step 8: Report API cost
    print(f"API Cost Consumed in dollars: {api_cost:.4f} USD")

if __name__ == "__main__":
    # Example usage
    # Load Synapse SQL and PySpark code from files or arguments
    with open("synapse_code.sql") as f:
        synapse_sql_code = f.read()
    with open("converted_pyspark_script.py") as f:
        pyspark_code = f.read()
    # Target tables and PKs (should be parsed from code in production)
    target_tables = ["Fact_Sales"]
    primary_keys = {"Fact_Sales": ["Transaction_ID"]}
    main(synapse_sql_code, pyspark_code, target_tables, primary_keys)