```
=============================================
Author:        AAVA
Created on:   
Description:   Convert Azure Synapse stored procedure for loading summarized holding metrics into FACT_EXECUTIVE_SUMMARY from staging, with validation and dimensional joins, into equivalent Databricks PySpark code.
=============================================

# Databricks PySpark Script: Load FACT_EXECUTIVE_SUMMARY from Staging

from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col, lit
from pyspark.sql.utils import AnalysisException

# Initialize Spark session
spark = SparkSession.builder.appName("Load_FACT_EXECUTIVE_SUMMARY").getOrCreate()

# Step 1: Prepare staging data (cache for performance)
print("*** Starting LOAD_FACT_EXECUTIVE_SUMMARY ***")

try:
    stg_holding_metrics = spark.table("STG_HOLDING_METRICS")
    stg_holding_metrics.cache()
    print("Staging data loaded and cached.")
except AnalysisException as e:
    print(f"Error loading staging data: {e}")
    raise

# Step 2: Load dimension tables (cache for repeated joins)
dim_date = spark.table("DIM_DATE").cache()
dim_institution = spark.table("DIM_INSTITUTION").cache()
dim_corporation = spark.table("DIM_CORPORATION").cache()
dim_product = spark.table("DIM_PRODUCT").cache()

# Step 3: Join staging with dimensions and apply business rules
print("Joining staging data with dimension tables and applying business rules...")

fact_df = (
    stg_holding_metrics
    .join(dim_date, stg_holding_metrics["date_value"] == dim_date["date_key"], "inner")
    .join(dim_institution, stg_holding_metrics["institution_id"] == dim_institution["institution_id"], "inner")
    .join(dim_corporation, stg_holding_metrics["corporation_id"] == dim_corporation["corporation_id"], "inner")
    .join(dim_product, stg_holding_metrics["product_id"] == dim_product["product_id"], "inner")
    .select(
        dim_date["date_key"].alias("date_key"),
        dim_institution["institution_id"].alias("institution_id"),
        dim_corporation["corporation_id"].alias("corporation_id"),
        dim_product["product_id"].alias("product_id"),
        stg_holding_metrics["a120_amount"],
        stg_holding_metrics["a120_count"],
        stg_holding_metrics["a30_to_59_amount"],
        stg_holding_metrics["a30_to_59_count"],
        stg_holding_metrics["a60_to_89_amount"],
        stg_holding_metrics["a60_to_89_count"],
        stg_holding_metrics["a90_to_119_amount"],
        stg_holding_metrics["a90_to_119_count"],
        stg_holding_metrics["charge_off_amount"],
        stg_holding_metrics["charge_off_count"],
        stg_holding_metrics["fraud_amount"],
        stg_holding_metrics["fraud_count"],
        when(
            (stg_holding_metrics["income_amount"].isNull()) | (stg_holding_metrics["income_amount"] < 0),
            lit(0)
        ).otherwise(stg_holding_metrics["income_amount"]).alias("income_amount"),
        stg_holding_metrics["number_of_accounts"],
        stg_holding_metrics["purchases_amount"],
        stg_holding_metrics["purchases_count"]
    )
)

# Step 4: Write to FACT_EXECUTIVE_SUMMARY (using Delta Lake for ACID compliance)
print("Writing records to FACT_EXECUTIVE_SUMMARY...")

fact_table_name = "FACT_EXECUTIVE_SUMMARY"
insert_mode = "append"  # Use "overwrite" if full reload is desired

fact_df.write.format("delta").mode(insert_mode).saveAsTable(fact_table_name)

# Step 5: Audit logging
v_row_count = fact_df.count()
print(f"{v_row_count} records inserted into FACT_EXECUTIVE_SUMMARY.")

print("*** LOAD_FACT_EXECUTIVE_SUMMARY completed successfully ***")

# Optional: Clean up cached tables
stg_holding_metrics.unpersist()
dim_date.unpersist()
dim_institution.unpersist()
dim_corporation.unpersist()
dim_product.unpersist()

# End of script

# API Cost Consumed in dollars:
apiCost: 0.0080 USD
```