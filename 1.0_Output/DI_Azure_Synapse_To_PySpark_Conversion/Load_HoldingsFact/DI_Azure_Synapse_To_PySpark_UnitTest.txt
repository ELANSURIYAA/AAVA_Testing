================================
Author: AAVA
Created on: 
Description: Pytest script to validate the Databricks PySpark ETL process for loading summarized holding metrics into FACT_EXECUTIVE_SUMMARY, including data joins, transformations, and error handling.
================================

## Test Case List

| Test Case ID | Test Case Description                                                                                  | Expected Outcome                                                                                           |
|--------------|-------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| TC01         | Happy path: All staging and dimension tables have valid data, all joins succeed, and business rules apply correctly. | Records are inserted into FACT_EXECUTIVE_SUMMARY with correct transformations and counts.                 |
| TC02         | Edge case: Staging table contains NULL values in income_amount.                                       | income_amount is set to 0 for those records in FACT_EXECUTIVE_SUMMARY.                                    |
| TC03         | Edge case: Staging table contains negative values in income_amount.                                   | income_amount is set to 0 for those records in FACT_EXECUTIVE_SUMMARY.                                    |
| TC04         | Edge case: Staging table is empty.                                                                   | No records are inserted into FACT_EXECUTIVE_SUMMARY; row count is zero.                                   |
| TC05         | Edge case: Dimension table is missing a matching key (e.g., date_key).                               | Only records with matching keys are inserted; unmatched records are excluded.                             |
| TC06         | Error handling: Staging table does not exist.                                                         | AnalysisException is raised and logged.                                                                   |
| TC07         | Error handling: Dimension table does not exist.                                                       | AnalysisException is raised and logged.                                                                   |
| TC08         | Error handling: Staging table missing required columns (e.g., income_amount).                        | AnalysisException or relevant error is raised and logged.                                                 |
| TC09         | Edge case: Unexpected data types in staging (e.g., string in income_amount).                         | Error is raised or handled gracefully; invalid records are excluded or set to default.                    |
| TC10         | Happy path: Large dataset performance (simulate with many rows).                                      | All records processed efficiently; row count matches input; no performance bottleneck.                    |

---

## Pytest Script

```python
# ================================================
# Author:        AAVA
# Created on:    
# Description:   Pytest script to validate the Databricks PySpark ETL process for loading summarized holding metrics into FACT_EXECUTIVE_SUMMARY, including data joins, transformations, and error handling.
# ================================================

import pytest
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.functions import lit
from pyspark.sql.utils import AnalysisException

# Helper function to create Spark session for testing
@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder.master("local[1]").appName("pytest_FACT_EXEC_SUMMARY").getOrCreate()
    yield spark
    spark.stop()

# Helper function to create DataFrames from Pandas
def create_spark_df(spark, data, schema):
    return spark.createDataFrame(pd.DataFrame(data, columns=schema))

# Setup and teardown for FACT_EXECUTIVE_SUMMARY table
@pytest.fixture
def setup_fact_table(spark):
    # Drop if exists and create empty table
    spark.sql("DROP TABLE IF EXISTS FACT_EXECUTIVE_SUMMARY")
    spark.createDataFrame([], schema="""date_key INT, institution_id INT, corporation_id INT, product_id INT, a120_amount DOUBLE, a120_count INT, a30_to_59_amount DOUBLE, a30_to_59_count INT, a60_to_89_amount DOUBLE, a60_to_89_count INT