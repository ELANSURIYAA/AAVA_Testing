1. **Test Cases Document:**

   **Test Case ID:** TC001  
   **Description:** Verify that the source file is read correctly with the specified delimiter and header row.  
   **Input Data:** `/path/to/source_file.csv`  
   **Expected Output:** DataFrame with the correct schema and data as per the source file.  

   **Test Case ID:** TC002  
   **Description:** Validate that rows with "status" not equal to "active" are filtered out.  
   **Input Data:** `/path/to/source_file.csv`  
   **Expected Output:** DataFrame containing only rows where "status" is "active".  

   **Test Case ID:** TC003  
   **Description:** Ensure that the column "old_column_name" is renamed to "new_column_name".  
   **Input Data:** `/path/to/source_file.csv`  
   **Expected Output:** DataFrame with the column "new_column_name" replacing "old_column_name".  

   **Test Case ID:** TC004  
   **Description:** Verify that the transformed data is loaded into the Snowflake table correctly.  
   **Input Data:** Transformed DataFrame  
   **Expected Output:** Data in the Snowflake table matches the transformed DataFrame.  

   **Test Case ID:** TC005  
   **Description:** Test handling of null values in the source file.  
   **Input Data:** `/path/to/source_file_with_nulls.csv`  
   **Expected Output:** Null values are processed without errors and handled as per the transformation logic.  

   **Test Case ID:** TC006  
   **Description:** Validate error handling when the source file is missing or inaccessible.  
   **Input Data:** `/path/to/missing_file.csv`  
   **Expected Output:** Appropriate error message is logged, and the process terminates gracefully.  

   **Test Case ID:** TC007  
   **Description:** Test performance with a large source file.  
   **Input Data:** `/path/to/large_source_file.csv`  
   **Expected Output:** ETL process completes within acceptable time limits without errors.  

   **Test Case ID:** TC008  
   **Description:** Validate that the Snowflake table is not truncated before loading new data.  
   **Input Data:** Existing Snowflake table data and new data to be loaded.  
   **Expected Output:** Existing data in the Snowflake table remains intact, and new data is appended.  

2. **Pytest Script:**

```python
import pytest
from pyspark.sql import SparkSession
from etl_job import run_etl  # Assuming the ETL job is implemented in a function named run_etl

@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder \
        .appName("ETL Test") \
        .master("local[*]") \
        .getOrCreate()
    yield spark
    spark.stop()

def test_read_source_file(spark):
    df = spark.read.csv("/path/to/source_file.csv", header=True, sep=",")
    assert df is not None
    assert "status" in df.columns

def test_filter_active_status(spark):
    df = spark.read.csv("/path/to/source_file.csv", header=True, sep=",")
    filtered_df = df.filter(df.status == "active")
    assert filtered_df.filter(filtered_df.status != "active").count() == 0

def test_rename_column(spark):
    df = spark.read.csv("/path/to/source_file.csv", header=True, sep=",")
    renamed_df = df.withColumnRenamed("old_column_name", "new_column_name")
    assert "new_column_name" in renamed_df.columns
    assert "old_column_name" not in renamed_df.columns

def test_load_into_snowflake(spark):
    # Mock Snowflake connection and table
    transformed_df = spark.read.csv("/path/to/transformed_file.csv", header=True, sep=",")
    # Mock loading logic
    assert transformed_df.count() > 0

def test_null_values_handling(spark):
    df = spark.read.csv("/path/to/source_file_with_nulls.csv", header=True, sep=",")
    assert df.filter(df["column_with_nulls"].isNull()).count() > 0

def test_missing_source_file(spark):
    with pytest.raises(Exception):
        spark.read.csv("/path/to/missing_file.csv", header=True, sep=",")

def test_large_file_performance(spark):
    df = spark.read.csv("/path/to/large_source_file.csv", header=True, sep=",")
    assert df.count() > 1000000  # Example threshold for large file

def test_no_truncate_on_target(spark):
    # Mock Snowflake connection and table
    existing_data_count = 100  # Mock existing data count
    new_data_count = 50  # Mock new data count
    total_count = existing_data_count + new_data_count
    assert total_count == 150
```

3. **Total Cost Incurred:** The cost incurred for the execution of the agent is included in the analysis and delegation process.