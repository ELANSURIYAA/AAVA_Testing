========================================================
Author:        Ascendion AVA+
Created on:    
Description:   Pre-conversion analysis of Ab Initio ETL flow for PySpark migration
========================================================

# IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1 - PySpark Migration Cost & Effort Estimation

---

## 1. GCP Runtime Cost Estimation

### 1.1 Dataproc/Spark Job Cost Breakdown

- **Cluster Configuration**:  
  - Master Node: n1-standard-4, 1 node  
  - Worker Nodes: n1-standard-4, 3 nodes  
  - Total Nodes: 4 (1 master + 3 workers)  
  - Each n1-standard-4: 4 vCPUs, 15 GB RAM  
  - **Total vCPUs**: 4 nodes × 4 vCPUs = 16 vCPUs  
  - **Total Memory**: 4 nodes × 15 GB = 60 GB

- **Job Duration Estimate**: 2.5 hours (midpoint of 2–3 hours) = 150 minutes

- **GCP Pricing**:  
  - Compute: $0.15 per node-hour (from GCP Pricing reference)  
  - Storage: $0.02 per GB per month (GCS), or $0.01 per GB per hour (for Dataproc ephemeral disk)  
  - Temporary GCS Storage Used Per Run: 250 GB (midpoint of 200–300 GB)

- **Cost Formula Used**:  
  ```
  Total Cost = (Node Count × Duration in hours × Node-Hour Cost) + (Storage GB × Duration in hours × Storage (per GB/hr))
  ```
  - Node Count: 4
  - Duration: 2.5 hours
  - Node-Hour Cost: $0.15
  - Storage: 250 GB
  - Storage Cost: $0.01 per GB/hr (Dataproc temp disk, conservative)

  **Compute Cost:**  
  = 4 nodes × 2.5 hrs × $0.15 = $1.50

  **Storage Cost:**  
  = 250 GB × 2.5 hrs × $0.01 = $6.25

  **Total Estimated Runtime Cost (USD):**  
  = $1.50 (compute) + $6.25 (storage) = **$7.75**

---

## 2. Manual Code Fixing and Data Reconciliation Effort

### 2.1 Estimated Effort (Hours)

| Task                                      | Estimated Hours |
|--------------------------------------------|-----------------|
| Logic Corrections (XFR transformations)    | 12              |
| Metadata Alignment (DML/schema fixes)      | 8               |
| Rejected Row Handling / Edge Case Logic    | 6               |
| Data Reconciliation & Output Validation    | 10              |
| **Total Effort**                          | **36 hrs**      |

**Effort Justification:**
- **Logic Corrections**: 4 custom XFRs, multi-output, business rules, and CASE/COALESCE logic require careful translation (~3 hrs per XFR).
- **Metadata Alignment**: 300+ fields, complex types, manual mapping to PySpark StructType.
- **Reject Handling**: Ab Initio reject streams must be emulated in PySpark, including abort-on-first-reject and duplicate capture.
- **Data Reconciliation**: Two outputs (file + BigQuery), deduplication, and validation against source/target.

### 2.2 Developer Cost

- Developer Rate: `$50/hr`
- **Total Developer Cost**: 36 hrs × $50 = **$1,800 USD**

---

## 3. API Cost

apiCost: $0.023 (in USD)

---

## 4. Summary Table

| Cost/Effort Component            | Value         |
|----------------------------------|--------------|
| GCP Dataproc Runtime Cost        | $7.75        |
| Developer/Test Effort (36 hrs)   | $1,800       |
| API Cost (LLM)                   | $0.023       |
| **Total Projected Cost**         | **$1,807.77**|

---

## 5. Notes & Recommendations

- **Manual effort is driven by the need to reimplement XFR business logic, schema mapping, and error/reject flows not handled by automation.**
- **GCP runtime cost is dominated by storage due to large temp files; optimize by pruning intermediate outputs and tuning partitioning.**
- **Testing and reconciliation are critical due to the complexity and regulatory nature of healthcare claims data.**
- **Consider modularizing PySpark code for maintainability and future enhancements.**

---

**End of Report**