```
========================================================
Author:        Ascendion AVA+
Created on:    
Description:   Cost and effort analysis for Ab Initio to PySpark migration of dental claim ETL
========================================================

# IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1.mp – PySpark Migration Cost & Effort Analysis

---

## 1. GCP Runtime Cost Estimation

### 1.1 Dataproc/Spark Job Cost Breakdown

- **Cluster Configuration**:  
  - Master Node: n1-standard-4, 1 node  
  - Worker Nodes: n1-standard-4, 3 nodes  
  - Total Nodes: 4  
  - vCPUs per node: 4  
  - Total vCPUs: 16  
  - Memory per node: (n1-standard-4 = 15 GB)  
  - Total Memory: 60 GB  

- **Job Duration Estimate**: 2–3 hours per run (using 3 hours for upper bound)

- **GCP Pricing**:  
  - Compute (per node-hour): $0.15  
  - Storage (per GB/month): $0.02 (GCS)  
  - Storage used per run: ~250 GB (average of 200–300 GB)  
  - Storage duration per run: Assume 1 day (0.033 month) for temp files

- **Cost Formula Used**:  
  - Compute Cost = (Number of nodes × Duration in hours × Cost per node-hour)  
  - Storage Cost = (Storage GB × Storage price per GB per month × (days/30))  

- **Calculation**:  
  - Compute: 4 nodes × 3 hours × $0.15 = $1.80  
  - Storage: 250 GB × $0.02 × (1/30) ≈ $0.17  

- **Estimated Runtime Cost (USD)**:  
  - **Total per run:** $1.80 (compute) + $0.17 (storage) = **$1.97**  
  - **Monthly (30 runs):** $1.97 × 30 = **$59.10**

---

## 2. Manual Code Fixing and Data Reconciliation Effort

### 2.1 Estimated Effort (Hours)

| Task                                    | Estimated Hours |
|------------------------------------------|-----------------|
| Logic Corrections (.xfr transformations) | 12              |
| Metadata Alignment (.dml type fixes)     | 6               |
| Rejected Row Handling / Edge Case Logic  | 4               |
| Data Reconciliation & Output Validation  | 8               |
| **Total Effort**                        | **30**          |

### 2.2 Developer Cost

- Developer Rate: `$50/hr`
- **Total Developer Cost**: 30 hrs × $50/hr = **$1,500 USD**

---

## 3. API Cost

apiCost: **0.017 USD**

---

## 4. Summary Table

| Category                  | Value (USD)      |
|---------------------------|------------------|
| GCP Runtime Cost (per run)| $1.97            |
| GCP Runtime Cost (monthly)| $59.10           |
| Developer Cost            | $1,500           |
| API Cost                  | $0.017           |

---

## 5. Manual Review Notes

- **Logic Gaps Requiring Manual Resolution**:
  - Custom `.xfr` business logic (FIRST_DEFINED, null handling, validation) must be re-implemented in PySpark.
  - DML schema translation for large integers, dates, and string fields.
  - Parameter sets (.pset) and dynamic configs must be mapped to PySpark environment variables or config files.
  - Reject/abort logic and error handling must be implemented using DataFrame filters and logging.
  - Dual-output management (CSV and BigQuery) requires separate transformation paths and schema validation.

- **Effort Drivers**:
  - Complexity of transformation logic and error handling.
  - Data reconciliation and output validation for regulatory reporting.
  - Schema mapping and metadata alignment between Ab Initio and PySpark/BigQuery.

---

## 6. GCP Cluster/Dataproc Configuration Reference

- Cluster Type: n1-standard-4 (1 master + 3 workers)
- Job Runtime Estimate: 2–3 hours per run
- Cost per node-hour: ~$0.15
- Temporary GCS Storage Used Per Run: ~200–300 GB
- Storage Cost Estimate: $0.02/GB/month

---

## 7. API Cost Calculation

- Tokens Used (Prompt + Completion): ~8,500 tokens
- Cost per 1K tokens: $0.002
- Final Cost in USD: $0.017

---

**End of Report**
```