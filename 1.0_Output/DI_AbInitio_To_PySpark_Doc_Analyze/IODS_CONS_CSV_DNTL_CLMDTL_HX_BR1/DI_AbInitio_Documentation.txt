# IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1 Ab Initio Graph Documentation

====================================================
Author:        AAVA
Date:          2024-12-19
Description:   Ab Initio ETL graph for consolidating CSV dental claim detail data from BigQuery sources
====================================================

## 1. Overview of Graph/Component

The IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1.mp graph is an Ab Initio ETL pipeline designed to consolidate dental claim detail data from multiple CSV 5010 format tables in BigQuery. The graph performs complex data integration by joining dental service line data with provider information and existing consolidated claim data, applies data quality transformations, removes duplicates, and outputs the consolidated results to both temporary files and staging tables. This pipeline supports regulatory reporting and enterprise data lake requirements for dental claims processing.

## 2. Component Structure and Design

The graph consists of 8 main components arranged in a sequential processing flow:

**Input Layer:**
- **V351S3P1 CSV 5010 DNTL CLMDTL (Input Table)**: BigQuery input component that executes a complex SQL query joining three tables: CSV_5010_DENTAL_SERVICE_LINE_HX (A), CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX (B), and CONS_CSV_DENTAL_CLM_HX (C)

**Transformation Layer:**
- **RFMT V351S3P1 Adaptor CSV 5010 DNTL CLMDTL (Reformat)**: Uses table_adaptor.xfr to standardize input data format
- **FD_RFMT-2 (Reformat)**: Applies GEN_CSV_FIRST_DEFINED.xfr transformation for data cleansing
- **RFMT V353S6 Xfm Jnr (Reformat)**: Multi-output reformat component with two transformation paths using V353S6P2.xfr and V353S6P3.xfr

**Data Quality Layer:**
- **Partition by Key-3**: Partitions data by key fields {AK_UCK_ID; AK_UCK_ID_PREFIX_CD; AK_UCK_ID_SEGMENT_NO}
- **SORT V353S0P3**: Sorts data on composite key {AK_UCK_ID; AK_UCK_ID_PREFIX_CD; AK_UCK_ID_SEGMENT_NO; AK_SUBMT_SVC_LN_NO}
- **DEDU V353S0 Rmv Dup**: Removes duplicates based on the sorted key, keeping first occurrence

**Output Layer:**
- **V353S5 DS CONS CSV DENTAL CLMDTL HX (Output File)**: Writes to temporary GCS location
- **Output Table**: Loads data into BigQuery staging table STG_CONS_CSV_DENTAL_CLM_DTL_HX using storage_load interface

The components are connected through a linear flow with branching at the multi-output reformat stage, utilizing parameters for environment-specific configurations and dynamic dataset references.

## 3. Data Flow and Processing Logic

**Processed Datasets:**
- CSV_5010_DENTAL_SERVICE_LINE_HX
- CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX  
- CONS_CSV_DENTAL_CLM_HX
- DNTLCLM_DTL_inserts_2019082807120189.tmp
- STG_CONS_CSV_DENTAL_CLM_DTL_HX

**Data Flow:**
1. **Data Extraction**: Input Table component executes complex BigQuery SQL with LEFT OUTER JOIN between dental service line and provider tables, INNER JOIN with consolidated claims, filtering by date range parameters
2. **Initial Transformation**: table_adaptor.xfr standardizes the extracted data format and field mappings
3. **Data Cleansing**: GEN_CSV_FIRST_DEFINED.xfr applies business rules for handling null values and data validation
4. **Multi-path Processing**: Data splits into two transformation paths - one for final output preparation (V353S6P2.xfr) and another for staging table format (V353S6P3.xfr)
5. **Data Partitioning**: Records are partitioned by claim identifier keys for parallel processing
6. **Sorting**: Data is sorted on composite key to prepare for deduplication
7. **Deduplication**: Removes duplicate records based on key fields, keeping first occurrence
8. **Dual Output**: Clean data flows to both temporary file storage and BigQuery staging table

## 4. Data Mapping (Lineage)

| Target Table | Target Column | Source Table | Source Column | Remarks |
|--------------|---------------|--------------|---------------|---------|
| Output | AK_UCK_ID | CSV_5010_DENTAL_SERVICE_LINE_HX | UCK_ID | 1:1 Mapping - Primary claim identifier |
| Output | AK_UCK_ID_PREFIX_CD | CSV_5010_DENTAL_SERVICE_LINE_HX | UCK_ID_PREFIX_CD | 1:1 Mapping - Claim ID prefix |
| Output | AK_UCK_ID_SEGMENT_NO | CSV_5010_DENTAL_SERVICE_LINE_HX | UCK_ID_SEGMENT_NO | 1:1 Mapping - Claim ID segment |
| Output | AK_SUBMT_SVC_LN_NO | CSV_5010_DENTAL_SERVICE_LINE_HX | SUBMT_SVC_LN_NO | 1:1 Mapping - Service line number |
| Output | ASRG_ENTY_TYP_QLFR_CD | CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX | ASRG_ENTY_TYP_QLFR_CD | Transformation - COALESCE with empty string default |
| Output | REND_PROV_ID | CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX | REND_PROV_ID | Transformation - COALESCE with empty string default |
| Output | STK_UCK_ID | CSV_5010_DENTAL_SERVICE_LINE_HX | STK_UCK_ID | Validation - Range check with 9223372036854775807 limit |
| Output | STK_REND_PROV_NO | CSV_5010_DENTAL_SERVICE_LINE_PROVIDER_HX | STK_REND_PROV_NO | Validation - Range check with 9223372036854775807 limit |
| Output | CONS_CSV_DENTAL_CLM_HX_ID | CONS_CSV_DENTAL_CLM_HX | CONS_CSV_DENTAL_CLM_HX_ID | 1:1 Mapping - Consolidated claim reference |
| Output | PROC_CD | CSV_5010_DENTAL_SERVICE_LINE_HX | PROC_CD | 1:1 Mapping - Procedure code |
| Output | SVC_DT | CSV_5010_DENTAL_SERVICE_LINE_HX | SVC_DT | 1:1 Mapping - Service date |
| Output | LN_ITEM_CHG_AMT | CSV_5010_DENTAL_SERVICE_LINE_HX | LN_ITEM_CHG_AMT | 1:1 Mapping - Line item charge amount |

## 5. Transformation Logic

**XFR Functions Used:**

1. **table_adaptor.xfr**: 
   - Purpose: Standardizes input data format from BigQuery to Ab Initio internal format
   - Fields Involved: All input fields from the complex SQL query
   - Function: Handles data type conversions and field mapping standardization

2. **GEN_CSV_FIRST_DEFINED.xfr**:
   - Purpose: Implements business logic for handling null values and data validation
   - Fields Involved: All fields requiring null handling and validation rules
   - Function: Applies FIRST_DEFINED logic to select non-null values from multiple sources

3. **IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2.xfr**:
   - Purpose: Transforms data for final output file format
   - Fields Involved: All output fields for temporary file storage
   - Function: Applies final formatting and business rule transformations

4. **IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P3.xfr**:
   - Purpose: Transforms data for staging table format
   - Fields Involved: All fields required for BigQuery staging table
   - Function: Formats data according to staging table schema requirements

## 6. Complexity Analysis

- **Number of Graph Components**: 8
- **Number of Lines of Code (in .xfr or .plan)**: ~2500 (estimated from SQL query and transformations)
- **Transform Functions Used**: 4
- **Joins Used**: LEFT OUTER JOIN, INNER JOIN
- **Lookup Files or Datasets**: 0
- **Parameter Sets (.pset) or Plan Files Used**: 1 (embedded parameter set)
- **Number of Output Datasets**: 2
- **Conditional Logic or if-else flows**: 2 (CASE statements in SQL, multi-output reformat)
- **External Dependencies**: BigQuery (JDBC), GCS storage, DML files, XFR transformation files
- **Overall Complexity Score**: 75

## 7. Key Outputs

**Primary Outputs:**
1. **DNTLCLM_DTL_inserts_2019082807120189.tmp**: Temporary file in GCS containing consolidated dental claim detail records in delimited format for downstream processing
2. **STG_CONS_CSV_DENTAL_CLM_DTL_HX**: BigQuery staging table loaded via storage_load interface containing validated and deduplicated dental claim details for reporting and analytics

**Output Format**: Delimited format for temporary files, native BigQuery format for staging table
**Intended Use**: Regulatory reporting, enterprise data lake population, and downstream analytical processing

## 8. Error Handling and Logging

**Error Handling Components:**
- **Reject Ports**: Each transformation component includes reject ports for handling data quality issues
- **Error Ports**: Standard error handling with error_info metadata for component-level error tracking
- **Log Ports**: Comprehensive logging capabilities with log_event metadata

**Error Management Strategy:**
- **Reject Threshold**: Set to "Abort on first reject" ensuring data quality standards
- **Deduplication Rejects**: Duplicate records are captured and can be analyzed separately
- **BigQuery Load Errors**: Storage_load interface provides native BigQuery error handling
- **Logging**: Component-level logging with configurable frequency settings

**Error Recovery:**
- Failed records are captured in reject streams rather than causing job failure
- Error metadata includes component, port, parameter, and message details for troubleshooting
- Log events provide detailed execution tracking and performance monitoring

## 9. API Cost (LLM Cost ONLY)

**Cost Calculation for this Documentation Generation:**
- **Tokens Used (Prompt + Completion)**: ~8,500 tokens
  - Input Prompt: ~6,000 tokens (Ab Initio .mp file content and instructions)
  - Generated Completion: ~2,500 tokens (this documentation)
- **Cost per 1K tokens**: $0.002 (estimated for GPT-4 class model)
- **Final Cost in USD**: $0.017

*Note: This cost represents only the LLM API consumption for generating this specific documentation and does not include any infrastructure, compute, or storage costs related to the actual Ab Initio job execution.*