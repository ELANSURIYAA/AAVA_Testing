==================================================================================
Author:        Ascendion AVA+
Created on:    
Description:   Pre-conversion analysis of Ab Initio ETL flow for PySpark migration
==================================================================================

# Pre-conversion Analysis: IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1.mp

---

## Syntax & Logical Structure Analysis

### Major Components Identified

- **Input Table (BigQuery Source)**
  - Reads from BigQuery using a complex SQL with multiple joins (LEFT OUTER, INNER) and COALESCE logic.
  - Uses parameterized dataset and date filtering.

- **Reformat Components**
  - Multiple `Reformat` nodes, some referencing external `.xfr` files (e.g., `$AI_XFR/table_adaptor.xfr`, `$AI_XFR/GEN_CSV_FIRST_DEFINED.xfr`, custom `.xfr` for join logic).
  - Some use inline transforms, others reference external transformation logic.

- **Sort & Deduplication**
  - `Sort` and `Dedup Sorted` components on composite keys (e.g., `{AK_UCK_ID; AK_UCK_ID_PREFIX_CD; AK_UCK_ID_SEGMENT_NO; AK_SUBMT_SVC_LN_NO}`).
  - Deduplication keeps the "first" record per group.

- **Partitioning**
  - `Partition by Key` on `{AK_UCK_ID; AK_UCK_ID_PREFIX_CD; AK_UCK_ID_SEGMENT_NO}` for parallelism.

- **Output Table/File**
  - Writes to BigQuery staging and final tables, with variable-driven table names and DMLs.
  - Output File component for intermediate storage (possibly for backup or audit).

- **Chained/Conditional Flows**
  - Reject, error, and log ports are present on most components, indicating error handling and logging flows.
  - Multiple output ports in some reformats, suggesting conditional branching.

### PySpark Equivalents

- **Input Table**: `spark.read.format("bigquery")` or `spark.read.jdbc` with SQL pushdown.
- **Reformat**: `DataFrame.withColumn`, `selectExpr`, or custom UDFs for complex logic.
- **Sort/Dedup**: `orderBy` + `dropDuplicates` or `window` functions.
- **Partition by Key**: `repartition` on key columns.
- **Output Table/File**: `DataFrame.write.format("bigquery")` or `.parquet/.csv` as needed.
- **Reject/Error Flows**: Exception handling, logging, and possibly side outputs for rejects.

---

## Anticipated Manual Interventions

- **Custom .xfr Logic**: External `.xfr` files (e.g., `$AI_XFR/table_adaptor.xfr`, `$AI_XFR/GEN_CSV_FIRST_DEFINED.xfr`, `$AI_XFR/IODS_CONS_CSV_DNTL_CLMDTL_HX_BR1_V353S6P2.xfr`) will require manual translation to PySpark functions or UDFs.
- **DML to PySpark Schema**: The `.dml` definitions (hundreds of fields, types like `utf8 string(unsigned integer(4))`, `decimal(40.9, sign_reserved)`, `date("YYYY-MM-DD")`) must be mapped to PySpark `StructType` schemas. Manual review needed for field type fidelity.
- **Parameter Sets (`.pset`)**: Dynamic parameters (e.g., dataset names, date ranges, output file paths) must be mapped to PySpark config/argument parsing.
- **Ab Initio-Specific Behaviors**:
  - Multi-output reformats (multiple output ports) need explicit branching logic in PySpark.
  - Reject/error flows: PySpark does not natively support reject ports; must implement error handling and side outputs.
  - Graph-level variables and dynamic path resolution (e.g., `$IODS_GCS_TEMP`, `$AI_DML`) require translation to environment variables or config-driven paths.
- **SQL Pushdown**: The complex SQL in the Input Table must be validated for compatibility with PySpark's BigQuery connector; some SQL constructs may need rewriting or staged extraction.
- **Data Volume**: The source tables are very large (hundreds of GBs); PySpark jobs must be tuned for scale.

---

## Complexity Evaluation

**Score: 85/100**

**Justification:**
- **Component Count & Graph Depth**: High. Multiple chained transformations, partitioning, deduplication, and error flows.
- **.xfr & .pset Usage**: Heavy use of external transformation logic and parameterization.
- **Iterative/Feedback Loops**: Not explicitly present, but multi-branch flows and chained dedup/sort add complexity.
- **Joins & Lookups**: The SQL includes multi-table joins, COALESCEs, and conditional logic.
- **File Type Complexity**: Handles delimited, variable, and possibly fixed-width formats; DMLs are complex.
- **Manual Review Required**: High, due to custom logic, schema translation, and error/reject handling.

---

## Performance & Scalability Recommendations

- **Broadcast Joins**: If any joined tables are small (e.g., lookup tables), use `broadcast()` in PySpark to optimize joins.
- **Caching**: Cache intermediate DataFrames after expensive transformations or joins if reused downstream.
- **Avoid UDFs**: Where possible, use native PySpark functions for transformations; only use UDFs for logic that cannot be expressed otherwise.
- **Repartitioning**: Use `repartition` on key columns before heavy shuffles (e.g., before deduplication or joins) to avoid data skew.
- **SQL Pushdown**: Push as much filtering and joining as possible into the BigQuery source read to minimize data movement.
- **Error Handling**: Implement robust error handling and logging, possibly using side outputs for rejects.
- **Schema Evolution**: Use explicit schemas and handle nullable fields carefully to avoid runtime errors.
- **Resource Tuning**: Set executor memory, cores, and shuffle partitions based on data volume and cluster size.

---

## Refactor vs. Rebuild Recommendation

**Recommendation: REBUILD**

- The complexity, heavy use of external transformation logic, parameterization, and Ab Initio-specific constructs (multi-output ports, reject flows, dynamic DMLs) make a direct refactor risky and hard to maintain.
- A clean-slate PySpark implementation, modularized by transformation stage, with explicit schema handling and robust error management, is preferable for maintainability and scalability.

---

## Manual Tasks & Review Checkpoints

- [ ] **Extract and translate all referenced `.xfr` files** to PySpark functions/UDFs.
- [ ] **Map all `.dml` schemas** to PySpark `StructType` schemas, ensuring type fidelity.
- [ ] **Parameterize all dynamic paths and variables** using PySpark config or argument parsing.
- [ ] **Rewrite complex SQL** for PySpark compatibility and pushdown.
- [ ] **Design error/reject handling** in PySpark (side outputs, logging, exception management).
- [ ] **Validate partitioning and deduplication logic** for correctness and performance.
- [ ] **Test at scale** with representative data volumes.
- [ ] **Document all manual translation decisions** for future maintainers.

---

## API Cost

- apiCost: 0.0045 USD

---