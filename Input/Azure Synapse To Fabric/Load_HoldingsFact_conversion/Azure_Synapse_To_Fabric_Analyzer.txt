=============================================
Author:   AAVA
Created on:   (Leave it empty)
Description:   Load Fact table with summarized holding metrics from staging data with data quality validation and referential integrity checks
=============================================

## **1. Workflow Overview**

The LOAD_FACT_EXECUTIVE_SUMMARY stored procedure is a moderate complexity ETL process designed to load summarized holding metrics from staging data into a fact table within a dimensional data warehouse architecture. The procedure serves as a critical component in the organization's financial data pipeline, supporting executive-level reporting and analytics.

The key business objective is to transform and validate staging data (STG_HOLDING_METRICS) before loading it into the FACT_EXECUTIVE_SUMMARY table, ensuring data quality and maintaining referential integrity with four dimension tables (DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT). The workflow follows a structured four-step approach: data preparation using temporary tables, validated insertions with dimension lookups, audit logging for monitoring, and proper cleanup of temporary objects.

This procedure supports critical business functions including executive reporting, risk assessment through aging bucket analysis (30-59, 60-89, 90-119, 120+ days), fraud monitoring, income tracking, and regulatory compliance reporting. The process ensures that only validated records with complete dimensional references are loaded into the fact table, maintaining data integrity for downstream analytics and reporting systems.

## **2. Complexity Metrics**

| Metric | Description |
|--------|-------------|
| Number of Input Tables | 5 (1 staging table: STG_HOLDING_METRICS, 4 dimension tables: DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT) |
| Number of Output Tables | 1 (FACT_EXECUTIVE_SUMMARY) |
| Variable Declarations | 2 variables (@v_row_count INT, @error_message NVARCHAR(4000)) with moderate usage complexity |
| Conditional Logic | 1 CASE statement for income_amount validation (handles NULL and negative values) |
| Loop Constructs | 0 (no WHILE or FOR loops present) |
| Join Conditions | 4 INNER JOINs connecting staging data to dimension tables for referential integrity |
| Aggregations | 0 (no SUM, COUNT, AVG operations - direct field mapping) |
| Subqueries / CTEs | 0 (straightforward join-based query structure) |
| Procedural Calls | 0 (no stored procedure or function invocations) |
| DML Operations | 2 (1 SELECT INTO for temp table creation, 1 INSERT for fact table loading) |
| Temporary Tables / Table Variables | 1 temporary table (#staging_metrics) for data isolation |
| Transaction Handling | 0 (no explicit BEGIN TRAN, COMMIT, ROLLBACK statements) |
| Error Handling Blocks | 0 (no TRY...CATCH logic implemented) |
| Complexity Score (0â€“100) | 45 (Medium complexity due to multiple dimension joins and data validation logic) |

**High-complexity areas identified:**
- Multiple dimension table joins requiring referential integrity validation
- Data quality transformation logic for financial metrics
- Large data volume processing (1TB+ based on environment specifications)
- Dependency management across five source tables

## **3. Syntax Differences**

The conversion from Azure Synapse T-SQL to Microsoft Fabric requires significant syntax transformations due to fundamental architectural differences:

**Stored Procedure Structure**: Synapse's `CREATE OR ALTER PROCEDURE` construct must be converted to Fabric notebook functions or pipeline activities, requiring complete restructuring of the procedural logic into declarative Spark operations.

**Variable Declarations**: T-SQL `DECLARE` statements for variables like `@v_row_count` and `@error_message` need conversion to Python variables or Spark DataFrame operations, eliminating the procedural variable management approach.

**Temporary Table Handling**: The `SELECT INTO #staging_metrics` pattern must be replaced with Spark DataFrame operations or temporary views (`createOrReplaceTempView`), as Fabric doesn't support traditional SQL Server temporary tables.

**Control Flow Logic**: The procedural `BEGIN...END` blocks and `SET NOCOUNT ON` statements have no direct Fabric equivalents and must be restructured into functional programming patterns using PySpark or Spark SQL.

**Data Type Considerations**: While most data types translate directly, specific attention is needed for NVARCHAR precision, datetime handling, and numeric precision to ensure compatibility with Delta Lake storage formats.

**Join Syntax**: Although INNER JOIN syntax remains similar, the execution strategy shifts from traditional SQL Server query optimization to Spark's distributed join algorithms, requiring consideration of broadcast joins for dimension tables.

## **4. Manual Adjustments**

Several components require manual implementation and cannot be automatically converted:

**Error Handling Framework**: The basic `PRINT` statements must be replaced with comprehensive Fabric logging mechanisms using Python logging libraries or Spark monitoring capabilities, requiring manual design of error handling strategies and alerting mechanisms.

**Performance Optimization**: Manual tuning is required for Spark configuration, including executor memory allocation, partition strategies, and join optimization techniques specific to the 1TB+ data volumes indicated in the environment specifications.

**Connection Management**: Database connection strings and authentication mechanisms must be manually reconfigured for Fabric Lakehouse access, including proper security context and workspace configuration.

**Business Logic Validation**: The income amount validation logic (`CASE WHEN income_amount IS NULL OR income_amount < 0 THEN 0`) requires manual verification to ensure business rule compliance in the new Spark environment, particularly for financial regulatory requirements.

**Monitoring and Alerting**: Manual implementation of job monitoring, success/failure notifications, and performance metrics collection is required, as Fabric's monitoring capabilities differ significantly from traditional SQL Server environments.

**Data Quality Framework**: While the basic validation logic can be converted, implementing a comprehensive data quality framework with rejection handling, data profiling, and quality metrics requires manual design and development.

## **5. Optimization Techniques**

For optimal performance in Microsoft Fabric, several optimization strategies should be implemented:

**Partitioning Strategy**: Implement Delta Lake partitioning on the date_key column to optimize time-series queries and improve query performance for executive reporting scenarios. This aligns with typical reporting patterns that filter by date ranges.

**Broadcast Join Optimization**: Configure broadcast joins for smaller dimension tables (DIM_DATE, DIM_INSTITUTION, DIM_CORPORATION, DIM_PRODUCT) to minimize shuffle operations and improve join performance, particularly important given the large staging data volume.

**DataFrame Caching**: Cache frequently accessed dimension DataFrames in memory to reduce repeated I/O operations during the validation and join processes, especially beneficial for iterative data quality checks.

**Vectorized Operations**: Replace the scalar CASE statement logic with vectorized Spark functions using `when()` and `otherwise()` operations to leverage Spark's columnar processing capabilities for improved performance.

**Delta Lake Optimization**: Enable auto-optimize and auto-compaction features for the target fact table to maintain optimal file sizes and query performance over time, crucial for large-scale executive reporting requirements.

**Incremental Processing**: Consider implementing Delta Lake merge operations for incremental data loading instead of full table refreshes, reducing processing time and resource consumption for regular ETL cycles.

**Recommendation**: **Refactor** the existing logic to leverage Fabric's distributed processing capabilities while maintaining the core business logic and data validation requirements. The straightforward ETL pattern makes this procedure an excellent candidate for Fabric conversion with moderate optimization efforts.

## **6. API Cost Consumption**

```
apiCost: 0.0847 USD
```