```
=============================================
Author:        Ascendion AVA
Date:   (Leave it empty)
Description:   Sales fact table loading procedure with data quality validation and audit logging
=============================================

## 1. Procedure Overview

The Azure Synapse stored procedure `dw.sp_load_sales_fact` implements a comprehensive ETL workflow for sales transaction data processing. This procedure serves as a critical component in the organization's data warehouse pipeline, responsible for extracting sales data from staging tables, performing data quality validations, transforming the data with business logic, and loading it into the fact table while maintaining complete audit trails.

**Key Business Objectives:**
- Data integration from staging to fact tables
- Data quality validation and cleansing
- Business logic enrichment through dimensional lookups
- Comprehensive audit logging and error tracking
- Automated data pipeline processing

**Workflow Structure:**
- **Number of Mappings per Workflow:** 1 main data flow with 8 logical processing steps
- **Session Count:** Single session execution with comprehensive error handling
- **Data Processing Pattern:** Batch-based ETL with validation framework

## 2. Complexity Metrics

| Metric | Count | Type/Details |
|--------|-------|--------------|
| Number of Source Qualifiers | 3 | stg.Sales_Transactions (SQL Server), dw.Dim_Customer (SQL Server), dw.Dim_Date (SQL Server) |
| Number of Transformations | 6 | Data validation, calculation, lookup enrichment, audit logging, error handling, cleanup |
| Lookup Usage | 2 | Connected lookups (Dim_Customer, Dim_Date) |
| Expression Logic | 4 | Total_Sales_Amount calculation, timestamp generation, batch ID assignment, validation conditions |
| Join Conditions | 3 | 2 INNER JOINs (dimension lookups), 1 INNER JOIN (invalid row removal) |
| Conditional Logic | 2 | Data quality validation filters (NULL Customer_ID, Invalid Quantity) |
| Reusable Components | 0 | No reusable transformations or mapplets |
| Data Sources | 3 | SQL Server staging table, SQL Server dimension tables |
| Data Targets | 3 | SQL Server fact table, SQL Server audit table, SQL Server DQ failures table |
| Pre/Post SQL Logic | 4 | Temp table creation/cleanup, staging truncation, audit initialization/completion |
| Session/Workflow Controls | 1 | TRY-CATCH error handling with transaction control |
| DML Logic | 6 | INSERT (4), DELETE (1), UPDATE (1), TRUNCATE (1) |
| **Complexity Score (0–100)** | **75** | High complexity due to multiple data quality checks, dimensional lookups, audit framework, and comprehensive error handling |

**High-Complexity Areas:**
- Multiple data quality validation rules with temporary table management
- Complex CTE with multiple INNER JOINs for dimensional enrichment
- Comprehensive audit logging framework with batch tracking
- Error handling with rollback and cleanup mechanisms

## 3. Syntax Differences

**Azure Synapse Functions Requiring BigQuery Conversion:**
- `NEWID()` → `GENERATE_UUID()` for unique identifier generation
- `SYSDATETIME()` → `CURRENT_DATETIME()` for timestamp functions
- `OBJECT_NAME(@@PROCID)` → Manual procedure name assignment (no direct equivalent)
- `@@ROWCOUNT` → `ROW_COUNT()` or explicit counting in BigQuery
- `CAST(s.Sales_Date AS DATE)` → `DATE(s.Sales_Date)` for date casting
- `CONCAT()` → `CONCAT()` (similar but may need adjustment for NULL handling)

**Data Type Conversions:**
- `UNIQUEIDENTIFIER` → `STRING` (UUID format)
- `DATETIME` → `DATETIME` (compatible)
- `NVARCHAR` → `STRING`
- `BIGINT` → `INT64`
- `INT` → `INT64`

**Control Flow Restructuring:**
- `TRY-CATCH` blocks → BigQuery exception handling with `BEGIN EXCEPTION` blocks
- `SET NOCOUNT ON` → Not applicable in BigQuery
- Temporary tables (`#InvalidRows`) → Common Table Expressions (CTEs) or temporary datasets
- `TRUNCATE TABLE` → `DELETE FROM table WHERE TRUE` in BigQuery

## 4. Manual Adjustments

**Components Requiring Manual Implementation:**
- **Procedure Structure:** BigQuery stored procedures use different syntax (`CREATE OR REPLACE PROCEDURE`)
- **Variable Declarations:** BigQuery uses `DECLARE` with different syntax patterns
- **Temporary Table Logic:** Convert `#InvalidRows` temporary table to CTE or array-based logic
- **Error Handling:** Restructure TRY-CATCH to BigQuery's exception handling model
- **Row Count Tracking:** Replace `@@ROWCOUNT` with explicit counting or `ROW_COUNT()` function calls
- **System Functions:** Manual implementation of procedure name tracking (no `@@PROCID` equivalent)

**External Dependencies:**
- **Audit Framework:** Verify audit table schemas are compatible with BigQuery
- **Data Quality Tables:** Ensure DQ_Failures table structure supports BigQuery data types
- **Scheduling Integration:** Update job scheduling to work with BigQuery's execution environment
- **Monitoring Systems:** Adjust monitoring tools to capture BigQuery procedure execution metrics

**Business Logic Validation Areas:**
- **Data Quality Rules:** Validate that validation logic produces same results in BigQuery
- **Calculation Accuracy:** Verify Total_Sales_Amount calculations maintain precision
- **Dimensional Lookup Logic:** Ensure JOIN conditions work correctly with BigQuery's execution engine
- **Audit Trail Completeness:** Confirm all audit information is captured properly

## 5. Optimization Techniques

**BigQuery Best Practices:**
- **Partitioning Strategy:** Partition `dw.Fact_Sales` table by `Sales_Date` for improved query performance
- **Clustering Implementation:** Cluster fact table on `Customer_ID` and `Product_ID` for efficient filtering
- **Materialized Views:** Consider materialized views for frequently accessed dimensional data
- **Query Optimization:** Use BigQuery's query optimizer by avoiding unnecessary CAST operations

**Data Processing Optimizations:**
- **Batch Processing:** Implement array-based processing for data quality checks instead of row-by-row validation
- **Streaming Inserts:** Consider BigQuery streaming inserts for real-time data processing requirements
- **Window Functions:** Replace complex subqueries with window functions for better performance
- **JOIN Optimization:** Use BigQuery's automatic JOIN optimization by ensuring proper table statistics

**Pipeline Improvements:**
- **Error Recovery:** Implement checkpoint-based recovery for large data volumes
- **Parallel Processing:** Leverage BigQuery's automatic parallelization for large datasets
- **Resource Management:** Use appropriate slot allocation for consistent performance
- **Cost Optimization:** Implement query result caching and avoid SELECT * operations

**Recommendation: REFACTOR**
The existing logic is well-structured and follows good ETL practices. The recommendation is to **REFACTOR** the current implementation to BigQuery syntax while maintaining the core business logic, audit framework, and data quality validation patterns. The procedure's modular design and comprehensive error handling make it suitable for adaptation rather than complete rebuilding.

**Migration Priority:**
1. Convert basic syntax and data types
2. Implement BigQuery-compatible error handling
3. Optimize dimensional lookups with BigQuery best practices
4. Enhance with BigQuery-specific performance optimizations
5. Validate data quality and audit functionality
```